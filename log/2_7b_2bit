Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 15, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 22, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 12, in <module>
    from lib.utils import gptq_data_utils
  File "/home/user/benchmarks/quip-sharp/lib/utils/__init__.py", line 1, in <module>
    from .data_utils import *
  File "/home/user/benchmarks/quip-sharp/lib/utils/data_utils.py", line 8, in <module>
    from lib import codebook
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_zeroshot.py", line 12, in <module>
    from lib.utils import LMEvalAdaptor
  File "/home/user/benchmarks/quip-sharp/lib/utils/__init__.py", line 1, in <module>
    from .data_utils import *
  File "/home/user/benchmarks/quip-sharp/lib/utils/data_utils.py", line 8, in <module>
    from lib import codebook
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 15, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 22, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 12, in <module>
    from lib.utils import gptq_data_utils
  File "/home/user/benchmarks/quip-sharp/lib/utils/__init__.py", line 1, in <module>
    from .data_utils import *
  File "/home/user/benchmarks/quip-sharp/lib/utils/data_utils.py", line 8, in <module>
    from lib import codebook
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_zeroshot.py", line 12, in <module>
    from lib.utils import LMEvalAdaptor
  File "/home/user/benchmarks/quip-sharp/lib/utils/__init__.py", line 1, in <module>
    from .data_utils import *
  File "/home/user/benchmarks/quip-sharp/lib/utils/data_utils.py", line 8, in <module>
    from lib import codebook
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
W0907 15:24:11.138020 62578 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:24:11.165940 62578 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 15:24:11.291017 62578 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 15:24:11.291213 62578 utils.py:164] NumExpr defaulting to 16 threads.
I0907 15:24:11.494396 62578 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 15:24:11.749377 62578 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 15:24:34.660044 62578 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 304, in hf_raise_for_status
    response.raise_for_status()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
    raise head_call_error
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1751, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1673, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 376, in _request_wrapper
    response = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 400, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 321, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-68bda3b2-062eec5f3cd5dc1f3035ba06;c1b81f13-31ce-4ed4-9315-2c20763d6962)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 242, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 135, in main
    model = AutoModelForCausalLM.from_pretrained(args.base_model,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 523, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 928, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 631, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 686, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.
401 Client Error. (Request ID: Root=1-68bda3b2-062eec5f3cd5dc1f3035ba06;c1b81f13-31ce-4ed4-9315-2c20763d6962)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.
W0907 15:24:37.064119 62703 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:24:37.090956 62703 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 15:24:37.207363 62703 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 15:24:37.207501 62703 utils.py:164] NumExpr defaulting to 16 threads.
I0907 15:24:37.392321 62703 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 15:24:37.649405 62703 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 40, in <module>
    from . import (latticee8_padded12, latticee8_padded12_rvq3bit,
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12_rvq3bit.py", line 107, in <module>
    _E8P_PACKED_ABS_CACHED = get_packed_abs_grid()
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12_rvq3bit.py", line 55, in get_packed_abs_grid
    d8 = torch.cartesian_prod(*[intr] * 8).float() + 1 / 2
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/functional.py", line 1419, in cartesian_prod
    return _VF.cartesian_prod(tensors)  # type: ignore[attr-defined]
KeyboardInterrupt
W0907 15:25:21.311019 62952 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:25:21.336924 62952 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 15:25:21.451843 62952 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 15:25:21.451986 62952 utils.py:164] NumExpr defaulting to 16 threads.
I0907 15:25:21.620272 62952 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 15:25:21.864167 62952 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 15:25:44.513867 62952 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 304, in hf_raise_for_status
    response.raise_for_status()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
    raise head_call_error
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1751, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1673, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 376, in _request_wrapper
    response = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 400, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 321, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-68bda3f8-17bc68665e34ee92045b8526;3bfec94f-e8ed-4412-99b9-7af82fe45419)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 242, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 135, in main
    model = AutoModelForCausalLM.from_pretrained(args.base_model,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 523, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 928, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 631, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 686, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.
401 Client Error. (Request ID: Root=1-68bda3f8-17bc68665e34ee92045b8526;3bfec94f-e8ed-4412-99b9-7af82fe45419)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.
W0907 15:25:46.799529 63091 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:25:46.826465 63091 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 15:25:46.947906 63091 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 15:25:46.948060 63091 utils.py:164] NumExpr defaulting to 16 threads.
I0907 15:25:47.125039 63091 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 15:25:47.373682 63091 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 108, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 23, in main
    saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'ckpt/2_7b_2bit/config.pt'
W0907 15:26:11.569584 63187 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:26:11.595147 63187 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 15:26:11.708209 63187 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 15:26:11.708309 63187 utils.py:164] NumExpr defaulting to 16 threads.
I0907 15:26:11.870058 63187 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 15:26:12.114592 63187 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 15:26:33.654260 63187 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 304, in hf_raise_for_status
    response.raise_for_status()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
    raise head_call_error
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1751, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1673, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 376, in _request_wrapper
    response = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 400, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 321, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-68bda429-2f0dffd74fbfbaf20fdcb646;e849c8f1-76b7-4d9c-b989-0712e23ff2c3)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 149, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 92, in main
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 819, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 928, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 631, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 686, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.
401 Client Error. (Request ID: Root=1-68bda429-2f0dffd74fbfbaf20fdcb646;e849c8f1-76b7-4d9c-b989-0712e23ff2c3)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.
W0907 15:26:36.030097 63282 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:26:36.055756 63282 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 15:26:36.168304 63282 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 15:26:36.168392 63282 utils.py:164] NumExpr defaulting to 16 threads.
I0907 15:26:36.327732 63282 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 15:26:36.571680 63282 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 108, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 23, in main
    saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'ckpt/2_7b_2bit/config.pt'
W0907 15:27:01.384788 63377 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:27:01.389145 63377 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0907 15:27:01.478953 63377 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 12, in <module>
    from lib.utils import gptq_data_utils
  File "/home/user/benchmarks/quip-sharp/lib/utils/__init__.py", line 1, in <module>
    from .data_utils import *
  File "/home/user/benchmarks/quip-sharp/lib/utils/data_utils.py", line 8, in <module>
    from lib import codebook
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 40, in <module>
    from . import (latticee8_padded12, latticee8_padded12_rvq3bit,
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12_rvq3bit.py", line 108, in <module>
    _E8P_GRID, _E8P_GRID_IDX, _PARITY_IDX = get_full_grid(_E8P_PACKED_ABS_CACHED)
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12_rvq3bit.py", line -1, in get_full_grid
KeyboardInterrupt
W0907 15:27:47.105750 63686 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:27:47.132088 63686 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 15:27:47.246319 63686 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 15:27:47.246464 63686 utils.py:164] NumExpr defaulting to 16 threads.
I0907 15:27:47.415120 63686 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 15:27:47.663402 63686 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 15, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 40, in <module>
    from . import (latticee8_padded12, latticee8_padded12_rvq3bit,
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12_rvq4bit.py", line 108, in <module>
    _E8P_GRID, _E8P_GRID_IDX, _PARITY_IDX = get_full_grid(_E8P_PACKED_ABS_CACHED)
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12_rvq4bit.py", line -1, in get_full_grid
KeyboardInterrupt
W0907 15:29:25.874364 64130 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:29:25.900552 64130 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 15:29:26.015914 64130 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 15:29:26.016058 64130 utils.py:164] NumExpr defaulting to 16 threads.
I0907 15:29:26.182477 64130 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 15:29:26.425909 64130 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 15:29:48.914433 64130 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  7.85it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.26it/s]
I0907 15:29:49.565686 64130 quantize_finetune_llama.py:156] loaded model
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 242, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 158, in main
    devset = utils.sample_rp1t(tokenizer, args.devset_size, args.ctx_size,
  File "/home/user/benchmarks/quip-sharp/lib/utils/data_utils.py", line 59, in sample_rp1t
    dataset = load_dataset('togethercomputer/RedPajama-Data-1T-Sample',
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 2594, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 2266, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 1914, in dataset_module_factory
    raise e1 from None
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 1866, in dataset_module_factory
    can_load_config_from_parquet_export = "DEFAULT_CONFIG_NAME" not in f.read()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/codecs.py", line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte
W0907 15:29:52.176069 64253 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:29:52.202869 64253 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 15:29:52.319324 64253 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 15:29:52.319470 64253 utils.py:164] NumExpr defaulting to 16 threads.
I0907 15:29:52.497960 64253 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 15:29:52.746755 64253 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 108, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 23, in main
    saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([argparse.Namespace])` or the `torch.serialization.safe_globals([argparse.Namespace])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
W0907 15:30:16.859486 64400 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:30:16.885911 64400 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 15:30:17.000930 64400 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 15:30:17.001070 64400 utils.py:164] NumExpr defaulting to 16 threads.
I0907 15:30:17.178210 64400 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 15:30:17.444110 64400 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 15:30:38.996952 64400 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 149, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 94, in main
    devset = utils.sample_rp1t(tokenizer, args.devset_size, args.ctx_size,
  File "/home/user/benchmarks/quip-sharp/lib/utils/data_utils.py", line 59, in sample_rp1t
    dataset = load_dataset('togethercomputer/RedPajama-Data-1T-Sample',
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 2594, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 2266, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 1914, in dataset_module_factory
    raise e1 from None
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 1866, in dataset_module_factory
    can_load_config_from_parquet_export = "DEFAULT_CONFIG_NAME" not in f.read()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/codecs.py", line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte
W0907 15:30:41.537299 64505 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:30:41.564115 64505 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 15:30:41.681786 64505 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 15:30:41.681938 64505 utils.py:164] NumExpr defaulting to 16 threads.
I0907 15:30:41.856471 64505 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 15:30:42.124769 64505 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 108, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 23, in main
    saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([argparse.Namespace])` or the `torch.serialization.safe_globals([argparse.Namespace])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
W0907 15:31:06.548083 64610 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:31:06.552997 64610 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0907 15:31:06.667600 64610 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 15:31:28.131031 64610 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 304, in hf_raise_for_status
    response.raise_for_status()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/hf/2_7b_2bit/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
    raise head_call_error
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1751, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1673, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 376, in _request_wrapper
    response = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 400, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 352, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-68bda550-4b5a46d7650544aa44f6d312;3f3b2467-fa49-4d54-a8ca-ec9cbbb1f2ca)

Repository Not Found for url: https://huggingface.co/hf/2_7b_2bit/resolve/main/config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 71, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 27, in main
    model, model_str = model_from_hf_path(
  File "/home/user/benchmarks/quip-sharp/lib/utils/unsafe_import.py", line 23, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 928, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 631, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 686, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 421, in cached_file
    raise EnvironmentError(
OSError: hf/2_7b_2bit is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
W0907 15:31:31.556460 64716 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 15:31:31.619530 64716 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0907 15:31:31.630942 64716 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 15:31:52.700746 64716 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 304, in hf_raise_for_status
    response.raise_for_status()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/hf/2_7b_2bit/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
    raise head_call_error
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1751, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1673, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 376, in _request_wrapper
    response = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 400, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py", line 352, in hf_raise_for_status
    raise RepositoryNotFoundError(message, response) from e
huggingface_hub.utils._errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-68bda568-2adbc10d3212ef0f3ce39e9f;6af8f977-cfa8-4178-a8f4-dd28aef7de86)

Repository Not Found for url: https://huggingface.co/hf/2_7b_2bit/resolve/main/config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_zeroshot.py", line 62, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/eval/eval_zeroshot.py", line 27, in main
    model, model_str = model_from_hf_path(
  File "/home/user/benchmarks/quip-sharp/lib/utils/unsafe_import.py", line 23, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 928, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 631, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 686, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 421, in cached_file
    raise EnvironmentError(
OSError: hf/2_7b_2bit is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
I0907 16:35:54.791549 67131 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 16:35:54.791707 67131 utils.py:164] NumExpr defaulting to 16 threads.
W0907 16:35:55.084934 67131 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 16:35:55.131961 67131 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 16:35:55.148385 67131 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 16:35:55.312539 67131 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 55.69it/s]
I0907 16:36:18.401852 67131 quantize_finetune_llama.py:156] loaded model
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 242, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 158, in main
    devset = utils.sample_rp1t(tokenizer, args.devset_size, args.ctx_size,
  File "/home/user/benchmarks/quip-sharp/lib/utils/data_utils.py", line 59, in sample_rp1t
    dataset = load_dataset('togethercomputer/RedPajama-Data-1T-Sample',
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 2594, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 2266, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 1914, in dataset_module_factory
    raise e1 from None
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 1866, in dataset_module_factory
    can_load_config_from_parquet_export = "DEFAULT_CONFIG_NAME" not in f.read()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/codecs.py", line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte
I0907 16:36:22.817774 67248 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 16:36:22.817898 67248 utils.py:164] NumExpr defaulting to 16 threads.
W0907 16:36:23.092538 67248 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 16:36:23.137897 67248 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 16:36:23.156753 67248 config.py:58] PyTorch version 2.8.0+cu126 available.
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 40, in <module>
    from . import (latticee8_padded12, latticee8_padded12_rvq3bit,
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12.py", line 19, in <module>
    from lib.utils.matmul_had import matmul_hadU_cuda, matmul_hadUt_cuda
  File "/home/user/benchmarks/quip-sharp/lib/utils/__init__.py", line 1, in <module>
    from .data_utils import *
  File "/home/user/benchmarks/quip-sharp/lib/utils/data_utils.py", line 5, in <module>
    from datasets import load_dataset
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/__init__.py", line 17, in <module>
    from .arrow_dataset import Dataset
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 76, in <module>
    from .arrow_reader import ArrowReader
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/arrow_reader.py", line 32, in <module>
    from .download.download_config import DownloadConfig
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/download/__init__.py", line 9, in <module>
    from .download_manager import DownloadManager, DownloadMode
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/download/download_manager.py", line 35, in <module>
    from ..utils.file_utils import (
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/utils/file_utils.py", line 37, in <module>
    from aiohttp.client_exceptions import ClientError
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/aiohttp/__init__.py", line 6, in <module>
    from .client import (
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/aiohttp/client.py", line 87, in <module>
    from .connector import (
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/aiohttp/connector.py", line 894, in <module>
    _SSL_CONTEXT_VERIFIED = _make_ssl_context(True)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/aiohttp/connector.py", line 877, in _make_ssl_context
    sslcontext = ssl.create_default_context()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/ssl.py", line 771, in create_default_context
    context.load_default_certs(purpose)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/ssl.py", line 593, in load_default_certs
    self.set_default_verify_paths()
KeyboardInterrupt
I0907 16:36:55.264417 67586 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 16:36:55.264557 67586 utils.py:164] NumExpr defaulting to 16 threads.
W0907 16:36:55.553115 67586 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 16:36:55.598906 67586 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 16:36:55.615024 67586 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 16:36:55.777234 67586 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 54.81it/s]
I0907 16:37:18.776413 67586 quantize_finetune_llama.py:156] loaded model
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 242, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 158, in main
    devset = utils.sample_rp1t(tokenizer, args.devset_size, args.ctx_size,
  File "/home/user/benchmarks/quip-sharp/lib/utils/data_utils.py", line 59, in sample_rp1t
    dataset = load_dataset('togethercomputer/RedPajama-Data-1T-Sample',
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 2594, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 2266, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 1914, in dataset_module_factory
    raise e1 from None
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 1866, in dataset_module_factory
    can_load_config_from_parquet_export = "DEFAULT_CONFIG_NAME" not in f.read()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/codecs.py", line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte
I0907 16:37:23.479793 67775 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 16:37:23.479950 67775 utils.py:164] NumExpr defaulting to 16 threads.
W0907 16:37:23.822886 67775 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 16:37:23.871374 67775 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 16:37:23.890494 67775 config.py:58] PyTorch version 2.8.0+cu126 available.
W0907 16:37:24.058311 67775 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 40, in <module>
    from . import (latticee8_padded12, latticee8_padded12_rvq3bit,
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12_rvq4bit.py", line 107, in <module>
    _E8P_PACKED_ABS_CACHED = get_packed_abs_grid()
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12_rvq4bit.py", line 58, in get_packed_abs_grid
    d8abs = torch.unique(d8[sorted(torch.where(d8m2 * d8n)[0])].abs(), dim=0)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/_jit_internal.py", line 627, in fn
    return if_false(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/_jit_internal.py", line 627, in fn
    return if_false(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/functional.py", line 1102, in _return_output
    output, _, _ = _unique_impl(input, sorted, return_inverse, return_counts, dim)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/functional.py", line 987, in _unique_impl
    output, inverse_indices, counts = _VF.unique_dim(
KeyboardInterrupt
I0907 16:41:18.025325 69057 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 16:41:18.026607 69057 utils.py:164] NumExpr defaulting to 16 threads.
W0907 16:41:18.940953 69057 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 16:41:18.996998 69057 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 16:41:19.047563 69057 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 16:41:19.607832 69057 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.38it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.17it/s]
I0907 16:41:45.102870 69057 quantize_finetune_llama.py:156] loaded model
I0907 16:41:58.150633 69057 quantize_finetune_llama.py:160] loaded dataset and devset
I0907 16:41:59.972350 69057 quantize_finetune_llama.py:179] layer 0 gpu 0
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 242, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 196, in main
    model.model.layers[i](
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 240, in forward
    cos, sin = position_embeddings
TypeError: cannot unpack non-iterable NoneType object
I0907 16:42:10.975184 69228 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 16:42:10.975324 69228 utils.py:164] NumExpr defaulting to 16 threads.
W0907 16:42:11.251837 69228 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 16:42:11.298572 69228 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 16:42:11.315682 69228 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 16:42:11.477879 69228 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 108, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 23, in main
    saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([argparse.Namespace])` or the `torch.serialization.safe_globals([argparse.Namespace])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
I0907 16:42:36.634708 69323 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 16:42:36.634850 69323 utils.py:164] NumExpr defaulting to 16 threads.
W0907 16:42:36.955119 69323 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 16:42:37.002833 69323 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 16:42:37.019582 69323 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 16:42:37.188522 69323 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

`torch_dtype` is deprecated! Use `dtype` instead!
I0907 16:43:09.123111 69323 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:56<00:56, 56.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 35.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 38.71s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 7, in <module>
    from transformers import AutoTokenizer
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/__init__.py", line 950, in <module>
    import_structure = define_import_structure(Path(__file__).parent / "models", prefix="models")
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2851, in define_import_structure
    import_structure = create_import_structure_from_path(module_path)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2564, in create_import_structure_from_path
    import_structure[f] = create_import_structure_from_path(os.path.join(module_path, f))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2562, in create_import_structure_from_path
    for f in os.listdir(module_path):
KeyboardInterrupt
I0907 17:39:03.391699 72286 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 17:39:03.393835 72286 utils.py:164] NumExpr defaulting to 16 threads.
W0907 17:39:03.955003 72286 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 17:39:04.010370 72286 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 17:39:04.038265 72286 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 17:39:04.580248 72286 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  2.24it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.66it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.34it/s]
I0907 17:39:28.554373 72286 quantize_finetune_llama.py:156] loaded model
I0907 17:39:43.937139 72286 quantize_finetune_llama.py:160] loaded dataset and devset
I0907 17:39:45.663279 72286 quantize_finetune_llama.py:179] layer 0 gpu 0
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 242, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 196, in main
    model.model.layers[i](
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 240, in forward
    cos, sin = position_embeddings
TypeError: cannot unpack non-iterable NoneType object
I0907 17:39:57.146832 72479 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 17:39:57.146994 72479 utils.py:164] NumExpr defaulting to 16 threads.
W0907 17:39:57.479520 72479 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 17:39:57.527670 72479 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 17:39:57.546953 72479 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 17:39:57.718444 72479 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 108, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 23, in main
    saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([argparse.Namespace])` or the `torch.serialization.safe_globals([argparse.Namespace])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
I0907 17:40:23.473271 72624 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 17:40:23.473418 72624 utils.py:164] NumExpr defaulting to 16 threads.
W0907 17:40:23.784835 72624 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 17:40:23.832736 72624 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 17:40:23.849486 72624 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 17:40:24.016988 72624 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

`torch_dtype` is deprecated! Use `dtype` instead!
I0907 17:40:55.896466 72624 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 149, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 97, in main
    orig_model = AutoModelForCausalLM.from_pretrained(args.base_model,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 604, in from_pretrained
    return model_class.from_pretrained(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/modeling_utils.py", line 288, in _wrapper
    return func(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5176, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5639, in _load_pretrained_model
    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/modeling_utils.py", line 946, in load_shard_file
    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/modeling_utils.py", line 815, in _load_state_dict_into_meta_model
    param = param.to(casting_dtype)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 11, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2302, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2330, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py", line 23, in <module>
    from .auto_factory import (
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 43, in <module>
    from ...generation import GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2302, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2330, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/generation/utils.py", line 45, in <module>
    from ..masking_utils import create_masks_for_generate
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/masking_utils.py", line 40, in <module>
    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/_dynamo/__init__.py", line 13, in <module>
    from . import config, convert_frame, eval_frame, resume_execution
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 53, in <module>
    from torch._dynamo.symbolic_convert import TensorifyState
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 52, in <module>
    from torch._dynamo.exc import TensorifyScalarRestartAnalysis
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/_dynamo/exc.py", line 41, in <module>
    from .utils import counters
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 2341, in <module>
    if has_triton_package():
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/utils/_triton.py", line 9, in has_triton_package
    from triton.compiler.compiler import triton_key
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/triton/__init__.py", line 8, in <module>
    from .runtime import (
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/triton/runtime/__init__.py", line 1, in <module>
    from .autotuner import (Autotuner, Config, Heuristics, autotune, heuristics)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/triton/runtime/autotuner.py", line 12, in <module>
    from .jit import KernelInterface
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/triton/runtime/jit.py", line 17, in <module>
    from ..runtime.driver import driver
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/triton/runtime/driver.py", line 3, in <module>
    from ..backends import backends, DriverBase
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/triton/backends/__init__.py", line 47, in <module>
    backends: dict[str, Backend] = _discover_backends()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/triton/backends/__init__.py", line 40, in _discover_backends
    compiler = importlib.import_module(f"{ep.value}.compiler")
  File "/home/user/miniconda3/envs/quip/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 2, in <module>
    from triton._C.libtriton import ir, passes, llvm, amd
KeyboardInterrupt
I0907 17:47:20.204824 74416 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 17:47:20.205709 74416 utils.py:164] NumExpr defaulting to 16 threads.
W0907 17:47:20.553614 74416 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 17:47:20.602241 74416 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 17:47:20.622622 74416 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 17:47:20.836025 74416 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  6.39it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 10.40it/s]
I0907 17:47:44.042145 74416 quantize_finetune_llama.py:156] loaded model
I0907 17:47:54.890481 74416 quantize_finetune_llama.py:160] loaded dataset and devset
I0907 17:47:56.729888 74416 quantize_finetune_llama.py:179] layer 0 gpu 0
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 242, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 196, in main
    model.model.layers[i](
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 240, in forward
    cos, sin = position_embeddings
TypeError: cannot unpack non-iterable NoneType object
I0907 17:48:03.555466 74582 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 17:48:03.555615 74582 utils.py:164] NumExpr defaulting to 16 threads.
W0907 17:48:03.847966 74582 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 17:48:03.895721 74582 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 17:48:03.913503 74582 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 17:48:04.081109 74582 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 40, in <module>
    from . import (latticee8_padded12, latticee8_padded12_rvq3bit,
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12.py", line 114, in <module>
    _E8P_GRID, _E8P_GRID_IDX, _PARITY_IDX = get_full_grid(_E8P_PACKED_ABS_CACHED)
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12.py", line -1, in get_full_grid
KeyboardInterrupt
I0907 17:51:02.597111 75538 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 17:51:02.597410 75538 utils.py:164] NumExpr defaulting to 16 threads.
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 11, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2302, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2330, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py", line 23, in <module>
    from .auto_factory import (
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 43, in <module>
    from ...generation import GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2302, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2330, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/generation/utils.py", line 58, in <module>
    from .candidate_generator import (
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/generation/candidate_generator.py", line 29, in <module>
    from sklearn.metrics import roc_curve
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/sklearn/__init__.py", line 73, in <module>
    from .base import clone  # noqa: E402
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/sklearn/base.py", line 19, in <module>
    from .utils._metadata_requests import _MetadataRequester, _routing_enabled
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/sklearn/utils/__init__.py", line 9, in <module>
    from ._chunking import gen_batches, gen_even_slices
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/sklearn/utils/_chunking.py", line 11, in <module>
    from ._param_validation import Interval, validate_params
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 17, in <module>
    from .validation import _is_arraylike_not_scalar
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/sklearn/utils/validation.py", line 21, in <module>
    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/sklearn/utils/_array_api.py", line 20, in <module>
    from .fixes import parse_version
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/sklearn/utils/fixes.py", line 20, in <module>
    import pandas as pd
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/pandas/core/api.py", line 28, in <module>
    from pandas.core.arrays import Categorical
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/pandas/core/arrays/__init__.py", line 1, in <module>
    from pandas.core.arrays.arrow import ArrowExtensionArray
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/pandas/core/arrays/arrow/__init__.py", line 5, in <module>
    from pandas.core.arrays.arrow.array import ArrowExtensionArray
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/pandas/core/arrays/arrow/array.py", line 66, in <module>
    from pandas.core.arrays.masked import BaseMaskedArray
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/pandas/core/arrays/masked.py", line 61, in <module>
    from pandas.core import (
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1012, in get_code
  File "<frozen importlib._bootstrap_external>", line 672, in _compile_bytecode
KeyboardInterrupt
W0907 18:51:35.255469 89385 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 18:51:35.279545 89385 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 18:51:35.385113 89385 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 18:51:35.385222 89385 utils.py:164] NumExpr defaulting to 16 threads.
I0907 18:51:35.535958 89385 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 18:51:35.770360 89385 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 18:51:57.940542 89385 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  9.83it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 10.56it/s]
I0907 18:51:58.502629 89385 quantize_finetune_llama.py:156] loaded model
I0907 18:52:17.870156 89385 quantize_finetune_llama.py:160] loaded dataset and devset
I0907 18:52:19.504677 89385 quantize_finetune_llama.py:179] layer 0 gpu 0
I0907 18:52:43.062963 89385 quantize_finetune_llama.py:211] computed original embedding for layer 0 in 23.479334354400635s, pre msv 4.959364014212042e-05, post msv 0.000485786673380062
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0907 18:52:50.509418 89385 quantize_finetune_llama.py:179] layer 1 gpu 0
W0907 18:52:51.834600 89551 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 18:52:51.859137 89551 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 18:52:51.969891 89551 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 18:52:51.970011 89551 utils.py:164] NumExpr defaulting to 16 threads.
I0907 18:52:52.126843 89551 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 18:52:52.364752 89551 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 18:53:11.795881 89551 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-1:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//0_qkv.pt'
I0907 18:53:37.020604 89385 quantize_finetune_llama.py:211] computed original embedding for layer 1 in 23.560860872268677s, pre msv 0.000485786673380062, post msv 0.5002250075340271
I0907 18:53:37.130828 89385 quantize_finetune_llama.py:179] layer 2 gpu 0
W0907 18:53:38.458566 89656 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 18:53:38.483846 89656 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 18:53:38.617603 89656 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 18:53:38.617762 89656 utils.py:164] NumExpr defaulting to 16 threads.
I0907 18:53:38.796984 89656 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 18:53:39.041621 89656 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 18:53:58.435132 89656 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-2:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//1_qkv.pt'
I0907 18:54:26.343963 89385 quantize_finetune_llama.py:211] computed original embedding for layer 2 in 26.611443758010864s, pre msv 0.5002250075340271, post msv 0.5026950836181641
I0907 18:54:26.459656 89385 quantize_finetune_llama.py:179] layer 3 gpu 0
W0907 18:54:27.719280 89794 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 18:54:27.743391 89794 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 18:54:27.852540 89794 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 18:54:27.852627 89794 utils.py:164] NumExpr defaulting to 16 threads.
I0907 18:54:28.002149 89794 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 18:54:28.239228 89794 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 18:54:47.680680 89794 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-3:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//2_qkv.pt'
I0907 18:55:14.985705 89385 quantize_finetune_llama.py:211] computed original embedding for layer 3 in 25.933621644973755s, pre msv 0.5026950836181641, post msv 0.5036399960517883
I0907 18:55:15.065701 89385 quantize_finetune_llama.py:179] layer 4 gpu 0
W0907 18:55:16.388966 89913 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 18:55:16.414034 89913 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 18:55:16.527713 89913 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 18:55:16.527849 89913 utils.py:164] NumExpr defaulting to 16 threads.
I0907 18:55:16.687187 89913 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 18:55:16.927018 89913 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 18:55:36.497849 89913 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-4:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//3_qkv.pt'
I0907 18:56:03.680670 89385 quantize_finetune_llama.py:211] computed original embedding for layer 4 in 25.922022819519043s, pre msv 0.5036399960517883, post msv 0.5094535946846008
I0907 18:56:03.778123 89385 quantize_finetune_llama.py:179] layer 5 gpu 0
W0907 18:56:05.041128 90003 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 18:56:05.065587 90003 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 18:56:05.175050 90003 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 18:56:05.175141 90003 utils.py:164] NumExpr defaulting to 16 threads.
I0907 18:56:05.326056 90003 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 18:56:05.561971 90003 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 18:56:24.867892 90003 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-5:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//4_qkv.pt'
I0907 18:56:53.104726 89385 quantize_finetune_llama.py:211] computed original embedding for layer 5 in 26.898730993270874s, pre msv 0.5094535946846008, post msv 0.5105752944946289
I0907 18:56:53.225169 89385 quantize_finetune_llama.py:179] layer 6 gpu 0
W0907 18:56:54.474728 90092 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 18:56:54.498384 90092 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 18:56:54.601976 90092 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 18:56:54.602091 90092 utils.py:164] NumExpr defaulting to 16 threads.
I0907 18:56:54.749805 90092 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 18:56:54.985991 90092 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 18:57:14.116423 90092 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-6:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//5_qkv.pt'
I0907 18:57:41.055976 89385 quantize_finetune_llama.py:211] computed original embedding for layer 6 in 25.790289640426636s, pre msv 0.5105752944946289, post msv 0.5122508406639099
I0907 18:57:41.139444 89385 quantize_finetune_llama.py:179] layer 7 gpu 0
W0907 18:57:42.433477 90181 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 18:57:42.459122 90181 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 18:57:42.575786 90181 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 18:57:42.575933 90181 utils.py:164] NumExpr defaulting to 16 threads.
I0907 18:57:42.758799 90181 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 18:57:43.008187 90181 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 18:58:02.556375 90181 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-7:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//6_qkv.pt'
I0907 18:58:28.627378 89385 quantize_finetune_llama.py:211] computed original embedding for layer 7 in 24.77099323272705s, pre msv 0.5122508406639099, post msv 0.5144863128662109
I0907 18:58:28.736036 89385 quantize_finetune_llama.py:179] layer 8 gpu 0
W0907 18:58:29.949313 90271 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 18:58:29.972880 90271 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 18:58:30.076884 90271 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 18:58:30.076982 90271 utils.py:164] NumExpr defaulting to 16 threads.
I0907 18:58:30.223892 90271 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 18:58:30.455743 90271 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 18:58:50.079143 90271 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-8:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//7_qkv.pt'
I0907 18:59:15.897191 89385 quantize_finetune_llama.py:211] computed original embedding for layer 8 in 24.513696670532227s, pre msv 0.5144863128662109, post msv 0.5168983340263367
I0907 18:59:16.012892 89385 quantize_finetune_llama.py:179] layer 9 gpu 0
W0907 18:59:17.235519 90361 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 18:59:17.259155 90361 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 18:59:17.361817 90361 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 18:59:17.361910 90361 utils.py:164] NumExpr defaulting to 16 threads.
I0907 18:59:17.506210 90361 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 18:59:17.742764 90361 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 18:59:37.885211 90361 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-9:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//8_qkv.pt'
I0907 19:00:04.811306 89385 quantize_finetune_llama.py:211] computed original embedding for layer 9 in 25.638296365737915s, pre msv 0.5168983340263367, post msv 0.5194587111473083
I0907 19:00:04.902537 89385 quantize_finetune_llama.py:179] layer 10 gpu 0
W0907 19:00:06.163510 90453 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:00:06.187897 90453 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:00:06.297075 90453 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:00:06.297161 90453 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:00:06.447079 90453 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:00:06.682271 90453 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:00:26.201320 90453 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-10:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//9_qkv.pt'
I0907 19:00:54.160642 89385 quantize_finetune_llama.py:211] computed original embedding for layer 10 in 26.51615023612976s, pre msv 0.5194587111473083, post msv 0.5221440196037292
I0907 19:00:54.274539 89385 quantize_finetune_llama.py:179] layer 11 gpu 0
W0907 19:00:55.584490 90542 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:00:55.608726 90542 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:00:55.715892 90542 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:00:55.715977 90542 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:00:55.866453 90542 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:00:56.106168 90542 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:01:15.443214 90542 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-11:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//10_qkv.pt'
I0907 19:01:43.133557 89385 quantize_finetune_llama.py:211] computed original embedding for layer 11 in 26.407008409500122s, pre msv 0.5221440196037292, post msv 0.5252853631973267
I0907 19:01:43.216723 89385 quantize_finetune_llama.py:179] layer 12 gpu 0
W0907 19:01:44.511770 90631 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:01:44.536128 90631 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:01:44.644377 90631 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:01:44.644487 90631 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:01:44.803127 90631 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:01:45.044884 90631 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:02:04.252070 90631 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-12:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//11_qkv.pt'
I0907 19:02:31.191193 89385 quantize_finetune_llama.py:211] computed original embedding for layer 12 in 25.49583649635315s, pre msv 0.5252853631973267, post msv 0.528696596622467
I0907 19:02:31.300902 89385 quantize_finetune_llama.py:179] layer 13 gpu 0
W0907 19:02:32.624017 90721 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:02:32.649230 90721 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:02:32.761851 90721 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:02:32.761989 90721 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:02:32.924227 90721 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:02:33.163339 90721 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:02:52.543356 90721 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-13:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//12_qkv.pt'
I0907 19:03:20.331770 89385 quantize_finetune_llama.py:211] computed original embedding for layer 13 in 26.565850734710693s, pre msv 0.528696596622467, post msv 0.5352429747581482
I0907 19:03:20.440843 89385 quantize_finetune_llama.py:179] layer 14 gpu 0
W0907 19:03:21.714305 90810 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:03:21.739078 90810 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:03:21.850719 90810 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:03:21.850860 90810 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:03:22.023481 90810 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:03:22.268110 90810 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:03:41.694367 90810 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-14:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//13_qkv.pt'
I0907 19:04:07.357393 89385 quantize_finetune_llama.py:211] computed original embedding for layer 14 in 24.389068841934204s, pre msv 0.5352429747581482, post msv 0.5414574146270752
I0907 19:04:07.469834 89385 quantize_finetune_llama.py:179] layer 15 gpu 0
W0907 19:04:08.758217 90899 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:04:08.782129 90899 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:04:08.889524 90899 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:04:08.889621 90899 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:04:09.040622 90899 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:04:09.272236 90899 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:04:28.330161 90899 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-15:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//14_qkv.pt'
I0907 19:04:51.679607 89385 quantize_finetune_llama.py:211] computed original embedding for layer 15 in 22.092593908309937s, pre msv 0.5414574146270752, post msv 0.5519701838493347
I0907 19:04:51.791432 89385 quantize_finetune_llama.py:179] layer 16 gpu 0
W0907 19:04:53.091753 90989 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:04:53.116158 90989 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:04:53.226693 90989 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:04:53.226798 90989 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:04:53.385816 90989 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:04:53.626506 90989 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:05:12.740115 90989 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-16:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//15_qkv.pt'
I0907 19:05:39.775664 89385 quantize_finetune_llama.py:211] computed original embedding for layer 16 in 25.62922739982605s, pre msv 0.5519701838493347, post msv 0.5729906558990479
I0907 19:05:39.871791 89385 quantize_finetune_llama.py:179] layer 17 gpu 0
W0907 19:05:41.126634 91082 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:05:41.150220 91082 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:05:41.258773 91082 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:05:41.258857 91082 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:05:41.403801 91082 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:05:41.640163 91082 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:06:00.712424 91082 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-17:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//16_qkv.pt'
I0907 19:06:25.852463 89385 quantize_finetune_llama.py:211] computed original embedding for layer 17 in 23.885738372802734s, pre msv 0.5729906558990479, post msv 0.5887737274169922
I0907 19:06:25.929558 89385 quantize_finetune_llama.py:179] layer 18 gpu 0
W0907 19:06:27.339470 91171 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:06:27.364981 91171 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:06:27.484102 91171 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:06:27.484260 91171 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:06:27.696979 91171 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:06:27.972609 91171 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:06:47.366715 91171 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-18:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//17_qkv.pt'
I0907 19:07:15.014937 89385 quantize_finetune_llama.py:211] computed original embedding for layer 18 in 26.46160578727722s, pre msv 0.5887737274169922, post msv 0.6145440936088562
I0907 19:07:15.120592 89385 quantize_finetune_llama.py:179] layer 19 gpu 0
W0907 19:07:16.440267 91260 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:07:16.465031 91260 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:07:16.577668 91260 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:07:16.577794 91260 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:07:16.750119 91260 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:07:17.007801 91260 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:07:36.107909 91260 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-19:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//18_qkv.pt'
I0907 19:08:03.834454 89385 quantize_finetune_llama.py:211] computed original embedding for layer 19 in 26.49665355682373s, pre msv 0.6145440936088562, post msv 0.651021420955658
I0907 19:08:03.912373 89385 quantize_finetune_llama.py:179] layer 20 gpu 0
W0907 19:08:05.179431 91351 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:08:05.202853 91351 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:08:05.310837 91351 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:08:05.310920 91351 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:08:05.459961 91351 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:08:05.697254 91351 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:08:24.956071 91351 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-20:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//19_qkv.pt'
I0907 19:08:52.345704 89385 quantize_finetune_llama.py:211] computed original embedding for layer 20 in 26.150404930114746s, pre msv 0.651021420955658, post msv 0.7045366764068604
I0907 19:08:52.465006 89385 quantize_finetune_llama.py:179] layer 21 gpu 0
W0907 19:08:53.738265 91440 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:08:53.762027 91440 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:08:53.870091 91440 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:08:53.870174 91440 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:08:54.022920 91440 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:08:54.266564 91440 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:09:13.134830 91440 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-21:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//20_qkv.pt'
I0907 19:09:40.776188 89385 quantize_finetune_llama.py:211] computed original embedding for layer 21 in 26.125985145568848s, pre msv 0.7045366764068604, post msv 0.757072925567627
I0907 19:09:40.887122 89385 quantize_finetune_llama.py:179] layer 22 gpu 0
W0907 19:09:42.151718 91529 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:09:42.175348 91529 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:09:42.284499 91529 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:09:42.284583 91529 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:09:42.430673 91529 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:09:42.666249 91529 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:10:01.638380 91529 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-22:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//21_qkv.pt'
I0907 19:10:29.176921 89385 quantize_finetune_llama.py:211] computed original embedding for layer 22 in 26.298217296600342s, pre msv 0.757072925567627, post msv 0.8597317337989807
I0907 19:10:29.290737 89385 quantize_finetune_llama.py:179] layer 23 gpu 0
W0907 19:10:30.552511 91623 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:10:30.576323 91623 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:10:30.685653 91623 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:10:30.685768 91623 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:10:30.839905 91623 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:10:31.078381 91623 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:10:49.824810 91623 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-23:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//22_qkv.pt'
I0907 19:11:16.379065 89385 quantize_finetune_llama.py:211] computed original embedding for layer 23 in 25.323302745819092s, pre msv 0.8597317337989807, post msv 0.9156923890113831
I0907 19:11:16.507160 89385 quantize_finetune_llama.py:179] layer 24 gpu 0
W0907 19:11:17.766141 91713 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:11:17.789781 91713 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:11:17.895568 91713 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:11:17.895678 91713 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:11:18.044065 91713 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:11:18.275434 91713 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:11:37.411219 91713 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-24:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//23_qkv.pt'
I0907 19:12:04.449231 89385 quantize_finetune_llama.py:211] computed original embedding for layer 24 in 25.784235954284668s, pre msv 0.9156923890113831, post msv 1.03025221824646
I0907 19:12:04.557414 89385 quantize_finetune_llama.py:179] layer 25 gpu 0
W0907 19:12:05.822347 91802 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:12:05.847331 91802 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:12:05.958572 91802 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:12:05.958684 91802 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:12:06.115604 91802 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:12:06.352278 91802 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:12:25.411624 91802 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-25:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//24_qkv.pt'
I0907 19:12:51.978659 89385 quantize_finetune_llama.py:211] computed original embedding for layer 25 in 25.03467559814453s, pre msv 1.03025221824646, post msv 1.061630368232727
I0907 19:12:52.059004 89385 quantize_finetune_llama.py:179] layer 26 gpu 0
W0907 19:12:53.320444 91891 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:12:53.344172 91891 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:12:53.453119 91891 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:12:53.453212 91891 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:12:53.603063 91891 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:12:53.839547 91891 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:13:12.732328 91891 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-26:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//25_qkv.pt'
I0907 19:13:38.778378 89385 quantize_finetune_llama.py:211] computed original embedding for layer 26 in 24.69959545135498s, pre msv 1.061630368232727, post msv 1.2449901103973389
I0907 19:13:38.888145 89385 quantize_finetune_llama.py:179] layer 27 gpu 0
W0907 19:13:40.189150 91980 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:13:40.213099 91980 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:13:40.322445 91980 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:13:40.322533 91980 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:13:40.475132 91980 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:13:40.728334 91980 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:13:59.827968 91980 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-27:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//26_qkv.pt'
I0907 19:14:23.173145 89385 quantize_finetune_llama.py:211] computed original embedding for layer 27 in 22.037710428237915s, pre msv 1.2449901103973389, post msv 1.3314666748046875
I0907 19:14:23.279477 89385 quantize_finetune_llama.py:179] layer 28 gpu 0
W0907 19:14:24.547067 92070 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:14:24.570629 92070 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:14:24.679546 92070 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:14:24.679635 92070 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:14:24.832221 92070 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:14:25.073389 92070 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:14:44.497290 92070 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-28:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//27_qkv.pt'
I0907 19:15:11.716402 89385 quantize_finetune_llama.py:211] computed original embedding for layer 28 in 25.875252962112427s, pre msv 1.3314666748046875, post msv 1.4843926429748535
I0907 19:15:11.839725 89385 quantize_finetune_llama.py:179] layer 29 gpu 0
W0907 19:15:13.159507 92163 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:15:13.184419 92163 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:15:13.297097 92163 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:15:13.297232 92163 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:15:13.462985 92163 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:15:13.704895 92163 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:15:32.966535 92163 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-29:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//28_qkv.pt'
I0907 19:16:00.615390 89385 quantize_finetune_llama.py:211] computed original embedding for layer 29 in 26.40515947341919s, pre msv 1.4843926429748535, post msv 1.685329556465149
I0907 19:16:00.699850 89385 quantize_finetune_llama.py:179] layer 30 gpu 0
W0907 19:16:02.056711 92252 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:16:02.081766 92252 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:16:02.193425 92252 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:16:02.193544 92252 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:16:02.352262 92252 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:16:02.593840 92252 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:16:22.091733 92252 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-30:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//29_qkv.pt'
I0907 19:16:49.706710 89385 quantize_finetune_llama.py:211] computed original embedding for layer 30 in 26.323933124542236s, pre msv 1.685329556465149, post msv 1.4689382314682007
I0907 19:16:49.821013 89385 quantize_finetune_llama.py:179] layer 31 gpu 0
W0907 19:16:51.095372 92419 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:16:51.119097 92419 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:16:51.228216 92419 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:16:51.228303 92419 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:16:51.384308 92419 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:16:51.640177 92419 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:17:10.987498 92419 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-31:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//30_qkv.pt'
I0907 19:17:36.269102 89385 quantize_finetune_llama.py:211] computed original embedding for layer 31 in 23.983838081359863s, pre msv 1.4689382314682007, post msv 3.4358034133911133
W0907 19:17:37.604184 92527 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:17:37.627686 92527 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:17:37.731631 92527 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:17:37.731745 92527 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:17:37.874692 92527 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:17:38.106118 92527 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:17:56.884611 92527 data_utils.py:205] using 256 training seqs, 128 validation seqs
Process Process-32:
Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 112, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/home/user/benchmarks/quip-sharp/lib/algo/finetune.py", line 105, in quantize_finetune_decoder_layer
    quip.quantize_linear(weights, save_path, hessian_path, cb, args,
  File "/home/user/benchmarks/quip-sharp/lib/algo/quip.py", line 490, in quantize_linear
    H_data = torch.load(hessian_path, map_location=torch.device('cpu'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'hess/llama2_7b_6144//31_qkv.pt'
W0907 19:18:03.550201 92632 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:18:03.574632 92632 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:18:03.686151 92632 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:18:03.686295 92632 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:18:03.857338 92632 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:18:04.106453 92632 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 108, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 23, in main
    saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([argparse.Namespace])` or the `torch.serialization.safe_globals([argparse.Namespace])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
W0907 19:18:27.237101 92732 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:18:27.261169 92732 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:18:27.369456 92732 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:18:27.369559 92732 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:18:27.528459 92732 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:18:27.764917 92732 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 19:18:48.727079 92732 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0907 19:18:59.770162 92732 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.31s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0907 19:21:15.910427 92936 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:21:15.943508 92936 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:21:17.296834 92936 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:21:17.296949 92936 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:21:17.806255 92936 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:21:19.346448 92936 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 108, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 23, in main
    saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([argparse.Namespace])` or the `torch.serialization.safe_globals([argparse.Namespace])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
W0907 19:21:46.950703 93049 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:21:46.954946 93049 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0907 19:21:47.047829 93049 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 19:22:08.588135 93049 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/hf/2_7b_2bit/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1010, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1117, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1658, in _raise_on_head_call_error
    raise head_call_error
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1546, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1463, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 310, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 459, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-68bddb60-4ec2be5d2d5eddf058d6a8f3;848c6ac0-a0d8-4cef-bf0d-eb99f6d878cf)

Repository Not Found for url: https://huggingface.co/hf/2_7b_2bit/resolve/main/config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 71, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 27, in main
    model, model_str = model_from_hf_path(
  File "/home/user/benchmarks/quip-sharp/lib/utils/unsafe_import.py", line 23, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 928, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 631, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 686, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 421, in cached_file
    raise EnvironmentError(
OSError: hf/2_7b_2bit is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
W0907 19:22:12.367845 93145 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:22:12.429541 93145 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0907 19:22:12.440127 93145 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 19:22:33.484736 93145 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/hf/2_7b_2bit/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1010, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1117, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1658, in _raise_on_head_call_error
    raise head_call_error
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1546, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1463, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 310, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 459, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-68bddb79-6f76a2680f8184604eca28b1;fb592758-11eb-4095-baf4-5847eb984bf3)

Repository Not Found for url: https://huggingface.co/hf/2_7b_2bit/resolve/main/config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_zeroshot.py", line 62, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/eval/eval_zeroshot.py", line 27, in main
    model, model_str = model_from_hf_path(
  File "/home/user/benchmarks/quip-sharp/lib/utils/unsafe_import.py", line 23, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 928, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 631, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 686, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 421, in cached_file
    raise EnvironmentError(
OSError: hf/2_7b_2bit is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
W0907 19:40:38.103145 93442 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:40:38.127134 93442 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:40:38.233875 93442 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:40:38.233977 93442 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:40:38.389672 93442 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:40:38.625612 93442 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 19:41:01.031714 93442 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.74it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.58it/s]
I0907 19:41:02.945445 93442 quantize_finetune_llama.py:156] loaded model
I0907 19:41:28.564795 93442 quantize_finetune_llama.py:160] loaded dataset and devset
I0907 19:41:30.346051 93442 quantize_finetune_llama.py:179] layer 0 gpu 0
I0907 19:42:00.030948 93442 quantize_finetune_llama.py:211] computed original embedding for layer 0 in 29.589252471923828s, pre msv 4.959364014212042e-05, post msv 0.000485786673380062
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0907 19:42:07.032005 93442 quantize_finetune_llama.py:179] layer 1 gpu 0
W0907 19:42:08.357518 93678 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:42:08.382526 93678 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:42:08.496018 93678 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:42:08.496145 93678 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:42:08.661686 93678 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:42:08.920016 93678 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:42:28.308762 93678 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 19:42:30.258777 93678 quip.py:388] mean square of W: 1.0
I0907 19:42:30.259235 93678 quip.py:389] mean square of Wr: 1.0
I0907 19:42:30.294726 93678 quip.py:390] difference between Hr and Hr.T: 9.5367431640625e-07
I0907 19:42:30.295124 93678 quip.py:391] max abs of Hr: 4.534232139587402
I0907 19:42:30.324275 93678 quip.py:392] min diag of Lhr: 0.13413164019584656
I0907 19:42:43.302139 93678 misc.py:19] ckpt/2_7b_2bit/0_qkv.pt frob  error: 0.6013572812080383
I0907 19:42:43.302322 93678 misc.py:20] ckpt/2_7b_2bit/0_qkv.pt proxy error: 0.00014375950559042394
I0907 19:42:52.284033 93678 finetune.py:25] layer 0_qkv initial loss 3.856013108816114e-07
W0907 19:42:52.284238 93678 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 19:42:52.504711 93678 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 19:43:05.189079 93678 finetune.py:49] layer 0_qkv @ epoch 0 new loss 1.8935894274818565e-07 old loss 3.856013108816114e-07 BETTER
I0907 19:43:17.754781 93678 finetune.py:49] layer 0_qkv @ epoch 1 new loss 1.6727126705973205e-07 old loss 1.8935894274818565e-07 BETTER
I0907 19:43:30.353027 93678 finetune.py:49] layer 0_qkv @ epoch 2 new loss 1.601862749112115e-07 old loss 1.6727126705973205e-07 BETTER
I0907 19:43:42.766211 93678 finetune.py:49] layer 0_qkv @ epoch 3 new loss 1.5513707296577195e-07 old loss 1.601862749112115e-07 BETTER
I0907 19:43:55.013962 93678 finetune.py:49] layer 0_qkv @ epoch 4 new loss 1.5150779120176594e-07 old loss 1.5513707296577195e-07 BETTER
I0907 19:43:56.552061 93678 quip.py:388] mean square of W: 1.0
I0907 19:43:56.552331 93678 quip.py:389] mean square of Wr: 1.0
I0907 19:43:56.552812 93678 quip.py:390] difference between Hr and Hr.T: 9.5367431640625e-07
I0907 19:43:56.553104 93678 quip.py:391] max abs of Hr: 6.745752811431885
I0907 19:43:56.553204 93678 quip.py:392] min diag of Lhr: 0.12213138490915298
I0907 19:44:06.610887 93678 misc.py:19] ckpt/2_7b_2bit/0_o.pt frob  error: 0.6810200810432434
I0907 19:44:06.611117 93678 misc.py:20] ckpt/2_7b_2bit/0_o.pt proxy error: 0.0009523002081550658
I0907 19:44:12.305074 93678 finetune.py:25] layer 0_o initial loss 3.939689747767261e-07
I0907 19:44:24.443971 93678 finetune.py:49] layer 0_o @ epoch 0 new loss 3.7124382856745797e-07 old loss 3.939689747767261e-07 BETTER
I0907 19:44:36.574803 93678 finetune.py:49] layer 0_o @ epoch 1 new loss 3.487852495709376e-07 old loss 3.7124382856745797e-07 BETTER
I0907 19:44:48.906922 93678 finetune.py:49] layer 0_o @ epoch 2 new loss 3.378384292318515e-07 old loss 3.487852495709376e-07 BETTER
I0907 19:45:00.995019 93678 finetune.py:49] layer 0_o @ epoch 3 new loss 3.368282932569855e-07 old loss 3.378384292318515e-07 BETTER
I0907 19:45:13.307064 93678 finetune.py:49] layer 0_o @ epoch 4 new loss 3.307601446067565e-07 old loss 3.368282932569855e-07 BETTER
I0907 19:45:15.315862 93678 quip.py:388] mean square of W: 1.0
I0907 19:45:15.316486 93678 quip.py:389] mean square of Wr: 1.0
I0907 19:45:15.316959 93678 quip.py:390] difference between Hr and Hr.T: 3.5762786865234375e-07
I0907 19:45:15.317257 93678 quip.py:391] max abs of Hr: 2.6488590240478516
I0907 19:45:15.317352 93678 quip.py:392] min diag of Lhr: 0.4706490635871887
I0907 19:45:31.138511 93678 misc.py:19] ckpt/2_7b_2bit/0_up.pt frob  error: 0.14889614284038544
I0907 19:45:31.138695 93678 misc.py:20] ckpt/2_7b_2bit/0_up.pt proxy error: 0.015588932670652866
I0907 19:45:35.437245 93678 finetune.py:25] layer 0_up initial loss 1.2126582760174642e-06
I0907 19:45:47.191880 93678 finetune.py:49] layer 0_up @ epoch 0 new loss 1.0601104349916568e-06 old loss 1.2126582760174642e-06 BETTER
I0907 19:45:58.777836 93678 finetune.py:49] layer 0_up @ epoch 1 new loss 1.0255936331304838e-06 old loss 1.0601104349916568e-06 BETTER
I0907 19:46:10.445317 93678 finetune.py:49] layer 0_up @ epoch 2 new loss 1.01839702892903e-06 old loss 1.0255936331304838e-06 BETTER
I0907 19:46:22.181037 93678 finetune.py:49] layer 0_up @ epoch 3 new loss 1.0109492905030493e-06 old loss 1.01839702892903e-06 BETTER
I0907 19:46:34.060394 93678 finetune.py:49] layer 0_up @ epoch 4 new loss 1.0034514161816332e-06 old loss 1.0109492905030493e-06 BETTER
I0907 19:46:38.335485 93678 quip.py:388] mean square of W: 1.0000001192092896
I0907 19:46:38.335904 93678 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 19:46:38.337735 93678 quip.py:390] difference between Hr and Hr.T: 6.556510925292969e-07
I0907 19:46:38.338317 93678 quip.py:391] max abs of Hr: 1.8101035356521606
I0907 19:46:38.338428 93678 quip.py:392] min diag of Lhr: 0.36613762378692627
I0907 19:47:05.476489 93678 misc.py:19] ckpt/2_7b_2bit/0_down.pt frob  error: 0.16125518083572388
I0907 19:47:05.476765 93678 misc.py:20] ckpt/2_7b_2bit/0_down.pt proxy error: 0.009706106968224049
I0907 19:47:37.305732 93442 quantize_finetune_llama.py:211] computed original embedding for layer 1 in 28.425520658493042s, pre msv 0.000485786673380062, post msv 0.5002250075340271
I0907 19:47:37.421722 93442 quantize_finetune_llama.py:179] layer 2 gpu 0
W0907 19:47:38.745223 93825 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:47:38.770303 93825 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:47:38.882476 93825 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:47:38.882605 93825 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:47:39.052309 93825 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:47:39.298212 93825 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:47:58.552602 93825 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 19:48:00.086864 93825 quip.py:388] mean square of W: 1.0000001192092896
I0907 19:48:00.087336 93825 quip.py:389] mean square of Wr: 1.0
I0907 19:48:00.096392 93825 quip.py:390] difference between Hr and Hr.T: 7.152557373046875e-07
I0907 19:48:00.096792 93825 quip.py:391] max abs of Hr: 3.316262722015381
I0907 19:48:00.105637 93825 quip.py:392] min diag of Lhr: 0.1720304638147354
I0907 19:48:13.144247 93825 misc.py:19] ckpt/2_7b_2bit/1_qkv.pt frob  error: 0.1717938780784607
I0907 19:48:13.144478 93825 misc.py:20] ckpt/2_7b_2bit/1_qkv.pt proxy error: 0.0006493374821729958
I0907 19:48:20.272084 93825 finetune.py:25] layer 1_qkv initial loss 8.393274038098752e-05
W0907 19:48:20.272372 93825 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 19:48:20.421622 93825 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 19:48:33.387535 93825 finetune.py:49] layer 1_qkv @ epoch 0 new loss 2.251148544019088e-05 old loss 8.393274038098752e-05 BETTER
I0907 19:48:46.352550 93825 finetune.py:49] layer 1_qkv @ epoch 1 new loss 1.4112242752162274e-05 old loss 2.251148544019088e-05 BETTER
I0907 19:48:58.938323 93825 finetune.py:49] layer 1_qkv @ epoch 2 new loss 7.293096132343635e-06 old loss 1.4112242752162274e-05 BETTER
I0907 19:49:11.468784 93825 finetune.py:56] layer 1_qkv @ epoch 3 new loss 7.774593541398644e-06 old loss 7.293096132343635e-06 WORSE
I0907 19:49:23.762541 93825 finetune.py:49] layer 1_qkv @ epoch 4 new loss 5.35817707714159e-06 old loss 7.293096132343635e-06 BETTER
I0907 19:49:25.185854 93825 quip.py:388] mean square of W: 0.9999999403953552
I0907 19:49:25.186144 93825 quip.py:389] mean square of Wr: 0.9999998807907104
I0907 19:49:25.186622 93825 quip.py:390] difference between Hr and Hr.T: 3.5762786865234375e-07
I0907 19:49:25.186919 93825 quip.py:391] max abs of Hr: 2.509366750717163
I0907 19:49:25.187024 93825 quip.py:392] min diag of Lhr: 0.15850964188575745
I0907 19:49:35.944715 93825 misc.py:19] ckpt/2_7b_2bit/1_o.pt frob  error: 0.2266434133052826
I0907 19:49:35.944945 93825 misc.py:20] ckpt/2_7b_2bit/1_o.pt proxy error: 0.004275206010788679
I0907 19:49:41.602410 93825 finetune.py:25] layer 1_o initial loss 8.404012987739407e-06
I0907 19:49:53.770552 93825 finetune.py:56] layer 1_o @ epoch 0 new loss 2.2839061784907244e-05 old loss 8.404012987739407e-06 WORSE
I0907 19:50:06.329839 93825 finetune.py:49] layer 1_o @ epoch 1 new loss 7.363546501437668e-06 old loss 8.404012987739407e-06 BETTER
I0907 19:50:18.577032 93825 finetune.py:49] layer 1_o @ epoch 2 new loss 7.280871159309754e-06 old loss 7.363546501437668e-06 BETTER
I0907 19:50:30.721127 93825 finetune.py:49] layer 1_o @ epoch 3 new loss 6.725323146383744e-06 old loss 7.280871159309754e-06 BETTER
I0907 19:50:43.102936 93825 finetune.py:49] layer 1_o @ epoch 4 new loss 6.567635864485055e-06 old loss 6.725323146383744e-06 BETTER
I0907 19:50:45.281718 93825 quip.py:388] mean square of W: 1.0
I0907 19:50:45.282331 93825 quip.py:389] mean square of Wr: 1.0
I0907 19:50:45.282817 93825 quip.py:390] difference between Hr and Hr.T: 1.1920928955078125e-07
I0907 19:50:45.283100 93825 quip.py:391] max abs of Hr: 1.7630040645599365
I0907 19:50:45.283202 93825 quip.py:392] min diag of Lhr: 0.5121211409568787
I0907 19:51:02.438898 93825 misc.py:19] ckpt/2_7b_2bit/1_up.pt frob  error: 0.15545809268951416
I0907 19:51:02.439126 93825 misc.py:20] ckpt/2_7b_2bit/1_up.pt proxy error: 0.0199696384370327
I0907 19:51:06.810380 93825 finetune.py:25] layer 1_up initial loss 0.00014746123633813113
I0907 19:51:18.921255 93825 finetune.py:49] layer 1_up @ epoch 0 new loss 2.1097244825796224e-05 old loss 0.00014746123633813113 BETTER
I0907 19:51:30.564989 93825 finetune.py:49] layer 1_up @ epoch 1 new loss 1.5294186596293002e-05 old loss 2.1097244825796224e-05 BETTER
I0907 19:51:42.188252 93825 finetune.py:49] layer 1_up @ epoch 2 new loss 1.4248986190068536e-05 old loss 1.5294186596293002e-05 BETTER
I0907 19:51:53.706529 93825 finetune.py:56] layer 1_up @ epoch 3 new loss 1.4396976439456921e-05 old loss 1.4248986190068536e-05 WORSE
I0907 19:52:05.242095 93825 finetune.py:56] layer 1_up @ epoch 4 new loss 1.5101361896086019e-05 old loss 1.4248986190068536e-05 WORSE
I0907 19:52:09.605055 93825 quip.py:388] mean square of W: 1.0000001192092896
I0907 19:52:09.605508 93825 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 19:52:09.607579 93825 quip.py:390] difference between Hr and Hr.T: 1.9073486328125e-06
I0907 19:52:09.608185 93825 quip.py:391] max abs of Hr: 1.2457023859024048
I0907 19:52:09.608315 93825 quip.py:392] min diag of Lhr: 0.10343188047409058
I0907 19:52:38.221559 93825 misc.py:19] ckpt/2_7b_2bit/1_down.pt frob  error: 0.11344141513109207
I0907 19:52:38.221829 93825 misc.py:20] ckpt/2_7b_2bit/1_down.pt proxy error: 0.00041166215669363737
I0907 19:53:09.747431 93442 quantize_finetune_llama.py:211] computed original embedding for layer 2 in 28.60756516456604s, pre msv 0.5002250075340271, post msv 0.5026950836181641
I0907 19:53:09.854753 93442 quantize_finetune_llama.py:179] layer 3 gpu 0
W0907 19:53:11.156607 93934 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:53:11.180794 93934 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:53:11.291982 93934 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:53:11.292092 93934 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:53:11.461244 93934 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:53:11.707277 93934 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:53:31.319493 93934 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 19:53:32.856996 93934 quip.py:388] mean square of W: 0.9999998807907104
I0907 19:53:32.857484 93934 quip.py:389] mean square of Wr: 0.9999998807907104
I0907 19:53:32.866506 93934 quip.py:390] difference between Hr and Hr.T: 1.1920928955078125e-06
I0907 19:53:32.866918 93934 quip.py:391] max abs of Hr: 6.851747989654541
I0907 19:53:32.875897 93934 quip.py:392] min diag of Lhr: 0.3530021011829376
I0907 19:53:45.611375 93934 misc.py:19] ckpt/2_7b_2bit/2_qkv.pt frob  error: 0.14540326595306396
I0907 19:53:45.611618 93934 misc.py:20] ckpt/2_7b_2bit/2_qkv.pt proxy error: 0.002206796547397971
I0907 19:53:52.677858 93934 finetune.py:25] layer 2_qkv initial loss 0.00010450309491716325
W0907 19:53:52.678132 93934 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 19:53:52.808744 93934 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 19:54:05.152269 93934 finetune.py:49] layer 2_qkv @ epoch 0 new loss 1.4193579772836529e-05 old loss 0.00010450309491716325 BETTER
I0907 19:54:17.612576 93934 finetune.py:49] layer 2_qkv @ epoch 1 new loss 1.2858688933192752e-05 old loss 1.4193579772836529e-05 BETTER
I0907 19:54:29.949703 93934 finetune.py:49] layer 2_qkv @ epoch 2 new loss 1.2451827387849335e-05 old loss 1.2858688933192752e-05 BETTER
I0907 19:54:42.409352 93934 finetune.py:56] layer 2_qkv @ epoch 3 new loss 1.252125275641447e-05 old loss 1.2451827387849335e-05 WORSE
I0907 19:54:54.877262 93934 finetune.py:49] layer 2_qkv @ epoch 4 new loss 1.1923330021090806e-05 old loss 1.2451827387849335e-05 BETTER
I0907 19:54:56.415487 93934 quip.py:388] mean square of W: 1.0
I0907 19:54:56.415765 93934 quip.py:389] mean square of Wr: 1.0
I0907 19:54:56.416240 93934 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 19:54:56.416521 93934 quip.py:391] max abs of Hr: 1.3465815782546997
I0907 19:54:56.416615 93934 quip.py:392] min diag of Lhr: 0.32262203097343445
I0907 19:55:06.426663 93934 misc.py:19] ckpt/2_7b_2bit/2_o.pt frob  error: 0.20776093006134033
I0907 19:55:06.426826 93934 misc.py:20] ckpt/2_7b_2bit/2_o.pt proxy error: 0.026302075013518333
I0907 19:55:12.073446 93934 finetune.py:25] layer 2_o initial loss 4.115753836231306e-05
I0907 19:55:24.228828 93934 finetune.py:49] layer 2_o @ epoch 0 new loss 2.5991628717747517e-05 old loss 4.115753836231306e-05 BETTER
I0907 19:55:36.425222 93934 finetune.py:49] layer 2_o @ epoch 1 new loss 2.4674161977600306e-05 old loss 2.5991628717747517e-05 BETTER
I0907 19:55:48.578739 93934 finetune.py:49] layer 2_o @ epoch 2 new loss 2.4120645321090706e-05 old loss 2.4674161977600306e-05 BETTER
I0907 19:56:00.847383 93934 finetune.py:49] layer 2_o @ epoch 3 new loss 2.3737329684081487e-05 old loss 2.4120645321090706e-05 BETTER
I0907 19:56:13.054861 93934 finetune.py:49] layer 2_o @ epoch 4 new loss 2.344296626688447e-05 old loss 2.3737329684081487e-05 BETTER
I0907 19:56:15.018972 93934 quip.py:388] mean square of W: 1.0
I0907 19:56:15.019595 93934 quip.py:389] mean square of Wr: 1.0
I0907 19:56:15.020074 93934 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 19:56:15.020358 93934 quip.py:391] max abs of Hr: 1.3044102191925049
I0907 19:56:15.020452 93934 quip.py:392] min diag of Lhr: 0.6145798563957214
I0907 19:56:31.039547 93934 misc.py:19] ckpt/2_7b_2bit/2_up.pt frob  error: 0.1423865109682083
I0907 19:56:31.039783 93934 misc.py:20] ckpt/2_7b_2bit/2_up.pt proxy error: 0.033899709582328796
I0907 19:56:35.272028 93934 finetune.py:25] layer 2_up initial loss 5.716124360333197e-05
I0907 19:56:47.108603 93934 finetune.py:49] layer 2_up @ epoch 0 new loss 5.628893632092513e-05 old loss 5.716124360333197e-05 BETTER
I0907 19:56:59.263710 93934 finetune.py:49] layer 2_up @ epoch 1 new loss 5.582470475928858e-05 old loss 5.628893632092513e-05 BETTER
I0907 19:57:11.037544 93934 finetune.py:49] layer 2_up @ epoch 2 new loss 5.5408610933227465e-05 old loss 5.582470475928858e-05 BETTER
I0907 19:57:22.956327 93934 finetune.py:49] layer 2_up @ epoch 3 new loss 5.509310358320363e-05 old loss 5.5408610933227465e-05 BETTER
I0907 19:57:34.811968 93934 finetune.py:49] layer 2_up @ epoch 4 new loss 5.479116953210905e-05 old loss 5.509310358320363e-05 BETTER
I0907 19:57:38.824203 93934 quip.py:388] mean square of W: 0.9999998807907104
I0907 19:57:38.824669 93934 quip.py:389] mean square of Wr: 1.0
I0907 19:57:38.826709 93934 quip.py:390] difference between Hr and Hr.T: 1.6577541828155518e-07
I0907 19:57:38.827311 93934 quip.py:391] max abs of Hr: 1.1600978374481201
I0907 19:57:38.827445 93934 quip.py:392] min diag of Lhr: 0.7287545800209045
I0907 19:58:08.038902 93934 misc.py:19] ckpt/2_7b_2bit/2_down.pt frob  error: 0.12486232817173004
I0907 19:58:08.039081 93934 misc.py:20] ckpt/2_7b_2bit/2_down.pt proxy error: 0.0519791916012764
I0907 19:58:37.211980 93442 quantize_finetune_llama.py:211] computed original embedding for layer 3 in 26.602603673934937s, pre msv 0.5026950836181641, post msv 0.5036399960517883
I0907 19:58:37.309776 93442 quantize_finetune_llama.py:179] layer 4 gpu 0
W0907 19:58:38.575163 94250 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 19:58:38.598981 94250 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 19:58:38.706663 94250 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 19:58:38.706773 94250 utils.py:164] NumExpr defaulting to 16 threads.
I0907 19:58:38.861104 94250 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 19:58:39.093102 94250 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 19:58:58.470258 94250 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 19:58:59.974237 94250 quip.py:388] mean square of W: 1.0000001192092896
I0907 19:58:59.974694 94250 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 19:58:59.983696 94250 quip.py:390] difference between Hr and Hr.T: 4.76837158203125e-07
I0907 19:58:59.984090 94250 quip.py:391] max abs of Hr: 5.524274826049805
I0907 19:58:59.992797 94250 quip.py:392] min diag of Lhr: 0.5148609280586243
I0907 19:59:12.412481 94250 misc.py:19] ckpt/2_7b_2bit/3_qkv.pt frob  error: 0.13240142166614532
I0907 19:59:12.412722 94250 misc.py:20] ckpt/2_7b_2bit/3_qkv.pt proxy error: 0.006631483323872089
I0907 19:59:19.358762 94250 finetune.py:25] layer 3_qkv initial loss 0.00027379937819205225
W0907 19:59:19.359021 94250 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 19:59:19.489766 94250 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 19:59:32.062896 94250 finetune.py:49] layer 3_qkv @ epoch 0 new loss 3.596476744860411e-05 old loss 0.00027379937819205225 BETTER
I0907 19:59:44.733307 94250 finetune.py:49] layer 3_qkv @ epoch 1 new loss 3.2752679544501007e-05 old loss 3.596476744860411e-05 BETTER
I0907 19:59:57.597508 94250 finetune.py:56] layer 3_qkv @ epoch 2 new loss 3.351218401803635e-05 old loss 3.2752679544501007e-05 WORSE
I0907 20:00:10.212838 94250 finetune.py:49] layer 3_qkv @ epoch 3 new loss 3.230148649890907e-05 old loss 3.2752679544501007e-05 BETTER
I0907 20:00:22.860987 94250 finetune.py:49] layer 3_qkv @ epoch 4 new loss 3.134254438919015e-05 old loss 3.230148649890907e-05 BETTER
I0907 20:00:25.033347 94250 quip.py:388] mean square of W: 0.9999999403953552
I0907 20:00:25.033676 94250 quip.py:389] mean square of Wr: 1.0
I0907 20:00:25.034166 94250 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 20:00:25.034453 94250 quip.py:391] max abs of Hr: 1.792499303817749
I0907 20:00:25.034574 94250 quip.py:392] min diag of Lhr: 0.47276198863983154
I0907 20:00:36.213839 94250 misc.py:19] ckpt/2_7b_2bit/3_o.pt frob  error: 0.15964728593826294
I0907 20:00:36.214081 94250 misc.py:20] ckpt/2_7b_2bit/3_o.pt proxy error: 0.02441229484975338
I0907 20:00:42.017278 94250 finetune.py:25] layer 3_o initial loss 9.16908320505172e-05
I0907 20:00:54.483898 94250 finetune.py:49] layer 3_o @ epoch 0 new loss 5.7656015997054055e-05 old loss 9.16908320505172e-05 BETTER
I0907 20:01:06.810676 94250 finetune.py:49] layer 3_o @ epoch 1 new loss 5.514791700989008e-05 old loss 5.7656015997054055e-05 BETTER
I0907 20:01:19.361273 94250 finetune.py:49] layer 3_o @ epoch 2 new loss 5.400306326919235e-05 old loss 5.514791700989008e-05 BETTER
I0907 20:01:31.980763 94250 finetune.py:49] layer 3_o @ epoch 3 new loss 5.3255960665410385e-05 old loss 5.400306326919235e-05 BETTER
I0907 20:01:44.399526 94250 finetune.py:49] layer 3_o @ epoch 4 new loss 5.2653227612609044e-05 old loss 5.3255960665410385e-05 BETTER
I0907 20:01:46.915133 94250 quip.py:388] mean square of W: 1.0
I0907 20:01:46.915756 94250 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 20:01:46.916247 94250 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 20:01:46.916524 94250 quip.py:391] max abs of Hr: 1.406893253326416
I0907 20:01:46.916627 94250 quip.py:392] min diag of Lhr: 0.6960368752479553
I0907 20:02:03.918100 94250 misc.py:19] ckpt/2_7b_2bit/3_up.pt frob  error: 0.12968981266021729
I0907 20:02:03.918322 94250 misc.py:20] ckpt/2_7b_2bit/3_up.pt proxy error: 0.04232248291373253
I0907 20:02:08.345775 94250 finetune.py:25] layer 3_up initial loss 0.0001394895080011338
I0907 20:02:20.235058 94250 finetune.py:49] layer 3_up @ epoch 0 new loss 0.00013656144437845796 old loss 0.0001394895080011338 BETTER
I0907 20:02:31.849233 94250 finetune.py:49] layer 3_up @ epoch 1 new loss 0.0001351229875581339 old loss 0.00013656144437845796 BETTER
I0907 20:02:43.343784 94250 finetune.py:49] layer 3_up @ epoch 2 new loss 0.00013407974620349705 old loss 0.0001351229875581339 BETTER
I0907 20:02:54.830318 94250 finetune.py:49] layer 3_up @ epoch 3 new loss 0.00013323545863386244 old loss 0.00013407974620349705 BETTER
I0907 20:03:06.386832 94250 finetune.py:49] layer 3_up @ epoch 4 new loss 0.0001325575722148642 old loss 0.00013323545863386244 BETTER
I0907 20:03:10.953325 94250 quip.py:388] mean square of W: 1.0
I0907 20:03:10.953767 94250 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 20:03:10.955783 94250 quip.py:390] difference between Hr and Hr.T: 2.086162567138672e-07
I0907 20:03:10.956360 94250 quip.py:391] max abs of Hr: 1.1965497732162476
I0907 20:03:10.956485 94250 quip.py:392] min diag of Lhr: 0.7492537498474121
I0907 20:03:38.711889 94250 misc.py:19] ckpt/2_7b_2bit/3_down.pt frob  error: 0.12207946181297302
I0907 20:03:38.712053 94250 misc.py:20] ckpt/2_7b_2bit/3_down.pt proxy error: 0.054559849202632904
I0907 20:04:10.664519 93442 quantize_finetune_llama.py:211] computed original embedding for layer 4 in 29.41037130355835s, pre msv 0.5036399960517883, post msv 0.5094535946846008
I0907 20:04:10.756892 93442 quantize_finetune_llama.py:179] layer 5 gpu 0
W0907 20:04:12.076787 94349 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 20:04:12.101975 94349 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 20:04:12.214455 94349 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 20:04:12.214600 94349 utils.py:164] NumExpr defaulting to 16 threads.
I0907 20:04:12.373836 94349 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 20:04:12.610601 94349 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 20:04:31.890529 94349 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 20:04:33.446827 94349 quip.py:388] mean square of W: 1.0
I0907 20:04:33.447334 94349 quip.py:389] mean square of Wr: 0.9999999403953552
I0907 20:04:33.456470 94349 quip.py:390] difference between Hr and Hr.T: 4.76837158203125e-07
I0907 20:04:33.456924 94349 quip.py:391] max abs of Hr: 3.93264102935791
I0907 20:04:33.465798 94349 quip.py:392] min diag of Lhr: 0.5239145755767822
I0907 20:04:46.038835 94349 misc.py:19] ckpt/2_7b_2bit/4_qkv.pt frob  error: 0.12816350162029266
I0907 20:04:46.039047 94349 misc.py:20] ckpt/2_7b_2bit/4_qkv.pt proxy error: 0.006481054704636335
I0907 20:04:53.037704 94349 finetune.py:25] layer 4_qkv initial loss 0.0003738900413736701
W0907 20:04:53.037984 94349 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 20:04:53.172574 94349 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 20:05:05.800588 94349 finetune.py:49] layer 4_qkv @ epoch 0 new loss 6.176741590024903e-05 old loss 0.0003738900413736701 BETTER
I0907 20:05:18.054619 94349 finetune.py:49] layer 4_qkv @ epoch 1 new loss 5.6662189308553934e-05 old loss 6.176741590024903e-05 BETTER
I0907 20:05:30.310331 94349 finetune.py:49] layer 4_qkv @ epoch 2 new loss 5.51673692825716e-05 old loss 5.6662189308553934e-05 BETTER
I0907 20:05:42.662188 94349 finetune.py:56] layer 4_qkv @ epoch 3 new loss 5.53572281205561e-05 old loss 5.51673692825716e-05 WORSE
I0907 20:05:55.195807 94349 finetune.py:49] layer 4_qkv @ epoch 4 new loss 5.281233097775839e-05 old loss 5.51673692825716e-05 BETTER
I0907 20:05:56.986783 94349 quip.py:388] mean square of W: 0.9999998807907104
I0907 20:05:56.987084 94349 quip.py:389] mean square of Wr: 0.9999999403953552
I0907 20:05:56.987564 94349 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 20:05:56.987851 94349 quip.py:391] max abs of Hr: 1.3949906826019287
I0907 20:05:56.987959 94349 quip.py:392] min diag of Lhr: 0.5348736047744751
I0907 20:06:07.694066 94349 misc.py:19] ckpt/2_7b_2bit/4_o.pt frob  error: 0.1524098664522171
I0907 20:06:07.694256 94349 misc.py:20] ckpt/2_7b_2bit/4_o.pt proxy error: 0.03171883895993233
I0907 20:06:13.380002 94349 finetune.py:25] layer 4_o initial loss 0.00014310864207800478
I0907 20:06:25.903738 94349 finetune.py:49] layer 4_o @ epoch 0 new loss 9.998738823924214e-05 old loss 0.00014310864207800478 BETTER
I0907 20:06:38.061665 94349 finetune.py:49] layer 4_o @ epoch 1 new loss 9.659616625867784e-05 old loss 9.998738823924214e-05 BETTER
I0907 20:06:50.204943 94349 finetune.py:49] layer 4_o @ epoch 2 new loss 9.490988304605708e-05 old loss 9.659616625867784e-05 BETTER
I0907 20:07:02.627390 94349 finetune.py:49] layer 4_o @ epoch 3 new loss 9.378181130159646e-05 old loss 9.490988304605708e-05 BETTER
I0907 20:07:14.718906 94349 finetune.py:49] layer 4_o @ epoch 4 new loss 9.260834485758096e-05 old loss 9.378181130159646e-05 BETTER
I0907 20:07:16.629280 94349 quip.py:388] mean square of W: 1.0000001192092896
I0907 20:07:16.629911 94349 quip.py:389] mean square of Wr: 1.0
I0907 20:07:16.630392 94349 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 20:07:16.630687 94349 quip.py:391] max abs of Hr: 1.5127931833267212
I0907 20:07:16.630791 94349 quip.py:392] min diag of Lhr: 0.7121403217315674
I0907 20:07:32.305017 94349 misc.py:19] ckpt/2_7b_2bit/4_up.pt frob  error: 0.12544642388820648
I0907 20:07:32.305195 94349 misc.py:20] ckpt/2_7b_2bit/4_up.pt proxy error: 0.03763134405016899
I0907 20:07:36.696199 94349 finetune.py:25] layer 4_up initial loss 0.000250927172601223
I0907 20:07:48.570241 94349 finetune.py:49] layer 4_up @ epoch 0 new loss 0.00024405785370618105 old loss 0.000250927172601223 BETTER
I0907 20:08:00.633870 94349 finetune.py:49] layer 4_up @ epoch 1 new loss 0.00024101932649500668 old loss 0.00024405785370618105 BETTER
I0907 20:08:12.394542 94349 finetune.py:49] layer 4_up @ epoch 2 new loss 0.0002388949360465631 old loss 0.00024101932649500668 BETTER
I0907 20:08:24.060278 94349 finetune.py:49] layer 4_up @ epoch 3 new loss 0.00023745627549942583 old loss 0.0002388949360465631 BETTER
I0907 20:08:35.749425 94349 finetune.py:49] layer 4_up @ epoch 4 new loss 0.0002361445949645713 old loss 0.00023745627549942583 BETTER
I0907 20:08:40.054667 94349 quip.py:388] mean square of W: 1.0000001192092896
I0907 20:08:40.055095 94349 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 20:08:40.057124 94349 quip.py:390] difference between Hr and Hr.T: 3.427267074584961e-07
I0907 20:08:40.057717 94349 quip.py:391] max abs of Hr: 1.3379390239715576
I0907 20:08:40.057849 94349 quip.py:392] min diag of Lhr: 0.7446703910827637
I0907 20:09:09.426135 94349 misc.py:19] ckpt/2_7b_2bit/4_down.pt frob  error: 0.12161984294652939
I0907 20:09:09.426317 94349 misc.py:20] ckpt/2_7b_2bit/4_down.pt proxy error: 0.0521753653883934
I0907 20:09:41.293607 93442 quantize_finetune_llama.py:211] computed original embedding for layer 5 in 29.093037843704224s, pre msv 0.5094535946846008, post msv 0.5105752944946289
I0907 20:09:41.410410 93442 quantize_finetune_llama.py:179] layer 6 gpu 0
W0907 20:09:42.698876 94455 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 20:09:42.722359 94455 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 20:09:42.831509 94455 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 20:09:42.831595 94455 utils.py:164] NumExpr defaulting to 16 threads.
I0907 20:09:42.979662 94455 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 20:09:43.237918 94455 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 20:10:02.515007 94455 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 20:10:03.928573 94455 quip.py:388] mean square of W: 1.0
I0907 20:10:03.929010 94455 quip.py:389] mean square of Wr: 0.9999999403953552
I0907 20:10:03.937308 94455 quip.py:390] difference between Hr and Hr.T: 4.76837158203125e-07
I0907 20:10:03.937701 94455 quip.py:391] max abs of Hr: 4.410794258117676
I0907 20:10:03.945794 94455 quip.py:392] min diag of Lhr: 0.5531601905822754
I0907 20:10:16.705542 94455 misc.py:19] ckpt/2_7b_2bit/5_qkv.pt frob  error: 0.1261979341506958
I0907 20:10:16.705782 94455 misc.py:20] ckpt/2_7b_2bit/5_qkv.pt proxy error: 0.0075864712707698345
I0907 20:10:23.870654 94455 finetune.py:25] layer 5_qkv initial loss 0.00045910297194495797
W0907 20:10:23.870930 94455 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 20:10:24.000517 94455 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 20:10:36.286689 94455 finetune.py:49] layer 5_qkv @ epoch 0 new loss 0.00010523336823098361 old loss 0.00045910297194495797 BETTER
I0907 20:10:48.634343 94455 finetune.py:49] layer 5_qkv @ epoch 1 new loss 9.758364467415959e-05 old loss 0.00010523336823098361 BETTER
I0907 20:11:01.124469 94455 finetune.py:56] layer 5_qkv @ epoch 2 new loss 9.93925568764098e-05 old loss 9.758364467415959e-05 WORSE
I0907 20:11:13.709333 94455 finetune.py:56] layer 5_qkv @ epoch 3 new loss 9.858576959231868e-05 old loss 9.758364467415959e-05 WORSE
I0907 20:11:25.982384 94455 finetune.py:49] layer 5_qkv @ epoch 4 new loss 9.397223038831726e-05 old loss 9.758364467415959e-05 BETTER
I0907 20:11:27.463384 94455 quip.py:388] mean square of W: 1.0
I0907 20:11:27.463672 94455 quip.py:389] mean square of Wr: 1.0
I0907 20:11:27.464162 94455 quip.py:390] difference between Hr and Hr.T: 1.6391277313232422e-07
I0907 20:11:27.464444 94455 quip.py:391] max abs of Hr: 1.5526294708251953
I0907 20:11:27.464548 94455 quip.py:392] min diag of Lhr: 0.42415520548820496
I0907 20:11:38.109771 94455 misc.py:19] ckpt/2_7b_2bit/5_o.pt frob  error: 0.15739034116268158
I0907 20:11:38.110001 94455 misc.py:20] ckpt/2_7b_2bit/5_o.pt proxy error: 0.034093618392944336
I0907 20:11:43.803692 94455 finetune.py:25] layer 5_o initial loss 0.00022492374409921467
I0907 20:11:56.086443 94455 finetune.py:49] layer 5_o @ epoch 0 new loss 0.0001633839710848406 old loss 0.00022492374409921467 BETTER
I0907 20:12:08.281914 94455 finetune.py:49] layer 5_o @ epoch 1 new loss 0.000157410409883596 old loss 0.0001633839710848406 BETTER
I0907 20:12:20.502042 94455 finetune.py:49] layer 5_o @ epoch 2 new loss 0.00015417835675179958 old loss 0.000157410409883596 BETTER
I0907 20:12:32.569928 94455 finetune.py:49] layer 5_o @ epoch 3 new loss 0.00015200945199467242 old loss 0.00015417835675179958 BETTER
I0907 20:12:44.614135 94455 finetune.py:49] layer 5_o @ epoch 4 new loss 0.0001504307147115469 old loss 0.00015200945199467242 BETTER
I0907 20:12:46.791414 94455 quip.py:388] mean square of W: 1.0
I0907 20:12:46.792064 94455 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 20:12:46.792566 94455 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 20:12:46.792848 94455 quip.py:391] max abs of Hr: 1.4282829761505127
I0907 20:12:46.792954 94455 quip.py:392] min diag of Lhr: 0.7206409573554993
I0907 20:13:04.105184 94455 misc.py:19] ckpt/2_7b_2bit/5_up.pt frob  error: 0.12479232996702194
I0907 20:13:04.105362 94455 misc.py:20] ckpt/2_7b_2bit/5_up.pt proxy error: 0.03608257323503494
I0907 20:13:08.420901 94455 finetune.py:25] layer 5_up initial loss 0.0003978548920713365
I0907 20:13:20.259905 94455 finetune.py:49] layer 5_up @ epoch 0 new loss 0.00038519769441336393 old loss 0.0003978548920713365 BETTER
I0907 20:13:31.907965 94455 finetune.py:49] layer 5_up @ epoch 1 new loss 0.00037994046579115093 old loss 0.00038519769441336393 BETTER
I0907 20:13:43.392893 94455 finetune.py:49] layer 5_up @ epoch 2 new loss 0.0003764074936043471 old loss 0.00037994046579115093 BETTER
I0907 20:13:55.113848 94455 finetune.py:49] layer 5_up @ epoch 3 new loss 0.00037383404560387135 old loss 0.0003764074936043471 BETTER
I0907 20:14:07.267374 94455 finetune.py:49] layer 5_up @ epoch 4 new loss 0.0003717915096785873 old loss 0.00037383404560387135 BETTER
I0907 20:14:11.725735 94455 quip.py:388] mean square of W: 1.0
I0907 20:14:11.726168 94455 quip.py:389] mean square of Wr: 1.0
I0907 20:14:11.728171 94455 quip.py:390] difference between Hr and Hr.T: 2.2351741790771484e-07
I0907 20:14:11.728770 94455 quip.py:391] max abs of Hr: 1.2673648595809937
I0907 20:14:11.728899 94455 quip.py:392] min diag of Lhr: 0.7697224617004395
I0907 20:14:40.782122 94455 misc.py:19] ckpt/2_7b_2bit/5_down.pt frob  error: 0.11959809809923172
I0907 20:14:40.782347 94455 misc.py:20] ckpt/2_7b_2bit/5_down.pt proxy error: 0.058255165815353394
I0907 20:15:11.848539 93442 quantize_finetune_llama.py:211] computed original embedding for layer 6 in 28.071030855178833s, pre msv 0.5105752944946289, post msv 0.5122508406639099
I0907 20:15:11.950686 93442 quantize_finetune_llama.py:179] layer 7 gpu 0
W0907 20:15:13.277716 94556 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 20:15:13.302795 94556 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 20:15:13.416604 94556 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 20:15:13.416759 94556 utils.py:164] NumExpr defaulting to 16 threads.
I0907 20:15:13.580332 94556 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 20:15:13.825681 94556 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 20:15:33.677736 94556 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 20:15:35.088842 94556 quip.py:388] mean square of W: 1.000000238418579
I0907 20:15:35.089289 94556 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 20:15:35.098187 94556 quip.py:390] difference between Hr and Hr.T: 4.76837158203125e-07
I0907 20:15:35.098577 94556 quip.py:391] max abs of Hr: 3.0379676818847656
I0907 20:15:35.107285 94556 quip.py:392] min diag of Lhr: 0.5855914950370789
I0907 20:15:47.814579 94556 misc.py:19] ckpt/2_7b_2bit/6_qkv.pt frob  error: 0.12458858639001846
I0907 20:15:47.814821 94556 misc.py:20] ckpt/2_7b_2bit/6_qkv.pt proxy error: 0.01031319797039032
I0907 20:15:54.957431 94556 finetune.py:25] layer 6_qkv initial loss 0.000522969348821789
W0907 20:15:54.957732 94556 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 20:15:55.094237 94556 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 20:16:07.903921 94556 finetune.py:49] layer 6_qkv @ epoch 0 new loss 0.00014977781393099576 old loss 0.000522969348821789 BETTER
I0907 20:16:20.504744 94556 finetune.py:49] layer 6_qkv @ epoch 1 new loss 0.0001414094731444493 old loss 0.00014977781393099576 BETTER
I0907 20:16:33.354117 94556 finetune.py:56] layer 6_qkv @ epoch 2 new loss 0.00015484337927773595 old loss 0.0001414094731444493 WORSE
I0907 20:16:46.025870 94556 finetune.py:56] layer 6_qkv @ epoch 3 new loss 0.0001430090342182666 old loss 0.0001414094731444493 WORSE
I0907 20:16:58.882888 94556 finetune.py:49] layer 6_qkv @ epoch 4 new loss 0.00013565662084147334 old loss 0.0001414094731444493 BETTER
I0907 20:17:00.972699 94556 quip.py:388] mean square of W: 1.0
I0907 20:17:00.973026 94556 quip.py:389] mean square of Wr: 1.0
I0907 20:17:00.973520 94556 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 20:17:00.973806 94556 quip.py:391] max abs of Hr: 1.4954500198364258
I0907 20:17:00.973914 94556 quip.py:392] min diag of Lhr: 0.5299257636070251
I0907 20:17:12.318324 94556 misc.py:19] ckpt/2_7b_2bit/6_o.pt frob  error: 0.14724406599998474
I0907 20:17:12.318552 94556 misc.py:20] ckpt/2_7b_2bit/6_o.pt proxy error: 0.035780251026153564
I0907 20:17:18.164388 94556 finetune.py:25] layer 6_o initial loss 0.0003039173607248813
I0907 20:17:30.559196 94556 finetune.py:49] layer 6_o @ epoch 0 new loss 0.00023662293097004294 old loss 0.0003039173607248813 BETTER
I0907 20:17:42.919785 94556 finetune.py:49] layer 6_o @ epoch 1 new loss 0.0002292534918524325 old loss 0.00023662293097004294 BETTER
I0907 20:17:55.305322 94556 finetune.py:49] layer 6_o @ epoch 2 new loss 0.00022601056843996048 old loss 0.0002292534918524325 BETTER
I0907 20:18:07.749820 94556 finetune.py:49] layer 6_o @ epoch 3 new loss 0.0002238978777313605 old loss 0.00022601056843996048 BETTER
I0907 20:18:20.164254 94556 finetune.py:49] layer 6_o @ epoch 4 new loss 0.0002216175926150754 old loss 0.0002238978777313605 BETTER
I0907 20:18:22.756440 94556 quip.py:388] mean square of W: 1.0
I0907 20:18:22.757117 94556 quip.py:389] mean square of Wr: 1.0
I0907 20:18:22.757605 94556 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 20:18:22.757898 94556 quip.py:391] max abs of Hr: 1.657125473022461
I0907 20:18:22.758015 94556 quip.py:392] min diag of Lhr: 0.7314727306365967
I0907 20:18:40.154518 94556 misc.py:19] ckpt/2_7b_2bit/6_up.pt frob  error: 0.1226009875535965
I0907 20:18:40.154755 94556 misc.py:20] ckpt/2_7b_2bit/6_up.pt proxy error: 0.034034691751003265
I0907 20:18:44.476832 94556 finetune.py:25] layer 6_up initial loss 0.00059780158335343
I0907 20:18:56.360625 94556 finetune.py:49] layer 6_up @ epoch 0 new loss 0.000577016151510179 old loss 0.00059780158335343 BETTER
I0907 20:19:08.010764 94556 finetune.py:49] layer 6_up @ epoch 1 new loss 0.0005680948961526155 old loss 0.000577016151510179 BETTER
I0907 20:19:19.576380 94556 finetune.py:49] layer 6_up @ epoch 2 new loss 0.000561990134883672 old loss 0.0005680948961526155 BETTER
I0907 20:19:31.493713 94556 finetune.py:49] layer 6_up @ epoch 3 new loss 0.0005577908013947308 old loss 0.000561990134883672 BETTER
I0907 20:19:43.515456 94556 finetune.py:49] layer 6_up @ epoch 4 new loss 0.0005545920575968921 old loss 0.0005577908013947308 BETTER
I0907 20:19:47.846266 94556 quip.py:388] mean square of W: 0.9999998211860657
I0907 20:19:47.846694 94556 quip.py:389] mean square of Wr: 1.0
I0907 20:19:47.848708 94556 quip.py:390] difference between Hr and Hr.T: 2.2351741790771484e-07
I0907 20:19:47.849285 94556 quip.py:391] max abs of Hr: 1.3263367414474487
I0907 20:19:47.849411 94556 quip.py:392] min diag of Lhr: 0.755719780921936
I0907 20:20:16.493711 94556 misc.py:19] ckpt/2_7b_2bit/6_down.pt frob  error: 0.12051890790462494
I0907 20:20:16.493939 94556 misc.py:20] ckpt/2_7b_2bit/6_down.pt proxy error: 0.05802950635552406
I0907 20:20:47.638690 93442 quantize_finetune_llama.py:211] computed original embedding for layer 7 in 28.544567108154297s, pre msv 0.5122508406639099, post msv 0.5144863128662109
I0907 20:20:47.748113 93442 quantize_finetune_llama.py:179] layer 8 gpu 0
W0907 20:20:49.104956 94658 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 20:20:49.130862 94658 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 20:20:49.251729 94658 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 20:20:49.251896 94658 utils.py:164] NumExpr defaulting to 16 threads.
I0907 20:20:49.447862 94658 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 20:20:49.705017 94658 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 20:21:09.750377 94658 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 20:21:11.432430 94658 quip.py:388] mean square of W: 1.0000001192092896
I0907 20:21:11.432965 94658 quip.py:389] mean square of Wr: 1.0
I0907 20:21:11.442287 94658 quip.py:390] difference between Hr and Hr.T: 4.76837158203125e-07
I0907 20:21:11.442742 94658 quip.py:391] max abs of Hr: 3.054899215698242
I0907 20:21:11.451802 94658 quip.py:392] min diag of Lhr: 0.5928442478179932
I0907 20:21:24.240329 94658 misc.py:19] ckpt/2_7b_2bit/7_qkv.pt frob  error: 0.12414774298667908
I0907 20:21:24.240553 94658 misc.py:20] ckpt/2_7b_2bit/7_qkv.pt proxy error: 0.011098156683146954
I0907 20:21:31.250220 94658 finetune.py:25] layer 7_qkv initial loss 0.0005991272628307343
W0907 20:21:31.250513 94658 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 20:21:31.390343 94658 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 20:21:44.441313 94658 finetune.py:49] layer 7_qkv @ epoch 0 new loss 0.00021640292834490538 old loss 0.0005991272628307343 BETTER
I0907 20:21:57.155902 94658 finetune.py:49] layer 7_qkv @ epoch 1 new loss 0.0002065529697574675 old loss 0.00021640292834490538 BETTER
I0907 20:22:09.907723 94658 finetune.py:56] layer 7_qkv @ epoch 2 new loss 0.000213514402275905 old loss 0.0002065529697574675 WORSE
I0907 20:22:22.543726 94658 finetune.py:49] layer 7_qkv @ epoch 3 new loss 0.00020438572391867638 old loss 0.0002065529697574675 BETTER
I0907 20:22:35.287189 94658 finetune.py:49] layer 7_qkv @ epoch 4 new loss 0.00020085074356757104 old loss 0.00020438572391867638 BETTER
I0907 20:22:36.806774 94658 quip.py:388] mean square of W: 0.9999999403953552
I0907 20:22:36.807038 94658 quip.py:389] mean square of Wr: 1.0
I0907 20:22:36.807513 94658 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 20:22:36.807798 94658 quip.py:391] max abs of Hr: 1.6483080387115479
I0907 20:22:36.807903 94658 quip.py:392] min diag of Lhr: 0.5250495076179504
I0907 20:22:47.007443 94658 misc.py:19] ckpt/2_7b_2bit/7_o.pt frob  error: 0.14507058262825012
I0907 20:22:47.007608 94658 misc.py:20] ckpt/2_7b_2bit/7_o.pt proxy error: 0.039352234452962875
I0907 20:22:52.804128 94658 finetune.py:25] layer 7_o initial loss 0.00041170718031935394
I0907 20:23:05.425529 94658 finetune.py:49] layer 7_o @ epoch 0 new loss 0.00033038645051419735 old loss 0.00041170718031935394 BETTER
I0907 20:23:17.992963 94658 finetune.py:49] layer 7_o @ epoch 1 new loss 0.0003191933792550117 old loss 0.00033038645051419735 BETTER
I0907 20:23:30.782061 94658 finetune.py:49] layer 7_o @ epoch 2 new loss 0.0003137658932246268 old loss 0.0003191933792550117 BETTER
I0907 20:23:43.213635 94658 finetune.py:49] layer 7_o @ epoch 3 new loss 0.00030896274256519973 old loss 0.0003137658932246268 BETTER
I0907 20:23:55.659617 94658 finetune.py:49] layer 7_o @ epoch 4 new loss 0.0003070310049224645 old loss 0.00030896274256519973 BETTER
I0907 20:23:57.931340 94658 quip.py:388] mean square of W: 1.0
I0907 20:23:57.931957 94658 quip.py:389] mean square of Wr: 1.0
I0907 20:23:57.932444 94658 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 20:23:57.932729 94658 quip.py:391] max abs of Hr: 1.827979326248169
I0907 20:23:57.932835 94658 quip.py:392] min diag of Lhr: 0.7295380234718323
I0907 20:24:15.088630 94658 misc.py:19] ckpt/2_7b_2bit/7_up.pt frob  error: 0.12151895463466644
I0907 20:24:15.088874 94658 misc.py:20] ckpt/2_7b_2bit/7_up.pt proxy error: 0.03303093835711479
I0907 20:24:19.406230 94658 finetune.py:25] layer 7_up initial loss 0.0008249470847658813
I0907 20:24:30.983871 94658 finetune.py:49] layer 7_up @ epoch 0 new loss 0.0007920077187009156 old loss 0.0008249470847658813 BETTER
I0907 20:24:42.489884 94658 finetune.py:49] layer 7_up @ epoch 1 new loss 0.0007781006279401481 old loss 0.0007920077187009156 BETTER
I0907 20:24:54.129593 94658 finetune.py:49] layer 7_up @ epoch 2 new loss 0.0007693029474467039 old loss 0.0007781006279401481 BETTER
I0907 20:25:06.002293 94658 finetune.py:49] layer 7_up @ epoch 3 new loss 0.0007629765314050019 old loss 0.0007693029474467039 BETTER
I0907 20:25:17.552636 94658 finetune.py:49] layer 7_up @ epoch 4 new loss 0.0007585091516375542 old loss 0.0007629765314050019 BETTER
I0907 20:25:21.708685 94658 quip.py:388] mean square of W: 0.9999998807907104
I0907 20:25:21.709114 94658 quip.py:389] mean square of Wr: 0.9999998807907104
I0907 20:25:21.711066 94658 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 20:25:21.711664 94658 quip.py:391] max abs of Hr: 1.304314136505127
I0907 20:25:21.711791 94658 quip.py:392] min diag of Lhr: 0.7499002814292908
I0907 20:25:50.661630 94658 misc.py:19] ckpt/2_7b_2bit/7_down.pt frob  error: 0.12078889459371567
I0907 20:25:50.661866 94658 misc.py:20] ckpt/2_7b_2bit/7_down.pt proxy error: 0.05830420181155205
I0907 20:26:22.597939 93442 quantize_finetune_llama.py:211] computed original embedding for layer 8 in 29.47510004043579s, pre msv 0.5144863128662109, post msv 0.5168983340263367
I0907 20:26:22.705254 93442 quantize_finetune_llama.py:179] layer 9 gpu 0
W0907 20:26:23.982971 94828 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 20:26:24.007024 94828 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 20:26:24.113696 94828 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 20:26:24.113799 94828 utils.py:164] NumExpr defaulting to 16 threads.
I0907 20:26:24.269172 94828 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 20:26:24.505780 94828 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 20:26:43.962596 94828 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 20:26:45.372536 94828 quip.py:388] mean square of W: 1.0
I0907 20:26:45.372984 94828 quip.py:389] mean square of Wr: 0.9999999403953552
I0907 20:26:45.381471 94828 quip.py:390] difference between Hr and Hr.T: 4.76837158203125e-07
I0907 20:26:45.381856 94828 quip.py:391] max abs of Hr: 3.5144081115722656
I0907 20:26:45.390049 94828 quip.py:392] min diag of Lhr: 0.5780870318412781
I0907 20:26:57.702163 94828 misc.py:19] ckpt/2_7b_2bit/8_qkv.pt frob  error: 0.12374932318925858
I0907 20:26:57.702392 94828 misc.py:20] ckpt/2_7b_2bit/8_qkv.pt proxy error: 0.010954038240015507
I0907 20:27:04.769117 94828 finetune.py:25] layer 8_qkv initial loss 0.000702309946063906
W0907 20:27:04.769389 94828 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 20:27:04.899209 94828 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 20:27:17.600723 94828 finetune.py:49] layer 8_qkv @ epoch 0 new loss 0.00030461352434940636 old loss 0.000702309946063906 BETTER
I0907 20:27:30.367243 94828 finetune.py:49] layer 8_qkv @ epoch 1 new loss 0.00029617667314596474 old loss 0.00030461352434940636 BETTER
I0907 20:27:42.906281 94828 finetune.py:49] layer 8_qkv @ epoch 2 new loss 0.00028553689480759203 old loss 0.00029617667314596474 BETTER
I0907 20:27:55.373641 94828 finetune.py:56] layer 8_qkv @ epoch 3 new loss 0.0002987940679304302 old loss 0.00028553689480759203 WORSE
I0907 20:28:08.182978 94828 finetune.py:49] layer 8_qkv @ epoch 4 new loss 0.0002845863054972142 old loss 0.00028553689480759203 BETTER
I0907 20:28:10.405662 94828 quip.py:388] mean square of W: 1.0
I0907 20:28:10.405947 94828 quip.py:389] mean square of Wr: 1.0
I0907 20:28:10.406450 94828 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 20:28:10.406755 94828 quip.py:391] max abs of Hr: 1.5076606273651123
I0907 20:28:10.406870 94828 quip.py:392] min diag of Lhr: 0.5139867663383484
I0907 20:28:21.696831 94828 misc.py:19] ckpt/2_7b_2bit/8_o.pt frob  error: 0.14680986106395721
I0907 20:28:21.697056 94828 misc.py:20] ckpt/2_7b_2bit/8_o.pt proxy error: 0.04475950449705124
I0907 20:28:27.554296 94828 finetune.py:25] layer 8_o initial loss 0.0005523203290067613
I0907 20:28:39.910909 94828 finetune.py:49] layer 8_o @ epoch 0 new loss 0.00046360527630895376 old loss 0.0005523203290067613 BETTER
I0907 20:28:52.309618 94828 finetune.py:49] layer 8_o @ epoch 1 new loss 0.00045033436617814004 old loss 0.00046360527630895376 BETTER
I0907 20:29:05.592442 94828 finetune.py:49] layer 8_o @ epoch 2 new loss 0.00044061048538424075 old loss 0.00045033436617814004 BETTER
I0907 20:29:18.296583 94828 finetune.py:49] layer 8_o @ epoch 3 new loss 0.0004365172062534839 old loss 0.00044061048538424075 BETTER
I0907 20:29:30.646342 94828 finetune.py:49] layer 8_o @ epoch 4 new loss 0.0004342109605204314 old loss 0.0004365172062534839 BETTER
I0907 20:29:32.949516 94828 quip.py:388] mean square of W: 1.0000001192092896
I0907 20:29:32.950173 94828 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 20:29:32.950674 94828 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 20:29:32.950956 94828 quip.py:391] max abs of Hr: 1.6322396993637085
I0907 20:29:32.951059 94828 quip.py:392] min diag of Lhr: 0.7313141226768494
I0907 20:29:49.860565 94828 misc.py:19] ckpt/2_7b_2bit/8_up.pt frob  error: 0.12124890089035034
I0907 20:29:49.860806 94828 misc.py:20] ckpt/2_7b_2bit/8_up.pt proxy error: 0.03208756446838379
I0907 20:29:54.247437 94828 finetune.py:25] layer 8_up initial loss 0.0010746086481958628
I0907 20:30:06.518751 94828 finetune.py:49] layer 8_up @ epoch 0 new loss 0.0010334684047847986 old loss 0.0010746086481958628 BETTER
I0907 20:30:18.860968 94828 finetune.py:49] layer 8_up @ epoch 1 new loss 0.001016906462609768 old loss 0.0010334684047847986 BETTER
I0907 20:30:31.270031 94828 finetune.py:49] layer 8_up @ epoch 2 new loss 0.001006747712381184 old loss 0.001016906462609768 BETTER
I0907 20:30:43.467467 94828 finetune.py:49] layer 8_up @ epoch 3 new loss 0.000999253592453897 old loss 0.001006747712381184 BETTER
I0907 20:30:55.728259 94828 finetune.py:49] layer 8_up @ epoch 4 new loss 0.0009935072157531977 old loss 0.000999253592453897 BETTER
I0907 20:31:00.092133 94828 quip.py:388] mean square of W: 1.0
I0907 20:31:00.092559 94828 quip.py:389] mean square of Wr: 1.0
I0907 20:31:00.094571 94828 quip.py:390] difference between Hr and Hr.T: 2.086162567138672e-07
I0907 20:31:00.095163 94828 quip.py:391] max abs of Hr: 1.3262529373168945
I0907 20:31:00.095295 94828 quip.py:392] min diag of Lhr: 0.7542017698287964
I0907 20:31:29.170157 94828 misc.py:19] ckpt/2_7b_2bit/8_down.pt frob  error: 0.12029484659433365
I0907 20:31:29.170392 94828 misc.py:20] ckpt/2_7b_2bit/8_down.pt proxy error: 0.0591655969619751
I0907 20:32:00.375734 93442 quantize_finetune_llama.py:211] computed original embedding for layer 9 in 28.787809133529663s, pre msv 0.5168983340263367, post msv 0.5194587111473083
I0907 20:32:00.476241 93442 quantize_finetune_llama.py:179] layer 10 gpu 0
W0907 20:32:01.814332 95203 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 20:32:01.839898 95203 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 20:32:01.957378 95203 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 20:32:01.957527 95203 utils.py:164] NumExpr defaulting to 16 threads.
I0907 20:32:02.119215 95203 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 20:32:02.367270 95203 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 20:32:22.035120 95203 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 20:32:23.369492 95203 quip.py:388] mean square of W: 1.0
I0907 20:32:23.369927 95203 quip.py:389] mean square of Wr: 1.0
I0907 20:32:23.378468 95203 quip.py:390] difference between Hr and Hr.T: 4.76837158203125e-07
I0907 20:32:23.378859 95203 quip.py:391] max abs of Hr: 3.044893264770508
I0907 20:32:23.387163 95203 quip.py:392] min diag of Lhr: 0.5997090935707092
I0907 20:32:35.357222 95203 misc.py:19] ckpt/2_7b_2bit/9_qkv.pt frob  error: 0.12297756224870682
I0907 20:32:35.357450 95203 misc.py:20] ckpt/2_7b_2bit/9_qkv.pt proxy error: 0.011826224625110626
I0907 20:32:42.418518 95203 finetune.py:25] layer 9_qkv initial loss 0.0008185378974303603
W0907 20:32:42.418802 95203 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 20:32:42.554002 95203 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 20:32:55.287319 95203 finetune.py:49] layer 9_qkv @ epoch 0 new loss 0.00040027705836109817 old loss 0.0008185378974303603 BETTER
I0907 20:33:08.083698 95203 finetune.py:56] layer 9_qkv @ epoch 1 new loss 0.0004212655476294458 old loss 0.00040027705836109817 WORSE
I0907 20:33:20.911512 95203 finetune.py:56] layer 9_qkv @ epoch 2 new loss 0.00040275155333802104 old loss 0.00040027705836109817 WORSE
I0907 20:33:34.017731 95203 finetune.py:49] layer 9_qkv @ epoch 3 new loss 0.0003899704315699637 old loss 0.00040027705836109817 BETTER
I0907 20:33:46.734029 95203 finetune.py:49] layer 9_qkv @ epoch 4 new loss 0.00038880761712789536 old loss 0.0003899704315699637 BETTER
I0907 20:33:48.338412 95203 quip.py:388] mean square of W: 1.0
I0907 20:33:48.338708 95203 quip.py:389] mean square of Wr: 1.0
I0907 20:33:48.339206 95203 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 20:33:48.339497 95203 quip.py:391] max abs of Hr: 1.4843412637710571
I0907 20:33:48.339605 95203 quip.py:392] min diag of Lhr: 0.4612729549407959
I0907 20:33:58.958393 95203 misc.py:19] ckpt/2_7b_2bit/9_o.pt frob  error: 0.16117753088474274
I0907 20:33:58.958594 95203 misc.py:20] ckpt/2_7b_2bit/9_o.pt proxy error: 0.04826077073812485
I0907 20:34:04.671602 95203 finetune.py:25] layer 9_o initial loss 0.0007602599798701704
I0907 20:34:17.120794 95203 finetune.py:49] layer 9_o @ epoch 0 new loss 0.0006298973457887769 old loss 0.0007602599798701704 BETTER
I0907 20:34:29.804091 95203 finetune.py:49] layer 9_o @ epoch 1 new loss 0.0006136828451417387 old loss 0.0006298973457887769 BETTER
I0907 20:34:42.796869 95203 finetune.py:49] layer 9_o @ epoch 2 new loss 0.0006012875819578767 old loss 0.0006136828451417387 BETTER
I0907 20:34:55.409624 95203 finetune.py:49] layer 9_o @ epoch 3 new loss 0.0005936754168942571 old loss 0.0006012875819578767 BETTER
I0907 20:35:08.090253 95203 finetune.py:49] layer 9_o @ epoch 4 new loss 0.0005891096079722047 old loss 0.0005936754168942571 BETTER
I0907 20:35:10.404959 95203 quip.py:388] mean square of W: 1.0000001192092896
I0907 20:35:10.405608 95203 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 20:35:10.406106 95203 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 20:35:10.406400 95203 quip.py:391] max abs of Hr: 1.7795839309692383
I0907 20:35:10.406510 95203 quip.py:392] min diag of Lhr: 0.7320356369018555
I0907 20:35:26.209204 95203 misc.py:19] ckpt/2_7b_2bit/9_up.pt frob  error: 0.12077388167381287
I0907 20:35:26.209437 95203 misc.py:20] ckpt/2_7b_2bit/9_up.pt proxy error: 0.03151480853557587
I0907 20:35:30.623259 95203 finetune.py:25] layer 9_up initial loss 0.001348154037259519
I0907 20:35:42.925947 95203 finetune.py:49] layer 9_up @ epoch 0 new loss 0.0012995487777516246 old loss 0.001348154037259519 BETTER
I0907 20:35:55.223073 95203 finetune.py:49] layer 9_up @ epoch 1 new loss 0.0012785486178472638 old loss 0.0012995487777516246 BETTER
I0907 20:36:07.772122 95203 finetune.py:49] layer 9_up @ epoch 2 new loss 0.0012653481680899858 old loss 0.0012785486178472638 BETTER
I0907 20:36:20.468955 95203 finetune.py:49] layer 9_up @ epoch 3 new loss 0.0012565696379169822 old loss 0.0012653481680899858 BETTER
I0907 20:36:32.604448 95203 finetune.py:49] layer 9_up @ epoch 4 new loss 0.0012498631840571761 old loss 0.0012565696379169822 BETTER
I0907 20:36:37.441449 95203 quip.py:388] mean square of W: 0.9999998211860657
I0907 20:36:37.441910 95203 quip.py:389] mean square of Wr: 0.9999998211860657
I0907 20:36:37.443939 95203 quip.py:390] difference between Hr and Hr.T: 1.862645149230957e-07
I0907 20:36:37.444540 95203 quip.py:391] max abs of Hr: 1.2511271238327026
I0907 20:36:37.444684 95203 quip.py:392] min diag of Lhr: 0.7601099610328674
I0907 20:37:06.199368 95203 misc.py:19] ckpt/2_7b_2bit/9_down.pt frob  error: 0.11972420662641525
I0907 20:37:06.199595 95203 misc.py:20] ckpt/2_7b_2bit/9_down.pt proxy error: 0.06075608730316162
I0907 20:37:31.894872 93442 quantize_finetune_llama.py:211] computed original embedding for layer 10 in 23.15335988998413s, pre msv 0.5194587111473083, post msv 0.5221440196037292
I0907 20:37:32.009079 93442 quantize_finetune_llama.py:179] layer 11 gpu 0
W0907 20:37:33.404868 95467 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 20:37:33.430148 95467 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 20:37:33.544490 95467 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 20:37:33.544631 95467 utils.py:164] NumExpr defaulting to 16 threads.
I0907 20:37:33.709984 95467 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 20:37:33.953426 95467 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 20:37:54.122654 95467 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 20:37:55.768398 95467 quip.py:388] mean square of W: 1.0
I0907 20:37:55.768907 95467 quip.py:389] mean square of Wr: 1.0
I0907 20:37:55.777902 95467 quip.py:390] difference between Hr and Hr.T: 3.5762786865234375e-07
I0907 20:37:55.778327 95467 quip.py:391] max abs of Hr: 3.40993070602417
I0907 20:37:55.787087 95467 quip.py:392] min diag of Lhr: 0.6040442585945129
I0907 20:38:08.787334 95467 misc.py:19] ckpt/2_7b_2bit/10_qkv.pt frob  error: 0.12252658605575562
I0907 20:38:08.787575 95467 misc.py:20] ckpt/2_7b_2bit/10_qkv.pt proxy error: 0.012324707582592964
I0907 20:38:16.047677 95467 finetune.py:25] layer 10_qkv initial loss 0.0008624499314464629
W0907 20:38:16.047962 95467 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 20:38:16.188498 95467 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 20:38:28.906948 95467 finetune.py:49] layer 10_qkv @ epoch 0 new loss 0.0005142676527611911 old loss 0.0008624499314464629 BETTER
I0907 20:38:41.599356 95467 finetune.py:49] layer 10_qkv @ epoch 1 new loss 0.0004889986594207585 old loss 0.0005142676527611911 BETTER
I0907 20:38:54.315155 95467 finetune.py:56] layer 10_qkv @ epoch 2 new loss 0.0005158696440048516 old loss 0.0004889986594207585 WORSE
I0907 20:39:06.976176 95467 finetune.py:56] layer 10_qkv @ epoch 3 new loss 0.0004947709967382252 old loss 0.0004889986594207585 WORSE
I0907 20:39:19.756839 95467 finetune.py:49] layer 10_qkv @ epoch 4 new loss 0.00047198610263876617 old loss 0.0004889986594207585 BETTER
I0907 20:39:21.655753 95467 quip.py:388] mean square of W: 1.0
I0907 20:39:21.656098 95467 quip.py:389] mean square of Wr: 1.0
I0907 20:39:21.656604 95467 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 20:39:21.656900 95467 quip.py:391] max abs of Hr: 1.4075995683670044
I0907 20:39:21.657008 95467 quip.py:392] min diag of Lhr: 0.5318868160247803
I0907 20:39:32.740439 95467 misc.py:19] ckpt/2_7b_2bit/10_o.pt frob  error: 0.1432017832994461
I0907 20:39:32.740687 95467 misc.py:20] ckpt/2_7b_2bit/10_o.pt proxy error: 0.04925588145852089
I0907 20:39:38.545145 95467 finetune.py:25] layer 10_o initial loss 0.0009150835103355348
I0907 20:39:50.959954 95467 finetune.py:49] layer 10_o @ epoch 0 new loss 0.00081296032294631 old loss 0.0009150835103355348 BETTER
I0907 20:40:03.185796 95467 finetune.py:49] layer 10_o @ epoch 1 new loss 0.0007881222409196198 old loss 0.00081296032294631 BETTER
I0907 20:40:15.405737 95467 finetune.py:49] layer 10_o @ epoch 2 new loss 0.0007790570962242782 old loss 0.0007881222409196198 BETTER
I0907 20:40:27.979091 95467 finetune.py:49] layer 10_o @ epoch 3 new loss 0.0007690099882893264 old loss 0.0007790570962242782 BETTER
I0907 20:40:40.193956 95467 finetune.py:49] layer 10_o @ epoch 4 new loss 0.0007627910235896707 old loss 0.0007690099882893264 BETTER
I0907 20:40:42.299772 95467 quip.py:388] mean square of W: 1.0
I0907 20:40:42.300414 95467 quip.py:389] mean square of Wr: 1.0
I0907 20:40:42.300904 95467 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 20:40:42.301191 95467 quip.py:391] max abs of Hr: 1.5585942268371582
I0907 20:40:42.301304 95467 quip.py:392] min diag of Lhr: 0.7301957011222839
I0907 20:40:58.781501 95467 misc.py:19] ckpt/2_7b_2bit/10_up.pt frob  error: 0.12086888402700424
I0907 20:40:58.781744 95467 misc.py:20] ckpt/2_7b_2bit/10_up.pt proxy error: 0.030702641233801842
I0907 20:41:03.014442 95467 finetune.py:25] layer 10_up initial loss 0.001650469028390944
I0907 20:41:15.093396 95467 finetune.py:49] layer 10_up @ epoch 0 new loss 0.0015894671669229865 old loss 0.001650469028390944 BETTER
I0907 20:41:27.474714 95467 finetune.py:49] layer 10_up @ epoch 1 new loss 0.0015646029496565461 old loss 0.0015894671669229865 BETTER
I0907 20:41:39.437419 95467 finetune.py:49] layer 10_up @ epoch 2 new loss 0.0015491221565753222 old loss 0.0015646029496565461 BETTER
I0907 20:41:51.227914 95467 finetune.py:49] layer 10_up @ epoch 3 new loss 0.0015380920376628637 old loss 0.0015491221565753222 BETTER
I0907 20:42:02.956296 95467 finetune.py:49] layer 10_up @ epoch 4 new loss 0.0015303061809390783 old loss 0.0015380920376628637 BETTER
I0907 20:42:07.459603 95467 quip.py:388] mean square of W: 1.0
I0907 20:42:07.460074 95467 quip.py:389] mean square of Wr: 1.0
I0907 20:42:07.462074 95467 quip.py:390] difference between Hr and Hr.T: 3.2782554626464844e-07
I0907 20:42:07.462664 95467 quip.py:391] max abs of Hr: 1.3829586505889893
I0907 20:42:07.462797 95467 quip.py:392] min diag of Lhr: 0.7611305713653564
I0907 20:42:36.636027 95467 misc.py:19] ckpt/2_7b_2bit/10_down.pt frob  error: 0.11966364085674286
I0907 20:42:36.636216 95467 misc.py:20] ckpt/2_7b_2bit/10_down.pt proxy error: 0.059029750525951385
I0907 20:43:07.236619 93442 quantize_finetune_llama.py:211] computed original embedding for layer 11 in 27.977490425109863s, pre msv 0.5221440196037292, post msv 0.5252853631973267
I0907 20:43:07.343539 93442 quantize_finetune_llama.py:179] layer 12 gpu 0
W0907 20:43:08.614854 95567 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 20:43:08.639355 95567 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 20:43:08.749966 95567 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 20:43:08.750082 95567 utils.py:164] NumExpr defaulting to 16 threads.
I0907 20:43:08.906322 95567 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 20:43:09.143092 95567 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 20:43:28.382694 95567 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 20:43:29.822407 95567 quip.py:388] mean square of W: 1.0
I0907 20:43:29.822914 95567 quip.py:389] mean square of Wr: 1.0
I0907 20:43:29.831904 95567 quip.py:390] difference between Hr and Hr.T: 3.5762786865234375e-07
I0907 20:43:29.832311 95567 quip.py:391] max abs of Hr: 2.9051599502563477
I0907 20:43:29.841098 95567 quip.py:392] min diag of Lhr: 0.6183326244354248
I0907 20:43:42.363904 95567 misc.py:19] ckpt/2_7b_2bit/11_qkv.pt frob  error: 0.12245280295610428
I0907 20:43:42.364124 95567 misc.py:20] ckpt/2_7b_2bit/11_qkv.pt proxy error: 0.013495588675141335
I0907 20:43:49.389529 95567 finetune.py:25] layer 11_qkv initial loss 0.0010312350932508707
W0907 20:43:49.389814 95567 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 20:43:49.522691 95567 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 20:44:02.269876 95567 finetune.py:49] layer 11_qkv @ epoch 0 new loss 0.0005655784043483436 old loss 0.0010312350932508707 BETTER
I0907 20:44:14.647170 95567 finetune.py:49] layer 11_qkv @ epoch 1 new loss 0.0005439405795186758 old loss 0.0005655784043483436 BETTER
I0907 20:44:27.066223 95567 finetune.py:56] layer 11_qkv @ epoch 2 new loss 0.0005499378312379122 old loss 0.0005439405795186758 WORSE
I0907 20:44:39.658448 95567 finetune.py:56] layer 11_qkv @ epoch 3 new loss 0.0005458686500787735 old loss 0.0005439405795186758 WORSE
I0907 20:44:52.184782 95567 finetune.py:49] layer 11_qkv @ epoch 4 new loss 0.0005187136121094227 old loss 0.0005439405795186758 BETTER
I0907 20:44:54.168634 95567 quip.py:388] mean square of W: 1.0
I0907 20:44:54.168957 95567 quip.py:389] mean square of Wr: 1.0
I0907 20:44:54.169458 95567 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 20:44:54.169741 95567 quip.py:391] max abs of Hr: 1.499609351158142
I0907 20:44:54.169851 95567 quip.py:392] min diag of Lhr: 0.5114615559577942
I0907 20:45:05.069307 95567 misc.py:19] ckpt/2_7b_2bit/11_o.pt frob  error: 0.14739695191383362
I0907 20:45:05.069529 95567 misc.py:20] ckpt/2_7b_2bit/11_o.pt proxy error: 0.052811149507761
I0907 20:45:10.981147 95567 finetune.py:25] layer 11_o initial loss 0.001050928607583046
I0907 20:45:23.484069 95567 finetune.py:49] layer 11_o @ epoch 0 new loss 0.0009211049182340503 old loss 0.001050928607583046 BETTER
I0907 20:45:36.136231 95567 finetune.py:49] layer 11_o @ epoch 1 new loss 0.0008977425168268383 old loss 0.0009211049182340503 BETTER
I0907 20:45:48.925323 95567 finetune.py:49] layer 11_o @ epoch 2 new loss 0.0008826359990052879 old loss 0.0008977425168268383 BETTER
I0907 20:46:01.307578 95567 finetune.py:49] layer 11_o @ epoch 3 new loss 0.0008760277414694428 old loss 0.0008826359990052879 BETTER
I0907 20:46:13.923939 95567 finetune.py:49] layer 11_o @ epoch 4 new loss 0.0008696612203493714 old loss 0.0008760277414694428 BETTER
I0907 20:46:16.324930 95567 quip.py:388] mean square of W: 1.0
I0907 20:46:16.325574 95567 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 20:46:16.326068 95567 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 20:46:16.326349 95567 quip.py:391] max abs of Hr: 1.607629418373108
I0907 20:46:16.326470 95567 quip.py:392] min diag of Lhr: 0.7424635291099548
I0907 20:46:32.984113 95567 misc.py:19] ckpt/2_7b_2bit/11_up.pt frob  error: 0.119828000664711
I0907 20:46:32.984341 95567 misc.py:20] ckpt/2_7b_2bit/11_up.pt proxy error: 0.03162688389420509
I0907 20:46:37.325987 95567 finetune.py:25] layer 11_up initial loss 0.0019035886507481337
I0907 20:46:49.479353 95567 finetune.py:49] layer 11_up @ epoch 0 new loss 0.0018350541358813643 old loss 0.0019035886507481337 BETTER
I0907 20:47:01.795153 95567 finetune.py:49] layer 11_up @ epoch 1 new loss 0.0018081567250192165 old loss 0.0018350541358813643 BETTER
I0907 20:47:13.766418 95567 finetune.py:49] layer 11_up @ epoch 2 new loss 0.0017909990856423974 old loss 0.0018081567250192165 BETTER
I0907 20:47:25.910312 95567 finetune.py:49] layer 11_up @ epoch 3 new loss 0.0017781652277335525 old loss 0.0017909990856423974 BETTER
I0907 20:47:37.452271 95567 finetune.py:49] layer 11_up @ epoch 4 new loss 0.0017692090477794409 old loss 0.0017781652277335525 BETTER
I0907 20:47:41.500256 95567 quip.py:388] mean square of W: 1.0
I0907 20:47:41.500670 95567 quip.py:389] mean square of Wr: 1.0
I0907 20:47:41.502680 95567 quip.py:390] difference between Hr and Hr.T: 2.980232238769531e-07
I0907 20:47:41.503260 95567 quip.py:391] max abs of Hr: 1.339529037475586
I0907 20:47:41.503389 95567 quip.py:392] min diag of Lhr: 0.7622519731521606
I0907 20:48:10.546249 95567 misc.py:19] ckpt/2_7b_2bit/11_down.pt frob  error: 0.11943253874778748
I0907 20:48:10.546483 95567 misc.py:20] ckpt/2_7b_2bit/11_down.pt proxy error: 0.06147272512316704
I0907 20:48:41.730146 93442 quantize_finetune_llama.py:211] computed original embedding for layer 12 in 28.6070556640625s, pre msv 0.5252853631973267, post msv 0.528696596622467
I0907 20:48:41.825067 93442 quantize_finetune_llama.py:179] layer 13 gpu 0
W0907 20:48:43.134732 95666 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 20:48:43.158527 95666 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 20:48:43.267865 95666 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 20:48:43.267949 95666 utils.py:164] NumExpr defaulting to 16 threads.
I0907 20:48:43.420362 95666 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 20:48:43.667269 95666 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 20:49:02.935229 95666 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 20:49:04.333590 95666 quip.py:388] mean square of W: 1.0000001192092896
I0907 20:49:04.334065 95666 quip.py:389] mean square of Wr: 1.0
I0907 20:49:04.343094 95666 quip.py:390] difference between Hr and Hr.T: 3.5762786865234375e-07
I0907 20:49:04.343513 95666 quip.py:391] max abs of Hr: 2.7813498973846436
I0907 20:49:04.352222 95666 quip.py:392] min diag of Lhr: 0.6286196708679199
I0907 20:49:16.567911 95666 misc.py:19] ckpt/2_7b_2bit/12_qkv.pt frob  error: 0.12161930650472641
I0907 20:49:16.568137 95666 misc.py:20] ckpt/2_7b_2bit/12_qkv.pt proxy error: 0.014423681423068047
I0907 20:49:23.744635 95666 finetune.py:25] layer 12_qkv initial loss 0.001119463238865137
W0907 20:49:23.744946 95666 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 20:49:23.881318 95666 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 20:49:36.758666 95666 finetune.py:49] layer 12_qkv @ epoch 0 new loss 0.0006309844320639968 old loss 0.001119463238865137 BETTER
I0907 20:49:49.358329 95666 finetune.py:49] layer 12_qkv @ epoch 1 new loss 0.0006075087003409863 old loss 0.0006309844320639968 BETTER
I0907 20:50:02.437409 95666 finetune.py:56] layer 12_qkv @ epoch 2 new loss 0.0006527319783344865 old loss 0.0006075087003409863 WORSE
I0907 20:50:14.767189 95666 finetune.py:56] layer 12_qkv @ epoch 3 new loss 0.0006140260375104845 old loss 0.0006075087003409863 WORSE
I0907 20:50:27.148985 95666 finetune.py:49] layer 12_qkv @ epoch 4 new loss 0.0005960419657640159 old loss 0.0006075087003409863 BETTER
I0907 20:50:28.710742 95666 quip.py:388] mean square of W: 1.0
I0907 20:50:28.711039 95666 quip.py:389] mean square of Wr: 0.9999999403953552
I0907 20:50:28.711533 95666 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 20:50:28.711822 95666 quip.py:391] max abs of Hr: 1.519035816192627
I0907 20:50:28.711929 95666 quip.py:392] min diag of Lhr: 0.5346666574478149
I0907 20:50:39.103667 95666 misc.py:19] ckpt/2_7b_2bit/12_o.pt frob  error: 0.14389561116695404
I0907 20:50:39.103895 95666 misc.py:20] ckpt/2_7b_2bit/12_o.pt proxy error: 0.0543975830078125
I0907 20:50:44.792966 95666 finetune.py:25] layer 12_o initial loss 0.0011616803240031004
I0907 20:50:57.372089 95666 finetune.py:49] layer 12_o @ epoch 0 new loss 0.0010158633813261986 old loss 0.0011616803240031004 BETTER
I0907 20:51:09.783982 95666 finetune.py:49] layer 12_o @ epoch 1 new loss 0.0009912396781146526 old loss 0.0010158633813261986 BETTER
I0907 20:51:22.259319 95666 finetune.py:49] layer 12_o @ epoch 2 new loss 0.0009730815654620528 old loss 0.0009912396781146526 BETTER
I0907 20:51:34.429573 95666 finetune.py:49] layer 12_o @ epoch 3 new loss 0.000962671940214932 old loss 0.0009730815654620528 BETTER
I0907 20:51:46.817705 95666 finetune.py:49] layer 12_o @ epoch 4 new loss 0.0009553449344821274 old loss 0.000962671940214932 BETTER
I0907 20:51:49.118351 95666 quip.py:388] mean square of W: 1.0000001192092896
I0907 20:51:49.118997 95666 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 20:51:49.119483 95666 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 20:51:49.119764 95666 quip.py:391] max abs of Hr: 1.669745922088623
I0907 20:51:49.119873 95666 quip.py:392] min diag of Lhr: 0.751442015171051
I0907 20:52:05.783858 95666 misc.py:19] ckpt/2_7b_2bit/12_up.pt frob  error: 0.11973932385444641
I0907 20:52:05.784087 95666 misc.py:20] ckpt/2_7b_2bit/12_up.pt proxy error: 0.03307740017771721
I0907 20:52:10.234623 95666 finetune.py:25] layer 12_up initial loss 0.0021403443533927202
I0907 20:52:22.007967 95666 finetune.py:49] layer 12_up @ epoch 0 new loss 0.0020644788164645433 old loss 0.0021403443533927202 BETTER
I0907 20:52:33.858450 95666 finetune.py:49] layer 12_up @ epoch 1 new loss 0.0020335728768259287 old loss 0.0020644788164645433 BETTER
I0907 20:52:45.865987 95666 finetune.py:49] layer 12_up @ epoch 2 new loss 0.002015042817220092 old loss 0.0020335728768259287 BETTER
I0907 20:52:57.671493 95666 finetune.py:49] layer 12_up @ epoch 3 new loss 0.0020010434091091156 old loss 0.002015042817220092 BETTER
I0907 20:53:09.215903 95666 finetune.py:49] layer 12_up @ epoch 4 new loss 0.001991446129977703 old loss 0.0020010434091091156 BETTER
I0907 20:53:13.489157 95666 quip.py:388] mean square of W: 0.9999998807907104
I0907 20:53:13.489589 95666 quip.py:389] mean square of Wr: 0.9999998807907104
I0907 20:53:13.491681 95666 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 20:53:13.492266 95666 quip.py:391] max abs of Hr: 1.38274347782135
I0907 20:53:13.492393 95666 quip.py:392] min diag of Lhr: 0.7648184895515442
I0907 20:53:42.664967 95666 misc.py:19] ckpt/2_7b_2bit/12_down.pt frob  error: 0.11944930255413055
I0907 20:53:42.665195 95666 misc.py:20] ckpt/2_7b_2bit/12_down.pt proxy error: 0.06214560195803642
I0907 20:54:13.262451 93442 quantize_finetune_llama.py:211] computed original embedding for layer 13 in 28.196319341659546s, pre msv 0.528696596622467, post msv 0.5352429747581482
I0907 20:54:13.347787 93442 quantize_finetune_llama.py:179] layer 14 gpu 0
W0907 20:54:14.610397 95764 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 20:54:14.634173 95764 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 20:54:14.742856 95764 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 20:54:14.742941 95764 utils.py:164] NumExpr defaulting to 16 threads.
I0907 20:54:14.890858 95764 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 20:54:15.129984 95764 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 20:54:34.276533 95764 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 20:54:35.715896 95764 quip.py:388] mean square of W: 1.0
I0907 20:54:35.716370 95764 quip.py:389] mean square of Wr: 0.9999999403953552
I0907 20:54:35.725252 95764 quip.py:390] difference between Hr and Hr.T: 2.980232238769531e-07
I0907 20:54:35.725638 95764 quip.py:391] max abs of Hr: 2.529111862182617
I0907 20:54:35.734296 95764 quip.py:392] min diag of Lhr: 0.6496500372886658
I0907 20:54:48.680936 95764 misc.py:19] ckpt/2_7b_2bit/13_qkv.pt frob  error: 0.12109079211950302
I0907 20:54:48.681164 95764 misc.py:20] ckpt/2_7b_2bit/13_qkv.pt proxy error: 0.015149853192269802
I0907 20:54:55.756401 95764 finetune.py:25] layer 13_qkv initial loss 0.0012861265568062663
W0907 20:54:55.756742 95764 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 20:54:55.898191 95764 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 20:55:08.493675 95764 finetune.py:49] layer 13_qkv @ epoch 0 new loss 0.0007066606776788831 old loss 0.0012861265568062663 BETTER
I0907 20:55:21.092149 95764 finetune.py:49] layer 13_qkv @ epoch 1 new loss 0.0006751992623321712 old loss 0.0007066606776788831 BETTER
I0907 20:55:33.314017 95764 finetune.py:56] layer 13_qkv @ epoch 2 new loss 0.0006754714995622635 old loss 0.0006751992623321712 WORSE
I0907 20:55:45.529888 95764 finetune.py:49] layer 13_qkv @ epoch 3 new loss 0.0006612845463678241 old loss 0.0006751992623321712 BETTER
I0907 20:55:57.795591 95764 finetune.py:49] layer 13_qkv @ epoch 4 new loss 0.0006356561789289117 old loss 0.0006612845463678241 BETTER
I0907 20:55:59.346184 95764 quip.py:388] mean square of W: 1.0
I0907 20:55:59.346481 95764 quip.py:389] mean square of Wr: 1.0
I0907 20:55:59.346973 95764 quip.py:390] difference between Hr and Hr.T: 1.1920928955078125e-07
I0907 20:55:59.347258 95764 quip.py:391] max abs of Hr: 1.3842369318008423
I0907 20:55:59.347367 95764 quip.py:392] min diag of Lhr: 0.5276800990104675
I0907 20:56:09.634193 95764 misc.py:19] ckpt/2_7b_2bit/13_o.pt frob  error: 0.14619937539100647
I0907 20:56:09.634365 95764 misc.py:20] ckpt/2_7b_2bit/13_o.pt proxy error: 0.051998116075992584
I0907 20:56:15.450151 95764 finetune.py:25] layer 13_o initial loss 0.001276050927117467
I0907 20:56:27.990827 95764 finetune.py:49] layer 13_o @ epoch 0 new loss 0.0011251780670136213 old loss 0.001276050927117467 BETTER
I0907 20:56:40.644510 95764 finetune.py:49] layer 13_o @ epoch 1 new loss 0.0010977046331390738 old loss 0.0011251780670136213 BETTER
I0907 20:56:53.172977 95764 finetune.py:49] layer 13_o @ epoch 2 new loss 0.0010832069674506783 old loss 0.0010977046331390738 BETTER
I0907 20:57:05.687918 95764 finetune.py:49] layer 13_o @ epoch 3 new loss 0.001078594708815217 old loss 0.0010832069674506783 BETTER
I0907 20:57:17.877858 95764 finetune.py:49] layer 13_o @ epoch 4 new loss 0.0010706543689593673 old loss 0.001078594708815217 BETTER
I0907 20:57:20.008511 95764 quip.py:388] mean square of W: 1.0000001192092896
I0907 20:57:20.009164 95764 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 20:57:20.009660 95764 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 20:57:20.009939 95764 quip.py:391] max abs of Hr: 1.6771539449691772
I0907 20:57:20.010043 95764 quip.py:392] min diag of Lhr: 0.741897702217102
I0907 20:57:36.385351 95764 misc.py:19] ckpt/2_7b_2bit/13_up.pt frob  error: 0.11999677121639252
I0907 20:57:36.385513 95764 misc.py:20] ckpt/2_7b_2bit/13_up.pt proxy error: 0.031835127621889114
I0907 20:57:40.551962 95764 finetune.py:25] layer 13_up initial loss 0.0024954602122306824
I0907 20:57:52.198803 95764 finetune.py:49] layer 13_up @ epoch 0 new loss 0.0023986860178411007 old loss 0.0024954602122306824 BETTER
I0907 20:58:03.769597 95764 finetune.py:49] layer 13_up @ epoch 1 new loss 0.002359634032472968 old loss 0.0023986860178411007 BETTER
I0907 20:58:15.651121 95764 finetune.py:49] layer 13_up @ epoch 2 new loss 0.0023354869335889816 old loss 0.002359634032472968 BETTER
I0907 20:58:27.203137 95764 finetune.py:49] layer 13_up @ epoch 3 new loss 0.0023181738797575235 old loss 0.0023354869335889816 BETTER
I0907 20:58:38.647365 95764 finetune.py:49] layer 13_up @ epoch 4 new loss 0.002306228270754218 old loss 0.0023181738797575235 BETTER
I0907 20:58:43.056201 95764 quip.py:388] mean square of W: 0.9999998807907104
I0907 20:58:43.056638 95764 quip.py:389] mean square of Wr: 0.9999998807907104
I0907 20:58:43.058680 95764 quip.py:390] difference between Hr and Hr.T: 2.682209014892578e-07
I0907 20:58:43.059271 95764 quip.py:391] max abs of Hr: 1.3170613050460815
I0907 20:58:43.059404 95764 quip.py:392] min diag of Lhr: 0.750086784362793
I0907 20:59:11.900097 95764 misc.py:19] ckpt/2_7b_2bit/13_down.pt frob  error: 0.12045913189649582
I0907 20:59:11.900316 95764 misc.py:20] ckpt/2_7b_2bit/13_down.pt proxy error: 0.06170005723834038
I0907 20:59:42.583315 93442 quantize_finetune_llama.py:211] computed original embedding for layer 14 in 28.294025659561157s, pre msv 0.5352429747581482, post msv 0.5414574146270752
I0907 20:59:42.693230 93442 quantize_finetune_llama.py:179] layer 15 gpu 0
W0907 20:59:44.060473 95862 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 20:59:44.085950 95862 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 20:59:44.202586 95862 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 20:59:44.202736 95862 utils.py:164] NumExpr defaulting to 16 threads.
I0907 20:59:44.360956 95862 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 20:59:44.602554 95862 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 21:00:04.063306 95862 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 21:00:05.628064 95862 quip.py:388] mean square of W: 1.0
I0907 21:00:05.628569 95862 quip.py:389] mean square of Wr: 0.9999999403953552
I0907 21:00:05.637602 95862 quip.py:390] difference between Hr and Hr.T: 2.980232238769531e-07
I0907 21:00:05.638035 95862 quip.py:391] max abs of Hr: 2.679999828338623
I0907 21:00:05.646879 95862 quip.py:392] min diag of Lhr: 0.6391745805740356
I0907 21:00:18.503420 95862 misc.py:19] ckpt/2_7b_2bit/14_qkv.pt frob  error: 0.12191355228424072
I0907 21:00:18.503660 95862 misc.py:20] ckpt/2_7b_2bit/14_qkv.pt proxy error: 0.014875615946948528
I0907 21:00:25.621585 95862 finetune.py:25] layer 14_qkv initial loss 0.0014027799479663372
W0907 21:00:25.621876 95862 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 21:00:25.752976 95862 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 21:00:38.494408 95862 finetune.py:49] layer 14_qkv @ epoch 0 new loss 0.0008441017707809806 old loss 0.0014027799479663372 BETTER
I0907 21:00:50.915246 95862 finetune.py:49] layer 14_qkv @ epoch 1 new loss 0.0007961999508552253 old loss 0.0008441017707809806 BETTER
I0907 21:01:03.242625 95862 finetune.py:56] layer 14_qkv @ epoch 2 new loss 0.0008109095506370068 old loss 0.0007961999508552253 WORSE
I0907 21:01:15.742596 95862 finetune.py:49] layer 14_qkv @ epoch 3 new loss 0.0007774866535328329 old loss 0.0007961999508552253 BETTER
I0907 21:01:28.384981 95862 finetune.py:49] layer 14_qkv @ epoch 4 new loss 0.0007607545121572912 old loss 0.0007774866535328329 BETTER
I0907 21:01:30.276561 95862 quip.py:388] mean square of W: 1.0
I0907 21:01:30.276891 95862 quip.py:389] mean square of Wr: 1.0
I0907 21:01:30.277386 95862 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 21:01:30.277670 95862 quip.py:391] max abs of Hr: 1.2960017919540405
I0907 21:01:30.277781 95862 quip.py:392] min diag of Lhr: 0.5634075999259949
I0907 21:01:41.342049 95862 misc.py:19] ckpt/2_7b_2bit/14_o.pt frob  error: 0.13845214247703552
I0907 21:01:41.342276 95862 misc.py:20] ckpt/2_7b_2bit/14_o.pt proxy error: 0.05664553493261337
I0907 21:01:47.212255 95862 finetune.py:25] layer 14_o initial loss 0.0015043426537886262
I0907 21:01:59.834435 95862 finetune.py:49] layer 14_o @ epoch 0 new loss 0.001346398494206369 old loss 0.0015043426537886262 BETTER
I0907 21:02:12.212637 95862 finetune.py:49] layer 14_o @ epoch 1 new loss 0.001312446198426187 old loss 0.001346398494206369 BETTER
I0907 21:02:24.868474 95862 finetune.py:49] layer 14_o @ epoch 2 new loss 0.0012920316075906157 old loss 0.001312446198426187 BETTER
I0907 21:02:37.222003 95862 finetune.py:49] layer 14_o @ epoch 3 new loss 0.0012858716072514653 old loss 0.0012920316075906157 BETTER
I0907 21:02:50.165232 95862 finetune.py:49] layer 14_o @ epoch 4 new loss 0.0012847938342019916 old loss 0.0012858716072514653 BETTER
I0907 21:02:52.644937 95862 quip.py:388] mean square of W: 1.0
I0907 21:02:52.645546 95862 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 21:02:52.646035 95862 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 21:02:52.646322 95862 quip.py:391] max abs of Hr: 1.512378454208374
I0907 21:02:52.646471 95862 quip.py:392] min diag of Lhr: 0.7522744536399841
I0907 21:03:09.668426 95862 misc.py:19] ckpt/2_7b_2bit/14_up.pt frob  error: 0.1197916716337204
I0907 21:03:09.668665 95862 misc.py:20] ckpt/2_7b_2bit/14_up.pt proxy error: 0.03342527523636818
I0907 21:03:13.899281 95862 finetune.py:25] layer 14_up initial loss 0.0029329948592931032
I0907 21:03:25.646885 95862 finetune.py:49] layer 14_up @ epoch 0 new loss 0.0028213635087013245 old loss 0.0029329948592931032 BETTER
I0907 21:03:37.683534 95862 finetune.py:49] layer 14_up @ epoch 1 new loss 0.002777322893962264 old loss 0.0028213635087013245 BETTER
I0907 21:03:49.595808 95862 finetune.py:49] layer 14_up @ epoch 2 new loss 0.0027522938326001167 old loss 0.002777322893962264 BETTER
I0907 21:04:01.524631 95862 finetune.py:49] layer 14_up @ epoch 3 new loss 0.00273566460236907 old loss 0.0027522938326001167 BETTER
I0907 21:04:13.282760 95862 finetune.py:49] layer 14_up @ epoch 4 new loss 0.002722943900153041 old loss 0.00273566460236907 BETTER
I0907 21:04:17.861194 95862 quip.py:388] mean square of W: 1.0
I0907 21:04:17.861633 95862 quip.py:389] mean square of Wr: 1.0
I0907 21:04:17.863674 95862 quip.py:390] difference between Hr and Hr.T: 2.980232238769531e-07
I0907 21:04:17.864282 95862 quip.py:391] max abs of Hr: 1.4841065406799316
I0907 21:04:17.864411 95862 quip.py:392] min diag of Lhr: 0.7513996958732605
I0907 21:04:47.270105 95862 misc.py:19] ckpt/2_7b_2bit/14_down.pt frob  error: 0.12068929523229599
I0907 21:04:47.270335 95862 misc.py:20] ckpt/2_7b_2bit/14_down.pt proxy error: 0.063315749168396
I0907 21:05:19.732484 93442 quantize_finetune_llama.py:211] computed original embedding for layer 15 in 29.741420030593872s, pre msv 0.5414574146270752, post msv 0.5519701838493347
I0907 21:05:19.852993 93442 quantize_finetune_llama.py:179] layer 16 gpu 0
W0907 21:05:21.133471 96192 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 21:05:21.157140 96192 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 21:05:21.262363 96192 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 21:05:21.262471 96192 utils.py:164] NumExpr defaulting to 16 threads.
I0907 21:05:21.410863 96192 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 21:05:21.645962 96192 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 21:05:40.596858 96192 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 21:05:42.006590 96192 quip.py:388] mean square of W: 1.0
I0907 21:05:42.007081 96192 quip.py:389] mean square of Wr: 1.0
I0907 21:05:42.016181 96192 quip.py:390] difference between Hr and Hr.T: 3.5762786865234375e-07
I0907 21:05:42.016588 96192 quip.py:391] max abs of Hr: 2.8040716648101807
I0907 21:05:42.025485 96192 quip.py:392] min diag of Lhr: 0.6371439099311829
I0907 21:05:54.258870 96192 misc.py:19] ckpt/2_7b_2bit/15_qkv.pt frob  error: 0.12144621461629868
I0907 21:05:54.259033 96192 misc.py:20] ckpt/2_7b_2bit/15_qkv.pt proxy error: 0.014364444650709629
I0907 21:06:01.345469 96192 finetune.py:25] layer 15_qkv initial loss 0.0015868564369156957
W0907 21:06:01.345686 96192 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 21:06:01.474945 96192 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 21:06:14.102745 96192 finetune.py:49] layer 15_qkv @ epoch 0 new loss 0.0009137072484008968 old loss 0.0015868564369156957 BETTER
I0907 21:06:26.757998 96192 finetune.py:49] layer 15_qkv @ epoch 1 new loss 0.0008637630962766707 old loss 0.0009137072484008968 BETTER
I0907 21:06:38.866840 96192 finetune.py:49] layer 15_qkv @ epoch 2 new loss 0.0008495509973727167 old loss 0.0008637630962766707 BETTER
I0907 21:06:51.275689 96192 finetune.py:56] layer 15_qkv @ epoch 3 new loss 0.0008787332917563617 old loss 0.0008495509973727167 WORSE
I0907 21:07:04.172110 96192 finetune.py:49] layer 15_qkv @ epoch 4 new loss 0.0008313583675771952 old loss 0.0008495509973727167 BETTER
I0907 21:07:05.797953 96192 quip.py:388] mean square of W: 1.0
I0907 21:07:05.798248 96192 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 21:07:05.798741 96192 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 21:07:05.799025 96192 quip.py:391] max abs of Hr: 1.3341503143310547
I0907 21:07:05.799134 96192 quip.py:392] min diag of Lhr: 0.5583981871604919
I0907 21:07:16.807415 96192 misc.py:19] ckpt/2_7b_2bit/15_o.pt frob  error: 0.13981270790100098
I0907 21:07:16.807570 96192 misc.py:20] ckpt/2_7b_2bit/15_o.pt proxy error: 0.05180530995130539
I0907 21:07:22.650434 96192 finetune.py:25] layer 15_o initial loss 0.001653903047554195
I0907 21:07:34.887376 96192 finetune.py:49] layer 15_o @ epoch 0 new loss 0.0014760164776816964 old loss 0.001653903047554195 BETTER
I0907 21:07:47.229162 96192 finetune.py:49] layer 15_o @ epoch 1 new loss 0.0014301729388535023 old loss 0.0014760164776816964 BETTER
I0907 21:07:59.440137 96192 finetune.py:49] layer 15_o @ epoch 2 new loss 0.001409324468113482 old loss 0.0014301729388535023 BETTER
I0907 21:08:11.511931 96192 finetune.py:49] layer 15_o @ epoch 3 new loss 0.0013951992150396109 old loss 0.001409324468113482 BETTER
I0907 21:08:23.546383 96192 finetune.py:49] layer 15_o @ epoch 4 new loss 0.0013924365630373359 old loss 0.0013951992150396109 BETTER
I0907 21:08:25.293998 96192 quip.py:388] mean square of W: 1.0
I0907 21:08:25.294625 96192 quip.py:389] mean square of Wr: 1.0
I0907 21:08:25.295116 96192 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 21:08:25.295405 96192 quip.py:391] max abs of Hr: 1.694295883178711
I0907 21:08:25.295500 96192 quip.py:392] min diag of Lhr: 0.7436192035675049
I0907 21:08:41.134914 96192 misc.py:19] ckpt/2_7b_2bit/15_up.pt frob  error: 0.1200660690665245
I0907 21:08:41.135068 96192 misc.py:20] ckpt/2_7b_2bit/15_up.pt proxy error: 0.03305038809776306
I0907 21:08:45.377049 96192 finetune.py:25] layer 15_up initial loss 0.0034111831337213516
I0907 21:08:57.276127 96192 finetune.py:49] layer 15_up @ epoch 0 new loss 0.003273413982242346 old loss 0.0034111831337213516 BETTER
I0907 21:09:09.427519 96192 finetune.py:49] layer 15_up @ epoch 1 new loss 0.003220158163458109 old loss 0.003273413982242346 BETTER
I0907 21:09:21.510894 96192 finetune.py:49] layer 15_up @ epoch 2 new loss 0.0031893765553832054 old loss 0.003220158163458109 BETTER
I0907 21:09:33.550798 96192 finetune.py:49] layer 15_up @ epoch 3 new loss 0.0031687661539763212 old loss 0.0031893765553832054 BETTER
I0907 21:09:46.022382 96192 finetune.py:49] layer 15_up @ epoch 4 new loss 0.0031542161013931036 old loss 0.0031687661539763212 BETTER
I0907 21:09:50.343719 96192 quip.py:388] mean square of W: 1.0
I0907 21:09:50.344151 96192 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 21:09:50.346177 96192 quip.py:390] difference between Hr and Hr.T: 3.427267074584961e-07
I0907 21:09:50.346765 96192 quip.py:391] max abs of Hr: 1.403406023979187
I0907 21:09:50.346904 96192 quip.py:392] min diag of Lhr: 0.7395076751708984
I0907 21:10:19.572414 96192 misc.py:19] ckpt/2_7b_2bit/15_down.pt frob  error: 0.12147803604602814
I0907 21:10:19.572666 96192 misc.py:20] ckpt/2_7b_2bit/15_down.pt proxy error: 0.0621008537709713
I0907 21:10:49.750303 93442 quantize_finetune_llama.py:211] computed original embedding for layer 16 in 27.416149377822876s, pre msv 0.5519701838493347, post msv 0.5729906558990479
I0907 21:10:49.861498 93442 quantize_finetune_llama.py:179] layer 17 gpu 0
W0907 21:10:51.180858 96385 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 21:10:51.205876 96385 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 21:10:51.318773 96385 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 21:10:51.318905 96385 utils.py:164] NumExpr defaulting to 16 threads.
I0907 21:10:51.483268 96385 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 21:10:51.726589 96385 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 21:11:11.020904 96385 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 21:11:12.409717 96385 quip.py:388] mean square of W: 1.0
I0907 21:11:12.410169 96385 quip.py:389] mean square of Wr: 0.9999999403953552
I0907 21:11:12.419121 96385 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 21:11:12.419517 96385 quip.py:391] max abs of Hr: 2.4932186603546143
I0907 21:11:12.428208 96385 quip.py:392] min diag of Lhr: 0.6478118300437927
I0907 21:11:24.961799 96385 misc.py:19] ckpt/2_7b_2bit/16_qkv.pt frob  error: 0.12187284231185913
I0907 21:11:24.962024 96385 misc.py:20] ckpt/2_7b_2bit/16_qkv.pt proxy error: 0.014540874399244785
I0907 21:11:32.011398 96385 finetune.py:25] layer 16_qkv initial loss 0.0021391853224486113
W0907 21:11:32.011662 96385 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 21:11:32.145701 96385 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 21:11:44.792641 96385 finetune.py:49] layer 16_qkv @ epoch 0 new loss 0.001184662338346243 old loss 0.0021391853224486113 BETTER
I0907 21:11:57.262565 96385 finetune.py:49] layer 16_qkv @ epoch 1 new loss 0.0011340848868712783 old loss 0.001184662338346243 BETTER
I0907 21:12:09.867932 96385 finetune.py:49] layer 16_qkv @ epoch 2 new loss 0.001129998010583222 old loss 0.0011340848868712783 BETTER
I0907 21:12:22.703992 96385 finetune.py:56] layer 16_qkv @ epoch 3 new loss 0.0011352191213518381 old loss 0.001129998010583222 WORSE
I0907 21:12:35.180721 96385 finetune.py:49] layer 16_qkv @ epoch 4 new loss 0.0010912275174632668 old loss 0.001129998010583222 BETTER
I0907 21:12:36.870714 96385 quip.py:388] mean square of W: 1.0000001192092896
I0907 21:12:36.871037 96385 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 21:12:36.871526 96385 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 21:12:36.871809 96385 quip.py:391] max abs of Hr: 1.3034236431121826
I0907 21:12:36.871916 96385 quip.py:392] min diag of Lhr: 0.5550949573516846
I0907 21:12:47.972606 96385 misc.py:19] ckpt/2_7b_2bit/16_o.pt frob  error: 0.1463712751865387
I0907 21:12:47.972849 96385 misc.py:20] ckpt/2_7b_2bit/16_o.pt proxy error: 0.04667866975069046
I0907 21:12:53.736480 96385 finetune.py:25] layer 16_o initial loss 0.002301044762134552
I0907 21:13:06.119723 96385 finetune.py:49] layer 16_o @ epoch 0 new loss 0.002047297079116106 old loss 0.002301044762134552 BETTER
I0907 21:13:18.588907 96385 finetune.py:49] layer 16_o @ epoch 1 new loss 0.001991866622120142 old loss 0.002047297079116106 BETTER
I0907 21:13:30.836415 96385 finetune.py:49] layer 16_o @ epoch 2 new loss 0.001953505678102374 old loss 0.001991866622120142 BETTER
I0907 21:13:42.958168 96385 finetune.py:49] layer 16_o @ epoch 3 new loss 0.0019426381913945079 old loss 0.001953505678102374 BETTER
I0907 21:13:55.390887 96385 finetune.py:49] layer 16_o @ epoch 4 new loss 0.001927066477946937 old loss 0.0019426381913945079 BETTER
I0907 21:13:57.672375 96385 quip.py:388] mean square of W: 1.0
I0907 21:13:57.673023 96385 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 21:13:57.673513 96385 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 21:13:57.673804 96385 quip.py:391] max abs of Hr: 1.520342469215393
I0907 21:13:57.673909 96385 quip.py:392] min diag of Lhr: 0.7342354655265808
I0907 21:14:14.002612 96385 misc.py:19] ckpt/2_7b_2bit/16_up.pt frob  error: 0.12170568853616714
I0907 21:14:14.002859 96385 misc.py:20] ckpt/2_7b_2bit/16_up.pt proxy error: 0.03233160078525543
I0907 21:14:18.342524 96385 finetune.py:25] layer 16_up initial loss 0.004527225624769926
I0907 21:14:30.260004 96385 finetune.py:49] layer 16_up @ epoch 0 new loss 0.004366906825453043 old loss 0.004527225624769926 BETTER
I0907 21:14:42.109312 96385 finetune.py:49] layer 16_up @ epoch 1 new loss 0.00430185254663229 old loss 0.004366906825453043 BETTER
I0907 21:14:54.171956 96385 finetune.py:49] layer 16_up @ epoch 2 new loss 0.004263340495526791 old loss 0.00430185254663229 BETTER
I0907 21:15:06.199338 96385 finetune.py:49] layer 16_up @ epoch 3 new loss 0.004239237867295742 old loss 0.004263340495526791 BETTER
I0907 21:15:18.206009 96385 finetune.py:49] layer 16_up @ epoch 4 new loss 0.004221638664603233 old loss 0.004239237867295742 BETTER
I0907 21:15:22.787040 96385 quip.py:388] mean square of W: 1.0
I0907 21:15:22.787499 96385 quip.py:389] mean square of Wr: 1.0
I0907 21:15:22.789553 96385 quip.py:390] difference between Hr and Hr.T: 4.76837158203125e-07
I0907 21:15:22.790137 96385 quip.py:391] max abs of Hr: 1.4748960733413696
I0907 21:15:22.790269 96385 quip.py:392] min diag of Lhr: 0.7339634299278259
I0907 21:15:51.845374 96385 misc.py:19] ckpt/2_7b_2bit/16_down.pt frob  error: 0.12191786617040634
I0907 21:15:51.845599 96385 misc.py:20] ckpt/2_7b_2bit/16_down.pt proxy error: 0.06212597340345383
I0907 21:16:23.221492 93442 quantize_finetune_llama.py:211] computed original embedding for layer 17 in 28.6826913356781s, pre msv 0.5729906558990479, post msv 0.5887737274169922
I0907 21:16:23.323075 93442 quantize_finetune_llama.py:179] layer 18 gpu 0
W0907 21:16:24.638849 96566 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 21:16:24.663400 96566 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 21:16:24.773604 96566 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 21:16:24.773725 96566 utils.py:164] NumExpr defaulting to 16 threads.
I0907 21:16:24.984125 96566 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 21:16:25.307698 96566 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 21:16:44.545265 96566 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 21:16:45.993307 96566 quip.py:388] mean square of W: 1.0
I0907 21:16:45.993813 96566 quip.py:389] mean square of Wr: 1.0
I0907 21:16:46.002978 96566 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 21:16:46.003414 96566 quip.py:391] max abs of Hr: 2.133504867553711
I0907 21:16:46.012338 96566 quip.py:392] min diag of Lhr: 0.6615934371948242
I0907 21:16:57.896034 96566 misc.py:19] ckpt/2_7b_2bit/17_qkv.pt frob  error: 0.12286293506622314
I0907 21:16:57.896194 96566 misc.py:20] ckpt/2_7b_2bit/17_qkv.pt proxy error: 0.015716852620244026
I0907 21:17:04.740739 96566 finetune.py:25] layer 17_qkv initial loss 0.0021403415594249964
W0907 21:17:04.741003 96566 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 21:17:04.874969 96566 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 21:17:17.297332 96566 finetune.py:49] layer 17_qkv @ epoch 0 new loss 0.0010406101355329156 old loss 0.0021403415594249964 BETTER
I0907 21:17:29.851322 96566 finetune.py:49] layer 17_qkv @ epoch 1 new loss 0.0009820939740166068 old loss 0.0010406101355329156 BETTER
I0907 21:17:42.764017 96566 finetune.py:56] layer 17_qkv @ epoch 2 new loss 0.000983045669272542 old loss 0.0009820939740166068 WORSE
I0907 21:17:55.561394 96566 finetune.py:49] layer 17_qkv @ epoch 3 new loss 0.0009765838040038943 old loss 0.0009820939740166068 BETTER
I0907 21:18:08.459665 96566 finetune.py:49] layer 17_qkv @ epoch 4 new loss 0.0009561534388922155 old loss 0.0009765838040038943 BETTER
I0907 21:18:10.388360 96566 quip.py:388] mean square of W: 0.9999998807907104
I0907 21:18:10.388681 96566 quip.py:389] mean square of Wr: 0.9999998211860657
I0907 21:18:10.389180 96566 quip.py:390] difference between Hr and Hr.T: 1.341104507446289e-07
I0907 21:18:10.389465 96566 quip.py:391] max abs of Hr: 1.2246818542480469
I0907 21:18:10.389570 96566 quip.py:392] min diag of Lhr: 0.5773912072181702
I0907 21:18:21.126627 96566 misc.py:19] ckpt/2_7b_2bit/17_o.pt frob  error: 0.1425255835056305
I0907 21:18:21.126865 96566 misc.py:20] ckpt/2_7b_2bit/17_o.pt proxy error: 0.05327035114169121
I0907 21:18:27.015599 96566 finetune.py:25] layer 17_o initial loss 0.0019338305573910475
I0907 21:18:39.656939 96566 finetune.py:49] layer 17_o @ epoch 0 new loss 0.0016826593782752752 old loss 0.0019338305573910475 BETTER
I0907 21:18:51.952830 96566 finetune.py:49] layer 17_o @ epoch 1 new loss 0.0016397625440731645 old loss 0.0016826593782752752 BETTER
I0907 21:19:04.419060 96566 finetune.py:49] layer 17_o @ epoch 2 new loss 0.0016183080151677132 old loss 0.0016397625440731645 BETTER
I0907 21:19:16.919941 96566 finetune.py:49] layer 17_o @ epoch 3 new loss 0.0016076667234301567 old loss 0.0016183080151677132 BETTER
I0907 21:19:29.547193 96566 finetune.py:49] layer 17_o @ epoch 4 new loss 0.0016041162889450788 old loss 0.0016076667234301567 BETTER
I0907 21:19:31.381546 96566 quip.py:388] mean square of W: 1.0
I0907 21:19:31.382178 96566 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 21:19:31.382663 96566 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 21:19:31.382948 96566 quip.py:391] max abs of Hr: 1.4836547374725342
I0907 21:19:31.383042 96566 quip.py:392] min diag of Lhr: 0.7538803815841675
I0907 21:19:47.149549 96566 misc.py:19] ckpt/2_7b_2bit/17_up.pt frob  error: 0.1210625097155571
I0907 21:19:47.149727 96566 misc.py:20] ckpt/2_7b_2bit/17_up.pt proxy error: 0.03571784123778343
I0907 21:19:51.335171 96566 finetune.py:25] layer 17_up initial loss 0.004620067775249481
I0907 21:20:02.971059 96566 finetune.py:49] layer 17_up @ epoch 0 new loss 0.004455298651009798 old loss 0.004620067775249481 BETTER
I0907 21:20:15.121726 96566 finetune.py:49] layer 17_up @ epoch 1 new loss 0.004389026667922735 old loss 0.004455298651009798 BETTER
I0907 21:20:27.344417 96566 finetune.py:49] layer 17_up @ epoch 2 new loss 0.004352125339210033 old loss 0.004389026667922735 BETTER
I0907 21:20:39.575661 96566 finetune.py:49] layer 17_up @ epoch 3 new loss 0.004329972434788942 old loss 0.004352125339210033 BETTER
I0907 21:20:51.991276 96566 finetune.py:49] layer 17_up @ epoch 4 new loss 0.00431478675454855 old loss 0.004329972434788942 BETTER
I0907 21:20:56.471213 96566 quip.py:388] mean square of W: 0.9999998807907104
I0907 21:20:56.471675 96566 quip.py:389] mean square of Wr: 1.0
I0907 21:20:56.473723 96566 quip.py:390] difference between Hr and Hr.T: 3.2782554626464844e-07
I0907 21:20:56.474311 96566 quip.py:391] max abs of Hr: 1.357744812965393
I0907 21:20:56.474452 96566 quip.py:392] min diag of Lhr: 0.7581937313079834
I0907 21:21:25.691092 96566 misc.py:19] ckpt/2_7b_2bit/17_down.pt frob  error: 0.12052622437477112
I0907 21:21:25.691341 96566 misc.py:20] ckpt/2_7b_2bit/17_down.pt proxy error: 0.0643136277794838
I0907 21:21:57.917350 93442 quantize_finetune_llama.py:211] computed original embedding for layer 18 in 29.196656942367554s, pre msv 0.5887737274169922, post msv 0.6145440936088562
I0907 21:21:58.035969 93442 quantize_finetune_llama.py:179] layer 19 gpu 0
W0907 21:21:59.365168 96669 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 21:21:59.390587 96669 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 21:21:59.506027 96669 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 21:21:59.506175 96669 utils.py:164] NumExpr defaulting to 16 threads.
I0907 21:21:59.704207 96669 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 21:21:59.952188 96669 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 21:22:19.257731 96669 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 21:22:20.788625 96669 quip.py:388] mean square of W: 1.0
I0907 21:22:20.789108 96669 quip.py:389] mean square of Wr: 0.9999999403953552
I0907 21:22:20.798071 96669 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 21:22:20.798479 96669 quip.py:391] max abs of Hr: 2.2570996284484863
I0907 21:22:20.807202 96669 quip.py:392] min diag of Lhr: 0.6808871030807495
I0907 21:22:33.908194 96669 misc.py:19] ckpt/2_7b_2bit/18_qkv.pt frob  error: 0.1221802607178688
I0907 21:22:33.908425 96669 misc.py:20] ckpt/2_7b_2bit/18_qkv.pt proxy error: 0.016873866319656372
I0907 21:22:41.159359 96669 finetune.py:25] layer 18_qkv initial loss 0.0030581371393054724
W0907 21:22:41.159668 96669 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 21:22:41.308256 96669 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 21:22:54.262579 96669 finetune.py:49] layer 18_qkv @ epoch 0 new loss 0.0012669148854911327 old loss 0.0030581371393054724 BETTER
I0907 21:23:07.254914 96669 finetune.py:49] layer 18_qkv @ epoch 1 new loss 0.001197701203636825 old loss 0.0012669148854911327 BETTER
I0907 21:23:20.005610 96669 finetune.py:49] layer 18_qkv @ epoch 2 new loss 0.0011902627302333713 old loss 0.001197701203636825 BETTER
I0907 21:23:33.167274 96669 finetune.py:56] layer 18_qkv @ epoch 3 new loss 0.0012001119321212173 old loss 0.0011902627302333713 WORSE
I0907 21:23:46.171484 96669 finetune.py:49] layer 18_qkv @ epoch 4 new loss 0.0011543508153408766 old loss 0.0011902627302333713 BETTER
I0907 21:23:47.683957 96669 quip.py:388] mean square of W: 0.9999998807907104
I0907 21:23:47.684219 96669 quip.py:389] mean square of Wr: 0.9999998807907104
I0907 21:23:47.684692 96669 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 21:23:47.684981 96669 quip.py:391] max abs of Hr: 1.2546857595443726
I0907 21:23:47.685081 96669 quip.py:392] min diag of Lhr: 0.5906053185462952
I0907 21:23:57.595531 96669 misc.py:19] ckpt/2_7b_2bit/18_o.pt frob  error: 0.14037005603313446
I0907 21:23:57.595705 96669 misc.py:20] ckpt/2_7b_2bit/18_o.pt proxy error: 0.05053688958287239
I0907 21:24:03.313975 96669 finetune.py:25] layer 18_o initial loss 0.002383213257417083
I0907 21:24:15.686941 96669 finetune.py:49] layer 18_o @ epoch 0 new loss 0.002038805978372693 old loss 0.002383213257417083 BETTER
I0907 21:24:28.184236 96669 finetune.py:49] layer 18_o @ epoch 1 new loss 0.0019918486941605806 old loss 0.002038805978372693 BETTER
I0907 21:24:40.553025 96669 finetune.py:49] layer 18_o @ epoch 2 new loss 0.0019642591942101717 old loss 0.0019918486941605806 BETTER
I0907 21:24:53.094242 96669 finetune.py:49] layer 18_o @ epoch 3 new loss 0.0019525341922417283 old loss 0.0019642591942101717 BETTER
I0907 21:25:05.774526 96669 finetune.py:56] layer 18_o @ epoch 4 new loss 0.001955935498699546 old loss 0.0019525341922417283 WORSE
I0907 21:25:08.006534 96669 quip.py:388] mean square of W: 1.0
I0907 21:25:08.007153 96669 quip.py:389] mean square of Wr: 1.0
I0907 21:25:08.007636 96669 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 21:25:08.007927 96669 quip.py:391] max abs of Hr: 1.3511686325073242
I0907 21:25:08.008028 96669 quip.py:392] min diag of Lhr: 0.7524869441986084
I0907 21:25:25.187794 96669 misc.py:19] ckpt/2_7b_2bit/18_up.pt frob  error: 0.12147501856088638
I0907 21:25:25.187979 96669 misc.py:20] ckpt/2_7b_2bit/18_up.pt proxy error: 0.03828033432364464
I0907 21:25:29.593627 96669 finetune.py:25] layer 18_up initial loss 0.005579861346632242
I0907 21:25:41.503909 96669 finetune.py:49] layer 18_up @ epoch 0 new loss 0.005401776172220707 old loss 0.005579861346632242 BETTER
I0907 21:25:53.349075 96669 finetune.py:49] layer 18_up @ epoch 1 new loss 0.0053311861120164394 old loss 0.005401776172220707 BETTER
I0907 21:26:05.245036 96669 finetune.py:49] layer 18_up @ epoch 2 new loss 0.00529263261705637 old loss 0.0053311861120164394 BETTER
I0907 21:26:17.366267 96669 finetune.py:49] layer 18_up @ epoch 3 new loss 0.005268119741231203 old loss 0.00529263261705637 BETTER
I0907 21:26:29.122972 96669 finetune.py:49] layer 18_up @ epoch 4 new loss 0.005251675378531218 old loss 0.005268119741231203 BETTER
I0907 21:26:33.435878 96669 quip.py:388] mean square of W: 1.0
I0907 21:26:33.436327 96669 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 21:26:33.438365 96669 quip.py:390] difference between Hr and Hr.T: 3.5762786865234375e-07
I0907 21:26:33.438969 96669 quip.py:391] max abs of Hr: 1.559623122215271
I0907 21:26:33.439101 96669 quip.py:392] min diag of Lhr: 0.7522649168968201
I0907 21:27:02.852799 96669 misc.py:19] ckpt/2_7b_2bit/18_down.pt frob  error: 0.12027180194854736
I0907 21:27:02.852976 96669 misc.py:20] ckpt/2_7b_2bit/18_down.pt proxy error: 0.06351013481616974
I0907 21:27:34.910571 93442 quantize_finetune_llama.py:211] computed original embedding for layer 19 in 29.270206689834595s, pre msv 0.6145440936088562, post msv 0.651021420955658
I0907 21:27:35.012055 93442 quantize_finetune_llama.py:179] layer 20 gpu 0
W0907 21:27:36.331799 96771 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 21:27:36.356347 96771 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 21:27:36.467077 96771 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 21:27:36.467186 96771 utils.py:164] NumExpr defaulting to 16 threads.
I0907 21:27:36.628698 96771 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 21:27:36.912443 96771 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 21:27:56.516912 96771 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 21:27:58.139781 96771 quip.py:388] mean square of W: 1.0
I0907 21:27:58.140286 96771 quip.py:389] mean square of Wr: 1.0
I0907 21:27:58.149484 96771 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 21:27:58.149979 96771 quip.py:391] max abs of Hr: 2.4189491271972656
I0907 21:27:58.158996 96771 quip.py:392] min diag of Lhr: 0.6834058165550232
I0907 21:28:11.030518 96771 misc.py:19] ckpt/2_7b_2bit/19_qkv.pt frob  error: 0.12281321734189987
I0907 21:28:11.030701 96771 misc.py:20] ckpt/2_7b_2bit/19_qkv.pt proxy error: 0.016790011897683144
I0907 21:28:18.049057 96771 finetune.py:25] layer 19_qkv initial loss 0.0030680785421282053
W0907 21:28:18.049371 96771 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 21:28:18.183826 96771 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 21:28:30.672214 96771 finetune.py:49] layer 19_qkv @ epoch 0 new loss 0.0011993148364126682 old loss 0.0030680785421282053 BETTER
I0907 21:28:42.856353 96771 finetune.py:49] layer 19_qkv @ epoch 1 new loss 0.001129521755501628 old loss 0.0011993148364126682 BETTER
I0907 21:28:55.515211 96771 finetune.py:49] layer 19_qkv @ epoch 2 new loss 0.0011226969072595239 old loss 0.001129521755501628 BETTER
I0907 21:29:08.065113 96771 finetune.py:56] layer 19_qkv @ epoch 3 new loss 0.0011294553987681866 old loss 0.0011226969072595239 WORSE
I0907 21:29:20.741572 96771 finetune.py:56] layer 19_qkv @ epoch 4 new loss 0.001136056613177061 old loss 0.0011226969072595239 WORSE
I0907 21:29:22.433506 96771 quip.py:388] mean square of W: 1.0
I0907 21:29:22.433826 96771 quip.py:389] mean square of Wr: 1.0
I0907 21:29:22.434318 96771 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 21:29:22.434606 96771 quip.py:391] max abs of Hr: 1.3877114057540894
I0907 21:29:22.434724 96771 quip.py:392] min diag of Lhr: 0.6172828078269958
I0907 21:29:33.118651 96771 misc.py:19] ckpt/2_7b_2bit/19_o.pt frob  error: 0.13703089952468872
I0907 21:29:33.118872 96771 misc.py:20] ckpt/2_7b_2bit/19_o.pt proxy error: 0.052702002227306366
I0907 21:29:38.883577 96771 finetune.py:25] layer 19_o initial loss 0.0023277688305824995
I0907 21:29:51.896035 96771 finetune.py:49] layer 19_o @ epoch 0 new loss 0.0020003935787826777 old loss 0.0023277688305824995 BETTER
I0907 21:30:04.477075 96771 finetune.py:49] layer 19_o @ epoch 1 new loss 0.0019578742794692516 old loss 0.0020003935787826777 BETTER
I0907 21:30:17.158980 96771 finetune.py:49] layer 19_o @ epoch 2 new loss 0.0019424458732828498 old loss 0.0019578742794692516 BETTER
I0907 21:30:29.764894 96771 finetune.py:49] layer 19_o @ epoch 3 new loss 0.0019313885131850839 old loss 0.0019424458732828498 BETTER
I0907 21:30:42.278778 96771 finetune.py:49] layer 19_o @ epoch 4 new loss 0.001923980307765305 old loss 0.0019313885131850839 BETTER
I0907 21:30:44.645277 96771 quip.py:388] mean square of W: 1.0
I0907 21:30:44.645930 96771 quip.py:389] mean square of Wr: 1.0
I0907 21:30:44.646435 96771 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 21:30:44.646723 96771 quip.py:391] max abs of Hr: 1.4347660541534424
I0907 21:30:44.646829 96771 quip.py:392] min diag of Lhr: 0.7564179301261902
I0907 21:31:01.924969 96771 misc.py:19] ckpt/2_7b_2bit/19_up.pt frob  error: 0.12103372067213058
I0907 21:31:01.925231 96771 misc.py:20] ckpt/2_7b_2bit/19_up.pt proxy error: 0.041196878999471664
I0907 21:31:06.377063 96771 finetune.py:25] layer 19_up initial loss 0.006128231529146433
I0907 21:31:18.690907 96771 finetune.py:49] layer 19_up @ epoch 0 new loss 0.005934165325015783 old loss 0.006128231529146433 BETTER
I0907 21:31:30.659665 96771 finetune.py:49] layer 19_up @ epoch 1 new loss 0.005859964061528444 old loss 0.005934165325015783 BETTER
I0907 21:31:42.918402 96771 finetune.py:49] layer 19_up @ epoch 2 new loss 0.0058214678429067135 old loss 0.005859964061528444 BETTER
I0907 21:31:54.920539 96771 finetune.py:49] layer 19_up @ epoch 3 new loss 0.0057980273850262165 old loss 0.0058214678429067135 BETTER
I0907 21:32:06.709616 96771 finetune.py:49] layer 19_up @ epoch 4 new loss 0.005784410517662764 old loss 0.0057980273850262165 BETTER
I0907 21:32:10.903687 96771 quip.py:388] mean square of W: 0.9999998807907104
I0907 21:32:10.904095 96771 quip.py:389] mean square of Wr: 0.9999998807907104
I0907 21:32:10.906128 96771 quip.py:390] difference between Hr and Hr.T: 3.129243850708008e-07
I0907 21:32:10.906764 96771 quip.py:391] max abs of Hr: 1.3528770208358765
I0907 21:32:10.906902 96771 quip.py:392] min diag of Lhr: 0.7592872977256775
I0907 21:32:40.444191 96771 misc.py:19] ckpt/2_7b_2bit/19_down.pt frob  error: 0.11964874714612961
I0907 21:32:40.444414 96771 misc.py:20] ckpt/2_7b_2bit/19_down.pt proxy error: 0.06294611096382141
I0907 21:33:12.268677 93442 quantize_finetune_llama.py:211] computed original embedding for layer 20 in 28.899850606918335s, pre msv 0.651021420955658, post msv 0.7045366764068604
I0907 21:33:12.382233 93442 quantize_finetune_llama.py:179] layer 21 gpu 0
W0907 21:33:13.696521 97042 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 21:33:13.720324 97042 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 21:33:13.828490 97042 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 21:33:13.828595 97042 utils.py:164] NumExpr defaulting to 16 threads.
I0907 21:33:13.986101 97042 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 21:33:14.226377 97042 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 21:33:33.394914 97042 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 21:33:34.688226 97042 quip.py:388] mean square of W: 1.0
I0907 21:33:34.688623 97042 quip.py:389] mean square of Wr: 1.0
I0907 21:33:34.696752 97042 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 21:33:34.697122 97042 quip.py:391] max abs of Hr: 2.2784316539764404
I0907 21:33:34.705150 97042 quip.py:392] min diag of Lhr: 0.6877040863037109
I0907 21:33:46.187344 97042 misc.py:19] ckpt/2_7b_2bit/20_qkv.pt frob  error: 0.12267029285430908
I0907 21:33:46.187495 97042 misc.py:20] ckpt/2_7b_2bit/20_qkv.pt proxy error: 0.017303764820098877
I0907 21:33:53.114855 97042 finetune.py:25] layer 20_qkv initial loss 0.0034333853982388973
W0907 21:33:53.115045 97042 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 21:33:53.259638 97042 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 21:34:05.820307 97042 finetune.py:49] layer 20_qkv @ epoch 0 new loss 0.001429114956408739 old loss 0.0034333853982388973 BETTER
I0907 21:34:18.408132 97042 finetune.py:49] layer 20_qkv @ epoch 1 new loss 0.0013645159779116511 old loss 0.001429114956408739 BETTER
I0907 21:34:31.465792 97042 finetune.py:56] layer 20_qkv @ epoch 2 new loss 0.001373978448100388 old loss 0.0013645159779116511 WORSE
I0907 21:34:44.407470 97042 finetune.py:56] layer 20_qkv @ epoch 3 new loss 0.0014179946156218648 old loss 0.0013645159779116511 WORSE
I0907 21:34:57.074737 97042 finetune.py:49] layer 20_qkv @ epoch 4 new loss 0.0013515993487089872 old loss 0.0013645159779116511 BETTER
I0907 21:34:58.943701 97042 quip.py:388] mean square of W: 0.9999999403953552
I0907 21:34:58.944009 97042 quip.py:389] mean square of Wr: 1.0
I0907 21:34:58.944488 97042 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 21:34:58.944775 97042 quip.py:391] max abs of Hr: 1.4715776443481445
I0907 21:34:58.944878 97042 quip.py:392] min diag of Lhr: 0.5890703797340393
I0907 21:35:10.280866 97042 misc.py:19] ckpt/2_7b_2bit/20_o.pt frob  error: 0.13771989941596985
I0907 21:35:10.281090 97042 misc.py:20] ckpt/2_7b_2bit/20_o.pt proxy error: 0.038611769676208496
I0907 21:35:16.098429 97042 finetune.py:25] layer 20_o initial loss 0.002804758492857218
I0907 21:35:28.820871 97042 finetune.py:49] layer 20_o @ epoch 0 new loss 0.002414316637441516 old loss 0.002804758492857218 BETTER
I0907 21:35:41.300390 97042 finetune.py:49] layer 20_o @ epoch 1 new loss 0.002363947220146656 old loss 0.002414316637441516 BETTER
I0907 21:35:53.796824 97042 finetune.py:49] layer 20_o @ epoch 2 new loss 0.002338759135454893 old loss 0.002363947220146656 BETTER
I0907 21:36:06.310702 97042 finetune.py:49] layer 20_o @ epoch 3 new loss 0.0023224730975925922 old loss 0.002338759135454893 BETTER
I0907 21:36:18.943440 97042 finetune.py:49] layer 20_o @ epoch 4 new loss 0.002315349178388715 old loss 0.0023224730975925922 BETTER
I0907 21:36:21.106580 97042 quip.py:388] mean square of W: 1.0000001192092896
I0907 21:36:21.107234 97042 quip.py:389] mean square of Wr: 1.0
I0907 21:36:21.107723 97042 quip.py:390] difference between Hr and Hr.T: 1.1920928955078125e-07
I0907 21:36:21.108005 97042 quip.py:391] max abs of Hr: 1.3190252780914307
I0907 21:36:21.108106 97042 quip.py:392] min diag of Lhr: 0.750399649143219
I0907 21:36:38.607746 97042 misc.py:19] ckpt/2_7b_2bit/20_up.pt frob  error: 0.12186156958341599
I0907 21:36:38.607979 97042 misc.py:20] ckpt/2_7b_2bit/20_up.pt proxy error: 0.04009447246789932
I0907 21:36:43.047409 97042 finetune.py:25] layer 20_up initial loss 0.007275494746863842
I0907 21:36:55.087899 97042 finetune.py:49] layer 20_up @ epoch 0 new loss 0.00705958204343915 old loss 0.007275494746863842 BETTER
I0907 21:37:06.943077 97042 finetune.py:49] layer 20_up @ epoch 1 new loss 0.006977670826017857 old loss 0.00705958204343915 BETTER
I0907 21:37:18.861975 97042 finetune.py:49] layer 20_up @ epoch 2 new loss 0.006935517769306898 old loss 0.006977670826017857 BETTER
I0907 21:37:30.837657 97042 finetune.py:49] layer 20_up @ epoch 3 new loss 0.006912651937454939 old loss 0.006935517769306898 BETTER
I0907 21:37:42.667174 97042 finetune.py:49] layer 20_up @ epoch 4 new loss 0.006896392907947302 old loss 0.006912651937454939 BETTER
I0907 21:37:47.144159 97042 quip.py:388] mean square of W: 1.0
I0907 21:37:47.144581 97042 quip.py:389] mean square of Wr: 1.0
I0907 21:37:47.146593 97042 quip.py:390] difference between Hr and Hr.T: 4.172325134277344e-07
I0907 21:37:47.147188 97042 quip.py:391] max abs of Hr: 1.4464685916900635
I0907 21:37:47.147319 97042 quip.py:392] min diag of Lhr: 0.739830732345581
I0907 21:38:15.968288 97042 misc.py:19] ckpt/2_7b_2bit/20_down.pt frob  error: 0.12079466134309769
I0907 21:38:15.968513 97042 misc.py:20] ckpt/2_7b_2bit/20_down.pt proxy error: 0.059967584908008575
I0907 21:38:48.695212 93442 quantize_finetune_llama.py:211] computed original embedding for layer 21 in 29.556129932403564s, pre msv 0.7045366764068604, post msv 0.757072925567627
I0907 21:38:48.829132 93442 quantize_finetune_llama.py:179] layer 22 gpu 0
W0907 21:38:50.100797 97152 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 21:38:50.124675 97152 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 21:38:50.230296 97152 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 21:38:50.230399 97152 utils.py:164] NumExpr defaulting to 16 threads.
I0907 21:38:50.383566 97152 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 21:38:50.614851 97152 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 21:39:09.966092 97152 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 21:39:11.401925 97152 quip.py:388] mean square of W: 1.0
I0907 21:39:11.402385 97152 quip.py:389] mean square of Wr: 1.0
I0907 21:39:11.411341 97152 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 21:39:11.411742 97152 quip.py:391] max abs of Hr: 2.198132276535034
I0907 21:39:11.420429 97152 quip.py:392] min diag of Lhr: 0.6904162168502808
I0907 21:39:23.998641 97152 misc.py:19] ckpt/2_7b_2bit/21_qkv.pt frob  error: 0.1228722408413887
I0907 21:39:23.998882 97152 misc.py:20] ckpt/2_7b_2bit/21_qkv.pt proxy error: 0.018741045147180557
I0907 21:39:31.122890 97152 finetune.py:25] layer 21_qkv initial loss 0.004210314713418484
W0907 21:39:31.123177 97152 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 21:39:31.262982 97152 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 21:39:43.910214 97152 finetune.py:49] layer 21_qkv @ epoch 0 new loss 0.001380888163112104 old loss 0.004210314713418484 BETTER
I0907 21:39:56.209213 97152 finetune.py:49] layer 21_qkv @ epoch 1 new loss 0.0013071075081825256 old loss 0.001380888163112104 BETTER
I0907 21:40:08.915870 97152 finetune.py:56] layer 21_qkv @ epoch 2 new loss 0.0013084356905892491 old loss 0.0013071075081825256 WORSE
I0907 21:40:21.724456 97152 finetune.py:56] layer 21_qkv @ epoch 3 new loss 0.0013107160339131951 old loss 0.0013071075081825256 WORSE
I0907 21:40:34.437622 97152 finetune.py:49] layer 21_qkv @ epoch 4 new loss 0.0012942524626851082 old loss 0.0013071075081825256 BETTER
I0907 21:40:35.980244 97152 quip.py:388] mean square of W: 0.9999999403953552
I0907 21:40:35.980528 97152 quip.py:389] mean square of Wr: 1.0
I0907 21:40:35.981016 97152 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 21:40:35.981305 97152 quip.py:391] max abs of Hr: 1.305168628692627
I0907 21:40:35.981414 97152 quip.py:392] min diag of Lhr: 0.6413515210151672
I0907 21:40:46.556238 97152 misc.py:19] ckpt/2_7b_2bit/21_o.pt frob  error: 0.13617445528507233
I0907 21:40:46.556465 97152 misc.py:20] ckpt/2_7b_2bit/21_o.pt proxy error: 0.047980014234781265
I0907 21:40:52.293850 97152 finetune.py:25] layer 21_o initial loss 0.0028188249561935663
I0907 21:41:04.741319 97152 finetune.py:49] layer 21_o @ epoch 0 new loss 0.002348495414480567 old loss 0.0028188249561935663 BETTER
I0907 21:41:17.230828 97152 finetune.py:49] layer 21_o @ epoch 1 new loss 0.0023088075686246157 old loss 0.002348495414480567 BETTER
I0907 21:41:29.707191 97152 finetune.py:49] layer 21_o @ epoch 2 new loss 0.002287684939801693 old loss 0.0023088075686246157 BETTER
I0907 21:41:42.675232 97152 finetune.py:49] layer 21_o @ epoch 3 new loss 0.0022760883439332247 old loss 0.002287684939801693 BETTER
I0907 21:41:55.145400 97152 finetune.py:49] layer 21_o @ epoch 4 new loss 0.002275766571983695 old loss 0.0022760883439332247 BETTER
I0907 21:41:57.061347 97152 quip.py:388] mean square of W: 1.0
I0907 21:41:57.061976 97152 quip.py:389] mean square of Wr: 1.0
I0907 21:41:57.062452 97152 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 21:41:57.062742 97152 quip.py:391] max abs of Hr: 1.2897318601608276
I0907 21:41:57.062840 97152 quip.py:392] min diag of Lhr: 0.7578758001327515
I0907 21:42:12.655777 97152 misc.py:19] ckpt/2_7b_2bit/21_up.pt frob  error: 0.12144787609577179
I0907 21:42:12.655941 97152 misc.py:20] ckpt/2_7b_2bit/21_up.pt proxy error: 0.04323432967066765
I0907 21:42:16.929248 97152 finetune.py:25] layer 21_up initial loss 0.007801781874150038
I0907 21:42:28.432097 97152 finetune.py:49] layer 21_up @ epoch 0 new loss 0.007605480961501598 old loss 0.007801781874150038 BETTER
I0907 21:42:40.022051 97152 finetune.py:49] layer 21_up @ epoch 1 new loss 0.007531697861850262 old loss 0.007605480961501598 BETTER
I0907 21:42:51.716791 97152 finetune.py:49] layer 21_up @ epoch 2 new loss 0.007495535537600517 old loss 0.007531697861850262 BETTER
I0907 21:43:03.664869 97152 finetune.py:49] layer 21_up @ epoch 3 new loss 0.007472018711268902 old loss 0.007495535537600517 BETTER
I0907 21:43:15.327377 97152 finetune.py:49] layer 21_up @ epoch 4 new loss 0.007462498266249895 old loss 0.007472018711268902 BETTER
I0907 21:43:19.582780 97152 quip.py:388] mean square of W: 1.0000001192092896
I0907 21:43:19.583179 97152 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 21:43:19.585125 97152 quip.py:390] difference between Hr and Hr.T: 3.5762786865234375e-07
I0907 21:43:19.585738 97152 quip.py:391] max abs of Hr: 1.4608759880065918
I0907 21:43:19.585864 97152 quip.py:392] min diag of Lhr: 0.7592426538467407
I0907 21:43:48.614562 97152 misc.py:19] ckpt/2_7b_2bit/21_down.pt frob  error: 0.1194692999124527
I0907 21:43:48.614818 97152 misc.py:20] ckpt/2_7b_2bit/21_down.pt proxy error: 0.06237852945923805
I0907 21:44:19.167721 93442 quantize_finetune_llama.py:211] computed original embedding for layer 22 in 27.969860315322876s, pre msv 0.757072925567627, post msv 0.8597317337989807
I0907 21:44:19.278216 93442 quantize_finetune_llama.py:179] layer 23 gpu 0
W0907 21:44:20.620360 97251 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 21:44:20.645691 97251 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 21:44:20.761601 97251 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 21:44:20.761758 97251 utils.py:164] NumExpr defaulting to 16 threads.
I0907 21:44:20.926292 97251 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 21:44:21.169229 97251 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 21:44:40.669767 97251 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 21:44:42.164748 97251 quip.py:388] mean square of W: 1.0000001192092896
I0907 21:44:42.165240 97251 quip.py:389] mean square of Wr: 1.0
I0907 21:44:42.174174 97251 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 21:44:42.174596 97251 quip.py:391] max abs of Hr: 2.0892131328582764
I0907 21:44:42.183673 97251 quip.py:392] min diag of Lhr: 0.6941390633583069
I0907 21:44:54.045710 97251 misc.py:19] ckpt/2_7b_2bit/22_qkv.pt frob  error: 0.12218113988637924
I0907 21:44:54.045938 97251 misc.py:20] ckpt/2_7b_2bit/22_qkv.pt proxy error: 0.018732160329818726
I0907 21:45:01.012121 97251 finetune.py:25] layer 22_qkv initial loss 0.005087903700768948
W0907 21:45:01.012408 97251 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 21:45:01.152319 97251 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 21:45:14.327252 97251 finetune.py:49] layer 22_qkv @ epoch 0 new loss 0.0019457078306004405 old loss 0.005087903700768948 BETTER
I0907 21:45:27.133789 97251 finetune.py:49] layer 22_qkv @ epoch 1 new loss 0.0018444614252075553 old loss 0.0019457078306004405 BETTER
I0907 21:45:39.932657 97251 finetune.py:49] layer 22_qkv @ epoch 2 new loss 0.0018241049256175756 old loss 0.0018444614252075553 BETTER
I0907 21:45:52.731510 97251 finetune.py:49] layer 22_qkv @ epoch 3 new loss 0.0017979053081944585 old loss 0.0018241049256175756 BETTER
I0907 21:46:05.490963 97251 finetune.py:56] layer 22_qkv @ epoch 4 new loss 0.0018116567516699433 old loss 0.0017979053081944585 WORSE
I0907 21:46:07.621788 97251 quip.py:388] mean square of W: 1.0000001192092896
I0907 21:46:07.622117 97251 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 21:46:07.622599 97251 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 21:46:07.622892 97251 quip.py:391] max abs of Hr: 1.3479831218719482
I0907 21:46:07.623003 97251 quip.py:392] min diag of Lhr: 0.6185938119888306
I0907 21:46:19.041901 97251 misc.py:19] ckpt/2_7b_2bit/22_o.pt frob  error: 0.1367870569229126
I0907 21:46:19.042139 97251 misc.py:20] ckpt/2_7b_2bit/22_o.pt proxy error: 0.03860454261302948
I0907 21:46:24.822095 97251 finetune.py:25] layer 22_o initial loss 0.003607639577239752
I0907 21:46:36.973880 97251 finetune.py:49] layer 22_o @ epoch 0 new loss 0.0031096884049475193 old loss 0.003607639577239752 BETTER
I0907 21:46:49.507577 97251 finetune.py:49] layer 22_o @ epoch 1 new loss 0.003049384569749236 old loss 0.0031096884049475193 BETTER
I0907 21:47:02.041121 97251 finetune.py:49] layer 22_o @ epoch 2 new loss 0.0030203089118003845 old loss 0.003049384569749236 BETTER
I0907 21:47:14.439781 97251 finetune.py:49] layer 22_o @ epoch 3 new loss 0.003008022904396057 old loss 0.0030203089118003845 BETTER
I0907 21:47:26.674442 97251 finetune.py:49] layer 22_o @ epoch 4 new loss 0.003001741599291563 old loss 0.003008022904396057 BETTER
I0907 21:47:28.953993 97251 quip.py:388] mean square of W: 1.0
I0907 21:47:28.954631 97251 quip.py:389] mean square of Wr: 1.0
I0907 21:47:28.955125 97251 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 21:47:28.955404 97251 quip.py:391] max abs of Hr: 1.3747978210449219
I0907 21:47:28.955509 97251 quip.py:392] min diag of Lhr: 0.7565540075302124
I0907 21:47:46.060622 97251 misc.py:19] ckpt/2_7b_2bit/22_up.pt frob  error: 0.12140502780675888
I0907 21:47:46.060865 97251 misc.py:20] ckpt/2_7b_2bit/22_up.pt proxy error: 0.04430711641907692
I0907 21:47:50.506392 97251 finetune.py:25] layer 22_up initial loss 0.009344509802758694
I0907 21:48:02.420242 97251 finetune.py:49] layer 22_up @ epoch 0 new loss 0.009139465168118477 old loss 0.009344509802758694 BETTER
I0907 21:48:14.236397 97251 finetune.py:49] layer 22_up @ epoch 1 new loss 0.009061215445399284 old loss 0.009139465168118477 BETTER
I0907 21:48:25.806479 97251 finetune.py:49] layer 22_up @ epoch 2 new loss 0.009025233797729015 old loss 0.009061215445399284 BETTER
I0907 21:48:37.354053 97251 finetune.py:49] layer 22_up @ epoch 3 new loss 0.009004280902445316 old loss 0.009025233797729015 BETTER
I0907 21:48:49.085627 97251 finetune.py:49] layer 22_up @ epoch 4 new loss 0.00899452343583107 old loss 0.009004280902445316 BETTER
I0907 21:48:53.146552 97251 quip.py:388] mean square of W: 1.0
I0907 21:48:53.147006 97251 quip.py:389] mean square of Wr: 1.0
I0907 21:48:53.149027 97251 quip.py:390] difference between Hr and Hr.T: 2.682209014892578e-07
I0907 21:48:53.149623 97251 quip.py:391] max abs of Hr: 1.4290611743927002
I0907 21:48:53.149757 97251 quip.py:392] min diag of Lhr: 0.7634076476097107
I0907 21:49:22.243561 97251 misc.py:19] ckpt/2_7b_2bit/22_down.pt frob  error: 0.11930669844150543
I0907 21:49:22.243759 97251 misc.py:20] ckpt/2_7b_2bit/22_down.pt proxy error: 0.06271766871213913
I0907 21:49:50.584318 93442 quantize_finetune_llama.py:211] computed original embedding for layer 23 in 25.78598713874817s, pre msv 0.8597317337989807, post msv 0.9156923890113831
I0907 21:49:50.692153 93442 quantize_finetune_llama.py:179] layer 24 gpu 0
W0907 21:49:52.019927 97349 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 21:49:52.044821 97349 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 21:49:52.156927 97349 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 21:49:52.157039 97349 utils.py:164] NumExpr defaulting to 16 threads.
I0907 21:49:52.317554 97349 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 21:49:52.585666 97349 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 21:50:12.456939 97349 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 21:50:14.217255 97349 quip.py:388] mean square of W: 1.0
I0907 21:50:14.217779 97349 quip.py:389] mean square of Wr: 1.0
I0907 21:50:14.226716 97349 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 21:50:14.227148 97349 quip.py:391] max abs of Hr: 2.156716823577881
I0907 21:50:14.235928 97349 quip.py:392] min diag of Lhr: 0.7118406295776367
I0907 21:50:27.149572 97349 misc.py:19] ckpt/2_7b_2bit/23_qkv.pt frob  error: 0.12156560271978378
I0907 21:50:27.149771 97349 misc.py:20] ckpt/2_7b_2bit/23_qkv.pt proxy error: 0.02150779590010643
I0907 21:50:34.227665 97349 finetune.py:25] layer 23_qkv initial loss 0.006112816743552685
W0907 21:50:34.227956 97349 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 21:50:34.361409 97349 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 21:50:46.720257 97349 finetune.py:49] layer 23_qkv @ epoch 0 new loss 0.0020037733484059572 old loss 0.006112816743552685 BETTER
I0907 21:50:59.112158 97349 finetune.py:49] layer 23_qkv @ epoch 1 new loss 0.0018982681212946773 old loss 0.0020037733484059572 BETTER
I0907 21:51:11.372044 97349 finetune.py:49] layer 23_qkv @ epoch 2 new loss 0.0018821322591975331 old loss 0.0018982681212946773 BETTER
I0907 21:51:24.076499 97349 finetune.py:56] layer 23_qkv @ epoch 3 new loss 0.001885601319372654 old loss 0.0018821322591975331 WORSE
I0907 21:51:37.026673 97349 finetune.py:56] layer 23_qkv @ epoch 4 new loss 0.0018829210894182324 old loss 0.0018821322591975331 WORSE
I0907 21:51:38.935760 97349 quip.py:388] mean square of W: 1.0000001192092896
I0907 21:51:38.936106 97349 quip.py:389] mean square of Wr: 1.0
I0907 21:51:38.936606 97349 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 21:51:38.936903 97349 quip.py:391] max abs of Hr: 1.2221475839614868
I0907 21:51:38.937034 97349 quip.py:392] min diag of Lhr: 0.6682307124137878
I0907 21:51:50.120783 97349 misc.py:19] ckpt/2_7b_2bit/23_o.pt frob  error: 0.12993882596492767
I0907 21:51:50.121014 97349 misc.py:20] ckpt/2_7b_2bit/23_o.pt proxy error: 0.05484335497021675
I0907 21:51:55.894668 97349 finetune.py:25] layer 23_o initial loss 0.0038607667665928602
I0907 21:52:07.999912 97349 finetune.py:49] layer 23_o @ epoch 0 new loss 0.0032200829591602087 old loss 0.0038607667665928602 BETTER
I0907 21:52:20.399287 97349 finetune.py:49] layer 23_o @ epoch 1 new loss 0.00316919875331223 old loss 0.0032200829591602087 BETTER
I0907 21:52:32.968599 97349 finetune.py:49] layer 23_o @ epoch 2 new loss 0.0031470279209315777 old loss 0.00316919875331223 BETTER
I0907 21:52:44.940265 97349 finetune.py:49] layer 23_o @ epoch 3 new loss 0.0031392392702400684 old loss 0.0031470279209315777 BETTER
I0907 21:52:56.922364 97349 finetune.py:49] layer 23_o @ epoch 4 new loss 0.003131704870611429 old loss 0.0031392392702400684 BETTER
I0907 21:52:58.692865 97349 quip.py:388] mean square of W: 1.0
I0907 21:52:58.693461 97349 quip.py:389] mean square of Wr: 1.0
I0907 21:52:58.693929 97349 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 21:52:58.694219 97349 quip.py:391] max abs of Hr: 1.3645200729370117
I0907 21:52:58.694317 97349 quip.py:392] min diag of Lhr: 0.7626333236694336
I0907 21:53:14.820332 97349 misc.py:19] ckpt/2_7b_2bit/23_up.pt frob  error: 0.12051982432603836
I0907 21:53:14.820567 97349 misc.py:20] ckpt/2_7b_2bit/23_up.pt proxy error: 0.04764384776353836
I0907 21:53:19.001256 97349 finetune.py:25] layer 23_up initial loss 0.010321020148694515
I0907 21:53:30.424717 97349 finetune.py:49] layer 23_up @ epoch 0 new loss 0.010087919421494007 old loss 0.010321020148694515 BETTER
I0907 21:53:41.750244 97349 finetune.py:49] layer 23_up @ epoch 1 new loss 0.010010037571191788 old loss 0.010087919421494007 BETTER
I0907 21:53:53.257985 97349 finetune.py:49] layer 23_up @ epoch 2 new loss 0.009971807710826397 old loss 0.010010037571191788 BETTER
I0907 21:54:04.926797 97349 finetune.py:49] layer 23_up @ epoch 3 new loss 0.009954610839486122 old loss 0.009971807710826397 BETTER
I0907 21:54:16.725565 97349 finetune.py:49] layer 23_up @ epoch 4 new loss 0.009945884346961975 old loss 0.009954610839486122 BETTER
I0907 21:54:21.120704 97349 quip.py:388] mean square of W: 1.0
I0907 21:54:21.121136 97349 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 21:54:21.123127 97349 quip.py:390] difference between Hr and Hr.T: 2.2351741790771484e-07
I0907 21:54:21.123715 97349 quip.py:391] max abs of Hr: 1.3099653720855713
I0907 21:54:21.123846 97349 quip.py:392] min diag of Lhr: 0.779833197593689
I0907 21:54:50.765469 97349 misc.py:19] ckpt/2_7b_2bit/23_down.pt frob  error: 0.11813835054636002
I0907 21:54:50.765704 97349 misc.py:20] ckpt/2_7b_2bit/23_down.pt proxy error: 0.06530799716711044
I0907 21:55:20.557470 93442 quantize_finetune_llama.py:211] computed original embedding for layer 24 in 27.126183032989502s, pre msv 0.9156923890113831, post msv 1.03025221824646
I0907 21:55:20.663143 93442 quantize_finetune_llama.py:179] layer 25 gpu 0
W0907 21:55:22.023739 97450 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 21:55:22.048629 97450 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 21:55:22.159279 97450 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 21:55:22.159407 97450 utils.py:164] NumExpr defaulting to 16 threads.
I0907 21:55:22.321165 97450 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 21:55:22.562510 97450 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 21:55:41.714880 97450 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 21:55:43.237889 97450 quip.py:388] mean square of W: 1.0
I0907 21:55:43.238365 97450 quip.py:389] mean square of Wr: 1.0
I0907 21:55:43.247421 97450 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 21:55:43.247805 97450 quip.py:391] max abs of Hr: 2.0772621631622314
I0907 21:55:43.256586 97450 quip.py:392] min diag of Lhr: 0.7074922323226929
I0907 21:55:56.333399 97450 misc.py:19] ckpt/2_7b_2bit/24_qkv.pt frob  error: 0.12096720188856125
I0907 21:55:56.333624 97450 misc.py:20] ckpt/2_7b_2bit/24_qkv.pt proxy error: 0.02016747184097767
I0907 21:56:03.500127 97450 finetune.py:25] layer 24_qkv initial loss 0.0062503996305167675
W0907 21:56:03.500362 97450 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 21:56:03.632823 97450 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 21:56:16.208183 97450 finetune.py:49] layer 24_qkv @ epoch 0 new loss 0.002241129521280527 old loss 0.0062503996305167675 BETTER
I0907 21:56:28.753306 97450 finetune.py:49] layer 24_qkv @ epoch 1 new loss 0.0021117841824889183 old loss 0.002241129521280527 BETTER
I0907 21:56:41.483345 97450 finetune.py:49] layer 24_qkv @ epoch 2 new loss 0.002083544386550784 old loss 0.0021117841824889183 BETTER
I0907 21:56:53.947808 97450 finetune.py:56] layer 24_qkv @ epoch 3 new loss 0.002084250096231699 old loss 0.002083544386550784 WORSE
I0907 21:57:06.463346 97450 finetune.py:49] layer 24_qkv @ epoch 4 new loss 0.002078480552881956 old loss 0.002083544386550784 BETTER
I0907 21:57:08.267203 97450 quip.py:388] mean square of W: 1.0
I0907 21:57:08.267497 97450 quip.py:389] mean square of Wr: 1.0
I0907 21:57:08.267989 97450 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 21:57:08.268275 97450 quip.py:391] max abs of Hr: 1.6177985668182373
I0907 21:57:08.268385 97450 quip.py:392] min diag of Lhr: 0.6075031757354736
I0907 21:57:19.014216 97450 misc.py:19] ckpt/2_7b_2bit/24_o.pt frob  error: 0.13663524389266968
I0907 21:57:19.014439 97450 misc.py:20] ckpt/2_7b_2bit/24_o.pt proxy error: 0.03755628690123558
I0907 21:57:24.632334 97450 finetune.py:25] layer 24_o initial loss 0.004309183452278376
I0907 21:57:36.768537 97450 finetune.py:49] layer 24_o @ epoch 0 new loss 0.0036611035466194153 old loss 0.004309183452278376 BETTER
I0907 21:57:49.010411 97450 finetune.py:49] layer 24_o @ epoch 1 new loss 0.003608972765505314 old loss 0.0036611035466194153 BETTER
I0907 21:58:01.301195 97450 finetune.py:49] layer 24_o @ epoch 2 new loss 0.0035842841025441885 old loss 0.003608972765505314 BETTER
I0907 21:58:13.730317 97450 finetune.py:49] layer 24_o @ epoch 3 new loss 0.003576211165636778 old loss 0.0035842841025441885 BETTER
I0907 21:58:25.784816 97450 finetune.py:56] layer 24_o @ epoch 4 new loss 0.0035826072562485933 old loss 0.003576211165636778 WORSE
I0907 21:58:27.960555 97450 quip.py:388] mean square of W: 1.0
I0907 21:58:27.961211 97450 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 21:58:27.961697 97450 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 21:58:27.961986 97450 quip.py:391] max abs of Hr: 1.364305019378662
I0907 21:58:27.962091 97450 quip.py:392] min diag of Lhr: 0.7721542119979858
I0907 21:58:44.460760 97450 misc.py:19] ckpt/2_7b_2bit/24_up.pt frob  error: 0.11977854371070862
I0907 21:58:44.460979 97450 misc.py:20] ckpt/2_7b_2bit/24_up.pt proxy error: 0.048759929835796356
I0907 21:58:48.844615 97450 finetune.py:25] layer 24_up initial loss 0.01158829964697361
I0907 21:59:00.936326 97450 finetune.py:49] layer 24_up @ epoch 0 new loss 0.01133221760392189 old loss 0.01158829964697361 BETTER
I0907 21:59:13.394484 97450 finetune.py:49] layer 24_up @ epoch 1 new loss 0.01125876884907484 old loss 0.01133221760392189 BETTER
I0907 21:59:25.522091 97450 finetune.py:49] layer 24_up @ epoch 2 new loss 0.011224862188100815 old loss 0.01125876884907484 BETTER
I0907 21:59:37.490811 97450 finetune.py:49] layer 24_up @ epoch 3 new loss 0.011209075339138508 old loss 0.011224862188100815 BETTER
I0907 21:59:49.576652 97450 finetune.py:49] layer 24_up @ epoch 4 new loss 0.011207876726984978 old loss 0.011209075339138508 BETTER
I0907 21:59:53.845102 97450 quip.py:388] mean square of W: 0.9999998807907104
I0907 21:59:53.845518 97450 quip.py:389] mean square of Wr: 1.0
I0907 21:59:53.847519 97450 quip.py:390] difference between Hr and Hr.T: 3.2782554626464844e-07
I0907 21:59:53.848098 97450 quip.py:391] max abs of Hr: 1.406270146369934
I0907 21:59:53.848240 97450 quip.py:392] min diag of Lhr: 0.7838900089263916
I0907 22:00:22.988161 97450 misc.py:19] ckpt/2_7b_2bit/24_down.pt frob  error: 0.11729197949171066
I0907 22:00:22.988393 97450 misc.py:20] ckpt/2_7b_2bit/24_down.pt proxy error: 0.0661584809422493
I0907 22:00:54.531958 93442 quantize_finetune_llama.py:211] computed original embedding for layer 25 in 28.996448278427124s, pre msv 1.03025221824646, post msv 1.061630368232727
I0907 22:00:54.608855 93442 quantize_finetune_llama.py:179] layer 26 gpu 0
W0907 22:00:55.929739 97558 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 22:00:55.955153 97558 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 22:00:56.069233 97558 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 22:00:56.069370 97558 utils.py:164] NumExpr defaulting to 16 threads.
I0907 22:00:56.235509 97558 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 22:00:56.481038 97558 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 22:01:15.568184 97558 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 22:01:17.050063 97558 quip.py:388] mean square of W: 1.0
I0907 22:01:17.050557 97558 quip.py:389] mean square of Wr: 1.0
I0907 22:01:17.059462 97558 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 22:01:17.059876 97558 quip.py:391] max abs of Hr: 1.8664391040802002
I0907 22:01:17.068662 97558 quip.py:392] min diag of Lhr: 0.7298693060874939
I0907 22:01:30.081891 97558 misc.py:19] ckpt/2_7b_2bit/25_qkv.pt frob  error: 0.11977487057447433
I0907 22:01:30.082066 97558 misc.py:20] ckpt/2_7b_2bit/25_qkv.pt proxy error: 0.024048766121268272
I0907 22:01:36.988266 97558 finetune.py:25] layer 25_qkv initial loss 0.008305429480969906
W0907 22:01:36.988481 97558 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 22:01:37.128989 97558 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 22:01:49.721615 97558 finetune.py:49] layer 25_qkv @ epoch 0 new loss 0.0021900448482483625 old loss 0.008305429480969906 BETTER
I0907 22:02:02.246686 97558 finetune.py:49] layer 25_qkv @ epoch 1 new loss 0.0020595311652868986 old loss 0.0021900448482483625 BETTER
I0907 22:02:14.394256 97558 finetune.py:49] layer 25_qkv @ epoch 2 new loss 0.002033520955592394 old loss 0.0020595311652868986 BETTER
I0907 22:02:26.768374 97558 finetune.py:49] layer 25_qkv @ epoch 3 new loss 0.0020247662905603647 old loss 0.002033520955592394 BETTER
I0907 22:02:38.944051 97558 finetune.py:56] layer 25_qkv @ epoch 4 new loss 0.002033873461186886 old loss 0.0020247662905603647 WORSE
I0907 22:02:40.204528 97558 quip.py:388] mean square of W: 0.9999999403953552
I0907 22:02:40.204779 97558 quip.py:389] mean square of Wr: 0.9999999403953552
I0907 22:02:40.205254 97558 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 22:02:40.205538 97558 quip.py:391] max abs of Hr: 1.1817424297332764
I0907 22:02:40.205633 97558 quip.py:392] min diag of Lhr: 0.6199069619178772
I0907 22:02:49.481154 97558 misc.py:19] ckpt/2_7b_2bit/25_o.pt frob  error: 0.13964490592479706
I0907 22:02:49.481308 97558 misc.py:20] ckpt/2_7b_2bit/25_o.pt proxy error: 0.051738739013671875
I0907 22:02:54.989045 97558 finetune.py:25] layer 25_o initial loss 0.004298659972846508
I0907 22:03:07.014853 97558 finetune.py:49] layer 25_o @ epoch 0 new loss 0.003361060982570052 old loss 0.004298659972846508 BETTER
I0907 22:03:19.072288 97558 finetune.py:49] layer 25_o @ epoch 1 new loss 0.003311990527436137 old loss 0.003361060982570052 BETTER
I0907 22:03:31.355946 97558 finetune.py:49] layer 25_o @ epoch 2 new loss 0.003290469292551279 old loss 0.003311990527436137 BETTER
I0907 22:03:43.668576 97558 finetune.py:49] layer 25_o @ epoch 3 new loss 0.0032814908772706985 old loss 0.003290469292551279 BETTER
I0907 22:03:56.059472 97558 finetune.py:49] layer 25_o @ epoch 4 new loss 0.0032769259996712208 old loss 0.0032814908772706985 BETTER
I0907 22:03:58.434432 97558 quip.py:388] mean square of W: 1.0000001192092896
I0907 22:03:58.435088 97558 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 22:03:58.435580 97558 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 22:03:58.435894 97558 quip.py:391] max abs of Hr: 1.4669183492660522
I0907 22:03:58.436000 97558 quip.py:392] min diag of Lhr: 0.7797879576683044
I0907 22:04:15.184277 97558 misc.py:19] ckpt/2_7b_2bit/25_up.pt frob  error: 0.11873745173215866
I0907 22:04:15.184473 97558 misc.py:20] ckpt/2_7b_2bit/25_up.pt proxy error: 0.048510488122701645
I0907 22:04:19.431107 97558 finetune.py:25] layer 25_up initial loss 0.012359017506241798
I0907 22:04:31.135957 97558 finetune.py:49] layer 25_up @ epoch 0 new loss 0.012035471387207508 old loss 0.012359017506241798 BETTER
I0907 22:04:42.734071 97558 finetune.py:49] layer 25_up @ epoch 1 new loss 0.011947216466069221 old loss 0.012035471387207508 BETTER
I0907 22:04:54.663499 97558 finetune.py:49] layer 25_up @ epoch 2 new loss 0.011909477412700653 old loss 0.011947216466069221 BETTER
I0907 22:05:06.142817 97558 finetune.py:49] layer 25_up @ epoch 3 new loss 0.011888121254742146 old loss 0.011909477412700653 BETTER
I0907 22:05:17.708353 97558 finetune.py:49] layer 25_up @ epoch 4 new loss 0.011883622035384178 old loss 0.011888121254742146 BETTER
I0907 22:05:21.540055 97558 quip.py:388] mean square of W: 0.9999998807907104
I0907 22:05:21.540456 97558 quip.py:389] mean square of Wr: 1.0
I0907 22:05:21.542376 97558 quip.py:390] difference between Hr and Hr.T: 3.2782554626464844e-07
I0907 22:05:21.542968 97558 quip.py:391] max abs of Hr: 1.442578911781311
I0907 22:05:21.543086 97558 quip.py:392] min diag of Lhr: 0.7789568305015564
I0907 22:05:48.874917 97558 misc.py:19] ckpt/2_7b_2bit/25_down.pt frob  error: 0.1167798563838005
I0907 22:05:48.875149 97558 misc.py:20] ckpt/2_7b_2bit/25_down.pt proxy error: 0.06523388624191284
I0907 22:06:20.158642 93442 quantize_finetune_llama.py:211] computed original embedding for layer 26 in 28.75501799583435s, pre msv 1.061630368232727, post msv 1.2449901103973389
I0907 22:06:20.256808 93442 quantize_finetune_llama.py:179] layer 27 gpu 0
W0907 22:06:21.548263 97656 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 22:06:21.572826 97656 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 22:06:21.683585 97656 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 22:06:21.683708 97656 utils.py:164] NumExpr defaulting to 16 threads.
I0907 22:06:21.845277 97656 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 22:06:22.084268 97656 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 22:06:41.433585 97656 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 22:06:42.948743 97656 quip.py:388] mean square of W: 1.0
I0907 22:06:42.949220 97656 quip.py:389] mean square of Wr: 1.0
I0907 22:06:42.958168 97656 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 22:06:42.958567 97656 quip.py:391] max abs of Hr: 2.0835063457489014
I0907 22:06:42.967316 97656 quip.py:392] min diag of Lhr: 0.7282485365867615
I0907 22:06:54.953849 97656 misc.py:19] ckpt/2_7b_2bit/26_qkv.pt frob  error: 0.11932998150587082
I0907 22:06:54.954006 97656 misc.py:20] ckpt/2_7b_2bit/26_qkv.pt proxy error: 0.02137979492545128
I0907 22:07:01.892331 97656 finetune.py:25] layer 26_qkv initial loss 0.00942266546189785
W0907 22:07:01.892576 97656 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 22:07:02.025527 97656 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 22:07:14.527718 97656 finetune.py:49] layer 26_qkv @ epoch 0 new loss 0.003286222694441676 old loss 0.00942266546189785 BETTER
I0907 22:07:26.966358 97656 finetune.py:49] layer 26_qkv @ epoch 1 new loss 0.003110553603619337 old loss 0.003286222694441676 BETTER
I0907 22:07:39.619682 97656 finetune.py:49] layer 26_qkv @ epoch 2 new loss 0.003073042258620262 old loss 0.003110553603619337 BETTER
I0907 22:07:51.958896 97656 finetune.py:49] layer 26_qkv @ epoch 3 new loss 0.0030471852514892817 old loss 0.003073042258620262 BETTER
I0907 22:08:04.147736 97656 finetune.py:49] layer 26_qkv @ epoch 4 new loss 0.0030080177821218967 old loss 0.0030471852514892817 BETTER
I0907 22:08:05.531288 97656 quip.py:388] mean square of W: 0.9999999403953552
I0907 22:08:05.531550 97656 quip.py:389] mean square of Wr: 0.9999999403953552
I0907 22:08:05.532011 97656 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 22:08:05.532303 97656 quip.py:391] max abs of Hr: 1.7557786703109741
I0907 22:08:05.532402 97656 quip.py:392] min diag of Lhr: 0.5262109041213989
I0907 22:08:15.681287 97656 misc.py:19] ckpt/2_7b_2bit/26_o.pt frob  error: 0.15034542977809906
I0907 22:08:15.681452 97656 misc.py:20] ckpt/2_7b_2bit/26_o.pt proxy error: 0.031795091927051544
I0907 22:08:21.313568 97656 finetune.py:25] layer 26_o initial loss 0.006007015239447355
I0907 22:08:33.239427 97656 finetune.py:49] layer 26_o @ epoch 0 new loss 0.004955211654305458 old loss 0.006007015239447355 BETTER
I0907 22:08:45.256606 97656 finetune.py:49] layer 26_o @ epoch 1 new loss 0.0048818388022482395 old loss 0.004955211654305458 BETTER
I0907 22:08:57.148153 97656 finetune.py:49] layer 26_o @ epoch 2 new loss 0.004857946187257767 old loss 0.0048818388022482395 BETTER
I0907 22:09:09.366892 97656 finetune.py:49] layer 26_o @ epoch 3 new loss 0.004845365881919861 old loss 0.004857946187257767 BETTER
I0907 22:09:21.590283 97656 finetune.py:49] layer 26_o @ epoch 4 new loss 0.00484491465613246 old loss 0.004845365881919861 BETTER
I0907 22:09:23.372581 97656 quip.py:388] mean square of W: 1.0
I0907 22:09:23.373194 97656 quip.py:389] mean square of Wr: 1.0
I0907 22:09:23.373672 97656 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 22:09:23.373956 97656 quip.py:391] max abs of Hr: 1.439748764038086
I0907 22:09:23.374050 97656 quip.py:392] min diag of Lhr: 0.78035569190979
I0907 22:09:39.604329 97656 misc.py:19] ckpt/2_7b_2bit/26_up.pt frob  error: 0.11851143091917038
I0907 22:09:39.604487 97656 misc.py:20] ckpt/2_7b_2bit/26_up.pt proxy error: 0.04541286826133728
I0907 22:09:43.799746 97656 finetune.py:25] layer 26_up initial loss 0.015029540285468102
I0907 22:09:55.472543 97656 finetune.py:49] layer 26_up @ epoch 0 new loss 0.014652851037681103 old loss 0.015029540285468102 BETTER
I0907 22:10:06.943653 97656 finetune.py:49] layer 26_up @ epoch 1 new loss 0.014546314254403114 old loss 0.014652851037681103 BETTER
I0907 22:10:18.439005 97656 finetune.py:49] layer 26_up @ epoch 2 new loss 0.014498713426291943 old loss 0.014546314254403114 BETTER
I0907 22:10:30.090406 97656 finetune.py:49] layer 26_up @ epoch 3 new loss 0.014480854384601116 old loss 0.014498713426291943 BETTER
I0907 22:10:41.759825 97656 finetune.py:49] layer 26_up @ epoch 4 new loss 0.014475964941084385 old loss 0.014480854384601116 BETTER
I0907 22:10:45.817406 97656 quip.py:388] mean square of W: 0.9999998807907104
I0907 22:10:45.817842 97656 quip.py:389] mean square of Wr: 1.0
I0907 22:10:45.819851 97656 quip.py:390] difference between Hr and Hr.T: 4.76837158203125e-07
I0907 22:10:45.820440 97656 quip.py:391] max abs of Hr: 1.7635287046432495
I0907 22:10:45.820573 97656 quip.py:392] min diag of Lhr: 0.7702463865280151
I0907 22:11:14.791909 97656 misc.py:19] ckpt/2_7b_2bit/26_down.pt frob  error: 0.11704045534133911
I0907 22:11:14.792122 97656 misc.py:20] ckpt/2_7b_2bit/26_down.pt proxy error: 0.06596272438764572
I0907 22:11:45.992560 93442 quantize_finetune_llama.py:211] computed original embedding for layer 27 in 28.794605255126953s, pre msv 1.2449901103973389, post msv 1.3314666748046875
I0907 22:11:46.101566 93442 quantize_finetune_llama.py:179] layer 28 gpu 0
W0907 22:11:47.564438 97754 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 22:11:47.589046 97754 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 22:11:47.699840 97754 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 22:11:47.699945 97754 utils.py:164] NumExpr defaulting to 16 threads.
I0907 22:11:47.854832 97754 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 22:11:48.098890 97754 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 22:12:07.858699 97754 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 22:12:09.495819 97754 quip.py:388] mean square of W: 1.0000001192092896
I0907 22:12:09.496329 97754 quip.py:389] mean square of Wr: 1.0
I0907 22:12:09.505489 97754 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 22:12:09.505939 97754 quip.py:391] max abs of Hr: 2.0464706420898438
I0907 22:12:09.514808 97754 quip.py:392] min diag of Lhr: 0.749915361404419
I0907 22:12:22.457343 97754 misc.py:19] ckpt/2_7b_2bit/27_qkv.pt frob  error: 0.11865093559026718
I0907 22:12:22.457564 97754 misc.py:20] ckpt/2_7b_2bit/27_qkv.pt proxy error: 0.022875741124153137
I0907 22:12:29.641787 97754 finetune.py:25] layer 27_qkv initial loss 0.009937702678143978
W0907 22:12:29.642080 97754 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 22:12:29.782935 97754 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 22:12:42.129349 97754 finetune.py:49] layer 27_qkv @ epoch 0 new loss 0.0031687209848314524 old loss 0.009937702678143978 BETTER
I0907 22:12:54.358018 97754 finetune.py:49] layer 27_qkv @ epoch 1 new loss 0.002987668849527836 old loss 0.0031687209848314524 BETTER
I0907 22:13:06.803612 97754 finetune.py:49] layer 27_qkv @ epoch 2 new loss 0.002939093392342329 old loss 0.002987668849527836 BETTER
I0907 22:13:19.287246 97754 finetune.py:49] layer 27_qkv @ epoch 3 new loss 0.002927323803305626 old loss 0.002939093392342329 BETTER
I0907 22:13:31.433143 97754 finetune.py:49] layer 27_qkv @ epoch 4 new loss 0.0029133902862668037 old loss 0.002927323803305626 BETTER
I0907 22:13:33.015290 97754 quip.py:388] mean square of W: 1.0000001192092896
I0907 22:13:33.015612 97754 quip.py:389] mean square of Wr: 1.0
I0907 22:13:33.016119 97754 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 22:13:33.016406 97754 quip.py:391] max abs of Hr: 1.4339325428009033
I0907 22:13:33.016512 97754 quip.py:392] min diag of Lhr: 0.6060526371002197
I0907 22:13:44.286839 97754 misc.py:19] ckpt/2_7b_2bit/27_o.pt frob  error: 0.13605327904224396
I0907 22:13:44.287069 97754 misc.py:20] ckpt/2_7b_2bit/27_o.pt proxy error: 0.047007858753204346
I0907 22:13:50.086063 97754 finetune.py:25] layer 27_o initial loss 0.005535497330129147
I0907 22:14:02.711883 97754 finetune.py:49] layer 27_o @ epoch 0 new loss 0.0045725759118795395 old loss 0.005535497330129147 BETTER
I0907 22:14:15.224462 97754 finetune.py:49] layer 27_o @ epoch 1 new loss 0.004524309188127518 old loss 0.0045725759118795395 BETTER
I0907 22:14:27.489352 97754 finetune.py:49] layer 27_o @ epoch 2 new loss 0.004506803583353758 old loss 0.004524309188127518 BETTER
I0907 22:14:39.943835 97754 finetune.py:49] layer 27_o @ epoch 3 new loss 0.004489339888095856 old loss 0.004506803583353758 BETTER
I0907 22:14:52.244921 97754 finetune.py:49] layer 27_o @ epoch 4 new loss 0.0044836741872131824 old loss 0.004489339888095856 BETTER
I0907 22:14:54.790862 97754 quip.py:388] mean square of W: 1.0000001192092896
I0907 22:14:54.791542 97754 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 22:14:54.792034 97754 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 22:14:54.792314 97754 quip.py:391] max abs of Hr: 1.3430464267730713
I0907 22:14:54.792420 97754 quip.py:392] min diag of Lhr: 0.7821486592292786
I0907 22:15:11.562816 97754 misc.py:19] ckpt/2_7b_2bit/27_up.pt frob  error: 0.11797346919775009
I0907 22:15:11.563042 97754 misc.py:20] ckpt/2_7b_2bit/27_up.pt proxy error: 0.042310040444135666
I0907 22:15:15.872158 97754 finetune.py:25] layer 27_up initial loss 0.016080906614661217
I0907 22:15:27.955411 97754 finetune.py:49] layer 27_up @ epoch 0 new loss 0.015607315115630627 old loss 0.016080906614661217 BETTER
I0907 22:15:40.052888 97754 finetune.py:49] layer 27_up @ epoch 1 new loss 0.015474949963390827 old loss 0.015607315115630627 BETTER
I0907 22:15:51.960431 97754 finetune.py:49] layer 27_up @ epoch 2 new loss 0.015419094823300838 old loss 0.015474949963390827 BETTER
I0907 22:16:03.821460 97754 finetune.py:49] layer 27_up @ epoch 3 new loss 0.015389250591397285 old loss 0.015419094823300838 BETTER
I0907 22:16:15.860507 97754 finetune.py:49] layer 27_up @ epoch 4 new loss 0.015381142497062683 old loss 0.015389250591397285 BETTER
I0907 22:16:19.811163 97754 quip.py:388] mean square of W: 0.9999998807907104
I0907 22:16:19.811587 97754 quip.py:389] mean square of Wr: 0.9999998807907104
I0907 22:16:19.813566 97754 quip.py:390] difference between Hr and Hr.T: 1.430511474609375e-06
I0907 22:16:19.814141 97754 quip.py:391] max abs of Hr: 2.3616342544555664
I0907 22:16:19.814267 97754 quip.py:392] min diag of Lhr: 0.743256151676178
I0907 22:16:48.657717 97754 misc.py:19] ckpt/2_7b_2bit/27_down.pt frob  error: 0.11799664795398712
I0907 22:16:48.657893 97754 misc.py:20] ckpt/2_7b_2bit/27_down.pt proxy error: 0.06446824967861176
I0907 22:17:19.594280 93442 quantize_finetune_llama.py:211] computed original embedding for layer 28 in 28.596867322921753s, pre msv 1.3314666748046875, post msv 1.4843926429748535
I0907 22:17:19.694694 93442 quantize_finetune_llama.py:179] layer 29 gpu 0
W0907 22:17:21.085277 97855 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 22:17:21.110268 97855 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 22:17:21.225637 97855 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 22:17:21.225791 97855 utils.py:164] NumExpr defaulting to 16 threads.
I0907 22:17:21.390975 97855 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 22:17:21.640263 97855 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 22:17:41.287708 97855 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 22:17:42.691710 97855 quip.py:388] mean square of W: 1.0
I0907 22:17:42.692176 97855 quip.py:389] mean square of Wr: 1.0
I0907 22:17:42.701050 97855 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 22:17:42.701438 97855 quip.py:391] max abs of Hr: 2.0548174381256104
I0907 22:17:42.710099 97855 quip.py:392] min diag of Lhr: 0.7490612268447876
I0907 22:17:55.308130 97855 misc.py:19] ckpt/2_7b_2bit/28_qkv.pt frob  error: 0.11871210485696793
I0907 22:17:55.308365 97855 misc.py:20] ckpt/2_7b_2bit/28_qkv.pt proxy error: 0.022477325052022934
I0907 22:18:02.379765 97855 finetune.py:25] layer 28_qkv initial loss 0.011515777558088303
W0907 22:18:02.380053 97855 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 22:18:02.508006 97855 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 22:18:15.330926 97855 finetune.py:49] layer 28_qkv @ epoch 0 new loss 0.004105020314455032 old loss 0.011515777558088303 BETTER
I0907 22:18:27.737508 97855 finetune.py:49] layer 28_qkv @ epoch 1 new loss 0.003896674606949091 old loss 0.004105020314455032 BETTER
I0907 22:18:40.527347 97855 finetune.py:49] layer 28_qkv @ epoch 2 new loss 0.003867637598887086 old loss 0.003896674606949091 BETTER
I0907 22:18:52.915738 97855 finetune.py:56] layer 28_qkv @ epoch 3 new loss 0.0038709966465830803 old loss 0.003867637598887086 WORSE
I0907 22:19:05.125723 97855 finetune.py:49] layer 28_qkv @ epoch 4 new loss 0.003799954429268837 old loss 0.003867637598887086 BETTER
I0907 22:19:06.837947 97855 quip.py:388] mean square of W: 1.0
I0907 22:19:06.838238 97855 quip.py:389] mean square of Wr: 1.0
I0907 22:19:06.838721 97855 quip.py:390] difference between Hr and Hr.T: 1.341104507446289e-07
I0907 22:19:06.839011 97855 quip.py:391] max abs of Hr: 1.676134467124939
I0907 22:19:06.839110 97855 quip.py:392] min diag of Lhr: 0.5308661460876465
I0907 22:19:18.232188 97855 misc.py:19] ckpt/2_7b_2bit/28_o.pt frob  error: 0.14789605140686035
I0907 22:19:18.232426 97855 misc.py:20] ckpt/2_7b_2bit/28_o.pt proxy error: 0.03667762503027916
I0907 22:19:24.066004 97855 finetune.py:25] layer 28_o initial loss 0.007181445136666298
I0907 22:19:36.355110 97855 finetune.py:49] layer 28_o @ epoch 0 new loss 0.005930481944233179 old loss 0.007181445136666298 BETTER
I0907 22:19:48.538574 97855 finetune.py:49] layer 28_o @ epoch 1 new loss 0.00586184486746788 old loss 0.005930481944233179 BETTER
I0907 22:20:00.822965 97855 finetune.py:49] layer 28_o @ epoch 2 new loss 0.005822357255965471 old loss 0.00586184486746788 BETTER
I0907 22:20:12.975156 97855 finetune.py:49] layer 28_o @ epoch 3 new loss 0.005799362435936928 old loss 0.005822357255965471 BETTER
I0907 22:20:24.980140 97855 finetune.py:49] layer 28_o @ epoch 4 new loss 0.005791523493826389 old loss 0.005799362435936928 BETTER
I0907 22:20:27.057766 97855 quip.py:388] mean square of W: 1.0
I0907 22:20:27.058420 97855 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 22:20:27.058910 97855 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 22:20:27.059198 97855 quip.py:391] max abs of Hr: 1.5929301977157593
I0907 22:20:27.059303 97855 quip.py:392] min diag of Lhr: 0.7772799730300903
I0907 22:20:43.611135 97855 misc.py:19] ckpt/2_7b_2bit/28_up.pt frob  error: 0.1177748516201973
I0907 22:20:43.611361 97855 misc.py:20] ckpt/2_7b_2bit/28_up.pt proxy error: 0.03826793283224106
I0907 22:20:47.855446 97855 finetune.py:25] layer 28_up initial loss 0.01951298490166664
I0907 22:20:59.576398 97855 finetune.py:49] layer 28_up @ epoch 0 new loss 0.01892082579433918 old loss 0.01951298490166664 BETTER
I0907 22:21:11.178068 97855 finetune.py:49] layer 28_up @ epoch 1 new loss 0.018752118572592735 old loss 0.01892082579433918 BETTER
I0907 22:21:22.639086 97855 finetune.py:49] layer 28_up @ epoch 2 new loss 0.01867343857884407 old loss 0.018752118572592735 BETTER
I0907 22:21:34.344660 97855 finetune.py:49] layer 28_up @ epoch 3 new loss 0.018636301159858704 old loss 0.01867343857884407 BETTER
I0907 22:21:45.994033 97855 finetune.py:49] layer 28_up @ epoch 4 new loss 0.01861291006207466 old loss 0.018636301159858704 BETTER
I0907 22:21:50.556224 97855 quip.py:388] mean square of W: 0.9999998807907104
I0907 22:21:50.556674 97855 quip.py:389] mean square of Wr: 0.9999998807907104
I0907 22:21:50.558681 97855 quip.py:390] difference between Hr and Hr.T: 1.5497207641601562e-06
I0907 22:21:50.559273 97855 quip.py:391] max abs of Hr: 3.2639288902282715
I0907 22:21:50.559405 97855 quip.py:392] min diag of Lhr: 0.6944859027862549
I0907 22:22:18.285615 97855 misc.py:19] ckpt/2_7b_2bit/28_down.pt frob  error: 0.11963825672864914
I0907 22:22:18.285794 97855 misc.py:20] ckpt/2_7b_2bit/28_down.pt proxy error: 0.05860934033989906
I0907 22:22:49.664282 93442 quantize_finetune_llama.py:211] computed original embedding for layer 29 in 28.739922523498535s, pre msv 1.4843926429748535, post msv 1.685329556465149
I0907 22:22:49.774788 93442 quantize_finetune_llama.py:179] layer 30 gpu 0
W0907 22:22:51.080618 97953 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 22:22:51.105090 97953 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 22:22:51.213027 97953 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 22:22:51.213123 97953 utils.py:164] NumExpr defaulting to 16 threads.
I0907 22:22:51.364658 97953 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 22:22:51.610018 97953 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 22:23:10.715126 97953 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 22:23:12.392156 97953 quip.py:388] mean square of W: 1.0
I0907 22:23:12.392659 97953 quip.py:389] mean square of Wr: 1.0
I0907 22:23:12.401698 97953 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0907 22:23:12.402115 97953 quip.py:391] max abs of Hr: 2.466193675994873
I0907 22:23:12.411006 97953 quip.py:392] min diag of Lhr: 0.7420397400856018
I0907 22:23:25.035458 97953 misc.py:19] ckpt/2_7b_2bit/29_qkv.pt frob  error: 0.1188485324382782
I0907 22:23:25.035687 97953 misc.py:20] ckpt/2_7b_2bit/29_qkv.pt proxy error: 0.020511198788881302
I0907 22:23:31.893670 97953 finetune.py:25] layer 29_qkv initial loss 0.00998482946306467
W0907 22:23:31.893942 97953 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 22:23:32.026045 97953 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 22:23:44.455176 97953 finetune.py:49] layer 29_qkv @ epoch 0 new loss 0.004183329641819 old loss 0.00998482946306467 BETTER
I0907 22:23:57.018327 97953 finetune.py:49] layer 29_qkv @ epoch 1 new loss 0.004036017693579197 old loss 0.004183329641819 BETTER
I0907 22:24:09.713014 97953 finetune.py:49] layer 29_qkv @ epoch 2 new loss 0.004024432972073555 old loss 0.004036017693579197 BETTER
I0907 22:24:22.297845 97953 finetune.py:49] layer 29_qkv @ epoch 3 new loss 0.003924116492271423 old loss 0.004024432972073555 BETTER
I0907 22:24:34.955257 97953 finetune.py:49] layer 29_qkv @ epoch 4 new loss 0.003851453075185418 old loss 0.003924116492271423 BETTER
I0907 22:24:36.659722 97953 quip.py:388] mean square of W: 0.9999998807907104
I0907 22:24:36.660034 97953 quip.py:389] mean square of Wr: 0.9999998807907104
I0907 22:24:36.660531 97953 quip.py:390] difference between Hr and Hr.T: 1.1920928955078125e-07
I0907 22:24:36.660813 97953 quip.py:391] max abs of Hr: 1.4700589179992676
I0907 22:24:36.660923 97953 quip.py:392] min diag of Lhr: 0.595476508140564
I0907 22:24:47.279500 97953 misc.py:19] ckpt/2_7b_2bit/29_o.pt frob  error: 0.1389763206243515
I0907 22:24:47.279731 97953 misc.py:20] ckpt/2_7b_2bit/29_o.pt proxy error: 0.035746097564697266
I0907 22:24:52.979948 97953 finetune.py:25] layer 29_o initial loss 0.00723810400813818
I0907 22:25:05.567785 97953 finetune.py:49] layer 29_o @ epoch 0 new loss 0.006300941575318575 old loss 0.00723810400813818 BETTER
I0907 22:25:18.286016 97953 finetune.py:49] layer 29_o @ epoch 1 new loss 0.006253681145608425 old loss 0.006300941575318575 BETTER
I0907 22:25:30.941519 97953 finetune.py:49] layer 29_o @ epoch 2 new loss 0.006206121761351824 old loss 0.006253681145608425 BETTER
I0907 22:25:43.274585 97953 finetune.py:49] layer 29_o @ epoch 3 new loss 0.006200949661433697 old loss 0.006206121761351824 BETTER
I0907 22:25:55.794036 97953 finetune.py:56] layer 29_o @ epoch 4 new loss 0.006206299643963575 old loss 0.006200949661433697 WORSE
I0907 22:25:58.363154 97953 quip.py:388] mean square of W: 1.0000001192092896
I0907 22:25:58.363783 97953 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 22:25:58.364269 97953 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 22:25:58.364556 97953 quip.py:391] max abs of Hr: 1.6176044940948486
I0907 22:25:58.364674 97953 quip.py:392] min diag of Lhr: 0.7713117003440857
I0907 22:26:15.846110 97953 misc.py:19] ckpt/2_7b_2bit/29_up.pt frob  error: 0.11790252476930618
I0907 22:26:15.846341 97953 misc.py:20] ckpt/2_7b_2bit/29_up.pt proxy error: 0.032890524715185165
I0907 22:26:20.123201 97953 finetune.py:25] layer 29_up initial loss 0.022484103217720985
I0907 22:26:31.945351 97953 finetune.py:49] layer 29_up @ epoch 0 new loss 0.021724620833992958 old loss 0.022484103217720985 BETTER
I0907 22:26:43.593099 97953 finetune.py:49] layer 29_up @ epoch 1 new loss 0.02149065025150776 old loss 0.021724620833992958 BETTER
I0907 22:26:55.652892 97953 finetune.py:49] layer 29_up @ epoch 2 new loss 0.021389588713645935 old loss 0.02149065025150776 BETTER
I0907 22:27:07.824023 97953 finetune.py:49] layer 29_up @ epoch 3 new loss 0.021332958713173866 old loss 0.021389588713645935 BETTER
I0907 22:27:19.658251 97953 finetune.py:49] layer 29_up @ epoch 4 new loss 0.021304627880454063 old loss 0.021332958713173866 BETTER
I0907 22:27:23.893182 97953 quip.py:388] mean square of W: 1.0
I0907 22:27:23.893632 97953 quip.py:389] mean square of Wr: 1.0
I0907 22:27:23.895687 97953 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-06
I0907 22:27:23.896275 97953 quip.py:391] max abs of Hr: 3.239778757095337
I0907 22:27:23.896402 97953 quip.py:392] min diag of Lhr: 0.6412152647972107
I0907 22:27:53.553465 97953 misc.py:19] ckpt/2_7b_2bit/29_down.pt frob  error: 0.12197142094373703
I0907 22:27:53.553721 97953 misc.py:20] ckpt/2_7b_2bit/29_down.pt proxy error: 0.052578434348106384
I0907 22:28:24.741731 93442 quantize_finetune_llama.py:211] computed original embedding for layer 30 in 28.431610345840454s, pre msv 1.685329556465149, post msv 1.4689382314682007
I0907 22:28:24.820536 93442 quantize_finetune_llama.py:179] layer 31 gpu 0
W0907 22:28:26.173776 98054 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 22:28:26.198778 98054 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 22:28:26.311256 98054 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 22:28:26.311374 98054 utils.py:164] NumExpr defaulting to 16 threads.
I0907 22:28:26.484717 98054 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 22:28:26.735605 98054 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 22:28:45.702038 98054 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 22:28:47.401904 98054 quip.py:388] mean square of W: 0.9999999403953552
I0907 22:28:47.402396 98054 quip.py:389] mean square of Wr: 0.9999998807907104
I0907 22:28:47.411412 98054 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0907 22:28:47.411829 98054 quip.py:391] max abs of Hr: 1.700053334236145
I0907 22:28:47.420547 98054 quip.py:392] min diag of Lhr: 0.7478322386741638
I0907 22:29:00.079429 98054 misc.py:19] ckpt/2_7b_2bit/30_qkv.pt frob  error: 0.11900659650564194
I0907 22:29:00.079656 98054 misc.py:20] ckpt/2_7b_2bit/30_qkv.pt proxy error: 0.020789165049791336
I0907 22:29:07.245569 98054 finetune.py:25] layer 30_qkv initial loss 0.01280747726559639
W0907 22:29:07.245875 98054 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 22:29:07.380345 98054 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 22:29:19.894365 98054 finetune.py:49] layer 30_qkv @ epoch 0 new loss 0.004762373398989439 old loss 0.01280747726559639 BETTER
I0907 22:29:32.411210 98054 finetune.py:56] layer 30_qkv @ epoch 1 new loss 0.004912348929792643 old loss 0.004762373398989439 WORSE
I0907 22:29:44.912988 98054 finetune.py:49] layer 30_qkv @ epoch 2 new loss 0.004578600637614727 old loss 0.004762373398989439 BETTER
I0907 22:29:57.266747 98054 finetune.py:56] layer 30_qkv @ epoch 3 new loss 0.004643761087208986 old loss 0.004578600637614727 WORSE
I0907 22:30:09.435551 98054 finetune.py:49] layer 30_qkv @ epoch 4 new loss 0.004397518467158079 old loss 0.004578600637614727 BETTER
I0907 22:30:10.803176 98054 quip.py:388] mean square of W: 1.0000001192092896
I0907 22:30:10.803422 98054 quip.py:389] mean square of Wr: 1.0
I0907 22:30:10.803891 98054 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 22:30:10.804172 98054 quip.py:391] max abs of Hr: 1.4175565242767334
I0907 22:30:10.804269 98054 quip.py:392] min diag of Lhr: 0.5216737985610962
I0907 22:30:21.069191 98054 misc.py:19] ckpt/2_7b_2bit/30_o.pt frob  error: 0.1441442370414734
I0907 22:30:21.069418 98054 misc.py:20] ckpt/2_7b_2bit/30_o.pt proxy error: 0.0329611599445343
I0907 22:30:26.650963 98054 finetune.py:25] layer 30_o initial loss 0.00824744813144207
I0907 22:30:38.687674 98054 finetune.py:49] layer 30_o @ epoch 0 new loss 0.0070028156042099 old loss 0.00824744813144207 BETTER
I0907 22:30:50.949623 98054 finetune.py:49] layer 30_o @ epoch 1 new loss 0.006907326169312 old loss 0.0070028156042099 BETTER
I0907 22:31:02.970474 98054 finetune.py:49] layer 30_o @ epoch 2 new loss 0.006810430902987719 old loss 0.006907326169312 BETTER
I0907 22:31:15.434010 98054 finetune.py:56] layer 30_o @ epoch 3 new loss 0.006838968954980373 old loss 0.006810430902987719 WORSE
I0907 22:31:27.865075 98054 finetune.py:49] layer 30_o @ epoch 4 new loss 0.006772254593670368 old loss 0.006810430902987719 BETTER
I0907 22:31:30.286905 98054 quip.py:388] mean square of W: 1.0
I0907 22:31:30.287584 98054 quip.py:389] mean square of Wr: 1.0
I0907 22:31:30.288086 98054 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 22:31:30.288366 98054 quip.py:391] max abs of Hr: 1.6489776372909546
I0907 22:31:30.288468 98054 quip.py:392] min diag of Lhr: 0.7555631399154663
I0907 22:31:47.028773 98054 misc.py:19] ckpt/2_7b_2bit/30_up.pt frob  error: 0.11834317445755005
I0907 22:31:47.029000 98054 misc.py:20] ckpt/2_7b_2bit/30_up.pt proxy error: 0.02123580314218998
I0907 22:31:51.335307 98054 finetune.py:25] layer 30_up initial loss 0.03105859085917473
I0907 22:32:03.302796 98054 finetune.py:49] layer 30_up @ epoch 0 new loss 0.028638601303100586 old loss 0.03105859085917473 BETTER
I0907 22:32:15.064870 98054 finetune.py:49] layer 30_up @ epoch 1 new loss 0.028020374476909637 old loss 0.028638601303100586 BETTER
I0907 22:32:26.667480 98054 finetune.py:49] layer 30_up @ epoch 2 new loss 0.027719072997570038 old loss 0.028020374476909637 BETTER
I0907 22:32:38.332442 98054 finetune.py:49] layer 30_up @ epoch 3 new loss 0.027527909725904465 old loss 0.027719072997570038 BETTER
I0907 22:32:49.979723 98054 finetune.py:49] layer 30_up @ epoch 4 new loss 0.027394825592637062 old loss 0.027527909725904465 BETTER
I0907 22:32:54.308599 98054 quip.py:388] mean square of W: 1.0
I0907 22:32:54.309078 98054 quip.py:389] mean square of Wr: 1.0
I0907 22:32:54.311094 98054 quip.py:390] difference between Hr and Hr.T: 2.1457672119140625e-06
I0907 22:32:54.311695 98054 quip.py:391] max abs of Hr: 2.841714382171631
I0907 22:32:54.311858 98054 quip.py:392] min diag of Lhr: 0.384689062833786
I0907 22:33:21.311330 98054 misc.py:19] ckpt/2_7b_2bit/30_down.pt frob  error: 0.12890633940696716
I0907 22:33:21.311496 98054 misc.py:20] ckpt/2_7b_2bit/30_down.pt proxy error: 0.010330782271921635
I0907 22:33:52.928857 93442 quantize_finetune_llama.py:211] computed original embedding for layer 31 in 29.308480262756348s, pre msv 1.4689382314682007, post msv 3.4358034133911133
W0907 22:33:54.311923 98156 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 22:33:54.335796 98156 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 22:33:54.443544 98156 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 22:33:54.443662 98156 utils.py:164] NumExpr defaulting to 16 threads.
I0907 22:33:54.596877 98156 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 22:33:54.833990 98156 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0907 22:34:14.006011 98156 data_utils.py:205] using 256 training seqs, 128 validation seqs
I0907 22:34:15.509890 98156 quip.py:388] mean square of W: 1.0
I0907 22:34:15.510375 98156 quip.py:389] mean square of Wr: 0.9999999403953552
I0907 22:34:15.519351 98156 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 22:34:15.519747 98156 quip.py:391] max abs of Hr: 1.7408766746520996
I0907 22:34:15.528431 98156 quip.py:392] min diag of Lhr: 0.7231066823005676
I0907 22:34:28.126922 98156 misc.py:19] ckpt/2_7b_2bit/31_qkv.pt frob  error: 0.12081442028284073
I0907 22:34:28.127093 98156 misc.py:20] ckpt/2_7b_2bit/31_qkv.pt proxy error: 0.015744317322969437
I0907 22:34:34.971096 98156 finetune.py:25] layer 31_qkv initial loss 0.011110235005617142
W0907 22:34:34.971298 98156 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0907 22:34:35.097975 98156 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0907 22:34:47.469217 98156 finetune.py:49] layer 31_qkv @ epoch 0 new loss 0.010025814175605774 old loss 0.011110235005617142 BETTER
I0907 22:34:59.873038 98156 finetune.py:49] layer 31_qkv @ epoch 1 new loss 0.009683896787464619 old loss 0.010025814175605774 BETTER
I0907 22:35:12.083664 98156 finetune.py:56] layer 31_qkv @ epoch 2 new loss 0.017259279265999794 old loss 0.009683896787464619 WORSE
I0907 22:35:24.329443 98156 finetune.py:56] layer 31_qkv @ epoch 3 new loss 0.032007183879613876 old loss 0.009683896787464619 WORSE
I0907 22:35:36.980550 98156 finetune.py:56] layer 31_qkv @ epoch 4 new loss 0.012613671831786633 old loss 0.009683896787464619 WORSE
I0907 22:35:38.730062 98156 quip.py:388] mean square of W: 1.0000001192092896
I0907 22:35:38.730371 98156 quip.py:389] mean square of Wr: 1.0000001192092896
I0907 22:35:38.730873 98156 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 22:35:38.731158 98156 quip.py:391] max abs of Hr: 1.638031244277954
I0907 22:35:38.731266 98156 quip.py:392] min diag of Lhr: 0.46512314677238464
I0907 22:35:49.602223 98156 misc.py:19] ckpt/2_7b_2bit/31_o.pt frob  error: 0.14334850013256073
I0907 22:35:49.602445 98156 misc.py:20] ckpt/2_7b_2bit/31_o.pt proxy error: 0.017175884917378426
I0907 22:35:55.201023 98156 finetune.py:25] layer 31_o initial loss 0.013224483467638493
I0907 22:36:07.313433 98156 finetune.py:49] layer 31_o @ epoch 0 new loss 0.011828497983515263 old loss 0.013224483467638493 BETTER
I0907 22:36:19.394008 98156 finetune.py:56] layer 31_o @ epoch 1 new loss 0.012446057051420212 old loss 0.011828497983515263 WORSE
I0907 22:36:31.305377 98156 finetune.py:49] layer 31_o @ epoch 2 new loss 0.01137445867061615 old loss 0.011828497983515263 BETTER
I0907 22:36:43.437533 98156 finetune.py:56] layer 31_o @ epoch 3 new loss 0.011542711406946182 old loss 0.01137445867061615 WORSE
I0907 22:36:55.708479 98156 finetune.py:49] layer 31_o @ epoch 4 new loss 0.010391738265752792 old loss 0.01137445867061615 BETTER
I0907 22:36:57.990027 98156 quip.py:388] mean square of W: 1.0
I0907 22:36:57.990688 98156 quip.py:389] mean square of Wr: 1.0
I0907 22:36:57.991174 98156 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0907 22:36:57.991453 98156 quip.py:391] max abs of Hr: 2.018998146057129
I0907 22:36:57.991557 98156 quip.py:392] min diag of Lhr: 0.7165087461471558
I0907 22:37:15.553794 98156 misc.py:19] ckpt/2_7b_2bit/31_up.pt frob  error: 0.12082419544458389
I0907 22:37:15.554027 98156 misc.py:20] ckpt/2_7b_2bit/31_up.pt proxy error: 0.012663934379816055
I0907 22:37:19.815399 98156 finetune.py:25] layer 31_up initial loss 0.05902289226651192
I0907 22:37:31.514533 98156 finetune.py:49] layer 31_up @ epoch 0 new loss 0.05486354976892471 old loss 0.05902289226651192 BETTER
I0907 22:37:43.674088 98156 finetune.py:49] layer 31_up @ epoch 1 new loss 0.053086038678884506 old loss 0.05486354976892471 BETTER
I0907 22:37:55.258596 98156 finetune.py:49] layer 31_up @ epoch 2 new loss 0.05176512897014618 old loss 0.053086038678884506 BETTER
I0907 22:38:06.949232 98156 finetune.py:49] layer 31_up @ epoch 3 new loss 0.05078888684511185 old loss 0.05176512897014618 BETTER
I0907 22:38:18.432442 98156 finetune.py:49] layer 31_up @ epoch 4 new loss 0.050057992339134216 old loss 0.05078888684511185 BETTER
I0907 22:38:22.768768 98156 quip.py:388] mean square of W: 1.0
I0907 22:38:22.769206 98156 quip.py:389] mean square of Wr: 1.0
I0907 22:38:22.771232 98156 quip.py:390] difference between Hr and Hr.T: 1.5497207641601562e-06
I0907 22:38:22.771824 98156 quip.py:391] max abs of Hr: 2.723031759262085
I0907 22:38:22.771954 98156 quip.py:392] min diag of Lhr: 0.26112303137779236
I0907 22:38:51.959506 98156 misc.py:19] ckpt/2_7b_2bit/31_down.pt frob  error: 0.13868534564971924
I0907 22:38:51.959787 98156 misc.py:20] ckpt/2_7b_2bit/31_down.pt proxy error: 0.004897447768598795
W0907 22:39:00.316507 98254 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 22:39:00.340638 98254 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 22:39:00.450248 98254 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 22:39:00.450336 98254 utils.py:164] NumExpr defaulting to 16 threads.
I0907 22:39:00.605791 98254 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 22:39:00.870684 98254 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 108, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 23, in main
    saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([argparse.Namespace])` or the `torch.serialization.safe_globals([argparse.Namespace])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
W0907 22:39:24.594208 98349 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 22:39:24.618047 98349 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 22:39:24.723120 98349 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 22:39:24.723216 98349 utils.py:164] NumExpr defaulting to 16 threads.
I0907 22:39:24.872584 98349 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 22:39:25.103146 98349 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 22:39:46.648534 98349 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0907 22:39:57.841195 98349 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0907 22:42:13.064186 98490 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 22:42:13.097816 98490 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0907 22:42:14.183158 98490 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0907 22:42:14.183303 98490 utils.py:164] NumExpr defaulting to 16 threads.
I0907 22:42:14.718054 98490 config.py:54] PyTorch version 2.8.0+cu126 available.
W0907 22:42:15.911790 98490 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 108, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/hfize_llama.py", line 23, in main
    saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([argparse.Namespace])` or the `torch.serialization.safe_globals([argparse.Namespace])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
W0907 22:42:43.575532 98603 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 22:42:43.578284 98603 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0907 22:42:43.682027 98603 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 22:43:04.918407 98603 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/hf/2_7b_2bit/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1010, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1117, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1658, in _raise_on_head_call_error
    raise head_call_error
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1546, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1463, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 310, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 459, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-68be0a79-71f07772015e5ebc2a1de318;0e539cd5-e70f-4faf-a395-0fd4b3201916)

Repository Not Found for url: https://huggingface.co/hf/2_7b_2bit/resolve/main/config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 71, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 27, in main
    model, model_str = model_from_hf_path(
  File "/home/user/benchmarks/quip-sharp/lib/utils/unsafe_import.py", line 23, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 928, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 631, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 686, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 421, in cached_file
    raise EnvironmentError(
OSError: hf/2_7b_2bit is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
W0907 22:43:09.069947 98698 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0907 22:43:09.133745 98698 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0907 22:43:09.145429 98698 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0907 22:43:30.287565 98698 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/hf/2_7b_2bit/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1010, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1117, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1658, in _raise_on_head_call_error
    raise head_call_error
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1546, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1463, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 310, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 459, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-68be0a92-62acf13d32ee9b772649d5ae;dbc6ee41-4178-4c20-933c-8f2be26f0d4c)

Repository Not Found for url: https://huggingface.co/hf/2_7b_2bit/resolve/main/config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_zeroshot.py", line 62, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/eval/eval_zeroshot.py", line 27, in main
    model, model_str = model_from_hf_path(
  File "/home/user/benchmarks/quip-sharp/lib/utils/unsafe_import.py", line 23, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 928, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 631, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/configuration_utils.py", line 686, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/hub.py", line 421, in cached_file
    raise EnvironmentError(
OSError: hf/2_7b_2bit is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
W0908 01:48:21.735011 99861 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 01:48:21.760405 99861 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 01:48:21.875116 99861 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 01:48:21.875247 99861 utils.py:164] NumExpr defaulting to 16 threads.
I0908 01:48:22.040667 99861 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 01:48:22.286156 99861 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 01:48:43.661778 99861 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]
Some weights of the model checkpoint at meta-llama/Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.Qidxs', 'model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.Wscale', 'model.layers.0.mlp.down_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.Qidxs', 'model.layers.0.mlp.upgate_proj.SU', 'model.layers.0.mlp.upgate_proj.SV', 'model.layers.0.mlp.upgate_proj.Wscale', 'model.layers.0.mlp.upgate_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.fuse_scales', 'model.layers.0.self_attn.o_proj.Qidxs', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.Wscale', 'model.layers.0.self_attn.o_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.Qidxs', 'model.layers.0.self_attn.qkv_proj.SU', 'model.layers.0.self_attn.qkv_proj.SV', 'model.layers.0.self_attn.qkv_proj.Wscale', 'model.layers.0.self_attn.qkv_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.fuse_scales', 'model.layers.1.mlp.down_proj.Qidxs', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.Wscale', 'model.layers.1.mlp.down_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.Qidxs', 'model.layers.1.mlp.upgate_proj.SU', 'model.layers.1.mlp.upgate_proj.SV', 'model.layers.1.mlp.upgate_proj.Wscale', 'model.layers.1.mlp.upgate_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.fuse_scales', 'model.layers.1.self_attn.o_proj.Qidxs', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.Wscale', 'model.layers.1.self_attn.o_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.Qidxs', 'model.layers.1.self_attn.qkv_proj.SU', 'model.layers.1.self_attn.qkv_proj.SV', 'model.layers.1.self_attn.qkv_proj.Wscale', 'model.layers.1.self_attn.qkv_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.fuse_scales', 'model.layers.10.mlp.down_proj.Qidxs', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.Wscale', 'model.layers.10.mlp.down_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.Qidxs', 'model.layers.10.mlp.upgate_proj.SU', 'model.layers.10.mlp.upgate_proj.SV', 'model.layers.10.mlp.upgate_proj.Wscale', 'model.layers.10.mlp.upgate_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.fuse_scales', 'model.layers.10.self_attn.o_proj.Qidxs', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.Wscale', 'model.layers.10.self_attn.o_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.Qidxs', 'model.layers.10.self_attn.qkv_proj.SU', 'model.layers.10.self_attn.qkv_proj.SV', 'model.layers.10.self_attn.qkv_proj.Wscale', 'model.layers.10.self_attn.qkv_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.fuse_scales', 'model.layers.11.mlp.down_proj.Qidxs', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.Wscale', 'model.layers.11.mlp.down_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.Qidxs', 'model.layers.11.mlp.upgate_proj.SU', 'model.layers.11.mlp.upgate_proj.SV', 'model.layers.11.mlp.upgate_proj.Wscale', 'model.layers.11.mlp.upgate_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.fuse_scales', 'model.layers.11.self_attn.o_proj.Qidxs', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.Wscale', 'model.layers.11.self_attn.o_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.Qidxs', 'model.layers.11.self_attn.qkv_proj.SU', 'model.layers.11.self_attn.qkv_proj.SV', 'model.layers.11.self_attn.qkv_proj.Wscale', 'model.layers.11.self_attn.qkv_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.fuse_scales', 'model.layers.12.mlp.down_proj.Qidxs', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.Wscale', 'model.layers.12.mlp.down_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.Qidxs', 'model.layers.12.mlp.upgate_proj.SU', 'model.layers.12.mlp.upgate_proj.SV', 'model.layers.12.mlp.upgate_proj.Wscale', 'model.layers.12.mlp.upgate_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.fuse_scales', 'model.layers.12.self_attn.o_proj.Qidxs', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.Wscale', 'model.layers.12.self_attn.o_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.Qidxs', 'model.layers.12.self_attn.qkv_proj.SU', 'model.layers.12.self_attn.qkv_proj.SV', 'model.layers.12.self_attn.qkv_proj.Wscale', 'model.layers.12.self_attn.qkv_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.fuse_scales', 'model.layers.13.mlp.down_proj.Qidxs', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.Wscale', 'model.layers.13.mlp.down_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.Qidxs', 'model.layers.13.mlp.upgate_proj.SU', 'model.layers.13.mlp.upgate_proj.SV', 'model.layers.13.mlp.upgate_proj.Wscale', 'model.layers.13.mlp.upgate_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.fuse_scales', 'model.layers.13.self_attn.o_proj.Qidxs', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.Wscale', 'model.layers.13.self_attn.o_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.Qidxs', 'model.layers.13.self_attn.qkv_proj.SU', 'model.layers.13.self_attn.qkv_proj.SV', 'model.layers.13.self_attn.qkv_proj.Wscale', 'model.layers.13.self_attn.qkv_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.fuse_scales', 'model.layers.14.mlp.down_proj.Qidxs', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.Wscale', 'model.layers.14.mlp.down_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.Qidxs', 'model.layers.14.mlp.upgate_proj.SU', 'model.layers.14.mlp.upgate_proj.SV', 'model.layers.14.mlp.upgate_proj.Wscale', 'model.layers.14.mlp.upgate_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.fuse_scales', 'model.layers.14.self_attn.o_proj.Qidxs', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.Wscale', 'model.layers.14.self_attn.o_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.Qidxs', 'model.layers.14.self_attn.qkv_proj.SU', 'model.layers.14.self_attn.qkv_proj.SV', 'model.layers.14.self_attn.qkv_proj.Wscale', 'model.layers.14.self_attn.qkv_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.fuse_scales', 'model.layers.15.mlp.down_proj.Qidxs', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.Wscale', 'model.layers.15.mlp.down_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.Qidxs', 'model.layers.15.mlp.upgate_proj.SU', 'model.layers.15.mlp.upgate_proj.SV', 'model.layers.15.mlp.upgate_proj.Wscale', 'model.layers.15.mlp.upgate_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.fuse_scales', 'model.layers.15.self_attn.o_proj.Qidxs', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.Wscale', 'model.layers.15.self_attn.o_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.Qidxs', 'model.layers.15.self_attn.qkv_proj.SU', 'model.layers.15.self_attn.qkv_proj.SV', 'model.layers.15.self_attn.qkv_proj.Wscale', 'model.layers.15.self_attn.qkv_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.fuse_scales', 'model.layers.16.mlp.down_proj.Qidxs', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.Wscale', 'model.layers.16.mlp.down_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.Qidxs', 'model.layers.16.mlp.upgate_proj.SU', 'model.layers.16.mlp.upgate_proj.SV', 'model.layers.16.mlp.upgate_proj.Wscale', 'model.layers.16.mlp.upgate_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.fuse_scales', 'model.layers.16.self_attn.o_proj.Qidxs', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.Wscale', 'model.layers.16.self_attn.o_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.Qidxs', 'model.layers.16.self_attn.qkv_proj.SU', 'model.layers.16.self_attn.qkv_proj.SV', 'model.layers.16.self_attn.qkv_proj.Wscale', 'model.layers.16.self_attn.qkv_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.fuse_scales', 'model.layers.17.mlp.down_proj.Qidxs', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.Wscale', 'model.layers.17.mlp.down_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.Qidxs', 'model.layers.17.mlp.upgate_proj.SU', 'model.layers.17.mlp.upgate_proj.SV', 'model.layers.17.mlp.upgate_proj.Wscale', 'model.layers.17.mlp.upgate_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.fuse_scales', 'model.layers.17.self_attn.o_proj.Qidxs', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.Wscale', 'model.layers.17.self_attn.o_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.Qidxs', 'model.layers.17.self_attn.qkv_proj.SU', 'model.layers.17.self_attn.qkv_proj.SV', 'model.layers.17.self_attn.qkv_proj.Wscale', 'model.layers.17.self_attn.qkv_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.fuse_scales', 'model.layers.18.mlp.down_proj.Qidxs', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.Wscale', 'model.layers.18.mlp.down_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.Qidxs', 'model.layers.18.mlp.upgate_proj.SU', 'model.layers.18.mlp.upgate_proj.SV', 'model.layers.18.mlp.upgate_proj.Wscale', 'model.layers.18.mlp.upgate_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.fuse_scales', 'model.layers.18.self_attn.o_proj.Qidxs', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.Wscale', 'model.layers.18.self_attn.o_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.Qidxs', 'model.layers.18.self_attn.qkv_proj.SU', 'model.layers.18.self_attn.qkv_proj.SV', 'model.layers.18.self_attn.qkv_proj.Wscale', 'model.layers.18.self_attn.qkv_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.fuse_scales', 'model.layers.19.mlp.down_proj.Qidxs', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.Wscale', 'model.layers.19.mlp.down_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.Qidxs', 'model.layers.19.mlp.upgate_proj.SU', 'model.layers.19.mlp.upgate_proj.SV', 'model.layers.19.mlp.upgate_proj.Wscale', 'model.layers.19.mlp.upgate_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.fuse_scales', 'model.layers.19.self_attn.o_proj.Qidxs', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.Wscale', 'model.layers.19.self_attn.o_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.Qidxs', 'model.layers.19.self_attn.qkv_proj.SU', 'model.layers.19.self_attn.qkv_proj.SV', 'model.layers.19.self_attn.qkv_proj.Wscale', 'model.layers.19.self_attn.qkv_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.fuse_scales', 'model.layers.2.mlp.down_proj.Qidxs', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.Wscale', 'model.layers.2.mlp.down_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.Qidxs', 'model.layers.2.mlp.upgate_proj.SU', 'model.layers.2.mlp.upgate_proj.SV', 'model.layers.2.mlp.upgate_proj.Wscale', 'model.layers.2.mlp.upgate_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.fuse_scales', 'model.layers.2.self_attn.o_proj.Qidxs', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.Wscale', 'model.layers.2.self_attn.o_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.Qidxs', 'model.layers.2.self_attn.qkv_proj.SU', 'model.layers.2.self_attn.qkv_proj.SV', 'model.layers.2.self_attn.qkv_proj.Wscale', 'model.layers.2.self_attn.qkv_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.fuse_scales', 'model.layers.20.mlp.down_proj.Qidxs', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.Wscale', 'model.layers.20.mlp.down_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.Qidxs', 'model.layers.20.mlp.upgate_proj.SU', 'model.layers.20.mlp.upgate_proj.SV', 'model.layers.20.mlp.upgate_proj.Wscale', 'model.layers.20.mlp.upgate_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.fuse_scales', 'model.layers.20.self_attn.o_proj.Qidxs', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.Wscale', 'model.layers.20.self_attn.o_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.Qidxs', 'model.layers.20.self_attn.qkv_proj.SU', 'model.layers.20.self_attn.qkv_proj.SV', 'model.layers.20.self_attn.qkv_proj.Wscale', 'model.layers.20.self_attn.qkv_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.fuse_scales', 'model.layers.21.mlp.down_proj.Qidxs', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.Wscale', 'model.layers.21.mlp.down_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.Qidxs', 'model.layers.21.mlp.upgate_proj.SU', 'model.layers.21.mlp.upgate_proj.SV', 'model.layers.21.mlp.upgate_proj.Wscale', 'model.layers.21.mlp.upgate_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.fuse_scales', 'model.layers.21.self_attn.o_proj.Qidxs', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.Wscale', 'model.layers.21.self_attn.o_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.Qidxs', 'model.layers.21.self_attn.qkv_proj.SU', 'model.layers.21.self_attn.qkv_proj.SV', 'model.layers.21.self_attn.qkv_proj.Wscale', 'model.layers.21.self_attn.qkv_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.fuse_scales', 'model.layers.22.mlp.down_proj.Qidxs', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.Wscale', 'model.layers.22.mlp.down_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.Qidxs', 'model.layers.22.mlp.upgate_proj.SU', 'model.layers.22.mlp.upgate_proj.SV', 'model.layers.22.mlp.upgate_proj.Wscale', 'model.layers.22.mlp.upgate_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.fuse_scales', 'model.layers.22.self_attn.o_proj.Qidxs', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.Wscale', 'model.layers.22.self_attn.o_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.Qidxs', 'model.layers.22.self_attn.qkv_proj.SU', 'model.layers.22.self_attn.qkv_proj.SV', 'model.layers.22.self_attn.qkv_proj.Wscale', 'model.layers.22.self_attn.qkv_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.fuse_scales', 'model.layers.23.mlp.down_proj.Qidxs', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.Wscale', 'model.layers.23.mlp.down_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.Qidxs', 'model.layers.23.mlp.upgate_proj.SU', 'model.layers.23.mlp.upgate_proj.SV', 'model.layers.23.mlp.upgate_proj.Wscale', 'model.layers.23.mlp.upgate_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.fuse_scales', 'model.layers.23.self_attn.o_proj.Qidxs', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.Wscale', 'model.layers.23.self_attn.o_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.Qidxs', 'model.layers.23.self_attn.qkv_proj.SU', 'model.layers.23.self_attn.qkv_proj.SV', 'model.layers.23.self_attn.qkv_proj.Wscale', 'model.layers.23.self_attn.qkv_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.fuse_scales', 'model.layers.24.mlp.down_proj.Qidxs', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.Wscale', 'model.layers.24.mlp.down_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.Qidxs', 'model.layers.24.mlp.upgate_proj.SU', 'model.layers.24.mlp.upgate_proj.SV', 'model.layers.24.mlp.upgate_proj.Wscale', 'model.layers.24.mlp.upgate_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.fuse_scales', 'model.layers.24.self_attn.o_proj.Qidxs', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.Wscale', 'model.layers.24.self_attn.o_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.Qidxs', 'model.layers.24.self_attn.qkv_proj.SU', 'model.layers.24.self_attn.qkv_proj.SV', 'model.layers.24.self_attn.qkv_proj.Wscale', 'model.layers.24.self_attn.qkv_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.fuse_scales', 'model.layers.25.mlp.down_proj.Qidxs', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.Wscale', 'model.layers.25.mlp.down_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.Qidxs', 'model.layers.25.mlp.upgate_proj.SU', 'model.layers.25.mlp.upgate_proj.SV', 'model.layers.25.mlp.upgate_proj.Wscale', 'model.layers.25.mlp.upgate_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.fuse_scales', 'model.layers.25.self_attn.o_proj.Qidxs', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.Wscale', 'model.layers.25.self_attn.o_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.Qidxs', 'model.layers.25.self_attn.qkv_proj.SU', 'model.layers.25.self_attn.qkv_proj.SV', 'model.layers.25.self_attn.qkv_proj.Wscale', 'model.layers.25.self_attn.qkv_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.fuse_scales', 'model.layers.26.mlp.down_proj.Qidxs', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.Wscale', 'model.layers.26.mlp.down_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.Qidxs', 'model.layers.26.mlp.upgate_proj.SU', 'model.layers.26.mlp.upgate_proj.SV', 'model.layers.26.mlp.upgate_proj.Wscale', 'model.layers.26.mlp.upgate_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.fuse_scales', 'model.layers.26.self_attn.o_proj.Qidxs', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.Wscale', 'model.layers.26.self_attn.o_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.Qidxs', 'model.layers.26.self_attn.qkv_proj.SU', 'model.layers.26.self_attn.qkv_proj.SV', 'model.layers.26.self_attn.qkv_proj.Wscale', 'model.layers.26.self_attn.qkv_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.fuse_scales', 'model.layers.27.mlp.down_proj.Qidxs', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.Wscale', 'model.layers.27.mlp.down_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.Qidxs', 'model.layers.27.mlp.upgate_proj.SU', 'model.layers.27.mlp.upgate_proj.SV', 'model.layers.27.mlp.upgate_proj.Wscale', 'model.layers.27.mlp.upgate_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.fuse_scales', 'model.layers.27.self_attn.o_proj.Qidxs', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.Wscale', 'model.layers.27.self_attn.o_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.Qidxs', 'model.layers.27.self_attn.qkv_proj.SU', 'model.layers.27.self_attn.qkv_proj.SV', 'model.layers.27.self_attn.qkv_proj.Wscale', 'model.layers.27.self_attn.qkv_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.fuse_scales', 'model.layers.28.mlp.down_proj.Qidxs', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.Wscale', 'model.layers.28.mlp.down_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.Qidxs', 'model.layers.28.mlp.upgate_proj.SU', 'model.layers.28.mlp.upgate_proj.SV', 'model.layers.28.mlp.upgate_proj.Wscale', 'model.layers.28.mlp.upgate_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.fuse_scales', 'model.layers.28.self_attn.o_proj.Qidxs', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.Wscale', 'model.layers.28.self_attn.o_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.Qidxs', 'model.layers.28.self_attn.qkv_proj.SU', 'model.layers.28.self_attn.qkv_proj.SV', 'model.layers.28.self_attn.qkv_proj.Wscale', 'model.layers.28.self_attn.qkv_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.fuse_scales', 'model.layers.29.mlp.down_proj.Qidxs', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.Wscale', 'model.layers.29.mlp.down_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.Qidxs', 'model.layers.29.mlp.upgate_proj.SU', 'model.layers.29.mlp.upgate_proj.SV', 'model.layers.29.mlp.upgate_proj.Wscale', 'model.layers.29.mlp.upgate_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.fuse_scales', 'model.layers.29.self_attn.o_proj.Qidxs', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.Wscale', 'model.layers.29.self_attn.o_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.Qidxs', 'model.layers.29.self_attn.qkv_proj.SU', 'model.layers.29.self_attn.qkv_proj.SV', 'model.layers.29.self_attn.qkv_proj.Wscale', 'model.layers.29.self_attn.qkv_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.fuse_scales', 'model.layers.3.mlp.down_proj.Qidxs', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.Wscale', 'model.layers.3.mlp.down_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.Qidxs', 'model.layers.3.mlp.upgate_proj.SU', 'model.layers.3.mlp.upgate_proj.SV', 'model.layers.3.mlp.upgate_proj.Wscale', 'model.layers.3.mlp.upgate_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.fuse_scales', 'model.layers.3.self_attn.o_proj.Qidxs', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.Wscale', 'model.layers.3.self_attn.o_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.Qidxs', 'model.layers.3.self_attn.qkv_proj.SU', 'model.layers.3.self_attn.qkv_proj.SV', 'model.layers.3.self_attn.qkv_proj.Wscale', 'model.layers.3.self_attn.qkv_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.fuse_scales', 'model.layers.30.mlp.down_proj.Qidxs', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.Wscale', 'model.layers.30.mlp.down_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.Qidxs', 'model.layers.30.mlp.upgate_proj.SU', 'model.layers.30.mlp.upgate_proj.SV', 'model.layers.30.mlp.upgate_proj.Wscale', 'model.layers.30.mlp.upgate_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.fuse_scales', 'model.layers.30.self_attn.o_proj.Qidxs', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.Wscale', 'model.layers.30.self_attn.o_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.Qidxs', 'model.layers.30.self_attn.qkv_proj.SU', 'model.layers.30.self_attn.qkv_proj.SV', 'model.layers.30.self_attn.qkv_proj.Wscale', 'model.layers.30.self_attn.qkv_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.fuse_scales', 'model.layers.31.mlp.down_proj.Qidxs', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.Wscale', 'model.layers.31.mlp.down_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.Qidxs', 'model.layers.31.mlp.upgate_proj.SU', 'model.layers.31.mlp.upgate_proj.SV', 'model.layers.31.mlp.upgate_proj.Wscale', 'model.layers.31.mlp.upgate_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.fuse_scales', 'model.layers.31.self_attn.o_proj.Qidxs', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.Wscale', 'model.layers.31.self_attn.o_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.Qidxs', 'model.layers.31.self_attn.qkv_proj.SU', 'model.layers.31.self_attn.qkv_proj.SV', 'model.layers.31.self_attn.qkv_proj.Wscale', 'model.layers.31.self_attn.qkv_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.fuse_scales', 'model.layers.4.mlp.down_proj.Qidxs', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.Wscale', 'model.layers.4.mlp.down_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.Qidxs', 'model.layers.4.mlp.upgate_proj.SU', 'model.layers.4.mlp.upgate_proj.SV', 'model.layers.4.mlp.upgate_proj.Wscale', 'model.layers.4.mlp.upgate_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.fuse_scales', 'model.layers.4.self_attn.o_proj.Qidxs', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.Wscale', 'model.layers.4.self_attn.o_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.Qidxs', 'model.layers.4.self_attn.qkv_proj.SU', 'model.layers.4.self_attn.qkv_proj.SV', 'model.layers.4.self_attn.qkv_proj.Wscale', 'model.layers.4.self_attn.qkv_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.fuse_scales', 'model.layers.5.mlp.down_proj.Qidxs', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.Wscale', 'model.layers.5.mlp.down_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.Qidxs', 'model.layers.5.mlp.upgate_proj.SU', 'model.layers.5.mlp.upgate_proj.SV', 'model.layers.5.mlp.upgate_proj.Wscale', 'model.layers.5.mlp.upgate_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.fuse_scales', 'model.layers.5.self_attn.o_proj.Qidxs', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.Wscale', 'model.layers.5.self_attn.o_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.Qidxs', 'model.layers.5.self_attn.qkv_proj.SU', 'model.layers.5.self_attn.qkv_proj.SV', 'model.layers.5.self_attn.qkv_proj.Wscale', 'model.layers.5.self_attn.qkv_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.fuse_scales', 'model.layers.6.mlp.down_proj.Qidxs', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.Wscale', 'model.layers.6.mlp.down_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.Qidxs', 'model.layers.6.mlp.upgate_proj.SU', 'model.layers.6.mlp.upgate_proj.SV', 'model.layers.6.mlp.upgate_proj.Wscale', 'model.layers.6.mlp.upgate_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.fuse_scales', 'model.layers.6.self_attn.o_proj.Qidxs', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.Wscale', 'model.layers.6.self_attn.o_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.Qidxs', 'model.layers.6.self_attn.qkv_proj.SU', 'model.layers.6.self_attn.qkv_proj.SV', 'model.layers.6.self_attn.qkv_proj.Wscale', 'model.layers.6.self_attn.qkv_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.fuse_scales', 'model.layers.7.mlp.down_proj.Qidxs', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.Wscale', 'model.layers.7.mlp.down_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.Qidxs', 'model.layers.7.mlp.upgate_proj.SU', 'model.layers.7.mlp.upgate_proj.SV', 'model.layers.7.mlp.upgate_proj.Wscale', 'model.layers.7.mlp.upgate_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.fuse_scales', 'model.layers.7.self_attn.o_proj.Qidxs', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.Wscale', 'model.layers.7.self_attn.o_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.Qidxs', 'model.layers.7.self_attn.qkv_proj.SU', 'model.layers.7.self_attn.qkv_proj.SV', 'model.layers.7.self_attn.qkv_proj.Wscale', 'model.layers.7.self_attn.qkv_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.fuse_scales', 'model.layers.8.mlp.down_proj.Qidxs', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.Wscale', 'model.layers.8.mlp.down_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.Qidxs', 'model.layers.8.mlp.upgate_proj.SU', 'model.layers.8.mlp.upgate_proj.SV', 'model.layers.8.mlp.upgate_proj.Wscale', 'model.layers.8.mlp.upgate_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.fuse_scales', 'model.layers.8.self_attn.o_proj.Qidxs', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.Wscale', 'model.layers.8.self_attn.o_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.Qidxs', 'model.layers.8.self_attn.qkv_proj.SU', 'model.layers.8.self_attn.qkv_proj.SV', 'model.layers.8.self_attn.qkv_proj.Wscale', 'model.layers.8.self_attn.qkv_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.fuse_scales', 'model.layers.9.mlp.down_proj.Qidxs', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.Wscale', 'model.layers.9.mlp.down_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.Qidxs', 'model.layers.9.mlp.upgate_proj.SU', 'model.layers.9.mlp.upgate_proj.SV', 'model.layers.9.mlp.upgate_proj.Wscale', 'model.layers.9.mlp.upgate_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.fuse_scales', 'model.layers.9.self_attn.o_proj.Qidxs', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.Wscale', 'model.layers.9.self_attn.o_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.Qidxs', 'model.layers.9.self_attn.qkv_proj.SU', 'model.layers.9.self_attn.qkv_proj.SV', 'model.layers.9.self_attn.qkv_proj.Wscale', 'model.layers.9.self_attn.qkv_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.fuse_scales']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
I0908 01:48:45.598591 99861 hfize_llama.py:78] loaded layer 0 down
I0908 01:48:45.740304 99861 hfize_llama.py:78] loaded layer 1 down
I0908 01:48:45.868184 99861 hfize_llama.py:78] loaded layer 2 down
I0908 01:48:46.014762 99861 hfize_llama.py:78] loaded layer 3 down
I0908 01:48:46.198088 99861 hfize_llama.py:78] loaded layer 4 down
I0908 01:48:46.498677 99861 hfize_llama.py:78] loaded layer 5 down
I0908 01:48:46.686856 99861 hfize_llama.py:78] loaded layer 6 down
I0908 01:48:46.804514 99861 hfize_llama.py:78] loaded layer 7 down
I0908 01:48:46.999145 99861 hfize_llama.py:78] loaded layer 8 down
I0908 01:48:47.140463 99861 hfize_llama.py:78] loaded layer 9 down
I0908 01:48:47.320688 99861 hfize_llama.py:78] loaded layer 10 down
I0908 01:48:47.471063 99861 hfize_llama.py:78] loaded layer 11 down
I0908 01:48:47.651165 99861 hfize_llama.py:78] loaded layer 12 down
I0908 01:48:47.804655 99861 hfize_llama.py:78] loaded layer 13 down
I0908 01:48:47.993672 99861 hfize_llama.py:78] loaded layer 14 down
I0908 01:48:48.131338 99861 hfize_llama.py:78] loaded layer 15 down
I0908 01:48:48.274016 99861 hfize_llama.py:78] loaded layer 16 down
I0908 01:48:48.401172 99861 hfize_llama.py:78] loaded layer 17 down
I0908 01:48:48.561818 99861 hfize_llama.py:78] loaded layer 18 down
I0908 01:48:48.777903 99861 hfize_llama.py:78] loaded layer 19 down
I0908 01:48:48.925458 99861 hfize_llama.py:78] loaded layer 20 down
I0908 01:48:49.078564 99861 hfize_llama.py:78] loaded layer 21 down
I0908 01:48:49.240140 99861 hfize_llama.py:78] loaded layer 22 down
I0908 01:48:49.430110 99861 hfize_llama.py:78] loaded layer 23 down
I0908 01:48:49.599691 99861 hfize_llama.py:78] loaded layer 24 down
I0908 01:48:49.721666 99861 hfize_llama.py:78] loaded layer 25 down
I0908 01:48:49.864459 99861 hfize_llama.py:78] loaded layer 26 down
I0908 01:48:50.030563 99861 hfize_llama.py:78] loaded layer 27 down
I0908 01:48:50.228562 99861 hfize_llama.py:78] loaded layer 28 down
I0908 01:48:50.368339 99861 hfize_llama.py:78] loaded layer 29 down
I0908 01:48:50.520782 99861 hfize_llama.py:78] loaded layer 30 down
I0908 01:48:50.666439 99861 hfize_llama.py:78] loaded layer 31 down
I0908 01:48:50.666620 99861 hfize_llama.py:80] saving model...
I0908 01:48:56.611795 99861 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
I0908 01:48:57.398517 99861 hfize_llama.py:87] successfully loaded hfized model
I0908 01:48:57.398723 99861 hfize_llama.py:89] generating some text...
I0908 01:49:15.781700 99861 hfize_llama.py:100] <s> It is a truth universally acknowledged that when one is in the midst of a divorce, one must find a way to deal with all the emotions, and to try to stay calm and rational. The divorce process can be a difficult time for all parties involved, and it is important to find a way to deal with the emotions that come with
I0908 01:49:15.781902 99861 hfize_llama.py:101] elapsed: 18.38313889503479
W0908 01:49:18.812348 100040 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 01:49:18.837591 100040 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 01:49:18.954177 100040 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 01:49:18.954325 100040 utils.py:164] NumExpr defaulting to 16 threads.
I0908 01:49:19.125999 100040 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 01:49:19.367347 100040 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 01:49:40.596869 100040 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 01:50:04.203045 100040 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:58<00:58, 58.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:16<00:00, 34.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:16<00:00, 38.14s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0908 01:53:22.554115 100274 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 01:53:22.589882 100274 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 01:53:23.917631 100274 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 01:53:23.917799 100274 utils.py:164] NumExpr defaulting to 16 threads.
I0908 01:53:24.404202 100274 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 01:53:25.551742 100274 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 01:53:48.560757 100274 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.78it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.81it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.58it/s]
Some weights of the model checkpoint at meta-llama/Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.Qidxs', 'model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.Wscale', 'model.layers.0.mlp.down_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.Qidxs', 'model.layers.0.mlp.upgate_proj.SU', 'model.layers.0.mlp.upgate_proj.SV', 'model.layers.0.mlp.upgate_proj.Wscale', 'model.layers.0.mlp.upgate_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.fuse_scales', 'model.layers.0.self_attn.o_proj.Qidxs', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.Wscale', 'model.layers.0.self_attn.o_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.Qidxs', 'model.layers.0.self_attn.qkv_proj.SU', 'model.layers.0.self_attn.qkv_proj.SV', 'model.layers.0.self_attn.qkv_proj.Wscale', 'model.layers.0.self_attn.qkv_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.fuse_scales', 'model.layers.1.mlp.down_proj.Qidxs', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.Wscale', 'model.layers.1.mlp.down_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.Qidxs', 'model.layers.1.mlp.upgate_proj.SU', 'model.layers.1.mlp.upgate_proj.SV', 'model.layers.1.mlp.upgate_proj.Wscale', 'model.layers.1.mlp.upgate_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.fuse_scales', 'model.layers.1.self_attn.o_proj.Qidxs', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.Wscale', 'model.layers.1.self_attn.o_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.Qidxs', 'model.layers.1.self_attn.qkv_proj.SU', 'model.layers.1.self_attn.qkv_proj.SV', 'model.layers.1.self_attn.qkv_proj.Wscale', 'model.layers.1.self_attn.qkv_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.fuse_scales', 'model.layers.10.mlp.down_proj.Qidxs', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.Wscale', 'model.layers.10.mlp.down_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.Qidxs', 'model.layers.10.mlp.upgate_proj.SU', 'model.layers.10.mlp.upgate_proj.SV', 'model.layers.10.mlp.upgate_proj.Wscale', 'model.layers.10.mlp.upgate_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.fuse_scales', 'model.layers.10.self_attn.o_proj.Qidxs', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.Wscale', 'model.layers.10.self_attn.o_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.Qidxs', 'model.layers.10.self_attn.qkv_proj.SU', 'model.layers.10.self_attn.qkv_proj.SV', 'model.layers.10.self_attn.qkv_proj.Wscale', 'model.layers.10.self_attn.qkv_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.fuse_scales', 'model.layers.11.mlp.down_proj.Qidxs', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.Wscale', 'model.layers.11.mlp.down_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.Qidxs', 'model.layers.11.mlp.upgate_proj.SU', 'model.layers.11.mlp.upgate_proj.SV', 'model.layers.11.mlp.upgate_proj.Wscale', 'model.layers.11.mlp.upgate_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.fuse_scales', 'model.layers.11.self_attn.o_proj.Qidxs', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.Wscale', 'model.layers.11.self_attn.o_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.Qidxs', 'model.layers.11.self_attn.qkv_proj.SU', 'model.layers.11.self_attn.qkv_proj.SV', 'model.layers.11.self_attn.qkv_proj.Wscale', 'model.layers.11.self_attn.qkv_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.fuse_scales', 'model.layers.12.mlp.down_proj.Qidxs', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.Wscale', 'model.layers.12.mlp.down_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.Qidxs', 'model.layers.12.mlp.upgate_proj.SU', 'model.layers.12.mlp.upgate_proj.SV', 'model.layers.12.mlp.upgate_proj.Wscale', 'model.layers.12.mlp.upgate_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.fuse_scales', 'model.layers.12.self_attn.o_proj.Qidxs', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.Wscale', 'model.layers.12.self_attn.o_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.Qidxs', 'model.layers.12.self_attn.qkv_proj.SU', 'model.layers.12.self_attn.qkv_proj.SV', 'model.layers.12.self_attn.qkv_proj.Wscale', 'model.layers.12.self_attn.qkv_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.fuse_scales', 'model.layers.13.mlp.down_proj.Qidxs', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.Wscale', 'model.layers.13.mlp.down_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.Qidxs', 'model.layers.13.mlp.upgate_proj.SU', 'model.layers.13.mlp.upgate_proj.SV', 'model.layers.13.mlp.upgate_proj.Wscale', 'model.layers.13.mlp.upgate_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.fuse_scales', 'model.layers.13.self_attn.o_proj.Qidxs', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.Wscale', 'model.layers.13.self_attn.o_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.Qidxs', 'model.layers.13.self_attn.qkv_proj.SU', 'model.layers.13.self_attn.qkv_proj.SV', 'model.layers.13.self_attn.qkv_proj.Wscale', 'model.layers.13.self_attn.qkv_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.fuse_scales', 'model.layers.14.mlp.down_proj.Qidxs', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.Wscale', 'model.layers.14.mlp.down_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.Qidxs', 'model.layers.14.mlp.upgate_proj.SU', 'model.layers.14.mlp.upgate_proj.SV', 'model.layers.14.mlp.upgate_proj.Wscale', 'model.layers.14.mlp.upgate_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.fuse_scales', 'model.layers.14.self_attn.o_proj.Qidxs', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.Wscale', 'model.layers.14.self_attn.o_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.Qidxs', 'model.layers.14.self_attn.qkv_proj.SU', 'model.layers.14.self_attn.qkv_proj.SV', 'model.layers.14.self_attn.qkv_proj.Wscale', 'model.layers.14.self_attn.qkv_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.fuse_scales', 'model.layers.15.mlp.down_proj.Qidxs', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.Wscale', 'model.layers.15.mlp.down_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.Qidxs', 'model.layers.15.mlp.upgate_proj.SU', 'model.layers.15.mlp.upgate_proj.SV', 'model.layers.15.mlp.upgate_proj.Wscale', 'model.layers.15.mlp.upgate_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.fuse_scales', 'model.layers.15.self_attn.o_proj.Qidxs', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.Wscale', 'model.layers.15.self_attn.o_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.Qidxs', 'model.layers.15.self_attn.qkv_proj.SU', 'model.layers.15.self_attn.qkv_proj.SV', 'model.layers.15.self_attn.qkv_proj.Wscale', 'model.layers.15.self_attn.qkv_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.fuse_scales', 'model.layers.16.mlp.down_proj.Qidxs', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.Wscale', 'model.layers.16.mlp.down_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.Qidxs', 'model.layers.16.mlp.upgate_proj.SU', 'model.layers.16.mlp.upgate_proj.SV', 'model.layers.16.mlp.upgate_proj.Wscale', 'model.layers.16.mlp.upgate_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.fuse_scales', 'model.layers.16.self_attn.o_proj.Qidxs', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.Wscale', 'model.layers.16.self_attn.o_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.Qidxs', 'model.layers.16.self_attn.qkv_proj.SU', 'model.layers.16.self_attn.qkv_proj.SV', 'model.layers.16.self_attn.qkv_proj.Wscale', 'model.layers.16.self_attn.qkv_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.fuse_scales', 'model.layers.17.mlp.down_proj.Qidxs', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.Wscale', 'model.layers.17.mlp.down_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.Qidxs', 'model.layers.17.mlp.upgate_proj.SU', 'model.layers.17.mlp.upgate_proj.SV', 'model.layers.17.mlp.upgate_proj.Wscale', 'model.layers.17.mlp.upgate_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.fuse_scales', 'model.layers.17.self_attn.o_proj.Qidxs', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.Wscale', 'model.layers.17.self_attn.o_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.Qidxs', 'model.layers.17.self_attn.qkv_proj.SU', 'model.layers.17.self_attn.qkv_proj.SV', 'model.layers.17.self_attn.qkv_proj.Wscale', 'model.layers.17.self_attn.qkv_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.fuse_scales', 'model.layers.18.mlp.down_proj.Qidxs', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.Wscale', 'model.layers.18.mlp.down_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.Qidxs', 'model.layers.18.mlp.upgate_proj.SU', 'model.layers.18.mlp.upgate_proj.SV', 'model.layers.18.mlp.upgate_proj.Wscale', 'model.layers.18.mlp.upgate_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.fuse_scales', 'model.layers.18.self_attn.o_proj.Qidxs', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.Wscale', 'model.layers.18.self_attn.o_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.Qidxs', 'model.layers.18.self_attn.qkv_proj.SU', 'model.layers.18.self_attn.qkv_proj.SV', 'model.layers.18.self_attn.qkv_proj.Wscale', 'model.layers.18.self_attn.qkv_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.fuse_scales', 'model.layers.19.mlp.down_proj.Qidxs', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.Wscale', 'model.layers.19.mlp.down_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.Qidxs', 'model.layers.19.mlp.upgate_proj.SU', 'model.layers.19.mlp.upgate_proj.SV', 'model.layers.19.mlp.upgate_proj.Wscale', 'model.layers.19.mlp.upgate_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.fuse_scales', 'model.layers.19.self_attn.o_proj.Qidxs', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.Wscale', 'model.layers.19.self_attn.o_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.Qidxs', 'model.layers.19.self_attn.qkv_proj.SU', 'model.layers.19.self_attn.qkv_proj.SV', 'model.layers.19.self_attn.qkv_proj.Wscale', 'model.layers.19.self_attn.qkv_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.fuse_scales', 'model.layers.2.mlp.down_proj.Qidxs', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.Wscale', 'model.layers.2.mlp.down_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.Qidxs', 'model.layers.2.mlp.upgate_proj.SU', 'model.layers.2.mlp.upgate_proj.SV', 'model.layers.2.mlp.upgate_proj.Wscale', 'model.layers.2.mlp.upgate_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.fuse_scales', 'model.layers.2.self_attn.o_proj.Qidxs', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.Wscale', 'model.layers.2.self_attn.o_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.Qidxs', 'model.layers.2.self_attn.qkv_proj.SU', 'model.layers.2.self_attn.qkv_proj.SV', 'model.layers.2.self_attn.qkv_proj.Wscale', 'model.layers.2.self_attn.qkv_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.fuse_scales', 'model.layers.20.mlp.down_proj.Qidxs', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.Wscale', 'model.layers.20.mlp.down_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.Qidxs', 'model.layers.20.mlp.upgate_proj.SU', 'model.layers.20.mlp.upgate_proj.SV', 'model.layers.20.mlp.upgate_proj.Wscale', 'model.layers.20.mlp.upgate_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.fuse_scales', 'model.layers.20.self_attn.o_proj.Qidxs', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.Wscale', 'model.layers.20.self_attn.o_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.Qidxs', 'model.layers.20.self_attn.qkv_proj.SU', 'model.layers.20.self_attn.qkv_proj.SV', 'model.layers.20.self_attn.qkv_proj.Wscale', 'model.layers.20.self_attn.qkv_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.fuse_scales', 'model.layers.21.mlp.down_proj.Qidxs', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.Wscale', 'model.layers.21.mlp.down_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.Qidxs', 'model.layers.21.mlp.upgate_proj.SU', 'model.layers.21.mlp.upgate_proj.SV', 'model.layers.21.mlp.upgate_proj.Wscale', 'model.layers.21.mlp.upgate_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.fuse_scales', 'model.layers.21.self_attn.o_proj.Qidxs', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.Wscale', 'model.layers.21.self_attn.o_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.Qidxs', 'model.layers.21.self_attn.qkv_proj.SU', 'model.layers.21.self_attn.qkv_proj.SV', 'model.layers.21.self_attn.qkv_proj.Wscale', 'model.layers.21.self_attn.qkv_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.fuse_scales', 'model.layers.22.mlp.down_proj.Qidxs', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.Wscale', 'model.layers.22.mlp.down_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.Qidxs', 'model.layers.22.mlp.upgate_proj.SU', 'model.layers.22.mlp.upgate_proj.SV', 'model.layers.22.mlp.upgate_proj.Wscale', 'model.layers.22.mlp.upgate_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.fuse_scales', 'model.layers.22.self_attn.o_proj.Qidxs', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.Wscale', 'model.layers.22.self_attn.o_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.Qidxs', 'model.layers.22.self_attn.qkv_proj.SU', 'model.layers.22.self_attn.qkv_proj.SV', 'model.layers.22.self_attn.qkv_proj.Wscale', 'model.layers.22.self_attn.qkv_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.fuse_scales', 'model.layers.23.mlp.down_proj.Qidxs', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.Wscale', 'model.layers.23.mlp.down_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.Qidxs', 'model.layers.23.mlp.upgate_proj.SU', 'model.layers.23.mlp.upgate_proj.SV', 'model.layers.23.mlp.upgate_proj.Wscale', 'model.layers.23.mlp.upgate_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.fuse_scales', 'model.layers.23.self_attn.o_proj.Qidxs', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.Wscale', 'model.layers.23.self_attn.o_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.Qidxs', 'model.layers.23.self_attn.qkv_proj.SU', 'model.layers.23.self_attn.qkv_proj.SV', 'model.layers.23.self_attn.qkv_proj.Wscale', 'model.layers.23.self_attn.qkv_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.fuse_scales', 'model.layers.24.mlp.down_proj.Qidxs', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.Wscale', 'model.layers.24.mlp.down_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.Qidxs', 'model.layers.24.mlp.upgate_proj.SU', 'model.layers.24.mlp.upgate_proj.SV', 'model.layers.24.mlp.upgate_proj.Wscale', 'model.layers.24.mlp.upgate_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.fuse_scales', 'model.layers.24.self_attn.o_proj.Qidxs', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.Wscale', 'model.layers.24.self_attn.o_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.Qidxs', 'model.layers.24.self_attn.qkv_proj.SU', 'model.layers.24.self_attn.qkv_proj.SV', 'model.layers.24.self_attn.qkv_proj.Wscale', 'model.layers.24.self_attn.qkv_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.fuse_scales', 'model.layers.25.mlp.down_proj.Qidxs', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.Wscale', 'model.layers.25.mlp.down_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.Qidxs', 'model.layers.25.mlp.upgate_proj.SU', 'model.layers.25.mlp.upgate_proj.SV', 'model.layers.25.mlp.upgate_proj.Wscale', 'model.layers.25.mlp.upgate_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.fuse_scales', 'model.layers.25.self_attn.o_proj.Qidxs', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.Wscale', 'model.layers.25.self_attn.o_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.Qidxs', 'model.layers.25.self_attn.qkv_proj.SU', 'model.layers.25.self_attn.qkv_proj.SV', 'model.layers.25.self_attn.qkv_proj.Wscale', 'model.layers.25.self_attn.qkv_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.fuse_scales', 'model.layers.26.mlp.down_proj.Qidxs', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.Wscale', 'model.layers.26.mlp.down_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.Qidxs', 'model.layers.26.mlp.upgate_proj.SU', 'model.layers.26.mlp.upgate_proj.SV', 'model.layers.26.mlp.upgate_proj.Wscale', 'model.layers.26.mlp.upgate_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.fuse_scales', 'model.layers.26.self_attn.o_proj.Qidxs', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.Wscale', 'model.layers.26.self_attn.o_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.Qidxs', 'model.layers.26.self_attn.qkv_proj.SU', 'model.layers.26.self_attn.qkv_proj.SV', 'model.layers.26.self_attn.qkv_proj.Wscale', 'model.layers.26.self_attn.qkv_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.fuse_scales', 'model.layers.27.mlp.down_proj.Qidxs', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.Wscale', 'model.layers.27.mlp.down_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.Qidxs', 'model.layers.27.mlp.upgate_proj.SU', 'model.layers.27.mlp.upgate_proj.SV', 'model.layers.27.mlp.upgate_proj.Wscale', 'model.layers.27.mlp.upgate_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.fuse_scales', 'model.layers.27.self_attn.o_proj.Qidxs', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.Wscale', 'model.layers.27.self_attn.o_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.Qidxs', 'model.layers.27.self_attn.qkv_proj.SU', 'model.layers.27.self_attn.qkv_proj.SV', 'model.layers.27.self_attn.qkv_proj.Wscale', 'model.layers.27.self_attn.qkv_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.fuse_scales', 'model.layers.28.mlp.down_proj.Qidxs', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.Wscale', 'model.layers.28.mlp.down_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.Qidxs', 'model.layers.28.mlp.upgate_proj.SU', 'model.layers.28.mlp.upgate_proj.SV', 'model.layers.28.mlp.upgate_proj.Wscale', 'model.layers.28.mlp.upgate_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.fuse_scales', 'model.layers.28.self_attn.o_proj.Qidxs', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.Wscale', 'model.layers.28.self_attn.o_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.Qidxs', 'model.layers.28.self_attn.qkv_proj.SU', 'model.layers.28.self_attn.qkv_proj.SV', 'model.layers.28.self_attn.qkv_proj.Wscale', 'model.layers.28.self_attn.qkv_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.fuse_scales', 'model.layers.29.mlp.down_proj.Qidxs', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.Wscale', 'model.layers.29.mlp.down_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.Qidxs', 'model.layers.29.mlp.upgate_proj.SU', 'model.layers.29.mlp.upgate_proj.SV', 'model.layers.29.mlp.upgate_proj.Wscale', 'model.layers.29.mlp.upgate_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.fuse_scales', 'model.layers.29.self_attn.o_proj.Qidxs', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.Wscale', 'model.layers.29.self_attn.o_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.Qidxs', 'model.layers.29.self_attn.qkv_proj.SU', 'model.layers.29.self_attn.qkv_proj.SV', 'model.layers.29.self_attn.qkv_proj.Wscale', 'model.layers.29.self_attn.qkv_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.fuse_scales', 'model.layers.3.mlp.down_proj.Qidxs', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.Wscale', 'model.layers.3.mlp.down_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.Qidxs', 'model.layers.3.mlp.upgate_proj.SU', 'model.layers.3.mlp.upgate_proj.SV', 'model.layers.3.mlp.upgate_proj.Wscale', 'model.layers.3.mlp.upgate_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.fuse_scales', 'model.layers.3.self_attn.o_proj.Qidxs', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.Wscale', 'model.layers.3.self_attn.o_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.Qidxs', 'model.layers.3.self_attn.qkv_proj.SU', 'model.layers.3.self_attn.qkv_proj.SV', 'model.layers.3.self_attn.qkv_proj.Wscale', 'model.layers.3.self_attn.qkv_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.fuse_scales', 'model.layers.30.mlp.down_proj.Qidxs', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.Wscale', 'model.layers.30.mlp.down_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.Qidxs', 'model.layers.30.mlp.upgate_proj.SU', 'model.layers.30.mlp.upgate_proj.SV', 'model.layers.30.mlp.upgate_proj.Wscale', 'model.layers.30.mlp.upgate_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.fuse_scales', 'model.layers.30.self_attn.o_proj.Qidxs', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.Wscale', 'model.layers.30.self_attn.o_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.Qidxs', 'model.layers.30.self_attn.qkv_proj.SU', 'model.layers.30.self_attn.qkv_proj.SV', 'model.layers.30.self_attn.qkv_proj.Wscale', 'model.layers.30.self_attn.qkv_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.fuse_scales', 'model.layers.31.mlp.down_proj.Qidxs', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.Wscale', 'model.layers.31.mlp.down_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.Qidxs', 'model.layers.31.mlp.upgate_proj.SU', 'model.layers.31.mlp.upgate_proj.SV', 'model.layers.31.mlp.upgate_proj.Wscale', 'model.layers.31.mlp.upgate_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.fuse_scales', 'model.layers.31.self_attn.o_proj.Qidxs', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.Wscale', 'model.layers.31.self_attn.o_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.Qidxs', 'model.layers.31.self_attn.qkv_proj.SU', 'model.layers.31.self_attn.qkv_proj.SV', 'model.layers.31.self_attn.qkv_proj.Wscale', 'model.layers.31.self_attn.qkv_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.fuse_scales', 'model.layers.4.mlp.down_proj.Qidxs', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.Wscale', 'model.layers.4.mlp.down_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.Qidxs', 'model.layers.4.mlp.upgate_proj.SU', 'model.layers.4.mlp.upgate_proj.SV', 'model.layers.4.mlp.upgate_proj.Wscale', 'model.layers.4.mlp.upgate_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.fuse_scales', 'model.layers.4.self_attn.o_proj.Qidxs', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.Wscale', 'model.layers.4.self_attn.o_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.Qidxs', 'model.layers.4.self_attn.qkv_proj.SU', 'model.layers.4.self_attn.qkv_proj.SV', 'model.layers.4.self_attn.qkv_proj.Wscale', 'model.layers.4.self_attn.qkv_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.fuse_scales', 'model.layers.5.mlp.down_proj.Qidxs', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.Wscale', 'model.layers.5.mlp.down_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.Qidxs', 'model.layers.5.mlp.upgate_proj.SU', 'model.layers.5.mlp.upgate_proj.SV', 'model.layers.5.mlp.upgate_proj.Wscale', 'model.layers.5.mlp.upgate_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.fuse_scales', 'model.layers.5.self_attn.o_proj.Qidxs', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.Wscale', 'model.layers.5.self_attn.o_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.Qidxs', 'model.layers.5.self_attn.qkv_proj.SU', 'model.layers.5.self_attn.qkv_proj.SV', 'model.layers.5.self_attn.qkv_proj.Wscale', 'model.layers.5.self_attn.qkv_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.fuse_scales', 'model.layers.6.mlp.down_proj.Qidxs', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.Wscale', 'model.layers.6.mlp.down_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.Qidxs', 'model.layers.6.mlp.upgate_proj.SU', 'model.layers.6.mlp.upgate_proj.SV', 'model.layers.6.mlp.upgate_proj.Wscale', 'model.layers.6.mlp.upgate_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.fuse_scales', 'model.layers.6.self_attn.o_proj.Qidxs', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.Wscale', 'model.layers.6.self_attn.o_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.Qidxs', 'model.layers.6.self_attn.qkv_proj.SU', 'model.layers.6.self_attn.qkv_proj.SV', 'model.layers.6.self_attn.qkv_proj.Wscale', 'model.layers.6.self_attn.qkv_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.fuse_scales', 'model.layers.7.mlp.down_proj.Qidxs', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.Wscale', 'model.layers.7.mlp.down_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.Qidxs', 'model.layers.7.mlp.upgate_proj.SU', 'model.layers.7.mlp.upgate_proj.SV', 'model.layers.7.mlp.upgate_proj.Wscale', 'model.layers.7.mlp.upgate_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.fuse_scales', 'model.layers.7.self_attn.o_proj.Qidxs', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.Wscale', 'model.layers.7.self_attn.o_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.Qidxs', 'model.layers.7.self_attn.qkv_proj.SU', 'model.layers.7.self_attn.qkv_proj.SV', 'model.layers.7.self_attn.qkv_proj.Wscale', 'model.layers.7.self_attn.qkv_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.fuse_scales', 'model.layers.8.mlp.down_proj.Qidxs', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.Wscale', 'model.layers.8.mlp.down_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.Qidxs', 'model.layers.8.mlp.upgate_proj.SU', 'model.layers.8.mlp.upgate_proj.SV', 'model.layers.8.mlp.upgate_proj.Wscale', 'model.layers.8.mlp.upgate_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.fuse_scales', 'model.layers.8.self_attn.o_proj.Qidxs', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.Wscale', 'model.layers.8.self_attn.o_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.Qidxs', 'model.layers.8.self_attn.qkv_proj.SU', 'model.layers.8.self_attn.qkv_proj.SV', 'model.layers.8.self_attn.qkv_proj.Wscale', 'model.layers.8.self_attn.qkv_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.fuse_scales', 'model.layers.9.mlp.down_proj.Qidxs', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.Wscale', 'model.layers.9.mlp.down_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.Qidxs', 'model.layers.9.mlp.upgate_proj.SU', 'model.layers.9.mlp.upgate_proj.SV', 'model.layers.9.mlp.upgate_proj.Wscale', 'model.layers.9.mlp.upgate_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.fuse_scales', 'model.layers.9.self_attn.o_proj.Qidxs', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.Wscale', 'model.layers.9.self_attn.o_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.Qidxs', 'model.layers.9.self_attn.qkv_proj.SU', 'model.layers.9.self_attn.qkv_proj.SV', 'model.layers.9.self_attn.qkv_proj.Wscale', 'model.layers.9.self_attn.qkv_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.fuse_scales']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
I0908 01:53:50.155833 100274 hfize_llama.py:78] loaded layer 0 down
I0908 01:53:50.338687 100274 hfize_llama.py:78] loaded layer 1 down
I0908 01:53:50.600096 100274 hfize_llama.py:78] loaded layer 2 down
I0908 01:53:50.767601 100274 hfize_llama.py:78] loaded layer 3 down
I0908 01:53:50.973164 100274 hfize_llama.py:78] loaded layer 4 down
I0908 01:53:51.174132 100274 hfize_llama.py:78] loaded layer 5 down
I0908 01:53:51.343172 100274 hfize_llama.py:78] loaded layer 6 down
I0908 01:53:51.500353 100274 hfize_llama.py:78] loaded layer 7 down
I0908 01:53:51.655499 100274 hfize_llama.py:78] loaded layer 8 down
I0908 01:53:51.776777 100274 hfize_llama.py:78] loaded layer 9 down
I0908 01:53:51.924205 100274 hfize_llama.py:78] loaded layer 10 down
I0908 01:53:52.067386 100274 hfize_llama.py:78] loaded layer 11 down
I0908 01:53:52.198230 100274 hfize_llama.py:78] loaded layer 12 down
I0908 01:53:52.319494 100274 hfize_llama.py:78] loaded layer 13 down
I0908 01:53:52.514402 100274 hfize_llama.py:78] loaded layer 14 down
I0908 01:53:52.702717 100274 hfize_llama.py:78] loaded layer 15 down
I0908 01:53:52.822598 100274 hfize_llama.py:78] loaded layer 16 down
I0908 01:53:53.008366 100274 hfize_llama.py:78] loaded layer 17 down
I0908 01:53:53.161361 100274 hfize_llama.py:78] loaded layer 18 down
I0908 01:53:53.342998 100274 hfize_llama.py:78] loaded layer 19 down
I0908 01:53:53.519761 100274 hfize_llama.py:78] loaded layer 20 down
I0908 01:53:53.687816 100274 hfize_llama.py:78] loaded layer 21 down
I0908 01:53:53.853777 100274 hfize_llama.py:78] loaded layer 22 down
I0908 01:53:54.071809 100274 hfize_llama.py:78] loaded layer 23 down
I0908 01:53:54.236741 100274 hfize_llama.py:78] loaded layer 24 down
I0908 01:53:54.373494 100274 hfize_llama.py:78] loaded layer 25 down
I0908 01:53:54.528397 100274 hfize_llama.py:78] loaded layer 26 down
I0908 01:53:54.662318 100274 hfize_llama.py:78] loaded layer 27 down
I0908 01:53:54.830341 100274 hfize_llama.py:78] loaded layer 28 down
I0908 01:53:55.005978 100274 hfize_llama.py:78] loaded layer 29 down
I0908 01:53:55.216985 100274 hfize_llama.py:78] loaded layer 30 down
I0908 01:53:55.357103 100274 hfize_llama.py:78] loaded layer 31 down
I0908 01:53:55.357283 100274 hfize_llama.py:80] saving model...
I0908 01:54:03.361078 100274 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
I0908 01:54:04.109991 100274 hfize_llama.py:87] successfully loaded hfized model
I0908 01:54:04.110167 100274 hfize_llama.py:89] generating some text...
I0908 01:54:22.384731 100274 hfize_llama.py:100] <s> It is a truth universally acknowledged that when one is in the midst of a divorce, one must find a way to deal with all the emotions, and to try to stay calm and rational. The divorce process can be a difficult time for all parties involved, and it is important to find a way to deal with the emotions that come with
I0908 01:54:22.384942 100274 hfize_llama.py:101] elapsed: 18.274733304977417
W0908 01:54:27.253375 100455 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 01:54:27.257573 100455 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0908 01:54:27.348846 100455 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0908 01:54:49.032407 100455 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 86615.32 examples/s]
Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 1053821.00 examples/s]
Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 837568.81 examples/s]
W0908 01:54:52.787768 100455 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 71, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 33, in main
    input_tok = gptq_data_utils.get_test_tokens(dataset,
  File "/home/user/benchmarks/quip-sharp/lib/utils/gptq_data_utils.py", line 198, in get_test_tokens
    return get_wikitext2(train_samples, seed, seqlen,
  File "/home/user/benchmarks/quip-sharp/lib/utils/gptq_data_utils.py", line 20, in get_wikitext2
    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 862, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1450, in __getattribute__
    requires_backends(cls, cls._backends)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1438, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
LlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

W0908 01:54:57.309140 100582 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 01:54:57.373013 100582 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0908 01:54:57.384631 100582 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0908 01:55:18.542494 100582 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
W0908 01:55:19.299586 100582 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 01:55:19.451807 100582 eval_zeroshot.py:33] loaded model!
Generating train split:   0%|          | 0/1119 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 1119/1119 [00:00<00:00, 164982.64 examples/s]
Generating test split:   0%|          | 0/1172 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1172/1172 [00:00<00:00, 297562.00 examples/s]
Generating validation split:   0%|          | 0/299 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 299/299 [00:00<00:00, 127358.27 examples/s]
Generating train split:   0%|          | 0/2251 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 2251/2251 [00:00<00:00, 439808.93 examples/s]
Generating test split:   0%|          | 0/2376 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 2376/2376 [00:00<00:00, 518478.03 examples/s]
Generating validation split:   0%|          | 0/570 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 570/570 [00:00<00:00, 261398.78 examples/s]
Generating train split:   0%|          | 0/9427 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 9427/9427 [00:00<00:00, 392465.32 examples/s]
Generating validation split:   0%|          | 0/3270 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 3270/3270 [00:00<00:00, 409414.15 examples/s]
Generating test split:   0%|          | 0/3245 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 3245/3245 [00:00<00:00, 424175.41 examples/s]
The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa.
You can avoid this prompt in future by passing the argument `trust_remote_code=True`.

Do you wish to run the custom code? [y/N] Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_zeroshot.py", line 62, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/eval/eval_zeroshot.py", line 39, in main
    results = evaluator.simple_evaluate(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/lm_eval/utils.py", line 161, in _wrapper
    return fn(*args, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/lm_eval/evaluator.py", line 81, in simple_evaluate
    task_dict = lm_eval.tasks.get_task_dict(tasks)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/lm_eval/tasks/__init__.py", line 317, in get_task_dict
    task_name_dict = {
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/lm_eval/tasks/__init__.py", line 318, in <dictcomp>
    task_name: get_task(task_name)()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/lm_eval/base.py", line 412, in __init__
    self.download(data_dir, cache_dir, download_mode)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/lm_eval/base.py", line 441, in download
    self.dataset = datasets.load_dataset(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 1664, in dataset_module_factory
    raise e1 from None
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 1614, in dataset_module_factory
    ).get_module()
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 1264, in get_module
    trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/datasets/load.py", line 137, in resolve_trust_remote_code
    raise ValueError(
ValueError: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
W0908 02:45:31.195632 101660 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 02:45:31.219898 101660 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 02:45:31.328299 101660 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 02:45:31.328401 101660 utils.py:164] NumExpr defaulting to 16 threads.
I0908 02:45:31.482809 101660 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 02:45:31.717626 101660 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 02:45:52.999228 101660 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 02:46:18.337408 101660 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:56<00:56, 56.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:14<00:00, 34.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:14<00:00, 37.47s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0908 02:49:31.596111 101896 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 02:49:31.631892 101896 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 02:49:33.032572 101896 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 02:49:33.032740 101896 utils.py:164] NumExpr defaulting to 16 threads.
I0908 02:49:33.482205 101896 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 02:49:34.681664 101896 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 02:49:57.599290 101896 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  2.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.29it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.01it/s]
Some weights of the model checkpoint at meta-llama/Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.Qidxs', 'model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.Wscale', 'model.layers.0.mlp.down_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.Qidxs', 'model.layers.0.mlp.upgate_proj.SU', 'model.layers.0.mlp.upgate_proj.SV', 'model.layers.0.mlp.upgate_proj.Wscale', 'model.layers.0.mlp.upgate_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.fuse_scales', 'model.layers.0.self_attn.o_proj.Qidxs', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.Wscale', 'model.layers.0.self_attn.o_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.Qidxs', 'model.layers.0.self_attn.qkv_proj.SU', 'model.layers.0.self_attn.qkv_proj.SV', 'model.layers.0.self_attn.qkv_proj.Wscale', 'model.layers.0.self_attn.qkv_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.fuse_scales', 'model.layers.1.mlp.down_proj.Qidxs', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.Wscale', 'model.layers.1.mlp.down_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.Qidxs', 'model.layers.1.mlp.upgate_proj.SU', 'model.layers.1.mlp.upgate_proj.SV', 'model.layers.1.mlp.upgate_proj.Wscale', 'model.layers.1.mlp.upgate_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.fuse_scales', 'model.layers.1.self_attn.o_proj.Qidxs', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.Wscale', 'model.layers.1.self_attn.o_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.Qidxs', 'model.layers.1.self_attn.qkv_proj.SU', 'model.layers.1.self_attn.qkv_proj.SV', 'model.layers.1.self_attn.qkv_proj.Wscale', 'model.layers.1.self_attn.qkv_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.fuse_scales', 'model.layers.10.mlp.down_proj.Qidxs', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.Wscale', 'model.layers.10.mlp.down_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.Qidxs', 'model.layers.10.mlp.upgate_proj.SU', 'model.layers.10.mlp.upgate_proj.SV', 'model.layers.10.mlp.upgate_proj.Wscale', 'model.layers.10.mlp.upgate_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.fuse_scales', 'model.layers.10.self_attn.o_proj.Qidxs', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.Wscale', 'model.layers.10.self_attn.o_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.Qidxs', 'model.layers.10.self_attn.qkv_proj.SU', 'model.layers.10.self_attn.qkv_proj.SV', 'model.layers.10.self_attn.qkv_proj.Wscale', 'model.layers.10.self_attn.qkv_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.fuse_scales', 'model.layers.11.mlp.down_proj.Qidxs', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.Wscale', 'model.layers.11.mlp.down_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.Qidxs', 'model.layers.11.mlp.upgate_proj.SU', 'model.layers.11.mlp.upgate_proj.SV', 'model.layers.11.mlp.upgate_proj.Wscale', 'model.layers.11.mlp.upgate_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.fuse_scales', 'model.layers.11.self_attn.o_proj.Qidxs', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.Wscale', 'model.layers.11.self_attn.o_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.Qidxs', 'model.layers.11.self_attn.qkv_proj.SU', 'model.layers.11.self_attn.qkv_proj.SV', 'model.layers.11.self_attn.qkv_proj.Wscale', 'model.layers.11.self_attn.qkv_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.fuse_scales', 'model.layers.12.mlp.down_proj.Qidxs', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.Wscale', 'model.layers.12.mlp.down_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.Qidxs', 'model.layers.12.mlp.upgate_proj.SU', 'model.layers.12.mlp.upgate_proj.SV', 'model.layers.12.mlp.upgate_proj.Wscale', 'model.layers.12.mlp.upgate_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.fuse_scales', 'model.layers.12.self_attn.o_proj.Qidxs', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.Wscale', 'model.layers.12.self_attn.o_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.Qidxs', 'model.layers.12.self_attn.qkv_proj.SU', 'model.layers.12.self_attn.qkv_proj.SV', 'model.layers.12.self_attn.qkv_proj.Wscale', 'model.layers.12.self_attn.qkv_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.fuse_scales', 'model.layers.13.mlp.down_proj.Qidxs', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.Wscale', 'model.layers.13.mlp.down_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.Qidxs', 'model.layers.13.mlp.upgate_proj.SU', 'model.layers.13.mlp.upgate_proj.SV', 'model.layers.13.mlp.upgate_proj.Wscale', 'model.layers.13.mlp.upgate_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.fuse_scales', 'model.layers.13.self_attn.o_proj.Qidxs', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.Wscale', 'model.layers.13.self_attn.o_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.Qidxs', 'model.layers.13.self_attn.qkv_proj.SU', 'model.layers.13.self_attn.qkv_proj.SV', 'model.layers.13.self_attn.qkv_proj.Wscale', 'model.layers.13.self_attn.qkv_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.fuse_scales', 'model.layers.14.mlp.down_proj.Qidxs', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.Wscale', 'model.layers.14.mlp.down_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.Qidxs', 'model.layers.14.mlp.upgate_proj.SU', 'model.layers.14.mlp.upgate_proj.SV', 'model.layers.14.mlp.upgate_proj.Wscale', 'model.layers.14.mlp.upgate_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.fuse_scales', 'model.layers.14.self_attn.o_proj.Qidxs', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.Wscale', 'model.layers.14.self_attn.o_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.Qidxs', 'model.layers.14.self_attn.qkv_proj.SU', 'model.layers.14.self_attn.qkv_proj.SV', 'model.layers.14.self_attn.qkv_proj.Wscale', 'model.layers.14.self_attn.qkv_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.fuse_scales', 'model.layers.15.mlp.down_proj.Qidxs', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.Wscale', 'model.layers.15.mlp.down_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.Qidxs', 'model.layers.15.mlp.upgate_proj.SU', 'model.layers.15.mlp.upgate_proj.SV', 'model.layers.15.mlp.upgate_proj.Wscale', 'model.layers.15.mlp.upgate_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.fuse_scales', 'model.layers.15.self_attn.o_proj.Qidxs', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.Wscale', 'model.layers.15.self_attn.o_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.Qidxs', 'model.layers.15.self_attn.qkv_proj.SU', 'model.layers.15.self_attn.qkv_proj.SV', 'model.layers.15.self_attn.qkv_proj.Wscale', 'model.layers.15.self_attn.qkv_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.fuse_scales', 'model.layers.16.mlp.down_proj.Qidxs', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.Wscale', 'model.layers.16.mlp.down_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.Qidxs', 'model.layers.16.mlp.upgate_proj.SU', 'model.layers.16.mlp.upgate_proj.SV', 'model.layers.16.mlp.upgate_proj.Wscale', 'model.layers.16.mlp.upgate_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.fuse_scales', 'model.layers.16.self_attn.o_proj.Qidxs', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.Wscale', 'model.layers.16.self_attn.o_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.Qidxs', 'model.layers.16.self_attn.qkv_proj.SU', 'model.layers.16.self_attn.qkv_proj.SV', 'model.layers.16.self_attn.qkv_proj.Wscale', 'model.layers.16.self_attn.qkv_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.fuse_scales', 'model.layers.17.mlp.down_proj.Qidxs', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.Wscale', 'model.layers.17.mlp.down_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.Qidxs', 'model.layers.17.mlp.upgate_proj.SU', 'model.layers.17.mlp.upgate_proj.SV', 'model.layers.17.mlp.upgate_proj.Wscale', 'model.layers.17.mlp.upgate_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.fuse_scales', 'model.layers.17.self_attn.o_proj.Qidxs', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.Wscale', 'model.layers.17.self_attn.o_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.Qidxs', 'model.layers.17.self_attn.qkv_proj.SU', 'model.layers.17.self_attn.qkv_proj.SV', 'model.layers.17.self_attn.qkv_proj.Wscale', 'model.layers.17.self_attn.qkv_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.fuse_scales', 'model.layers.18.mlp.down_proj.Qidxs', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.Wscale', 'model.layers.18.mlp.down_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.Qidxs', 'model.layers.18.mlp.upgate_proj.SU', 'model.layers.18.mlp.upgate_proj.SV', 'model.layers.18.mlp.upgate_proj.Wscale', 'model.layers.18.mlp.upgate_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.fuse_scales', 'model.layers.18.self_attn.o_proj.Qidxs', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.Wscale', 'model.layers.18.self_attn.o_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.Qidxs', 'model.layers.18.self_attn.qkv_proj.SU', 'model.layers.18.self_attn.qkv_proj.SV', 'model.layers.18.self_attn.qkv_proj.Wscale', 'model.layers.18.self_attn.qkv_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.fuse_scales', 'model.layers.19.mlp.down_proj.Qidxs', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.Wscale', 'model.layers.19.mlp.down_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.Qidxs', 'model.layers.19.mlp.upgate_proj.SU', 'model.layers.19.mlp.upgate_proj.SV', 'model.layers.19.mlp.upgate_proj.Wscale', 'model.layers.19.mlp.upgate_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.fuse_scales', 'model.layers.19.self_attn.o_proj.Qidxs', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.Wscale', 'model.layers.19.self_attn.o_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.Qidxs', 'model.layers.19.self_attn.qkv_proj.SU', 'model.layers.19.self_attn.qkv_proj.SV', 'model.layers.19.self_attn.qkv_proj.Wscale', 'model.layers.19.self_attn.qkv_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.fuse_scales', 'model.layers.2.mlp.down_proj.Qidxs', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.Wscale', 'model.layers.2.mlp.down_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.Qidxs', 'model.layers.2.mlp.upgate_proj.SU', 'model.layers.2.mlp.upgate_proj.SV', 'model.layers.2.mlp.upgate_proj.Wscale', 'model.layers.2.mlp.upgate_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.fuse_scales', 'model.layers.2.self_attn.o_proj.Qidxs', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.Wscale', 'model.layers.2.self_attn.o_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.Qidxs', 'model.layers.2.self_attn.qkv_proj.SU', 'model.layers.2.self_attn.qkv_proj.SV', 'model.layers.2.self_attn.qkv_proj.Wscale', 'model.layers.2.self_attn.qkv_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.fuse_scales', 'model.layers.20.mlp.down_proj.Qidxs', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.Wscale', 'model.layers.20.mlp.down_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.Qidxs', 'model.layers.20.mlp.upgate_proj.SU', 'model.layers.20.mlp.upgate_proj.SV', 'model.layers.20.mlp.upgate_proj.Wscale', 'model.layers.20.mlp.upgate_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.fuse_scales', 'model.layers.20.self_attn.o_proj.Qidxs', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.Wscale', 'model.layers.20.self_attn.o_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.Qidxs', 'model.layers.20.self_attn.qkv_proj.SU', 'model.layers.20.self_attn.qkv_proj.SV', 'model.layers.20.self_attn.qkv_proj.Wscale', 'model.layers.20.self_attn.qkv_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.fuse_scales', 'model.layers.21.mlp.down_proj.Qidxs', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.Wscale', 'model.layers.21.mlp.down_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.Qidxs', 'model.layers.21.mlp.upgate_proj.SU', 'model.layers.21.mlp.upgate_proj.SV', 'model.layers.21.mlp.upgate_proj.Wscale', 'model.layers.21.mlp.upgate_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.fuse_scales', 'model.layers.21.self_attn.o_proj.Qidxs', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.Wscale', 'model.layers.21.self_attn.o_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.Qidxs', 'model.layers.21.self_attn.qkv_proj.SU', 'model.layers.21.self_attn.qkv_proj.SV', 'model.layers.21.self_attn.qkv_proj.Wscale', 'model.layers.21.self_attn.qkv_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.fuse_scales', 'model.layers.22.mlp.down_proj.Qidxs', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.Wscale', 'model.layers.22.mlp.down_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.Qidxs', 'model.layers.22.mlp.upgate_proj.SU', 'model.layers.22.mlp.upgate_proj.SV', 'model.layers.22.mlp.upgate_proj.Wscale', 'model.layers.22.mlp.upgate_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.fuse_scales', 'model.layers.22.self_attn.o_proj.Qidxs', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.Wscale', 'model.layers.22.self_attn.o_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.Qidxs', 'model.layers.22.self_attn.qkv_proj.SU', 'model.layers.22.self_attn.qkv_proj.SV', 'model.layers.22.self_attn.qkv_proj.Wscale', 'model.layers.22.self_attn.qkv_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.fuse_scales', 'model.layers.23.mlp.down_proj.Qidxs', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.Wscale', 'model.layers.23.mlp.down_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.Qidxs', 'model.layers.23.mlp.upgate_proj.SU', 'model.layers.23.mlp.upgate_proj.SV', 'model.layers.23.mlp.upgate_proj.Wscale', 'model.layers.23.mlp.upgate_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.fuse_scales', 'model.layers.23.self_attn.o_proj.Qidxs', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.Wscale', 'model.layers.23.self_attn.o_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.Qidxs', 'model.layers.23.self_attn.qkv_proj.SU', 'model.layers.23.self_attn.qkv_proj.SV', 'model.layers.23.self_attn.qkv_proj.Wscale', 'model.layers.23.self_attn.qkv_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.fuse_scales', 'model.layers.24.mlp.down_proj.Qidxs', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.Wscale', 'model.layers.24.mlp.down_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.Qidxs', 'model.layers.24.mlp.upgate_proj.SU', 'model.layers.24.mlp.upgate_proj.SV', 'model.layers.24.mlp.upgate_proj.Wscale', 'model.layers.24.mlp.upgate_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.fuse_scales', 'model.layers.24.self_attn.o_proj.Qidxs', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.Wscale', 'model.layers.24.self_attn.o_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.Qidxs', 'model.layers.24.self_attn.qkv_proj.SU', 'model.layers.24.self_attn.qkv_proj.SV', 'model.layers.24.self_attn.qkv_proj.Wscale', 'model.layers.24.self_attn.qkv_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.fuse_scales', 'model.layers.25.mlp.down_proj.Qidxs', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.Wscale', 'model.layers.25.mlp.down_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.Qidxs', 'model.layers.25.mlp.upgate_proj.SU', 'model.layers.25.mlp.upgate_proj.SV', 'model.layers.25.mlp.upgate_proj.Wscale', 'model.layers.25.mlp.upgate_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.fuse_scales', 'model.layers.25.self_attn.o_proj.Qidxs', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.Wscale', 'model.layers.25.self_attn.o_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.Qidxs', 'model.layers.25.self_attn.qkv_proj.SU', 'model.layers.25.self_attn.qkv_proj.SV', 'model.layers.25.self_attn.qkv_proj.Wscale', 'model.layers.25.self_attn.qkv_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.fuse_scales', 'model.layers.26.mlp.down_proj.Qidxs', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.Wscale', 'model.layers.26.mlp.down_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.Qidxs', 'model.layers.26.mlp.upgate_proj.SU', 'model.layers.26.mlp.upgate_proj.SV', 'model.layers.26.mlp.upgate_proj.Wscale', 'model.layers.26.mlp.upgate_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.fuse_scales', 'model.layers.26.self_attn.o_proj.Qidxs', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.Wscale', 'model.layers.26.self_attn.o_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.Qidxs', 'model.layers.26.self_attn.qkv_proj.SU', 'model.layers.26.self_attn.qkv_proj.SV', 'model.layers.26.self_attn.qkv_proj.Wscale', 'model.layers.26.self_attn.qkv_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.fuse_scales', 'model.layers.27.mlp.down_proj.Qidxs', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.Wscale', 'model.layers.27.mlp.down_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.Qidxs', 'model.layers.27.mlp.upgate_proj.SU', 'model.layers.27.mlp.upgate_proj.SV', 'model.layers.27.mlp.upgate_proj.Wscale', 'model.layers.27.mlp.upgate_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.fuse_scales', 'model.layers.27.self_attn.o_proj.Qidxs', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.Wscale', 'model.layers.27.self_attn.o_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.Qidxs', 'model.layers.27.self_attn.qkv_proj.SU', 'model.layers.27.self_attn.qkv_proj.SV', 'model.layers.27.self_attn.qkv_proj.Wscale', 'model.layers.27.self_attn.qkv_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.fuse_scales', 'model.layers.28.mlp.down_proj.Qidxs', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.Wscale', 'model.layers.28.mlp.down_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.Qidxs', 'model.layers.28.mlp.upgate_proj.SU', 'model.layers.28.mlp.upgate_proj.SV', 'model.layers.28.mlp.upgate_proj.Wscale', 'model.layers.28.mlp.upgate_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.fuse_scales', 'model.layers.28.self_attn.o_proj.Qidxs', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.Wscale', 'model.layers.28.self_attn.o_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.Qidxs', 'model.layers.28.self_attn.qkv_proj.SU', 'model.layers.28.self_attn.qkv_proj.SV', 'model.layers.28.self_attn.qkv_proj.Wscale', 'model.layers.28.self_attn.qkv_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.fuse_scales', 'model.layers.29.mlp.down_proj.Qidxs', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.Wscale', 'model.layers.29.mlp.down_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.Qidxs', 'model.layers.29.mlp.upgate_proj.SU', 'model.layers.29.mlp.upgate_proj.SV', 'model.layers.29.mlp.upgate_proj.Wscale', 'model.layers.29.mlp.upgate_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.fuse_scales', 'model.layers.29.self_attn.o_proj.Qidxs', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.Wscale', 'model.layers.29.self_attn.o_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.Qidxs', 'model.layers.29.self_attn.qkv_proj.SU', 'model.layers.29.self_attn.qkv_proj.SV', 'model.layers.29.self_attn.qkv_proj.Wscale', 'model.layers.29.self_attn.qkv_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.fuse_scales', 'model.layers.3.mlp.down_proj.Qidxs', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.Wscale', 'model.layers.3.mlp.down_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.Qidxs', 'model.layers.3.mlp.upgate_proj.SU', 'model.layers.3.mlp.upgate_proj.SV', 'model.layers.3.mlp.upgate_proj.Wscale', 'model.layers.3.mlp.upgate_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.fuse_scales', 'model.layers.3.self_attn.o_proj.Qidxs', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.Wscale', 'model.layers.3.self_attn.o_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.Qidxs', 'model.layers.3.self_attn.qkv_proj.SU', 'model.layers.3.self_attn.qkv_proj.SV', 'model.layers.3.self_attn.qkv_proj.Wscale', 'model.layers.3.self_attn.qkv_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.fuse_scales', 'model.layers.30.mlp.down_proj.Qidxs', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.Wscale', 'model.layers.30.mlp.down_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.Qidxs', 'model.layers.30.mlp.upgate_proj.SU', 'model.layers.30.mlp.upgate_proj.SV', 'model.layers.30.mlp.upgate_proj.Wscale', 'model.layers.30.mlp.upgate_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.fuse_scales', 'model.layers.30.self_attn.o_proj.Qidxs', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.Wscale', 'model.layers.30.self_attn.o_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.Qidxs', 'model.layers.30.self_attn.qkv_proj.SU', 'model.layers.30.self_attn.qkv_proj.SV', 'model.layers.30.self_attn.qkv_proj.Wscale', 'model.layers.30.self_attn.qkv_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.fuse_scales', 'model.layers.31.mlp.down_proj.Qidxs', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.Wscale', 'model.layers.31.mlp.down_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.Qidxs', 'model.layers.31.mlp.upgate_proj.SU', 'model.layers.31.mlp.upgate_proj.SV', 'model.layers.31.mlp.upgate_proj.Wscale', 'model.layers.31.mlp.upgate_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.fuse_scales', 'model.layers.31.self_attn.o_proj.Qidxs', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.Wscale', 'model.layers.31.self_attn.o_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.Qidxs', 'model.layers.31.self_attn.qkv_proj.SU', 'model.layers.31.self_attn.qkv_proj.SV', 'model.layers.31.self_attn.qkv_proj.Wscale', 'model.layers.31.self_attn.qkv_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.fuse_scales', 'model.layers.4.mlp.down_proj.Qidxs', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.Wscale', 'model.layers.4.mlp.down_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.Qidxs', 'model.layers.4.mlp.upgate_proj.SU', 'model.layers.4.mlp.upgate_proj.SV', 'model.layers.4.mlp.upgate_proj.Wscale', 'model.layers.4.mlp.upgate_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.fuse_scales', 'model.layers.4.self_attn.o_proj.Qidxs', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.Wscale', 'model.layers.4.self_attn.o_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.Qidxs', 'model.layers.4.self_attn.qkv_proj.SU', 'model.layers.4.self_attn.qkv_proj.SV', 'model.layers.4.self_attn.qkv_proj.Wscale', 'model.layers.4.self_attn.qkv_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.fuse_scales', 'model.layers.5.mlp.down_proj.Qidxs', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.Wscale', 'model.layers.5.mlp.down_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.Qidxs', 'model.layers.5.mlp.upgate_proj.SU', 'model.layers.5.mlp.upgate_proj.SV', 'model.layers.5.mlp.upgate_proj.Wscale', 'model.layers.5.mlp.upgate_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.fuse_scales', 'model.layers.5.self_attn.o_proj.Qidxs', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.Wscale', 'model.layers.5.self_attn.o_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.Qidxs', 'model.layers.5.self_attn.qkv_proj.SU', 'model.layers.5.self_attn.qkv_proj.SV', 'model.layers.5.self_attn.qkv_proj.Wscale', 'model.layers.5.self_attn.qkv_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.fuse_scales', 'model.layers.6.mlp.down_proj.Qidxs', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.Wscale', 'model.layers.6.mlp.down_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.Qidxs', 'model.layers.6.mlp.upgate_proj.SU', 'model.layers.6.mlp.upgate_proj.SV', 'model.layers.6.mlp.upgate_proj.Wscale', 'model.layers.6.mlp.upgate_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.fuse_scales', 'model.layers.6.self_attn.o_proj.Qidxs', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.Wscale', 'model.layers.6.self_attn.o_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.Qidxs', 'model.layers.6.self_attn.qkv_proj.SU', 'model.layers.6.self_attn.qkv_proj.SV', 'model.layers.6.self_attn.qkv_proj.Wscale', 'model.layers.6.self_attn.qkv_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.fuse_scales', 'model.layers.7.mlp.down_proj.Qidxs', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.Wscale', 'model.layers.7.mlp.down_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.Qidxs', 'model.layers.7.mlp.upgate_proj.SU', 'model.layers.7.mlp.upgate_proj.SV', 'model.layers.7.mlp.upgate_proj.Wscale', 'model.layers.7.mlp.upgate_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.fuse_scales', 'model.layers.7.self_attn.o_proj.Qidxs', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.Wscale', 'model.layers.7.self_attn.o_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.Qidxs', 'model.layers.7.self_attn.qkv_proj.SU', 'model.layers.7.self_attn.qkv_proj.SV', 'model.layers.7.self_attn.qkv_proj.Wscale', 'model.layers.7.self_attn.qkv_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.fuse_scales', 'model.layers.8.mlp.down_proj.Qidxs', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.Wscale', 'model.layers.8.mlp.down_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.Qidxs', 'model.layers.8.mlp.upgate_proj.SU', 'model.layers.8.mlp.upgate_proj.SV', 'model.layers.8.mlp.upgate_proj.Wscale', 'model.layers.8.mlp.upgate_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.fuse_scales', 'model.layers.8.self_attn.o_proj.Qidxs', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.Wscale', 'model.layers.8.self_attn.o_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.Qidxs', 'model.layers.8.self_attn.qkv_proj.SU', 'model.layers.8.self_attn.qkv_proj.SV', 'model.layers.8.self_attn.qkv_proj.Wscale', 'model.layers.8.self_attn.qkv_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.fuse_scales', 'model.layers.9.mlp.down_proj.Qidxs', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.Wscale', 'model.layers.9.mlp.down_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.Qidxs', 'model.layers.9.mlp.upgate_proj.SU', 'model.layers.9.mlp.upgate_proj.SV', 'model.layers.9.mlp.upgate_proj.Wscale', 'model.layers.9.mlp.upgate_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.fuse_scales', 'model.layers.9.self_attn.o_proj.Qidxs', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.Wscale', 'model.layers.9.self_attn.o_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.Qidxs', 'model.layers.9.self_attn.qkv_proj.SU', 'model.layers.9.self_attn.qkv_proj.SV', 'model.layers.9.self_attn.qkv_proj.Wscale', 'model.layers.9.self_attn.qkv_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.fuse_scales']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
I0908 02:49:59.076778 101896 hfize_llama.py:78] loaded layer 0 down
I0908 02:49:59.291640 101896 hfize_llama.py:78] loaded layer 1 down
I0908 02:49:59.437754 101896 hfize_llama.py:78] loaded layer 2 down
I0908 02:49:59.580555 101896 hfize_llama.py:78] loaded layer 3 down
I0908 02:49:59.728252 101896 hfize_llama.py:78] loaded layer 4 down
I0908 02:49:59.879619 101896 hfize_llama.py:78] loaded layer 5 down
I0908 02:50:00.090803 101896 hfize_llama.py:78] loaded layer 6 down
I0908 02:50:00.272218 101896 hfize_llama.py:78] loaded layer 7 down
I0908 02:50:00.470922 101896 hfize_llama.py:78] loaded layer 8 down
I0908 02:50:00.592244 101896 hfize_llama.py:78] loaded layer 9 down
I0908 02:50:00.817909 101896 hfize_llama.py:78] loaded layer 10 down
I0908 02:50:00.977816 101896 hfize_llama.py:78] loaded layer 11 down
I0908 02:50:01.354821 101896 hfize_llama.py:78] loaded layer 12 down
I0908 02:50:01.534127 101896 hfize_llama.py:78] loaded layer 13 down
I0908 02:50:01.823439 101896 hfize_llama.py:78] loaded layer 14 down
I0908 02:50:01.990917 101896 hfize_llama.py:78] loaded layer 15 down
I0908 02:50:02.168953 101896 hfize_llama.py:78] loaded layer 16 down
I0908 02:50:02.376915 101896 hfize_llama.py:78] loaded layer 17 down
I0908 02:50:02.557214 101896 hfize_llama.py:78] loaded layer 18 down
I0908 02:50:02.699267 101896 hfize_llama.py:78] loaded layer 19 down
I0908 02:50:02.850203 101896 hfize_llama.py:78] loaded layer 20 down
I0908 02:50:03.011706 101896 hfize_llama.py:78] loaded layer 21 down
I0908 02:50:03.193498 101896 hfize_llama.py:78] loaded layer 22 down
I0908 02:50:03.348773 101896 hfize_llama.py:78] loaded layer 23 down
I0908 02:50:03.519228 101896 hfize_llama.py:78] loaded layer 24 down
I0908 02:50:03.676262 101896 hfize_llama.py:78] loaded layer 25 down
I0908 02:50:03.832189 101896 hfize_llama.py:78] loaded layer 26 down
I0908 02:50:04.076024 101896 hfize_llama.py:78] loaded layer 27 down
I0908 02:50:04.273968 101896 hfize_llama.py:78] loaded layer 28 down
I0908 02:50:04.388943 101896 hfize_llama.py:78] loaded layer 29 down
I0908 02:50:04.583762 101896 hfize_llama.py:78] loaded layer 30 down
I0908 02:50:04.760152 101896 hfize_llama.py:78] loaded layer 31 down
I0908 02:50:04.760335 101896 hfize_llama.py:80] saving model...
I0908 02:50:12.462735 101896 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
I0908 02:50:13.216700 101896 hfize_llama.py:87] successfully loaded hfized model
I0908 02:50:13.216891 101896 hfize_llama.py:89] generating some text...
I0908 02:50:31.485237 101896 hfize_llama.py:100] <s> It is a truth universally acknowledged that when one is in the midst of a divorce, one must find a way to deal with all the emotions, and to try to stay sane. The emotions are going to come, and if you are not prepared for them, they are going to come out in the form of anger, depression,
I0908 02:50:31.485415 101896 hfize_llama.py:101] elapsed: 18.268486499786377
W0908 02:50:36.273025 102041 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 02:50:36.277139 102041 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0908 02:50:36.366046 102041 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0908 02:50:58.108027 102041 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
W0908 02:51:01.185513 102041 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 71, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/eval/eval_ppl.py", line 33, in main
    input_tok = gptq_data_utils.get_test_tokens(dataset,
  File "/home/user/benchmarks/quip-sharp/lib/utils/gptq_data_utils.py", line 198, in get_test_tokens
    return get_wikitext2(train_samples, seed, seqlen,
  File "/home/user/benchmarks/quip-sharp/lib/utils/gptq_data_utils.py", line 20, in get_wikitext2
    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 862, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2089, in from_pretrained
    return cls._from_pretrained(
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2311, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py", line 169, in __init__
    self.sp_model = self.get_spm_processor(kwargs.pop("from_slow", False))
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py", line 201, in get_spm_processor
    model_pb2 = import_protobuf(f"The new behaviour of {self.__class__.__name__} (with `self.legacy = False`)")
  File "/home/user/miniconda3/envs/quip/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 43, in import_protobuf
    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))
ImportError: 
The new behaviour of LlamaTokenizer (with `self.legacy = False`) requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

W0908 02:51:05.559620 102152 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 02:51:05.623206 102152 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0908 02:51:05.636023 102152 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0908 02:51:26.984382 102152 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
W0908 02:51:27.770056 102152 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 02:51:27.927767 102152 eval_zeroshot.py:33] loaded model!
Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]Downloading data: 100%|██████████| 1.82M/1.82M [00:00<00:00, 74.1MB/s]
Downloading data:   0%|          | 0.00/220k [00:00<?, ?B/s]Downloading data: 815kB [00:00, 104MB/s]                    
Generating train split:   0%|          | 0/16113 [00:00<?, ? examples/s]Generating train split:  25%|██▌       | 4045/16113 [00:00<00:00, 40355.65 examples/s]Generating train split:  51%|█████     | 8174/16113 [00:00<00:00, 40322.01 examples/s]Generating train split:  81%|████████▏ | 13129/16113 [00:00<00:00, 44496.32 examples/s]Generating train split: 100%|██████████| 16113/16113 [00:00<00:00, 44186.54 examples/s]
Generating test split:   0%|          | 0/3084 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 3084/3084 [00:00<00:00, 49757.21 examples/s]
Generating validation split:   0%|          | 0/1838 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 1838/1838 [00:00<00:00, 47066.88 examples/s]
Generating train split:   0%|          | 0/40398 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 40398/40398 [00:00<00:00, 570477.42 examples/s]
Generating test split:   0%|          | 0/1767 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1767/1767 [00:00<00:00, 670042.05 examples/s]
Generating validation split:   0%|          | 0/1267 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 1267/1267 [00:00<00:00, 570191.33 examples/s]
Running loglikelihood requests
  0%|          | 0/26933 [00:00<?, ?it/s]  0%|          | 4/26933 [00:02<4:25:40,  1.69it/s]  0%|          | 8/26933 [00:02<2:27:03,  3.05it/s]  0%|          | 12/26933 [00:03<1:47:43,  4.17it/s]  0%|          | 16/26933 [00:04<1:30:58,  4.93it/s]  0%|          | 20/26933 [00:04<1:20:56,  5.54it/s]  0%|          | 24/26933 [00:05<1:14:21,  6.03it/s]  0%|          | 28/26933 [00:05<1:09:38,  6.44it/s]  0%|          | 32/26933 [00:06<1:06:27,  6.75it/s]  0%|          | 36/26933 [00:06<1:03:45,  7.03it/s]  0%|          | 40/26933 [00:07<1:01:31,  7.29it/s]  0%|          | 44/26933 [00:07<59:45,  7.50it/s]    0%|          | 48/26933 [00:08<58:23,  7.67it/s]  0%|          | 52/26933 [00:08<57:22,  7.81it/s]  0%|          | 56/26933 [00:09<56:08,  7.98it/s]  0%|          | 60/26933 [00:09<55:05,  8.13it/s]  0%|          | 64/26933 [00:10<54:05,  8.28it/s]  0%|          | 68/26933 [00:10<53:24,  8.38it/s]  0%|          | 72/26933 [00:11<52:44,  8.49it/s]  0%|          | 76/26933 [00:11<52:20,  8.55it/s]  0%|          | 80/26933 [00:11<51:48,  8.64it/s]  0%|          | 84/26933 [00:12<51:27,  8.69it/s]  0%|          | 88/26933 [00:12<51:00,  8.77it/s]  0%|          | 92/26933 [00:13<50:45,  8.81it/s]  0%|          | 96/26933 [00:13<50:13,  8.91it/s]  0%|          | 100/26933 [00:14<49:58,  8.95it/s]  0%|          | 104/26933 [00:14<49:38,  9.01it/s]  0%|          | 108/26933 [00:15<49:19,  9.06it/s]  0%|          | 112/26933 [00:15<48:59,  9.12it/s]  0%|          | 116/26933 [00:15<48:42,  9.18it/s]  0%|          | 120/26933 [00:16<48:24,  9.23it/s]  0%|          | 124/26933 [00:16<48:14,  9.26it/s]  0%|          | 128/26933 [00:17<48:02,  9.30it/s]  0%|          | 132/26933 [00:17<47:46,  9.35it/s]  1%|          | 136/26933 [00:18<47:25,  9.42it/s]  1%|          | 140/26933 [00:18<47:09,  9.47it/s]  1%|          | 144/26933 [00:18<46:55,  9.52it/s]  1%|          | 148/26933 [00:19<46:52,  9.53it/s]  1%|          | 152/26933 [00:19<46:47,  9.54it/s]  1%|          | 156/26933 [00:20<46:47,  9.54it/s]  1%|          | 160/26933 [00:20<46:34,  9.58it/s]  1%|          | 164/26933 [00:20<46:32,  9.59it/s]  1%|          | 168/26933 [00:21<46:23,  9.61it/s]  1%|          | 172/26933 [00:21<46:09,  9.66it/s]  1%|          | 176/26933 [00:22<46:03,  9.68it/s]  1%|          | 180/26933 [00:22<46:01,  9.69it/s]  1%|          | 184/26933 [00:23<46:10,  9.66it/s]  1%|          | 188/26933 [00:23<46:07,  9.66it/s]  1%|          | 192/26933 [00:23<46:09,  9.65it/s]  1%|          | 196/26933 [00:24<46:00,  9.69it/s]  1%|          | 200/26933 [00:24<45:47,  9.73it/s]  1%|          | 204/26933 [00:25<45:39,  9.76it/s]  1%|          | 208/26933 [00:25<45:36,  9.77it/s]  1%|          | 212/26933 [00:25<45:31,  9.78it/s]  1%|          | 216/26933 [00:26<45:27,  9.79it/s]  1%|          | 220/26933 [00:26<45:24,  9.81it/s]  1%|          | 224/26933 [00:27<45:16,  9.83it/s]  1%|          | 228/26933 [00:27<45:19,  9.82it/s]  1%|          | 232/26933 [00:27<45:19,  9.82it/s]  1%|          | 236/26933 [00:28<45:16,  9.83it/s]  1%|          | 240/26933 [00:28<45:10,  9.85it/s]  1%|          | 244/26933 [00:29<45:09,  9.85it/s]  1%|          | 248/26933 [00:29<45:10,  9.84it/s]  1%|          | 252/26933 [00:29<45:01,  9.88it/s]  1%|          | 256/26933 [00:30<44:56,  9.89it/s]  1%|          | 260/26933 [00:30<44:53,  9.90it/s]  1%|          | 264/26933 [00:31<44:48,  9.92it/s]  1%|          | 268/26933 [00:31<44:44,  9.93it/s]  1%|          | 272/26933 [00:31<44:33,  9.97it/s]  1%|          | 276/26933 [00:32<44:31,  9.98it/s]  1%|          | 280/26933 [00:32<44:24, 10.00it/s]  1%|          | 284/26933 [00:33<44:25, 10.00it/s]  1%|          | 288/26933 [00:33<44:16, 10.03it/s]  1%|          | 292/26933 [00:33<44:13, 10.04it/s]  1%|          | 296/26933 [00:34<44:17, 10.02it/s]  1%|          | 300/26933 [00:34<44:11, 10.04it/s]  1%|          | 304/26933 [00:35<44:11, 10.04it/s]  1%|          | 308/26933 [00:35<44:06, 10.06it/s]  1%|          | 312/26933 [00:35<43:57, 10.09it/s]  1%|          | 316/26933 [00:36<44:07, 10.05it/s]  1%|          | 320/26933 [00:36<44:06, 10.06it/s]  1%|          | 324/26933 [00:37<44:04, 10.06it/s]  1%|          | 328/26933 [00:37<43:48, 10.12it/s]  1%|          | 332/26933 [00:37<43:40, 10.15it/s]  1%|          | 336/26933 [00:38<43:38, 10.16it/s]  1%|▏         | 340/26933 [00:38<43:37, 10.16it/s]  1%|▏         | 344/26933 [00:39<43:29, 10.19it/s]  1%|▏         | 348/26933 [00:39<43:18, 10.23it/s]  1%|▏         | 352/26933 [00:39<43:09, 10.27it/s]  1%|▏         | 356/26933 [00:40<43:03, 10.29it/s]  1%|▏         | 360/26933 [00:40<42:59, 10.30it/s]  1%|▏         | 364/26933 [00:41<43:01, 10.29it/s]  1%|▏         | 368/26933 [00:41<43:01, 10.29it/s]  1%|▏         | 372/26933 [00:41<42:49, 10.34it/s]  1%|▏         | 376/26933 [00:42<42:47, 10.35it/s]  1%|▏         | 380/26933 [00:42<42:41, 10.37it/s]  1%|▏         | 384/26933 [00:42<42:39, 10.37it/s]  1%|▏         | 388/26933 [00:43<42:39, 10.37it/s]  1%|▏         | 392/26933 [00:43<42:37, 10.38it/s]  1%|▏         | 396/26933 [00:44<42:34, 10.39it/s]  1%|▏         | 400/26933 [00:44<42:41, 10.36it/s]  2%|▏         | 404/26933 [00:44<42:42, 10.35it/s]  2%|▏         | 408/26933 [00:45<42:47, 10.33it/s]  2%|▏         | 412/26933 [00:45<42:39, 10.36it/s]  2%|▏         | 416/26933 [00:46<42:37, 10.37it/s]  2%|▏         | 420/26933 [00:46<42:34, 10.38it/s]  2%|▏         | 424/26933 [00:46<42:29, 10.40it/s]  2%|▏         | 428/26933 [00:47<42:27, 10.40it/s]  2%|▏         | 432/26933 [00:47<42:26, 10.41it/s]  2%|▏         | 436/26933 [00:47<42:20, 10.43it/s]  2%|▏         | 440/26933 [00:48<42:21, 10.43it/s]  2%|▏         | 444/26933 [00:48<42:20, 10.43it/s]  2%|▏         | 448/26933 [00:49<42:17, 10.44it/s]  2%|▏         | 452/26933 [00:49<42:18, 10.43it/s]  2%|▏         | 456/26933 [00:49<42:07, 10.48it/s]  2%|▏         | 460/26933 [00:50<42:07, 10.47it/s]  2%|▏         | 464/26933 [00:50<42:02, 10.49it/s]  2%|▏         | 468/26933 [00:51<41:57, 10.51it/s]  2%|▏         | 472/26933 [00:51<41:58, 10.51it/s]  2%|▏         | 476/26933 [00:51<41:52, 10.53it/s]  2%|▏         | 480/26933 [00:52<41:53, 10.52it/s]  2%|▏         | 484/26933 [00:52<41:48, 10.54it/s]  2%|▏         | 488/26933 [00:52<41:42, 10.57it/s]  2%|▏         | 492/26933 [00:53<41:37, 10.59it/s]  2%|▏         | 496/26933 [00:53<41:29, 10.62it/s]  2%|▏         | 500/26933 [00:54<41:26, 10.63it/s]  2%|▏         | 504/26933 [00:54<41:32, 10.60it/s]  2%|▏         | 508/26933 [00:54<41:34, 10.59it/s]  2%|▏         | 512/26933 [00:55<41:41, 10.56it/s]  2%|▏         | 516/26933 [00:55<41:34, 10.59it/s]  2%|▏         | 520/26933 [00:55<41:25, 10.63it/s]  2%|▏         | 524/26933 [00:56<41:22, 10.64it/s]  2%|▏         | 528/26933 [00:56<41:17, 10.66it/s]  2%|▏         | 532/26933 [00:57<41:16, 10.66it/s]  2%|▏         | 536/26933 [00:57<41:08, 10.69it/s]  2%|▏         | 540/26933 [00:57<41:03, 10.71it/s]  2%|▏         | 544/26933 [00:58<41:13, 10.67it/s]  2%|▏         | 548/26933 [00:58<41:28, 10.60it/s]  2%|▏         | 552/26933 [00:58<41:22, 10.62it/s]  2%|▏         | 556/26933 [00:59<41:24, 10.62it/s]  2%|▏         | 560/26933 [00:59<41:18, 10.64it/s]  2%|▏         | 564/26933 [01:00<41:18, 10.64it/s]  2%|▏         | 568/26933 [01:00<41:12, 10.66it/s]  2%|▏         | 572/26933 [01:00<41:04, 10.69it/s]  2%|▏         | 576/26933 [01:01<41:06, 10.69it/s]  2%|▏         | 580/26933 [01:01<41:00, 10.71it/s]  2%|▏         | 584/26933 [01:01<40:55, 10.73it/s]  2%|▏         | 588/26933 [01:02<40:54, 10.73it/s]  2%|▏         | 592/26933 [01:02<40:50, 10.75it/s]  2%|▏         | 596/26933 [01:03<40:45, 10.77it/s]  2%|▏         | 600/26933 [01:03<40:48, 10.76it/s]  2%|▏         | 604/26933 [01:03<40:45, 10.77it/s]  2%|▏         | 608/26933 [01:04<40:45, 10.76it/s]  2%|▏         | 612/26933 [01:04<40:44, 10.77it/s]  2%|▏         | 616/26933 [01:04<40:42, 10.77it/s]  2%|▏         | 620/26933 [01:05<40:44, 10.76it/s]  2%|▏         | 624/26933 [01:05<40:39, 10.79it/s]  2%|▏         | 628/26933 [01:06<40:30, 10.82it/s]  2%|▏         | 632/26933 [01:06<40:30, 10.82it/s]  2%|▏         | 636/26933 [01:06<40:19, 10.87it/s]  2%|▏         | 640/26933 [01:07<40:10, 10.91it/s]  2%|▏         | 644/26933 [01:07<40:03, 10.94it/s]  2%|▏         | 648/26933 [01:07<40:04, 10.93it/s]  2%|▏         | 652/26933 [01:08<40:07, 10.92it/s]  2%|▏         | 656/26933 [01:08<38:54, 11.26it/s]  2%|▏         | 660/26933 [01:08<37:49, 11.58it/s]  2%|▏         | 664/26933 [01:09<37:07, 11.79it/s]  2%|▏         | 668/26933 [01:09<36:34, 11.97it/s]  2%|▏         | 672/26933 [01:09<37:37, 11.63it/s]  3%|▎         | 676/26933 [01:10<37:30, 11.67it/s]  3%|▎         | 680/26933 [01:10<37:06, 11.79it/s]  3%|▎         | 684/26933 [01:10<36:50, 11.87it/s]  3%|▎         | 688/26933 [01:11<36:22, 12.03it/s]  3%|▎         | 692/26933 [01:11<36:00, 12.14it/s]  3%|▎         | 696/26933 [01:11<35:45, 12.23it/s]  3%|▎         | 700/26933 [01:12<35:35, 12.28it/s]  3%|▎         | 704/26933 [01:12<35:27, 12.33it/s]  3%|▎         | 708/26933 [01:12<35:23, 12.35it/s]  3%|▎         | 712/26933 [01:13<36:46, 11.88it/s]  3%|▎         | 716/26933 [01:13<37:32, 11.64it/s]  3%|▎         | 720/26933 [01:13<38:04, 11.48it/s]  3%|▎         | 724/26933 [01:14<38:29, 11.35it/s]  3%|▎         | 728/26933 [01:14<38:49, 11.25it/s]  3%|▎         | 732/26933 [01:14<38:58, 11.21it/s]  3%|▎         | 736/26933 [01:15<39:07, 11.16it/s]  3%|▎         | 740/26933 [01:15<39:18, 11.11it/s]  3%|▎         | 744/26933 [01:16<39:14, 11.12it/s]  3%|▎         | 748/26933 [01:16<39:26, 11.06it/s]  3%|▎         | 752/26933 [01:16<39:22, 11.08it/s]  3%|▎         | 756/26933 [01:17<39:21, 11.08it/s]  3%|▎         | 760/26933 [01:17<39:18, 11.10it/s]  3%|▎         | 764/26933 [01:17<39:13, 11.12it/s]  3%|▎         | 768/26933 [01:18<39:14, 11.11it/s]  3%|▎         | 772/26933 [01:18<39:15, 11.10it/s]  3%|▎         | 776/26933 [01:18<39:13, 11.11it/s]  3%|▎         | 780/26933 [01:19<39:24, 11.06it/s]  3%|▎         | 784/26933 [01:19<39:17, 11.09it/s]  3%|▎         | 788/26933 [01:20<39:15, 11.10it/s]  3%|▎         | 792/26933 [01:20<39:22, 11.07it/s]  3%|▎         | 796/26933 [01:20<38:19, 11.37it/s]  3%|▎         | 800/26933 [01:21<37:31, 11.61it/s]  3%|▎         | 804/26933 [01:21<36:56, 11.79it/s]  3%|▎         | 808/26933 [01:21<36:32, 11.91it/s]  3%|▎         | 812/26933 [01:22<36:03, 12.07it/s]  3%|▎         | 816/26933 [01:22<35:51, 12.14it/s]  3%|▎         | 820/26933 [01:22<35:38, 12.21it/s]  3%|▎         | 824/26933 [01:23<35:33, 12.23it/s]  3%|▎         | 828/26933 [01:23<35:33, 12.24it/s]  3%|▎         | 832/26933 [01:23<35:25, 12.28it/s]  3%|▎         | 836/26933 [01:24<36:29, 11.92it/s]  3%|▎         | 840/26933 [01:24<37:19, 11.65it/s]  3%|▎         | 844/26933 [01:24<37:59, 11.45it/s]  3%|▎         | 848/26933 [01:25<38:16, 11.36it/s]  3%|▎         | 852/26933 [01:25<38:25, 11.31it/s]  3%|▎         | 856/26933 [01:25<38:27, 11.30it/s]  3%|▎         | 860/26933 [01:26<38:37, 11.25it/s]  3%|▎         | 864/26933 [01:26<38:34, 11.27it/s]  3%|▎         | 868/26933 [01:26<38:27, 11.30it/s]  3%|▎         | 872/26933 [01:27<38:24, 11.31it/s]  3%|▎         | 876/26933 [01:27<38:26, 11.30it/s]  3%|▎         | 880/26933 [01:27<38:22, 11.31it/s]  3%|▎         | 884/26933 [01:28<38:28, 11.28it/s]  3%|▎         | 888/26933 [01:28<38:33, 11.26it/s]  3%|▎         | 892/26933 [01:29<38:32, 11.26it/s]  3%|▎         | 896/26933 [01:29<38:37, 11.23it/s]  3%|▎         | 900/26933 [01:29<38:33, 11.25it/s]  3%|▎         | 904/26933 [01:30<38:54, 11.15it/s]  3%|▎         | 908/26933 [01:30<38:44, 11.20it/s]  3%|▎         | 912/26933 [01:30<38:45, 11.19it/s]  3%|▎         | 916/26933 [01:31<38:44, 11.19it/s]  3%|▎         | 920/26933 [01:31<38:41, 11.20it/s]  3%|▎         | 924/26933 [01:31<38:31, 11.25it/s]  3%|▎         | 928/26933 [01:32<38:28, 11.26it/s]  3%|▎         | 932/26933 [01:32<38:22, 11.29it/s]  3%|▎         | 936/26933 [01:32<38:20, 11.30it/s]  3%|▎         | 940/26933 [01:33<38:22, 11.29it/s]  4%|▎         | 944/26933 [01:33<38:22, 11.29it/s]  4%|▎         | 948/26933 [01:33<38:15, 11.32it/s]  4%|▎         | 952/26933 [01:34<38:17, 11.31it/s]  4%|▎         | 956/26933 [01:34<38:13, 11.33it/s]  4%|▎         | 960/26933 [01:35<38:11, 11.34it/s]  4%|▎         | 964/26933 [01:35<38:18, 11.30it/s]  4%|▎         | 968/26933 [01:35<38:10, 11.34it/s]  4%|▎         | 972/26933 [01:36<38:02, 11.38it/s]  4%|▎         | 976/26933 [01:36<38:01, 11.38it/s]  4%|▎         | 980/26933 [01:36<37:58, 11.39it/s]  4%|▎         | 984/26933 [01:37<37:56, 11.40it/s]  4%|▎         | 988/26933 [01:37<37:51, 11.42it/s]  4%|▎         | 992/26933 [01:37<37:47, 11.44it/s]  4%|▎         | 996/26933 [01:38<37:52, 11.41it/s]  4%|▎         | 1000/26933 [01:38<37:44, 11.45it/s]  4%|▎         | 1004/26933 [01:38<37:40, 11.47it/s]  4%|▎         | 1008/26933 [01:39<37:40, 11.47it/s]  4%|▍         | 1012/26933 [01:39<37:41, 11.46it/s]  4%|▍         | 1016/26933 [01:39<37:39, 11.47it/s]  4%|▍         | 1020/26933 [01:40<37:46, 11.43it/s]  4%|▍         | 1024/26933 [01:40<37:41, 11.46it/s]  4%|▍         | 1028/26933 [01:40<37:43, 11.44it/s]  4%|▍         | 1032/26933 [01:41<37:42, 11.45it/s]  4%|▍         | 1036/26933 [01:41<37:40, 11.46it/s]  4%|▍         | 1040/26933 [01:42<36:42, 11.75it/s]  4%|▍         | 1044/26933 [01:42<35:58, 11.99it/s]  4%|▍         | 1048/26933 [01:42<35:22, 12.20it/s]  4%|▍         | 1052/26933 [01:42<34:57, 12.34it/s]  4%|▍         | 1056/26933 [01:43<34:43, 12.42it/s]  4%|▍         | 1060/26933 [01:43<34:25, 12.53it/s]  4%|▍         | 1064/26933 [01:43<34:14, 12.59it/s]  4%|▍         | 1068/26933 [01:44<34:11, 12.61it/s]  4%|▍         | 1072/26933 [01:44<34:07, 12.63it/s]  4%|▍         | 1076/26933 [01:44<34:02, 12.66it/s]  4%|▍         | 1080/26933 [01:45<34:04, 12.65it/s]  4%|▍         | 1084/26933 [01:45<33:59, 12.67it/s]  4%|▍         | 1088/26933 [01:45<33:59, 12.67it/s]  4%|▍         | 1092/26933 [01:46<33:56, 12.69it/s]  4%|▍         | 1096/26933 [01:46<33:53, 12.71it/s]  4%|▍         | 1100/26933 [01:46<33:52, 12.71it/s]  4%|▍         | 1104/26933 [01:47<33:51, 12.72it/s]  4%|▍         | 1108/26933 [01:47<33:52, 12.71it/s]  4%|▍         | 1112/26933 [01:47<33:59, 12.66it/s]  4%|▍         | 1116/26933 [01:47<34:02, 12.64it/s]  4%|▍         | 1120/26933 [01:48<34:13, 12.57it/s]  4%|▍         | 1124/26933 [01:48<34:15, 12.56it/s]  4%|▍         | 1128/26933 [01:48<34:12, 12.57it/s]  4%|▍         | 1132/26933 [01:49<34:16, 12.54it/s]  4%|▍         | 1136/26933 [01:49<34:10, 12.58it/s]  4%|▍         | 1140/26933 [01:49<34:04, 12.62it/s]  4%|▍         | 1144/26933 [01:50<35:33, 12.09it/s]  4%|▍         | 1148/26933 [01:50<36:03, 11.92it/s]  4%|▍         | 1152/26933 [01:50<35:26, 12.12it/s]  4%|▍         | 1156/26933 [01:51<35:03, 12.26it/s]  4%|▍         | 1160/26933 [01:51<34:31, 12.44it/s]  4%|▍         | 1164/26933 [01:51<34:14, 12.54it/s]  4%|▍         | 1168/26933 [01:52<34:02, 12.61it/s]  4%|▍         | 1172/26933 [01:52<33:54, 12.66it/s]  4%|▍         | 1176/26933 [01:52<33:40, 12.75it/s]  4%|▍         | 1180/26933 [01:53<33:41, 12.74it/s]  4%|▍         | 1184/26933 [01:53<33:54, 12.65it/s]  4%|▍         | 1188/26933 [01:53<33:56, 12.64it/s]  4%|▍         | 1192/26933 [01:54<34:15, 12.52it/s]  4%|▍         | 1196/26933 [01:54<34:06, 12.57it/s]  4%|▍         | 1200/26933 [01:54<34:05, 12.58it/s]  4%|▍         | 1204/26933 [01:55<34:08, 12.56it/s]  4%|▍         | 1208/26933 [01:55<34:20, 12.49it/s]  5%|▍         | 1212/26933 [01:55<34:16, 12.51it/s]  5%|▍         | 1216/26933 [01:56<34:25, 12.45it/s]  5%|▍         | 1220/26933 [01:56<34:24, 12.46it/s]  5%|▍         | 1224/26933 [01:56<34:05, 12.57it/s]  5%|▍         | 1228/26933 [01:56<33:49, 12.67it/s]  5%|▍         | 1232/26933 [01:57<34:43, 12.34it/s]  5%|▍         | 1236/26933 [01:57<35:17, 12.14it/s]  5%|▍         | 1240/26933 [01:57<36:08, 11.85it/s]  5%|▍         | 1244/26933 [01:58<36:21, 11.78it/s]  5%|▍         | 1248/26933 [01:58<36:27, 11.74it/s]  5%|▍         | 1252/26933 [01:59<36:32, 11.71it/s]  5%|▍         | 1256/26933 [01:59<35:49, 11.95it/s]  5%|▍         | 1260/26933 [01:59<35:05, 12.20it/s]  5%|▍         | 1264/26933 [01:59<34:49, 12.29it/s]  5%|▍         | 1268/26933 [02:00<34:56, 12.24it/s]  5%|▍         | 1272/26933 [02:00<34:44, 12.31it/s]  5%|▍         | 1276/26933 [02:00<34:13, 12.49it/s]  5%|▍         | 1280/26933 [02:01<33:54, 12.61it/s]  5%|▍         | 1284/26933 [02:01<33:35, 12.73it/s]  5%|▍         | 1288/26933 [02:01<33:22, 12.81it/s]  5%|▍         | 1292/26933 [02:02<33:17, 12.83it/s]  5%|▍         | 1296/26933 [02:02<33:11, 12.87it/s]  5%|▍         | 1300/26933 [02:02<33:03, 12.93it/s]  5%|▍         | 1304/26933 [02:03<33:02, 12.93it/s]  5%|▍         | 1308/26933 [02:03<32:56, 12.97it/s]  5%|▍         | 1312/26933 [02:03<32:50, 13.00it/s]  5%|▍         | 1316/26933 [02:04<32:50, 13.00it/s]  5%|▍         | 1320/26933 [02:04<32:55, 12.96it/s]  5%|▍         | 1324/26933 [02:04<32:54, 12.97it/s]  5%|▍         | 1328/26933 [02:04<32:53, 12.97it/s]  5%|▍         | 1332/26933 [02:05<32:55, 12.96it/s]  5%|▍         | 1336/26933 [02:05<32:49, 13.00it/s]  5%|▍         | 1340/26933 [02:05<32:57, 12.94it/s]  5%|▍         | 1344/26933 [02:06<33:02, 12.91it/s]  5%|▌         | 1348/26933 [02:06<32:56, 12.95it/s]  5%|▌         | 1352/26933 [02:06<32:47, 13.00it/s]  5%|▌         | 1356/26933 [02:07<32:48, 12.99it/s]  5%|▌         | 1360/26933 [02:07<32:50, 12.98it/s]  5%|▌         | 1364/26933 [02:07<32:49, 12.98it/s]  5%|▌         | 1368/26933 [02:08<32:48, 12.99it/s]  5%|▌         | 1372/26933 [02:08<32:48, 12.98it/s]  5%|▌         | 1376/26933 [02:08<32:44, 13.01it/s]  5%|▌         | 1380/26933 [02:08<32:37, 13.05it/s]  5%|▌         | 1384/26933 [02:09<32:44, 13.00it/s]  5%|▌         | 1388/26933 [02:09<32:32, 13.08it/s]  5%|▌         | 1392/26933 [02:09<32:24, 13.14it/s]  5%|▌         | 1396/26933 [02:10<32:20, 13.16it/s]  5%|▌         | 1400/26933 [02:10<32:13, 13.20it/s]  5%|▌         | 1404/26933 [02:10<32:13, 13.21it/s]  5%|▌         | 1408/26933 [02:11<32:18, 13.16it/s]  5%|▌         | 1412/26933 [02:11<32:21, 13.14it/s]  5%|▌         | 1416/26933 [02:11<32:22, 13.14it/s]  5%|▌         | 1420/26933 [02:11<32:23, 13.13it/s]  5%|▌         | 1424/26933 [02:12<32:35, 13.04it/s]  5%|▌         | 1428/26933 [02:12<32:31, 13.07it/s]  5%|▌         | 1432/26933 [02:12<32:27, 13.10it/s]  5%|▌         | 1436/26933 [02:13<32:28, 13.08it/s]  5%|▌         | 1440/26933 [02:13<32:18, 13.15it/s]  5%|▌         | 1444/26933 [02:13<32:12, 13.19it/s]  5%|▌         | 1448/26933 [02:14<32:10, 13.20it/s]  5%|▌         | 1452/26933 [02:14<32:07, 13.22it/s]  5%|▌         | 1456/26933 [02:14<32:02, 13.25it/s]  5%|▌         | 1460/26933 [02:15<31:59, 13.27it/s]  5%|▌         | 1464/26933 [02:15<32:03, 13.24it/s]  5%|▌         | 1468/26933 [02:15<32:01, 13.25it/s]  5%|▌         | 1472/26933 [02:15<31:57, 13.28it/s]  5%|▌         | 1476/26933 [02:16<32:01, 13.25it/s]  5%|▌         | 1480/26933 [02:16<32:00, 13.26it/s]  6%|▌         | 1484/26933 [02:16<31:58, 13.27it/s]  6%|▌         | 1488/26933 [02:17<32:00, 13.25it/s]  6%|▌         | 1492/26933 [02:17<31:58, 13.26it/s]  6%|▌         | 1496/26933 [02:17<32:00, 13.24it/s]  6%|▌         | 1500/26933 [02:18<32:04, 13.22it/s]  6%|▌         | 1504/26933 [02:18<32:00, 13.24it/s]  6%|▌         | 1508/26933 [02:18<31:57, 13.26it/s]  6%|▌         | 1512/26933 [02:18<31:53, 13.28it/s]  6%|▌         | 1516/26933 [02:19<31:59, 13.24it/s]  6%|▌         | 1520/26933 [02:19<32:58, 12.85it/s]  6%|▌         | 1524/26933 [02:19<33:36, 12.60it/s]  6%|▌         | 1528/26933 [02:20<34:12, 12.38it/s]  6%|▌         | 1532/26933 [02:20<34:30, 12.27it/s]  6%|▌         | 1536/26933 [02:20<34:51, 12.14it/s]  6%|▌         | 1540/26933 [02:21<35:05, 12.06it/s]  6%|▌         | 1544/26933 [02:21<35:13, 12.01it/s]  6%|▌         | 1548/26933 [02:21<35:19, 11.98it/s]  6%|▌         | 1552/26933 [02:22<35:45, 11.83it/s]  6%|▌         | 1556/26933 [02:22<35:56, 11.77it/s]  6%|▌         | 1560/26933 [02:22<36:25, 11.61it/s]  6%|▌         | 1564/26933 [02:23<36:46, 11.50it/s]  6%|▌         | 1568/26933 [02:23<36:23, 11.62it/s]  6%|▌         | 1572/26933 [02:23<35:58, 11.75it/s]  6%|▌         | 1576/26933 [02:24<35:45, 11.82it/s]  6%|▌         | 1580/26933 [02:24<35:31, 11.90it/s]  6%|▌         | 1584/26933 [02:24<35:28, 11.91it/s]  6%|▌         | 1588/26933 [02:25<35:39, 11.85it/s]  6%|▌         | 1592/26933 [02:25<35:27, 11.91it/s]  6%|▌         | 1596/26933 [02:26<35:17, 11.96it/s]  6%|▌         | 1600/26933 [02:26<35:10, 12.00it/s]  6%|▌         | 1604/26933 [02:26<35:02, 12.05it/s]  6%|▌         | 1608/26933 [02:26<34:55, 12.08it/s]  6%|▌         | 1612/26933 [02:27<34:57, 12.07it/s]  6%|▌         | 1616/26933 [02:27<34:54, 12.09it/s]  6%|▌         | 1620/26933 [02:27<34:56, 12.07it/s]  6%|▌         | 1624/26933 [02:28<34:57, 12.07it/s]  6%|▌         | 1628/26933 [02:28<34:58, 12.06it/s]  6%|▌         | 1632/26933 [02:28<34:56, 12.07it/s]  6%|▌         | 1636/26933 [02:29<35:02, 12.03it/s]  6%|▌         | 1640/26933 [02:29<35:21, 11.92it/s]  6%|▌         | 1644/26933 [02:29<35:16, 11.95it/s]  6%|▌         | 1648/26933 [02:30<35:17, 11.94it/s]  6%|▌         | 1652/26933 [02:30<35:12, 11.96it/s]  6%|▌         | 1656/26933 [02:30<35:07, 12.00it/s]  6%|▌         | 1660/26933 [02:31<35:05, 12.00it/s]  6%|▌         | 1664/26933 [02:31<35:01, 12.02it/s]  6%|▌         | 1668/26933 [02:31<34:58, 12.04it/s]  6%|▌         | 1672/26933 [02:32<35:05, 12.00it/s]  6%|▌         | 1676/26933 [02:32<35:00, 12.02it/s]  6%|▌         | 1680/26933 [02:32<34:52, 12.07it/s]  6%|▋         | 1684/26933 [02:33<34:50, 12.08it/s]  6%|▋         | 1688/26933 [02:33<34:45, 12.11it/s]  6%|▋         | 1692/26933 [02:33<34:39, 12.14it/s]  6%|▋         | 1696/26933 [02:34<34:44, 12.11it/s]  6%|▋         | 1700/26933 [02:34<34:47, 12.09it/s]  6%|▋         | 1704/26933 [02:34<34:45, 12.10it/s]  6%|▋         | 1708/26933 [02:35<34:51, 12.06it/s]  6%|▋         | 1712/26933 [02:35<34:45, 12.09it/s]  6%|▋         | 1716/26933 [02:35<34:43, 12.10it/s]  6%|▋         | 1720/26933 [02:36<34:43, 12.10it/s]  6%|▋         | 1724/26933 [02:36<34:43, 12.10it/s]  6%|▋         | 1728/26933 [02:36<34:42, 12.10it/s]  6%|▋         | 1732/26933 [02:37<34:42, 12.10it/s]  6%|▋         | 1736/26933 [02:37<34:42, 12.10it/s]  6%|▋         | 1740/26933 [02:37<34:42, 12.10it/s]  6%|▋         | 1744/26933 [02:38<34:36, 12.13it/s]  6%|▋         | 1748/26933 [02:38<34:30, 12.16it/s]  7%|▋         | 1752/26933 [02:38<34:30, 12.16it/s]  7%|▋         | 1756/26933 [02:39<34:30, 12.16it/s]  7%|▋         | 1760/26933 [02:39<34:30, 12.16it/s]  7%|▋         | 1764/26933 [02:39<34:27, 12.18it/s]  7%|▋         | 1768/26933 [02:40<34:30, 12.16it/s]  7%|▋         | 1772/26933 [02:40<34:23, 12.19it/s]  7%|▋         | 1776/26933 [02:40<34:22, 12.20it/s]  7%|▋         | 1780/26933 [02:41<34:25, 12.18it/s]  7%|▋         | 1784/26933 [02:41<34:23, 12.19it/s]  7%|▋         | 1788/26933 [02:41<34:18, 12.22it/s]  7%|▋         | 1792/26933 [02:42<34:23, 12.18it/s]  7%|▋         | 1796/26933 [02:42<34:22, 12.19it/s]  7%|▋         | 1800/26933 [02:42<34:22, 12.19it/s]  7%|▋         | 1804/26933 [02:43<34:20, 12.19it/s]  7%|▋         | 1808/26933 [02:43<34:15, 12.22it/s]  7%|▋         | 1812/26933 [02:43<34:09, 12.26it/s]  7%|▋         | 1816/26933 [02:44<34:06, 12.28it/s]  7%|▋         | 1820/26933 [02:44<34:05, 12.28it/s]  7%|▋         | 1824/26933 [02:44<34:02, 12.30it/s]  7%|▋         | 1828/26933 [02:45<34:04, 12.28it/s]  7%|▋         | 1832/26933 [02:45<34:00, 12.30it/s]  7%|▋         | 1836/26933 [02:45<33:59, 12.31it/s]  7%|▋         | 1840/26933 [02:46<34:02, 12.29it/s]  7%|▋         | 1844/26933 [02:46<33:17, 12.56it/s]  7%|▋         | 1848/26933 [02:46<32:34, 12.83it/s]  7%|▋         | 1852/26933 [02:47<32:06, 13.02it/s]  7%|▋         | 1856/26933 [02:47<31:47, 13.15it/s]  7%|▋         | 1860/26933 [02:47<31:29, 13.27it/s]  7%|▋         | 1864/26933 [02:47<31:14, 13.38it/s]  7%|▋         | 1868/26933 [02:48<31:10, 13.40it/s]  7%|▋         | 1872/26933 [02:48<31:03, 13.45it/s]  7%|▋         | 1876/26933 [02:48<30:56, 13.49it/s]  7%|▋         | 1880/26933 [02:49<30:51, 13.53it/s]  7%|▋         | 1884/26933 [02:49<30:52, 13.52it/s]  7%|▋         | 1888/26933 [02:49<30:50, 13.54it/s]  7%|▋         | 1892/26933 [02:49<30:49, 13.54it/s]  7%|▋         | 1896/26933 [02:50<31:17, 13.34it/s]  7%|▋         | 1900/26933 [02:50<31:13, 13.36it/s]  7%|▋         | 1904/26933 [02:50<31:06, 13.41it/s]  7%|▋         | 1908/26933 [02:51<31:04, 13.42it/s]  7%|▋         | 1912/26933 [02:51<31:01, 13.44it/s]  7%|▋         | 1916/26933 [02:51<30:54, 13.49it/s]  7%|▋         | 1920/26933 [02:52<30:50, 13.52it/s]  7%|▋         | 1924/26933 [02:52<31:50, 13.09it/s]  7%|▋         | 1928/26933 [02:52<32:27, 12.84it/s]  7%|▋         | 1932/26933 [02:53<32:52, 12.68it/s]  7%|▋         | 1936/26933 [02:53<33:12, 12.55it/s]  7%|▋         | 1940/26933 [02:53<33:22, 12.48it/s]  7%|▋         | 1944/26933 [02:54<33:27, 12.45it/s]  7%|▋         | 1948/26933 [02:54<33:39, 12.37it/s]  7%|▋         | 1952/26933 [02:54<33:36, 12.39it/s]  7%|▋         | 1956/26933 [02:54<33:30, 12.42it/s]  7%|▋         | 1960/26933 [02:55<33:45, 12.33it/s]  7%|▋         | 1964/26933 [02:55<33:43, 12.34it/s]  7%|▋         | 1968/26933 [02:55<33:43, 12.34it/s]  7%|▋         | 1972/26933 [02:56<33:48, 12.30it/s]  7%|▋         | 1976/26933 [02:56<33:44, 12.33it/s]  7%|▋         | 1980/26933 [02:56<33:39, 12.35it/s]  7%|▋         | 1984/26933 [02:57<33:52, 12.28it/s]  7%|▋         | 1988/26933 [02:57<33:49, 12.29it/s]  7%|▋         | 1992/26933 [02:57<33:47, 12.30it/s]  7%|▋         | 1996/26933 [02:58<34:03, 12.20it/s]  7%|▋         | 2000/26933 [02:58<34:03, 12.20it/s]  7%|▋         | 2004/26933 [02:58<33:58, 12.23it/s]  7%|▋         | 2008/26933 [02:59<34:12, 12.15it/s]  7%|▋         | 2012/26933 [02:59<34:09, 12.16it/s]  7%|▋         | 2016/26933 [02:59<34:28, 12.05it/s]  8%|▊         | 2020/26933 [03:00<34:31, 12.03it/s]  8%|▊         | 2024/26933 [03:00<34:17, 12.11it/s]  8%|▊         | 2028/26933 [03:00<34:02, 12.20it/s]  8%|▊         | 2032/26933 [03:01<34:05, 12.18it/s]  8%|▊         | 2036/26933 [03:01<33:54, 12.24it/s]  8%|▊         | 2040/26933 [03:01<33:53, 12.24it/s]  8%|▊         | 2044/26933 [03:02<33:56, 12.22it/s]  8%|▊         | 2048/26933 [03:02<33:49, 12.26it/s]  8%|▊         | 2052/26933 [03:02<33:46, 12.28it/s]  8%|▊         | 2056/26933 [03:03<33:55, 12.22it/s]  8%|▊         | 2060/26933 [03:03<33:52, 12.24it/s]  8%|▊         | 2064/26933 [03:03<33:43, 12.29it/s]  8%|▊         | 2068/26933 [03:04<33:44, 12.28it/s]  8%|▊         | 2072/26933 [03:04<33:41, 12.30it/s]  8%|▊         | 2076/26933 [03:04<33:35, 12.33it/s]  8%|▊         | 2080/26933 [03:05<33:28, 12.37it/s]  8%|▊         | 2084/26933 [03:05<33:27, 12.38it/s]  8%|▊         | 2088/26933 [03:05<33:26, 12.38it/s]  8%|▊         | 2092/26933 [03:06<33:26, 12.38it/s]  8%|▊         | 2096/26933 [03:06<33:32, 12.34it/s]  8%|▊         | 2100/26933 [03:06<33:26, 12.38it/s]  8%|▊         | 2104/26933 [03:07<33:21, 12.40it/s]  8%|▊         | 2108/26933 [03:07<33:36, 12.31it/s]  8%|▊         | 2112/26933 [03:07<33:32, 12.34it/s]  8%|▊         | 2116/26933 [03:08<33:26, 12.37it/s]  8%|▊         | 2120/26933 [03:08<33:31, 12.34it/s]  8%|▊         | 2124/26933 [03:08<33:29, 12.35it/s]  8%|▊         | 2128/26933 [03:08<33:24, 12.38it/s]  8%|▊         | 2132/26933 [03:09<33:31, 12.33it/s]  8%|▊         | 2136/26933 [03:09<33:29, 12.34it/s]  8%|▊         | 2140/26933 [03:09<33:25, 12.36it/s]  8%|▊         | 2144/26933 [03:10<33:36, 12.29it/s]  8%|▊         | 2148/26933 [03:10<33:35, 12.30it/s]  8%|▊         | 2152/26933 [03:10<33:31, 12.32it/s]  8%|▊         | 2156/26933 [03:11<33:35, 12.29it/s]  8%|▊         | 2160/26933 [03:11<33:23, 12.36it/s]  8%|▊         | 2164/26933 [03:11<33:14, 12.42it/s]  8%|▊         | 2168/26933 [03:12<33:17, 12.40it/s]  8%|▊         | 2172/26933 [03:12<33:12, 12.43it/s]  8%|▊         | 2176/26933 [03:12<33:09, 12.45it/s]  8%|▊         | 2180/26933 [03:13<33:12, 12.42it/s]  8%|▊         | 2184/26933 [03:13<33:04, 12.47it/s]  8%|▊         | 2188/26933 [03:13<32:59, 12.50it/s]  8%|▊         | 2192/26933 [03:14<33:10, 12.43it/s]  8%|▊         | 2196/26933 [03:14<33:13, 12.41it/s]  8%|▊         | 2200/26933 [03:14<33:12, 12.42it/s]  8%|▊         | 2204/26933 [03:15<33:16, 12.39it/s]  8%|▊         | 2208/26933 [03:15<33:17, 12.38it/s]  8%|▊         | 2212/26933 [03:15<33:23, 12.34it/s]  8%|▊         | 2216/26933 [03:16<33:17, 12.37it/s]  8%|▊         | 2220/26933 [03:16<33:08, 12.43it/s]  8%|▊         | 2224/26933 [03:16<32:58, 12.49it/s]  8%|▊         | 2228/26933 [03:17<32:52, 12.52it/s]  8%|▊         | 2232/26933 [03:17<32:50, 12.54it/s]  8%|▊         | 2236/26933 [03:17<32:48, 12.55it/s]  8%|▊         | 2240/26933 [03:17<32:49, 12.54it/s]  8%|▊         | 2244/26933 [03:18<33:31, 12.27it/s]  8%|▊         | 2248/26933 [03:18<33:14, 12.37it/s]  8%|▊         | 2252/26933 [03:18<33:07, 12.42it/s]  8%|▊         | 2256/26933 [03:19<33:08, 12.41it/s]  8%|▊         | 2260/26933 [03:19<32:59, 12.47it/s]  8%|▊         | 2264/26933 [03:19<32:50, 12.52it/s]  8%|▊         | 2268/26933 [03:20<32:51, 12.51it/s]  8%|▊         | 2272/26933 [03:20<32:43, 12.56it/s]  8%|▊         | 2276/26933 [03:20<32:40, 12.58it/s]  8%|▊         | 2280/26933 [03:21<32:02, 12.82it/s]  8%|▊         | 2284/26933 [03:21<31:26, 13.07it/s]  8%|▊         | 2288/26933 [03:21<30:58, 13.26it/s]  9%|▊         | 2292/26933 [03:22<30:31, 13.45it/s]  9%|▊         | 2296/26933 [03:22<31:11, 13.17it/s]  9%|▊         | 2300/26933 [03:22<31:37, 12.98it/s]  9%|▊         | 2304/26933 [03:23<31:54, 12.86it/s]  9%|▊         | 2308/26933 [03:23<31:33, 13.00it/s]  9%|▊         | 2312/26933 [03:23<31:00, 13.24it/s]  9%|▊         | 2316/26933 [03:23<30:32, 13.43it/s]  9%|▊         | 2320/26933 [03:24<30:19, 13.53it/s]  9%|▊         | 2324/26933 [03:24<30:01, 13.66it/s]  9%|▊         | 2328/26933 [03:24<29:48, 13.75it/s]  9%|▊         | 2332/26933 [03:25<29:44, 13.79it/s]  9%|▊         | 2336/26933 [03:25<29:41, 13.81it/s]  9%|▊         | 2340/26933 [03:25<29:41, 13.80it/s]  9%|▊         | 2344/26933 [03:25<29:40, 13.81it/s]  9%|▊         | 2348/26933 [03:26<29:49, 13.74it/s]  9%|▊         | 2352/26933 [03:26<29:43, 13.79it/s]  9%|▊         | 2356/26933 [03:26<29:37, 13.82it/s]  9%|▉         | 2360/26933 [03:27<29:40, 13.80it/s]  9%|▉         | 2364/26933 [03:27<29:40, 13.80it/s]  9%|▉         | 2368/26933 [03:27<29:42, 13.78it/s]  9%|▉         | 2372/26933 [03:27<29:41, 13.79it/s]  9%|▉         | 2376/26933 [03:28<29:50, 13.72it/s]  9%|▉         | 2380/26933 [03:28<29:46, 13.74it/s]  9%|▉         | 2384/26933 [03:28<29:39, 13.80it/s]  9%|▉         | 2388/26933 [03:29<29:39, 13.79it/s]  9%|▉         | 2392/26933 [03:29<29:34, 13.83it/s]  9%|▉         | 2396/26933 [03:29<29:32, 13.84it/s]  9%|▉         | 2400/26933 [03:29<29:27, 13.88it/s]  9%|▉         | 2404/26933 [03:30<29:28, 13.87it/s]  9%|▉         | 2408/26933 [03:30<29:24, 13.90it/s]  9%|▉         | 2412/26933 [03:30<29:20, 13.93it/s]  9%|▉         | 2416/26933 [03:31<30:12, 13.53it/s]  9%|▉         | 2420/26933 [03:31<31:30, 12.97it/s]  9%|▉         | 2424/26933 [03:31<31:42, 12.88it/s]  9%|▉         | 2428/26933 [03:32<32:05, 12.73it/s]  9%|▉         | 2432/26933 [03:32<32:08, 12.71it/s]  9%|▉         | 2436/26933 [03:32<32:11, 12.68it/s]  9%|▉         | 2440/26933 [03:33<32:17, 12.64it/s]  9%|▉         | 2444/26933 [03:33<32:24, 12.59it/s]  9%|▉         | 2448/26933 [03:33<32:21, 12.61it/s]  9%|▉         | 2452/26933 [03:34<32:17, 12.64it/s]  9%|▉         | 2456/26933 [03:34<32:19, 12.62it/s]  9%|▉         | 2460/26933 [03:34<32:16, 12.64it/s]  9%|▉         | 2464/26933 [03:34<32:09, 12.68it/s]  9%|▉         | 2468/26933 [03:35<32:10, 12.67it/s]  9%|▉         | 2472/26933 [03:35<32:06, 12.69it/s]  9%|▉         | 2476/26933 [03:35<32:07, 12.69it/s]  9%|▉         | 2480/26933 [03:36<32:07, 12.69it/s]  9%|▉         | 2484/26933 [03:36<32:07, 12.69it/s]  9%|▉         | 2488/26933 [03:36<32:03, 12.71it/s]  9%|▉         | 2492/26933 [03:37<32:08, 12.68it/s]  9%|▉         | 2496/26933 [03:37<32:06, 12.68it/s]  9%|▉         | 2500/26933 [03:37<32:06, 12.68it/s]  9%|▉         | 2504/26933 [03:38<32:11, 12.65it/s]  9%|▉         | 2508/26933 [03:38<32:12, 12.64it/s]  9%|▉         | 2512/26933 [03:38<32:10, 12.65it/s]  9%|▉         | 2516/26933 [03:39<32:11, 12.64it/s]  9%|▉         | 2520/26933 [03:39<32:20, 12.58it/s]  9%|▉         | 2524/26933 [03:39<32:19, 12.59it/s]  9%|▉         | 2528/26933 [03:40<32:13, 12.62it/s]  9%|▉         | 2532/26933 [03:40<32:17, 12.60it/s]  9%|▉         | 2536/26933 [03:40<32:08, 12.65it/s]  9%|▉         | 2540/26933 [03:40<32:01, 12.69it/s]  9%|▉         | 2544/26933 [03:41<32:01, 12.70it/s]  9%|▉         | 2548/26933 [03:41<31:55, 12.73it/s]  9%|▉         | 2552/26933 [03:41<31:51, 12.75it/s]  9%|▉         | 2556/26933 [03:42<32:00, 12.69it/s] 10%|▉         | 2560/26933 [03:42<31:54, 12.73it/s] 10%|▉         | 2564/26933 [03:42<31:48, 12.77it/s] 10%|▉         | 2568/26933 [03:43<31:50, 12.75it/s] 10%|▉         | 2572/26933 [03:43<31:49, 12.76it/s] 10%|▉         | 2576/26933 [03:43<31:46, 12.77it/s] 10%|▉         | 2580/26933 [03:44<31:51, 12.74it/s] 10%|▉         | 2584/26933 [03:44<31:55, 12.71it/s] 10%|▉         | 2588/26933 [03:44<31:53, 12.72it/s] 10%|▉         | 2592/26933 [03:45<31:53, 12.72it/s] 10%|▉         | 2596/26933 [03:45<31:52, 12.72it/s] 10%|▉         | 2600/26933 [03:45<31:46, 12.76it/s] 10%|▉         | 2604/26933 [03:45<31:39, 12.81it/s] 10%|▉         | 2608/26933 [03:46<31:41, 12.79it/s] 10%|▉         | 2612/26933 [03:46<30:59, 13.08it/s] 10%|▉         | 2616/26933 [03:46<30:21, 13.35it/s] 10%|▉         | 2620/26933 [03:47<30:01, 13.49it/s] 10%|▉         | 2624/26933 [03:47<29:45, 13.61it/s] 10%|▉         | 2628/26933 [03:47<29:29, 13.74it/s] 10%|▉         | 2632/26933 [03:48<29:16, 13.83it/s] 10%|▉         | 2636/26933 [03:48<29:11, 13.87it/s] 10%|▉         | 2640/26933 [03:48<29:03, 13.94it/s] 10%|▉         | 2644/26933 [03:48<28:59, 13.96it/s] 10%|▉         | 2648/26933 [03:49<28:58, 13.97it/s] 10%|▉         | 2652/26933 [03:49<28:53, 14.01it/s] 10%|▉         | 2656/26933 [03:49<28:54, 13.99it/s] 10%|▉         | 2660/26933 [03:50<28:52, 14.01it/s] 10%|▉         | 2664/26933 [03:50<28:57, 13.97it/s] 10%|▉         | 2668/26933 [03:50<28:55, 13.98it/s] 10%|▉         | 2672/26933 [03:50<28:51, 14.01it/s] 10%|▉         | 2676/26933 [03:51<28:55, 13.98it/s] 10%|▉         | 2680/26933 [03:51<28:50, 14.02it/s] 10%|▉         | 2684/26933 [03:51<28:59, 13.94it/s] 10%|▉         | 2688/26933 [03:52<29:05, 13.89it/s] 10%|▉         | 2692/26933 [03:52<29:10, 13.84it/s] 10%|█         | 2696/26933 [03:52<29:06, 13.88it/s] 10%|█         | 2700/26933 [03:52<29:02, 13.90it/s] 10%|█         | 2704/26933 [03:53<29:13, 13.82it/s] 10%|█         | 2708/26933 [03:53<29:07, 13.86it/s] 10%|█         | 2712/26933 [03:53<29:02, 13.90it/s] 10%|█         | 2716/26933 [03:54<28:58, 13.93it/s] 10%|█         | 2720/26933 [03:54<29:01, 13.90it/s] 10%|█         | 2724/26933 [03:54<28:59, 13.92it/s] 10%|█         | 2728/26933 [03:54<28:59, 13.91it/s] 10%|█         | 2732/26933 [03:55<29:01, 13.89it/s] 10%|█         | 2736/26933 [03:55<29:00, 13.91it/s] 10%|█         | 2740/26933 [03:55<29:00, 13.90it/s] 10%|█         | 2744/26933 [03:56<28:55, 13.94it/s] 10%|█         | 2748/26933 [03:56<28:56, 13.93it/s] 10%|█         | 2752/26933 [03:56<28:48, 13.99it/s] 10%|█         | 2756/26933 [03:56<28:45, 14.01it/s] 10%|█         | 2760/26933 [03:57<28:49, 13.98it/s] 10%|█         | 2764/26933 [03:57<28:50, 13.97it/s] 10%|█         | 2768/26933 [03:57<28:54, 13.93it/s] 10%|█         | 2772/26933 [03:58<28:49, 13.97it/s] 10%|█         | 2776/26933 [03:58<28:49, 13.97it/s] 10%|█         | 2780/26933 [03:58<28:43, 14.01it/s] 10%|█         | 2784/26933 [03:58<28:39, 14.05it/s] 10%|█         | 2788/26933 [03:59<28:42, 14.02it/s] 10%|█         | 2792/26933 [03:59<28:44, 14.00it/s] 10%|█         | 2796/26933 [03:59<28:39, 14.04it/s] 10%|█         | 2800/26933 [04:00<28:36, 14.06it/s] 10%|█         | 2804/26933 [04:00<28:40, 14.02it/s] 10%|█         | 2808/26933 [04:00<28:40, 14.03it/s] 10%|█         | 2812/26933 [04:00<28:39, 14.03it/s] 10%|█         | 2816/26933 [04:01<28:46, 13.97it/s] 10%|█         | 2820/26933 [04:01<28:49, 13.95it/s] 10%|█         | 2824/26933 [04:01<28:52, 13.92it/s] 11%|█         | 2828/26933 [04:02<28:48, 13.94it/s] 11%|█         | 2832/26933 [04:02<28:48, 13.94it/s] 11%|█         | 2836/26933 [04:02<28:43, 13.98it/s] 11%|█         | 2840/26933 [04:02<28:40, 14.00it/s] 11%|█         | 2844/26933 [04:03<28:41, 13.99it/s] 11%|█         | 2848/26933 [04:03<28:38, 14.01it/s] 11%|█         | 2852/26933 [04:03<28:36, 14.03it/s] 11%|█         | 2856/26933 [04:04<28:33, 14.05it/s] 11%|█         | 2860/26933 [04:04<28:30, 14.08it/s] 11%|█         | 2864/26933 [04:04<28:28, 14.09it/s] 11%|█         | 2868/26933 [04:04<28:31, 14.06it/s] 11%|█         | 2872/26933 [04:05<28:34, 14.03it/s] 11%|█         | 2876/26933 [04:05<28:30, 14.07it/s] 11%|█         | 2880/26933 [04:05<28:27, 14.08it/s] 11%|█         | 2884/26933 [04:06<28:27, 14.09it/s] 11%|█         | 2888/26933 [04:06<28:27, 14.08it/s] 11%|█         | 2892/26933 [04:06<28:28, 14.07it/s] 11%|█         | 2896/26933 [04:06<28:26, 14.08it/s] 11%|█         | 2900/26933 [04:07<28:30, 14.05it/s] 11%|█         | 2904/26933 [04:07<28:28, 14.07it/s] 11%|█         | 2908/26933 [04:07<28:25, 14.09it/s] 11%|█         | 2912/26933 [04:08<28:25, 14.09it/s] 11%|█         | 2916/26933 [04:08<28:27, 14.06it/s] 11%|█         | 2920/26933 [04:08<28:26, 14.07it/s] 11%|█         | 2924/26933 [04:08<28:21, 14.11it/s] 11%|█         | 2928/26933 [04:09<28:21, 14.11it/s] 11%|█         | 2932/26933 [04:09<28:21, 14.11it/s] 11%|█         | 2936/26933 [04:09<28:21, 14.11it/s] 11%|█         | 2940/26933 [04:10<28:15, 14.15it/s] 11%|█         | 2944/26933 [04:10<28:15, 14.14it/s] 11%|█         | 2948/26933 [04:10<28:15, 14.14it/s] 11%|█         | 2952/26933 [04:10<28:11, 14.18it/s] 11%|█         | 2956/26933 [04:11<28:15, 14.15it/s] 11%|█         | 2960/26933 [04:11<28:12, 14.16it/s] 11%|█         | 2964/26933 [04:11<28:11, 14.17it/s] 11%|█         | 2968/26933 [04:11<28:11, 14.17it/s] 11%|█         | 2972/26933 [04:12<28:12, 14.15it/s] 11%|█         | 2976/26933 [04:12<28:37, 13.95it/s] 11%|█         | 2980/26933 [04:12<29:17, 13.63it/s] 11%|█         | 2984/26933 [04:13<29:46, 13.41it/s] 11%|█         | 2988/26933 [04:13<30:10, 13.22it/s] 11%|█         | 2992/26933 [04:13<30:36, 13.03it/s] 11%|█         | 2996/26933 [04:14<31:12, 12.78it/s] 11%|█         | 3000/26933 [04:14<31:06, 12.82it/s] 11%|█         | 3004/26933 [04:14<31:00, 12.86it/s] 11%|█         | 3008/26933 [04:15<30:55, 12.90it/s] 11%|█         | 3012/26933 [04:15<30:59, 12.87it/s] 11%|█         | 3016/26933 [04:15<30:55, 12.89it/s] 11%|█         | 3020/26933 [04:16<30:54, 12.90it/s] 11%|█         | 3024/26933 [04:16<31:08, 12.80it/s] 11%|█         | 3028/26933 [04:16<31:10, 12.78it/s] 11%|█▏        | 3032/26933 [04:16<31:02, 12.84it/s] 11%|█▏        | 3036/26933 [04:17<31:13, 12.75it/s] 11%|█▏        | 3040/26933 [04:17<31:05, 12.81it/s] 11%|█▏        | 3044/26933 [04:17<31:03, 12.82it/s] 11%|█▏        | 3048/26933 [04:18<31:04, 12.81it/s] 11%|█▏        | 3052/26933 [04:18<30:58, 12.85it/s] 11%|█▏        | 3056/26933 [04:18<30:51, 12.89it/s] 11%|█▏        | 3060/26933 [04:19<30:50, 12.90it/s] 11%|█▏        | 3064/26933 [04:19<30:50, 12.90it/s] 11%|█▏        | 3068/26933 [04:19<30:44, 12.94it/s] 11%|█▏        | 3072/26933 [04:20<30:36, 12.99it/s] 11%|█▏        | 3076/26933 [04:20<30:34, 13.01it/s] 11%|█▏        | 3080/26933 [04:20<30:31, 13.03it/s] 11%|█▏        | 3084/26933 [04:20<30:24, 13.07it/s] 11%|█▏        | 3088/26933 [04:21<30:26, 13.06it/s] 11%|█▏        | 3092/26933 [04:21<30:29, 13.03it/s] 11%|█▏        | 3096/26933 [04:21<30:30, 13.02it/s] 12%|█▏        | 3100/26933 [04:22<30:33, 13.00it/s] 12%|█▏        | 3104/26933 [04:22<30:34, 12.99it/s] 12%|█▏        | 3108/26933 [04:22<30:34, 12.99it/s] 12%|█▏        | 3112/26933 [04:23<30:38, 12.96it/s] 12%|█▏        | 3116/26933 [04:23<30:37, 12.96it/s] 12%|█▏        | 3120/26933 [04:23<30:35, 12.97it/s] 12%|█▏        | 3124/26933 [04:24<30:37, 12.96it/s] 12%|█▏        | 3128/26933 [04:24<30:42, 12.92it/s] 12%|█▏        | 3132/26933 [04:24<30:41, 12.93it/s] 12%|█▏        | 3136/26933 [04:24<30:36, 12.96it/s] 12%|█▏        | 3140/26933 [04:25<30:37, 12.95it/s] 12%|█▏        | 3144/26933 [04:25<30:37, 12.95it/s] 12%|█▏        | 3148/26933 [04:25<30:33, 12.97it/s] 12%|█▏        | 3152/26933 [04:26<30:32, 12.98it/s] 12%|█▏        | 3156/26933 [04:26<30:31, 12.98it/s] 12%|█▏        | 3160/26933 [04:26<30:28, 13.00it/s] 12%|█▏        | 3164/26933 [04:27<30:30, 12.98it/s] 12%|█▏        | 3168/26933 [04:27<30:28, 13.00it/s] 12%|█▏        | 3172/26933 [04:27<30:25, 13.02it/s] 12%|█▏        | 3176/26933 [04:28<30:21, 13.04it/s] 12%|█▏        | 3180/26933 [04:28<30:23, 13.02it/s] 12%|█▏        | 3184/26933 [04:28<30:21, 13.04it/s] 12%|█▏        | 3188/26933 [04:28<30:32, 12.96it/s] 12%|█▏        | 3192/26933 [04:29<30:31, 12.97it/s] 12%|█▏        | 3196/26933 [04:29<30:27, 12.99it/s] 12%|█▏        | 3200/26933 [04:29<30:22, 13.03it/s] 12%|█▏        | 3204/26933 [04:30<30:25, 13.00it/s] 12%|█▏        | 3208/26933 [04:30<30:18, 13.05it/s] 12%|█▏        | 3212/26933 [04:30<30:13, 13.08it/s] 12%|█▏        | 3216/26933 [04:31<30:11, 13.09it/s] 12%|█▏        | 3220/26933 [04:31<30:11, 13.09it/s] 12%|█▏        | 3224/26933 [04:31<30:08, 13.11it/s] 12%|█▏        | 3228/26933 [04:32<30:11, 13.09it/s] 12%|█▏        | 3232/26933 [04:32<30:18, 13.04it/s] 12%|█▏        | 3236/26933 [04:32<30:11, 13.08it/s] 12%|█▏        | 3240/26933 [04:32<30:06, 13.12it/s] 12%|█▏        | 3244/26933 [04:33<30:09, 13.10it/s] 12%|█▏        | 3248/26933 [04:33<30:03, 13.13it/s] 12%|█▏        | 3252/26933 [04:33<29:58, 13.17it/s] 12%|█▏        | 3256/26933 [04:34<29:55, 13.18it/s] 12%|█▏        | 3260/26933 [04:34<29:54, 13.19it/s] 12%|█▏        | 3264/26933 [04:34<29:55, 13.18it/s] 12%|█▏        | 3268/26933 [04:35<29:59, 13.15it/s] 12%|█▏        | 3272/26933 [04:35<29:57, 13.17it/s] 12%|█▏        | 3276/26933 [04:35<30:01, 13.13it/s] 12%|█▏        | 3280/26933 [04:35<29:56, 13.17it/s] 12%|█▏        | 3284/26933 [04:36<29:58, 13.15it/s] 12%|█▏        | 3288/26933 [04:36<29:57, 13.15it/s] 12%|█▏        | 3292/26933 [04:36<29:57, 13.15it/s] 12%|█▏        | 3296/26933 [04:37<29:59, 13.13it/s] 12%|█▏        | 3300/26933 [04:37<29:59, 13.14it/s] 12%|█▏        | 3304/26933 [04:37<32:41, 12.05it/s] 12%|█▏        | 3308/26933 [04:38<31:57, 12.32it/s] 12%|█▏        | 3312/26933 [04:38<31:20, 12.56it/s] 12%|█▏        | 3316/26933 [04:38<30:58, 12.71it/s] 12%|█▏        | 3320/26933 [04:39<30:46, 12.79it/s] 12%|█▏        | 3324/26933 [04:39<30:33, 12.88it/s] 12%|█▏        | 3328/26933 [04:39<30:20, 12.97it/s] 12%|█▏        | 3332/26933 [04:40<30:20, 12.96it/s] 12%|█▏        | 3336/26933 [04:40<30:17, 12.99it/s] 12%|█▏        | 3340/26933 [04:40<30:10, 13.03it/s] 12%|█▏        | 3344/26933 [04:40<30:03, 13.08it/s] 12%|█▏        | 3348/26933 [04:41<30:02, 13.09it/s] 12%|█▏        | 3352/26933 [04:41<30:12, 13.01it/s] 12%|█▏        | 3356/26933 [04:41<30:11, 13.02it/s] 12%|█▏        | 3360/26933 [04:42<30:07, 13.04it/s] 12%|█▏        | 3364/26933 [04:42<30:00, 13.09it/s] 13%|█▎        | 3368/26933 [04:42<29:56, 13.12it/s] 13%|█▎        | 3372/26933 [04:43<29:52, 13.14it/s] 13%|█▎        | 3376/26933 [04:43<29:46, 13.19it/s] 13%|█▎        | 3380/26933 [04:43<29:50, 13.15it/s] 13%|█▎        | 3384/26933 [04:44<29:50, 13.16it/s] 13%|█▎        | 3388/26933 [04:44<29:52, 13.14it/s] 13%|█▎        | 3392/26933 [04:44<31:15, 12.55it/s] 13%|█▎        | 3396/26933 [04:44<30:57, 12.67it/s] 13%|█▎        | 3400/26933 [04:45<30:41, 12.78it/s] 13%|█▎        | 3404/26933 [04:45<30:29, 12.86it/s] 13%|█▎        | 3408/26933 [04:45<30:18, 12.94it/s] 13%|█▎        | 3412/26933 [04:46<30:13, 12.97it/s] 13%|█▎        | 3416/26933 [04:46<30:15, 12.96it/s] 13%|█▎        | 3420/26933 [04:46<30:04, 13.03it/s] 13%|█▎        | 3424/26933 [04:47<29:19, 13.36it/s] 13%|█▎        | 3428/26933 [04:47<28:42, 13.65it/s] 13%|█▎        | 3432/26933 [04:47<28:15, 13.86it/s] 13%|█▎        | 3436/26933 [04:47<27:52, 14.05it/s] 13%|█▎        | 3440/26933 [04:48<28:20, 13.81it/s] 13%|█▎        | 3444/26933 [04:48<28:46, 13.60it/s] 13%|█▎        | 3448/26933 [04:48<28:58, 13.51it/s] 13%|█▎        | 3452/26933 [04:49<29:14, 13.39it/s] 13%|█▎        | 3456/26933 [04:49<29:16, 13.37it/s] 13%|█▎        | 3460/26933 [04:49<29:19, 13.34it/s] 13%|█▎        | 3464/26933 [04:50<29:19, 13.34it/s] 13%|█▎        | 3468/26933 [04:50<29:19, 13.33it/s] 13%|█▎        | 3472/26933 [04:50<29:24, 13.30it/s] 13%|█▎        | 3476/26933 [04:50<29:22, 13.31it/s] 13%|█▎        | 3480/26933 [04:51<29:25, 13.29it/s] 13%|█▎        | 3484/26933 [04:51<29:27, 13.26it/s] 13%|█▎        | 3488/26933 [04:51<29:27, 13.27it/s] 13%|█▎        | 3492/26933 [04:52<29:25, 13.28it/s] 13%|█▎        | 3496/26933 [04:52<29:19, 13.32it/s] 13%|█▎        | 3500/26933 [04:52<29:19, 13.32it/s] 13%|█▎        | 3504/26933 [04:53<29:19, 13.32it/s] 13%|█▎        | 3508/26933 [04:53<29:16, 13.34it/s] 13%|█▎        | 3512/26933 [04:53<29:27, 13.25it/s] 13%|█▎        | 3516/26933 [04:53<29:26, 13.26it/s] 13%|█▎        | 3520/26933 [04:54<29:03, 13.43it/s] 13%|█▎        | 3524/26933 [04:54<28:29, 13.70it/s] 13%|█▎        | 3528/26933 [04:54<28:04, 13.90it/s] 13%|█▎        | 3532/26933 [04:55<27:42, 14.08it/s] 13%|█▎        | 3536/26933 [04:55<27:23, 14.23it/s] 13%|█▎        | 3540/26933 [04:55<27:06, 14.38it/s] 13%|█▎        | 3544/26933 [04:55<26:54, 14.48it/s] 13%|█▎        | 3548/26933 [04:56<26:48, 14.54it/s] 13%|█▎        | 3552/26933 [04:56<26:38, 14.62it/s] 13%|█▎        | 3556/26933 [04:56<26:32, 14.68it/s] 13%|█▎        | 3560/26933 [04:56<26:29, 14.70it/s] 13%|█▎        | 3564/26933 [04:57<26:28, 14.71it/s] 13%|█▎        | 3568/26933 [04:57<26:28, 14.71it/s] 13%|█▎        | 3572/26933 [04:57<26:29, 14.70it/s] 13%|█▎        | 3576/26933 [04:58<26:32, 14.66it/s] 13%|█▎        | 3580/26933 [04:58<26:35, 14.64it/s] 13%|█▎        | 3584/26933 [04:58<26:36, 14.62it/s] 13%|█▎        | 3588/26933 [04:58<26:31, 14.67it/s] 13%|█▎        | 3592/26933 [04:59<26:34, 14.64it/s] 13%|█▎        | 3596/26933 [04:59<27:21, 14.22it/s] 13%|█▎        | 3600/26933 [04:59<27:59, 13.90it/s] 13%|█▎        | 3604/26933 [05:00<28:43, 13.54it/s] 13%|█▎        | 3608/26933 [05:00<29:01, 13.40it/s] 13%|█▎        | 3612/26933 [05:00<29:06, 13.35it/s] 13%|█▎        | 3616/26933 [05:00<29:07, 13.35it/s] 13%|█▎        | 3620/26933 [05:01<28:37, 13.57it/s] 13%|█▎        | 3624/26933 [05:01<28:04, 13.84it/s] 13%|█▎        | 3628/26933 [05:01<27:38, 14.05it/s] 13%|█▎        | 3632/26933 [05:02<27:23, 14.18it/s] 14%|█▎        | 3636/26933 [05:02<27:11, 14.28it/s] 14%|█▎        | 3640/26933 [05:02<27:01, 14.37it/s] 14%|█▎        | 3644/26933 [05:02<26:53, 14.43it/s] 14%|█▎        | 3648/26933 [05:03<26:50, 14.46it/s] 14%|█▎        | 3652/26933 [05:03<26:45, 14.50it/s] 14%|█▎        | 3656/26933 [05:03<26:40, 14.55it/s] 14%|█▎        | 3660/26933 [05:04<26:43, 14.51it/s] 14%|█▎        | 3664/26933 [05:04<26:47, 14.47it/s] 14%|█▎        | 3668/26933 [05:04<26:44, 14.50it/s] 14%|█▎        | 3672/26933 [05:04<26:40, 14.53it/s] 14%|█▎        | 3676/26933 [05:05<26:43, 14.51it/s] 14%|█▎        | 3680/26933 [05:05<26:40, 14.53it/s] 14%|█▎        | 3684/26933 [05:05<26:37, 14.55it/s] 14%|█▎        | 3688/26933 [05:05<26:35, 14.57it/s] 14%|█▎        | 3692/26933 [05:06<26:36, 14.56it/s] 14%|█▎        | 3696/26933 [05:06<26:36, 14.55it/s] 14%|█▎        | 3700/26933 [05:06<26:44, 14.48it/s] 14%|█▍        | 3704/26933 [05:07<26:43, 14.48it/s] 14%|█▍        | 3708/26933 [05:07<26:41, 14.50it/s] 14%|█▍        | 3712/26933 [05:07<26:43, 14.48it/s] 14%|█▍        | 3716/26933 [05:07<26:41, 14.50it/s] 14%|█▍        | 3720/26933 [05:08<26:46, 14.45it/s] 14%|█▍        | 3724/26933 [05:08<26:41, 14.49it/s] 14%|█▍        | 3728/26933 [05:08<26:41, 14.49it/s] 14%|█▍        | 3732/26933 [05:08<26:34, 14.55it/s] 14%|█▍        | 3736/26933 [05:09<26:35, 14.54it/s] 14%|█▍        | 3740/26933 [05:09<26:32, 14.56it/s] 14%|█▍        | 3744/26933 [05:09<26:31, 14.57it/s] 14%|█▍        | 3748/26933 [05:10<27:15, 14.18it/s] 14%|█▍        | 3752/26933 [05:10<27:12, 14.20it/s] 14%|█▍        | 3756/26933 [05:10<27:48, 13.89it/s] 14%|█▍        | 3760/26933 [05:10<28:02, 13.78it/s] 14%|█▍        | 3764/26933 [05:11<28:18, 13.64it/s] 14%|█▍        | 3768/26933 [05:11<28:27, 13.57it/s] 14%|█▍        | 3772/26933 [05:11<28:34, 13.51it/s] 14%|█▍        | 3776/26933 [05:12<28:51, 13.37it/s] 14%|█▍        | 3780/26933 [05:12<28:53, 13.36it/s] 14%|█▍        | 3784/26933 [05:12<29:00, 13.30it/s] 14%|█▍        | 3788/26933 [05:13<29:00, 13.30it/s] 14%|█▍        | 3792/26933 [05:13<29:09, 13.23it/s] 14%|█▍        | 3796/26933 [05:13<29:13, 13.20it/s] 14%|█▍        | 3800/26933 [05:14<29:06, 13.25it/s] 14%|█▍        | 3804/26933 [05:14<28:59, 13.30it/s] 14%|█▍        | 3808/26933 [05:14<28:55, 13.33it/s] 14%|█▍        | 3812/26933 [05:14<28:51, 13.35it/s] 14%|█▍        | 3816/26933 [05:15<28:53, 13.34it/s] 14%|█▍        | 3820/26933 [05:15<28:46, 13.38it/s] 14%|█▍        | 3824/26933 [05:15<28:43, 13.41it/s] 14%|█▍        | 3828/26933 [05:16<28:44, 13.40it/s] 14%|█▍        | 3832/26933 [05:16<28:45, 13.39it/s] 14%|█▍        | 3836/26933 [05:16<28:44, 13.40it/s] 14%|█▍        | 3840/26933 [05:16<28:41, 13.41it/s] 14%|█▍        | 3844/26933 [05:17<28:44, 13.39it/s] 14%|█▍        | 3848/26933 [05:17<28:43, 13.39it/s] 14%|█▍        | 3852/26933 [05:17<28:40, 13.41it/s] 14%|█▍        | 3856/26933 [05:18<28:56, 13.29it/s] 14%|█▍        | 3860/26933 [05:18<29:00, 13.26it/s] 14%|█▍        | 3864/26933 [05:18<28:54, 13.30it/s] 14%|█▍        | 3868/26933 [05:19<28:49, 13.34it/s] 14%|█▍        | 3872/26933 [05:19<28:46, 13.36it/s] 14%|█▍        | 3876/26933 [05:19<28:39, 13.41it/s] 14%|█▍        | 3880/26933 [05:19<28:33, 13.45it/s] 14%|█▍        | 3884/26933 [05:20<28:38, 13.41it/s] 14%|█▍        | 3888/26933 [05:20<28:36, 13.42it/s] 14%|█▍        | 3892/26933 [05:20<28:40, 13.39it/s] 14%|█▍        | 3896/26933 [05:21<28:46, 13.35it/s] 14%|█▍        | 3900/26933 [05:21<28:41, 13.38it/s] 14%|█▍        | 3904/26933 [05:21<28:38, 13.40it/s] 15%|█▍        | 3908/26933 [05:22<28:35, 13.43it/s] 15%|█▍        | 3912/26933 [05:22<28:34, 13.43it/s] 15%|█▍        | 3916/26933 [05:22<28:33, 13.43it/s] 15%|█▍        | 3920/26933 [05:22<28:32, 13.44it/s] 15%|█▍        | 3924/26933 [05:23<28:38, 13.39it/s] 15%|█▍        | 3928/26933 [05:23<28:44, 13.34it/s] 15%|█▍        | 3932/26933 [05:23<28:42, 13.35it/s] 15%|█▍        | 3936/26933 [05:24<28:49, 13.29it/s] 15%|█▍        | 3940/26933 [05:24<28:46, 13.32it/s] 15%|█▍        | 3944/26933 [05:24<28:43, 13.34it/s] 15%|█▍        | 3948/26933 [05:25<28:39, 13.37it/s] 15%|█▍        | 3952/26933 [05:25<28:40, 13.36it/s] 15%|█▍        | 3956/26933 [05:25<28:12, 13.58it/s] 15%|█▍        | 3960/26933 [05:25<27:45, 13.79it/s] 15%|█▍        | 3964/26933 [05:26<27:32, 13.90it/s] 15%|█▍        | 3968/26933 [05:26<27:21, 13.99it/s] 15%|█▍        | 3972/26933 [05:26<27:11, 14.07it/s] 15%|█▍        | 3976/26933 [05:27<27:02, 14.15it/s] 15%|█▍        | 3980/26933 [05:27<26:58, 14.18it/s] 15%|█▍        | 3984/26933 [05:27<26:52, 14.23it/s] 15%|█▍        | 3988/26933 [05:27<26:48, 14.26it/s] 15%|█▍        | 3992/26933 [05:28<27:04, 14.12it/s] 15%|█▍        | 3996/26933 [05:28<26:50, 14.24it/s] 15%|█▍        | 4000/26933 [05:28<26:43, 14.30it/s] 15%|█▍        | 4004/26933 [05:29<26:37, 14.35it/s] 15%|█▍        | 4008/26933 [05:29<26:36, 14.36it/s] 15%|█▍        | 4012/26933 [05:29<26:30, 14.41it/s] 15%|█▍        | 4016/26933 [05:29<26:24, 14.46it/s] 15%|█▍        | 4020/26933 [05:30<26:28, 14.42it/s] 15%|█▍        | 4024/26933 [05:30<26:27, 14.43it/s] 15%|█▍        | 4028/26933 [05:30<26:28, 14.42it/s] 15%|█▍        | 4032/26933 [05:30<26:25, 14.45it/s] 15%|█▍        | 4036/26933 [05:31<26:28, 14.42it/s] 15%|█▌        | 4040/26933 [05:31<26:24, 14.45it/s] 15%|█▌        | 4044/26933 [05:31<26:23, 14.46it/s] 15%|█▌        | 4048/26933 [05:32<26:28, 14.41it/s] 15%|█▌        | 4052/26933 [05:32<26:27, 14.41it/s] 15%|█▌        | 4056/26933 [05:32<26:26, 14.42it/s] 15%|█▌        | 4060/26933 [05:32<26:22, 14.45it/s] 15%|█▌        | 4064/26933 [05:33<26:30, 14.38it/s] 15%|█▌        | 4068/26933 [05:33<26:27, 14.41it/s] 15%|█▌        | 4072/26933 [05:33<26:26, 14.41it/s] 15%|█▌        | 4076/26933 [05:34<26:25, 14.41it/s] 15%|█▌        | 4080/26933 [05:34<26:27, 14.40it/s] 15%|█▌        | 4084/26933 [05:34<26:24, 14.42it/s] 15%|█▌        | 4088/26933 [05:34<26:22, 14.43it/s] 15%|█▌        | 4092/26933 [05:35<26:25, 14.40it/s] 15%|█▌        | 4096/26933 [05:35<26:18, 14.47it/s] 15%|█▌        | 4100/26933 [05:35<26:14, 14.51it/s] 15%|█▌        | 4104/26933 [05:35<26:13, 14.51it/s] 15%|█▌        | 4108/26933 [05:36<26:16, 14.48it/s] 15%|█▌        | 4112/26933 [05:36<26:30, 14.35it/s] 15%|█▌        | 4116/26933 [05:36<27:16, 13.95it/s] 15%|█▌        | 4120/26933 [05:37<27:38, 13.76it/s] 15%|█▌        | 4124/26933 [05:37<27:44, 13.70it/s] 15%|█▌        | 4128/26933 [05:37<27:47, 13.68it/s] 15%|█▌        | 4132/26933 [05:37<27:50, 13.65it/s] 15%|█▌        | 4136/26933 [05:38<28:07, 13.51it/s] 15%|█▌        | 4140/26933 [05:38<28:07, 13.51it/s] 15%|█▌        | 4144/26933 [05:38<28:04, 13.53it/s] 15%|█▌        | 4148/26933 [05:39<28:13, 13.45it/s] 15%|█▌        | 4152/26933 [05:39<28:08, 13.49it/s] 15%|█▌        | 4156/26933 [05:39<28:13, 13.45it/s] 15%|█▌        | 4160/26933 [05:40<28:12, 13.46it/s] 15%|█▌        | 4164/26933 [05:40<28:17, 13.41it/s] 15%|█▌        | 4168/26933 [05:40<28:11, 13.46it/s] 15%|█▌        | 4172/26933 [05:40<28:04, 13.51it/s] 16%|█▌        | 4176/26933 [05:41<28:05, 13.50it/s] 16%|█▌        | 4180/26933 [05:41<28:02, 13.53it/s] 16%|█▌        | 4184/26933 [05:41<28:00, 13.54it/s] 16%|█▌        | 4188/26933 [05:42<28:02, 13.52it/s] 16%|█▌        | 4192/26933 [05:42<28:04, 13.50it/s] 16%|█▌        | 4196/26933 [05:42<28:10, 13.45it/s] 16%|█▌        | 4200/26933 [05:43<28:10, 13.45it/s] 16%|█▌        | 4204/26933 [05:43<28:17, 13.39it/s] 16%|█▌        | 4208/26933 [05:43<28:14, 13.41it/s] 16%|█▌        | 4212/26933 [05:43<28:11, 13.43it/s] 16%|█▌        | 4216/26933 [05:44<28:08, 13.45it/s] 16%|█▌        | 4220/26933 [05:44<28:02, 13.50it/s] 16%|█▌        | 4224/26933 [05:44<27:57, 13.54it/s] 16%|█▌        | 4228/26933 [05:45<27:59, 13.51it/s] 16%|█▌        | 4232/26933 [05:45<27:57, 13.53it/s] 16%|█▌        | 4236/26933 [05:45<27:53, 13.56it/s] 16%|█▌        | 4240/26933 [05:45<27:55, 13.54it/s] 16%|█▌        | 4244/26933 [05:46<27:56, 13.54it/s] 16%|█▌        | 4248/26933 [05:46<27:59, 13.51it/s] 16%|█▌        | 4252/26933 [05:46<27:53, 13.55it/s] 16%|█▌        | 4256/26933 [05:47<27:58, 13.51it/s] 16%|█▌        | 4260/26933 [05:47<27:58, 13.51it/s] 16%|█▌        | 4264/26933 [05:47<27:53, 13.55it/s] 16%|█▌        | 4268/26933 [05:48<27:50, 13.57it/s] 16%|█▌        | 4272/26933 [05:48<27:47, 13.59it/s] 16%|█▌        | 4276/26933 [05:48<27:44, 13.61it/s] 16%|█▌        | 4280/26933 [05:48<27:43, 13.62it/s] 16%|█▌        | 4284/26933 [05:49<27:59, 13.49it/s] 16%|█▌        | 4288/26933 [05:49<27:57, 13.50it/s] 16%|█▌        | 4292/26933 [05:49<27:23, 13.78it/s] 16%|█▌        | 4296/26933 [05:50<26:55, 14.01it/s] 16%|█▌        | 4300/26933 [05:50<26:29, 14.24it/s] 16%|█▌        | 4304/26933 [05:50<26:14, 14.37it/s] 16%|█▌        | 4308/26933 [05:50<26:00, 14.50it/s] 16%|█▌        | 4312/26933 [05:51<25:54, 14.55it/s] 16%|█▌        | 4316/26933 [05:51<25:46, 14.62it/s] 16%|█▌        | 4320/26933 [05:51<25:44, 14.64it/s] 16%|█▌        | 4324/26933 [05:51<25:45, 14.63it/s] 16%|█▌        | 4328/26933 [05:52<25:42, 14.66it/s] 16%|█▌        | 4332/26933 [05:52<25:39, 14.68it/s] 16%|█▌        | 4336/26933 [05:52<25:35, 14.72it/s] 16%|█▌        | 4340/26933 [05:53<25:35, 14.71it/s] 16%|█▌        | 4344/26933 [05:53<25:31, 14.75it/s] 16%|█▌        | 4348/26933 [05:53<25:28, 14.77it/s] 16%|█▌        | 4352/26933 [05:53<25:29, 14.76it/s] 16%|█▌        | 4356/26933 [05:54<25:28, 14.77it/s] 16%|█▌        | 4360/26933 [05:54<25:26, 14.78it/s] 16%|█▌        | 4364/26933 [05:54<25:26, 14.78it/s] 16%|█▌        | 4368/26933 [05:54<25:35, 14.70it/s] 16%|█▌        | 4372/26933 [05:55<25:33, 14.71it/s] 16%|█▌        | 4376/26933 [05:55<25:30, 14.74it/s] 16%|█▋        | 4380/26933 [05:55<25:26, 14.78it/s] 16%|█▋        | 4384/26933 [05:56<25:24, 14.79it/s] 16%|█▋        | 4388/26933 [05:56<25:24, 14.79it/s] 16%|█▋        | 4392/26933 [05:56<25:25, 14.78it/s] 16%|█▋        | 4396/26933 [05:56<25:22, 14.80it/s] 16%|█▋        | 4400/26933 [05:57<25:27, 14.75it/s] 16%|█▋        | 4404/26933 [05:57<25:23, 14.79it/s] 16%|█▋        | 4408/26933 [05:57<25:29, 14.72it/s] 16%|█▋        | 4412/26933 [05:57<25:26, 14.75it/s] 16%|█▋        | 4416/26933 [05:58<25:30, 14.71it/s] 16%|█▋        | 4420/26933 [05:58<25:27, 14.74it/s] 16%|█▋        | 4424/26933 [05:58<25:24, 14.76it/s] 16%|█▋        | 4428/26933 [05:59<25:22, 14.78it/s] 16%|█▋        | 4432/26933 [05:59<25:18, 14.82it/s] 16%|█▋        | 4436/26933 [05:59<25:17, 14.82it/s] 16%|█▋        | 4440/26933 [05:59<25:23, 14.77it/s] 17%|█▋        | 4444/26933 [06:00<25:29, 14.70it/s] 17%|█▋        | 4448/26933 [06:00<25:32, 14.67it/s] 17%|█▋        | 4452/26933 [06:00<25:33, 14.66it/s] 17%|█▋        | 4456/26933 [06:00<25:34, 14.65it/s] 17%|█▋        | 4460/26933 [06:01<26:07, 14.33it/s] 17%|█▋        | 4464/26933 [06:01<26:31, 14.12it/s] 17%|█▋        | 4468/26933 [06:01<26:47, 13.97it/s] 17%|█▋        | 4472/26933 [06:02<27:07, 13.80it/s] 17%|█▋        | 4476/26933 [06:02<27:11, 13.76it/s] 17%|█▋        | 4480/26933 [06:02<26:44, 13.99it/s] 17%|█▋        | 4484/26933 [06:02<26:18, 14.22it/s] 17%|█▋        | 4488/26933 [06:03<26:33, 14.09it/s] 17%|█▋        | 4492/26933 [06:03<26:44, 13.99it/s] 17%|█▋        | 4496/26933 [06:03<26:48, 13.95it/s] 17%|█▋        | 4500/26933 [06:04<26:56, 13.87it/s] 17%|█▋        | 4504/26933 [06:04<26:25, 14.14it/s] 17%|█▋        | 4508/26933 [06:04<26:02, 14.36it/s] 17%|█▋        | 4512/26933 [06:04<25:50, 14.46it/s] 17%|█▋        | 4516/26933 [06:05<25:41, 14.54it/s] 17%|█▋        | 4520/26933 [06:05<25:38, 14.56it/s] 17%|█▋        | 4524/26933 [06:05<25:30, 14.64it/s] 17%|█▋        | 4528/26933 [06:06<25:25, 14.69it/s] 17%|█▋        | 4532/26933 [06:06<25:24, 14.69it/s] 17%|█▋        | 4536/26933 [06:06<25:19, 14.74it/s] 17%|█▋        | 4540/26933 [06:06<25:18, 14.75it/s] 17%|█▋        | 4544/26933 [06:07<25:59, 14.35it/s] 17%|█▋        | 4548/26933 [06:07<26:26, 14.11it/s] 17%|█▋        | 4552/26933 [06:07<26:40, 13.99it/s] 17%|█▋        | 4556/26933 [06:08<26:51, 13.89it/s] 17%|█▋        | 4560/26933 [06:08<26:59, 13.81it/s] 17%|█▋        | 4564/26933 [06:08<27:05, 13.76it/s] 17%|█▋        | 4568/26933 [06:08<27:02, 13.78it/s] 17%|█▋        | 4572/26933 [06:09<27:06, 13.75it/s] 17%|█▋        | 4576/26933 [06:09<27:02, 13.78it/s] 17%|█▋        | 4580/26933 [06:09<26:47, 13.90it/s] 17%|█▋        | 4584/26933 [06:10<26:18, 14.16it/s] 17%|█▋        | 4588/26933 [06:10<25:58, 14.34it/s] 17%|█▋        | 4592/26933 [06:10<25:38, 14.52it/s] 17%|█▋        | 4596/26933 [06:10<25:26, 14.63it/s] 17%|█▋        | 4600/26933 [06:11<25:20, 14.69it/s] 17%|█▋        | 4604/26933 [06:11<25:13, 14.75it/s] 17%|█▋        | 4608/26933 [06:11<25:10, 14.78it/s] 17%|█▋        | 4612/26933 [06:11<25:06, 14.81it/s] 17%|█▋        | 4616/26933 [06:12<25:05, 14.82it/s] 17%|█▋        | 4620/26933 [06:12<25:02, 14.85it/s] 17%|█▋        | 4624/26933 [06:12<25:00, 14.87it/s] 17%|█▋        | 4628/26933 [06:12<24:59, 14.88it/s] 17%|█▋        | 4632/26933 [06:13<24:59, 14.87it/s] 17%|█▋        | 4636/26933 [06:13<24:57, 14.89it/s] 17%|█▋        | 4640/26933 [06:13<24:56, 14.90it/s] 17%|█▋        | 4644/26933 [06:14<24:52, 14.94it/s] 17%|█▋        | 4648/26933 [06:14<25:00, 14.85it/s] 17%|█▋        | 4652/26933 [06:14<24:58, 14.87it/s] 17%|█▋        | 4656/26933 [06:14<24:52, 14.92it/s] 17%|█▋        | 4660/26933 [06:15<24:54, 14.91it/s] 17%|█▋        | 4664/26933 [06:15<24:51, 14.93it/s] 17%|█▋        | 4668/26933 [06:15<25:23, 14.61it/s] 17%|█▋        | 4672/26933 [06:15<25:54, 14.32it/s] 17%|█▋        | 4676/26933 [06:16<26:16, 14.12it/s] 17%|█▋        | 4680/26933 [06:16<26:30, 13.99it/s] 17%|█▋        | 4684/26933 [06:16<26:36, 13.94it/s] 17%|█▋        | 4688/26933 [06:17<26:53, 13.79it/s] 17%|█▋        | 4692/26933 [06:17<26:50, 13.81it/s] 17%|█▋        | 4696/26933 [06:17<26:47, 13.83it/s] 17%|█▋        | 4700/26933 [06:18<26:45, 13.85it/s] 17%|█▋        | 4704/26933 [06:18<26:44, 13.86it/s] 17%|█▋        | 4708/26933 [06:18<26:47, 13.83it/s] 17%|█▋        | 4712/26933 [06:18<27:11, 13.62it/s] 18%|█▊        | 4716/26933 [06:19<27:25, 13.50it/s] 18%|█▊        | 4720/26933 [06:19<27:18, 13.56it/s] 18%|█▊        | 4724/26933 [06:19<27:18, 13.55it/s] 18%|█▊        | 4728/26933 [06:20<27:07, 13.65it/s] 18%|█▊        | 4732/26933 [06:20<27:11, 13.61it/s] 18%|█▊        | 4736/26933 [06:20<27:05, 13.66it/s] 18%|█▊        | 4740/26933 [06:20<27:09, 13.62it/s] 18%|█▊        | 4744/26933 [06:21<27:27, 13.47it/s] 18%|█▊        | 4748/26933 [06:21<26:44, 13.83it/s] 18%|█▊        | 4752/26933 [06:21<26:03, 14.19it/s] 18%|█▊        | 4756/26933 [06:22<25:43, 14.36it/s] 18%|█▊        | 4760/26933 [06:22<25:28, 14.51it/s] 18%|█▊        | 4764/26933 [06:22<25:16, 14.62it/s] 18%|█▊        | 4768/26933 [06:22<25:16, 14.61it/s] 18%|█▊        | 4772/26933 [06:23<25:13, 14.64it/s] 18%|█▊        | 4776/26933 [06:23<25:01, 14.76it/s] 18%|█▊        | 4780/26933 [06:23<24:55, 14.81it/s] 18%|█▊        | 4784/26933 [06:23<24:51, 14.85it/s] 18%|█▊        | 4788/26933 [06:24<24:55, 14.81it/s] 18%|█▊        | 4792/26933 [06:24<24:51, 14.84it/s] 18%|█▊        | 4796/26933 [06:24<24:45, 14.90it/s] 18%|█▊        | 4800/26933 [06:25<24:38, 14.97it/s] 18%|█▊        | 4804/26933 [06:25<24:40, 14.94it/s] 18%|█▊        | 4808/26933 [06:25<24:34, 15.00it/s] 18%|█▊        | 4812/26933 [06:25<24:32, 15.02it/s] 18%|█▊        | 4816/26933 [06:26<24:34, 15.00it/s] 18%|█▊        | 4820/26933 [06:26<24:32, 15.02it/s] 18%|█▊        | 4824/26933 [06:26<24:30, 15.03it/s] 18%|█▊        | 4828/26933 [06:26<24:32, 15.02it/s] 18%|█▊        | 4832/26933 [06:27<24:32, 15.01it/s] 18%|█▊        | 4836/26933 [06:27<25:06, 14.67it/s] 18%|█▊        | 4840/26933 [06:27<25:38, 14.36it/s] 18%|█▊        | 4844/26933 [06:28<25:54, 14.21it/s] 18%|█▊        | 4848/26933 [06:28<26:06, 14.10it/s] 18%|█▊        | 4852/26933 [06:28<26:16, 14.01it/s] 18%|█▊        | 4856/26933 [06:28<26:17, 13.99it/s] 18%|█▊        | 4860/26933 [06:29<26:19, 13.97it/s] 18%|█▊        | 4864/26933 [06:29<26:20, 13.96it/s] 18%|█▊        | 4868/26933 [06:29<26:19, 13.97it/s] 18%|█▊        | 4872/26933 [06:30<26:19, 13.97it/s] 18%|█▊        | 4876/26933 [06:30<26:20, 13.95it/s] 18%|█▊        | 4880/26933 [06:30<26:19, 13.96it/s] 18%|█▊        | 4884/26933 [06:30<26:21, 13.94it/s] 18%|█▊        | 4888/26933 [06:31<26:21, 13.94it/s] 18%|█▊        | 4892/26933 [06:31<26:27, 13.89it/s] 18%|█▊        | 4896/26933 [06:31<26:32, 13.84it/s] 18%|█▊        | 4900/26933 [06:32<26:29, 13.86it/s] 18%|█▊        | 4904/26933 [06:32<26:22, 13.92it/s] 18%|█▊        | 4908/26933 [06:32<26:20, 13.93it/s] 18%|█▊        | 4912/26933 [06:32<26:25, 13.89it/s] 18%|█▊        | 4916/26933 [06:33<26:24, 13.89it/s] 18%|█▊        | 4920/26933 [06:33<26:24, 13.89it/s] 18%|█▊        | 4924/26933 [06:33<26:17, 13.95it/s] 18%|█▊        | 4928/26933 [06:34<26:12, 14.00it/s] 18%|█▊        | 4932/26933 [06:34<26:11, 14.00it/s] 18%|█▊        | 4936/26933 [06:34<26:15, 13.96it/s] 18%|█▊        | 4940/26933 [06:34<26:17, 13.95it/s] 18%|█▊        | 4944/26933 [06:35<26:15, 13.96it/s] 18%|█▊        | 4948/26933 [06:35<26:11, 13.99it/s] 18%|█▊        | 4952/26933 [06:35<26:09, 14.00it/s] 18%|█▊        | 4956/26933 [06:36<26:09, 14.01it/s] 18%|█▊        | 4960/26933 [06:36<26:05, 14.03it/s] 18%|█▊        | 4964/26933 [06:36<26:05, 14.03it/s] 18%|█▊        | 4968/26933 [06:36<26:04, 14.04it/s] 18%|█▊        | 4972/26933 [06:37<26:07, 14.01it/s] 18%|█▊        | 4976/26933 [06:37<26:02, 14.05it/s] 18%|█▊        | 4980/26933 [06:37<26:09, 13.99it/s] 19%|█▊        | 4984/26933 [06:38<26:06, 14.02it/s] 19%|█▊        | 4988/26933 [06:38<26:18, 13.90it/s] 19%|█▊        | 4992/26933 [06:38<26:25, 13.83it/s] 19%|█▊        | 4996/26933 [06:38<26:12, 13.95it/s] 19%|█▊        | 5000/26933 [06:39<25:44, 14.21it/s] 19%|█▊        | 5004/26933 [06:39<25:27, 14.35it/s] 19%|█▊        | 5008/26933 [06:39<25:06, 14.56it/s] 19%|█▊        | 5012/26933 [06:39<24:53, 14.68it/s] 19%|█▊        | 5016/26933 [06:40<24:52, 14.69it/s] 19%|█▊        | 5020/26933 [06:40<24:49, 14.71it/s] 19%|█▊        | 5024/26933 [06:40<24:43, 14.77it/s] 19%|█▊        | 5028/26933 [06:41<25:07, 14.53it/s] 19%|█▊        | 5032/26933 [06:41<25:21, 14.40it/s] 19%|█▊        | 5036/26933 [06:41<25:29, 14.31it/s] 19%|█▊        | 5040/26933 [06:41<25:36, 14.25it/s] 19%|█▊        | 5044/26933 [06:42<25:46, 14.15it/s] 19%|█▊        | 5048/26933 [06:42<25:44, 14.17it/s] 19%|█▉        | 5052/26933 [06:42<25:45, 14.16it/s] 19%|█▉        | 5056/26933 [06:43<25:44, 14.17it/s] 19%|█▉        | 5060/26933 [06:43<25:45, 14.15it/s] 19%|█▉        | 5064/26933 [06:43<25:44, 14.16it/s] 19%|█▉        | 5068/26933 [06:43<25:59, 14.02it/s] 19%|█▉        | 5072/26933 [06:44<25:54, 14.06it/s] 19%|█▉        | 5076/26933 [06:44<25:49, 14.10it/s] 19%|█▉        | 5080/26933 [06:44<25:46, 14.13it/s] 19%|█▉        | 5084/26933 [06:45<25:50, 14.09it/s] 19%|█▉        | 5088/26933 [06:45<25:56, 14.03it/s] 19%|█▉        | 5092/26933 [06:45<25:33, 14.24it/s] 19%|█▉        | 5096/26933 [06:45<25:13, 14.43it/s] 19%|█▉        | 5100/26933 [06:46<25:04, 14.52it/s] 19%|█▉        | 5104/26933 [06:46<24:56, 14.59it/s] 19%|█▉        | 5108/26933 [06:46<25:33, 14.23it/s] 19%|█▉        | 5112/26933 [06:47<25:37, 14.19it/s] 19%|█▉        | 5116/26933 [06:47<25:58, 14.00it/s] 19%|█▉        | 5120/26933 [06:47<26:00, 13.98it/s] 19%|█▉        | 5124/26933 [06:47<25:58, 13.99it/s] 19%|█▉        | 5128/26933 [06:48<26:01, 13.96it/s] 19%|█▉        | 5132/26933 [06:48<26:15, 13.83it/s] 19%|█▉        | 5136/26933 [06:48<26:15, 13.84it/s] 19%|█▉        | 5140/26933 [06:49<26:08, 13.90it/s] 19%|█▉        | 5144/26933 [06:49<26:12, 13.85it/s] 19%|█▉        | 5148/26933 [06:49<26:06, 13.91it/s] 19%|█▉        | 5152/26933 [06:49<26:04, 13.92it/s] 19%|█▉        | 5156/26933 [06:50<26:07, 13.90it/s] 19%|█▉        | 5160/26933 [06:50<26:02, 13.93it/s] 19%|█▉        | 5164/26933 [06:50<25:55, 14.00it/s] 19%|█▉        | 5168/26933 [06:51<25:48, 14.05it/s] 19%|█▉        | 5172/26933 [06:51<25:49, 14.04it/s] 19%|█▉        | 5176/26933 [06:51<25:45, 14.07it/s] 19%|█▉        | 5180/26933 [06:51<25:41, 14.11it/s] 19%|█▉        | 5184/26933 [06:52<25:42, 14.10it/s] 19%|█▉        | 5188/26933 [06:52<25:16, 14.34it/s] 19%|█▉        | 5192/26933 [06:52<24:53, 14.56it/s] 19%|█▉        | 5196/26933 [06:52<24:37, 14.71it/s] 19%|█▉        | 5200/26933 [06:53<24:28, 14.80it/s] 19%|█▉        | 5204/26933 [06:53<24:17, 14.91it/s] 19%|█▉        | 5208/26933 [06:53<24:11, 14.96it/s] 19%|█▉        | 5212/26933 [06:54<24:35, 14.72it/s] 19%|█▉        | 5216/26933 [06:54<24:49, 14.58it/s] 19%|█▉        | 5220/26933 [06:54<24:58, 14.49it/s] 19%|█▉        | 5224/26933 [06:54<25:01, 14.46it/s] 19%|█▉        | 5228/26933 [06:55<25:05, 14.42it/s] 19%|█▉        | 5232/26933 [06:55<25:08, 14.39it/s] 19%|█▉        | 5236/26933 [06:55<25:12, 14.34it/s] 19%|█▉        | 5240/26933 [06:56<25:15, 14.32it/s] 19%|█▉        | 5244/26933 [06:56<25:18, 14.28it/s] 19%|█▉        | 5248/26933 [06:56<25:20, 14.26it/s] 20%|█▉        | 5252/26933 [06:56<25:16, 14.30it/s] 20%|█▉        | 5256/26933 [06:57<25:18, 14.28it/s] 20%|█▉        | 5260/26933 [06:57<25:16, 14.30it/s] 20%|█▉        | 5264/26933 [06:57<25:13, 14.32it/s] 20%|█▉        | 5268/26933 [06:57<25:18, 14.26it/s] 20%|█▉        | 5272/26933 [06:58<25:22, 14.23it/s] 20%|█▉        | 5276/26933 [06:58<25:28, 14.17it/s] 20%|█▉        | 5280/26933 [06:58<25:33, 14.12it/s] 20%|█▉        | 5284/26933 [06:59<25:12, 14.32it/s] 20%|█▉        | 5288/26933 [06:59<25:09, 14.34it/s] 20%|█▉        | 5292/26933 [06:59<25:01, 14.41it/s] 20%|█▉        | 5296/26933 [06:59<24:59, 14.43it/s] 20%|█▉        | 5300/26933 [07:00<25:10, 14.32it/s] 20%|█▉        | 5304/26933 [07:00<24:49, 14.52it/s] 20%|█▉        | 5308/26933 [07:00<24:37, 14.63it/s] 20%|█▉        | 5312/26933 [07:01<24:26, 14.75it/s] 20%|█▉        | 5316/26933 [07:01<24:30, 14.71it/s] 20%|█▉        | 5320/26933 [07:01<24:20, 14.79it/s] 20%|█▉        | 5324/26933 [07:01<24:15, 14.84it/s] 20%|█▉        | 5328/26933 [07:02<24:15, 14.85it/s] 20%|█▉        | 5332/26933 [07:02<24:15, 14.84it/s] 20%|█▉        | 5336/26933 [07:02<24:15, 14.84it/s] 20%|█▉        | 5340/26933 [07:02<24:12, 14.87it/s] 20%|█▉        | 5344/26933 [07:03<24:10, 14.88it/s] 20%|█▉        | 5348/26933 [07:03<24:10, 14.89it/s] 20%|█▉        | 5352/26933 [07:03<24:06, 14.92it/s] 20%|█▉        | 5356/26933 [07:03<24:05, 14.93it/s] 20%|█▉        | 5360/26933 [07:04<24:11, 14.87it/s] 20%|█▉        | 5364/26933 [07:04<24:08, 14.89it/s] 20%|█▉        | 5368/26933 [07:04<24:11, 14.86it/s] 20%|█▉        | 5372/26933 [07:05<24:11, 14.85it/s] 20%|█▉        | 5376/26933 [07:05<24:20, 14.76it/s] 20%|█▉        | 5380/26933 [07:05<24:19, 14.76it/s] 20%|█▉        | 5384/26933 [07:05<24:15, 14.81it/s] 20%|██        | 5388/26933 [07:06<24:13, 14.83it/s] 20%|██        | 5392/26933 [07:06<24:12, 14.83it/s] 20%|██        | 5396/26933 [07:06<24:11, 14.83it/s] 20%|██        | 5400/26933 [07:06<24:06, 14.88it/s] 20%|██        | 5404/26933 [07:07<24:04, 14.90it/s] 20%|██        | 5408/26933 [07:07<24:02, 14.92it/s] 20%|██        | 5412/26933 [07:07<24:28, 14.66it/s] 20%|██        | 5416/26933 [07:08<24:10, 14.84it/s] 20%|██        | 5420/26933 [07:08<23:56, 14.98it/s] 20%|██        | 5424/26933 [07:08<23:46, 15.08it/s] 20%|██        | 5428/26933 [07:08<23:38, 15.16it/s] 20%|██        | 5432/26933 [07:09<23:33, 15.21it/s] 20%|██        | 5436/26933 [07:09<23:36, 15.18it/s] 20%|██        | 5440/26933 [07:09<23:32, 15.22it/s] 20%|██        | 5444/26933 [07:09<23:29, 15.25it/s] 20%|██        | 5448/26933 [07:10<23:27, 15.26it/s] 20%|██        | 5452/26933 [07:10<23:39, 15.14it/s] 20%|██        | 5456/26933 [07:10<23:32, 15.21it/s] 20%|██        | 5460/26933 [07:10<23:31, 15.21it/s] 20%|██        | 5464/26933 [07:11<23:28, 15.24it/s] 20%|██        | 5468/26933 [07:11<23:26, 15.26it/s] 20%|██        | 5472/26933 [07:11<23:20, 15.32it/s] 20%|██        | 5476/26933 [07:11<23:12, 15.40it/s] 20%|██        | 5480/26933 [07:12<23:10, 15.42it/s] 20%|██        | 5484/26933 [07:12<23:06, 15.47it/s] 20%|██        | 5488/26933 [07:12<23:00, 15.54it/s] 20%|██        | 5492/26933 [07:12<22:57, 15.57it/s] 20%|██        | 5496/26933 [07:13<23:08, 15.44it/s] 20%|██        | 5500/26933 [07:13<23:06, 15.46it/s] 20%|██        | 5504/26933 [07:13<23:06, 15.46it/s] 20%|██        | 5508/26933 [07:13<23:06, 15.45it/s] 20%|██        | 5512/26933 [07:14<23:10, 15.41it/s] 20%|██        | 5516/26933 [07:14<23:16, 15.34it/s] 20%|██        | 5520/26933 [07:14<23:16, 15.33it/s] 21%|██        | 5524/26933 [07:15<23:22, 15.27it/s] 21%|██        | 5528/26933 [07:15<23:26, 15.22it/s] 21%|██        | 5532/26933 [07:15<23:25, 15.23it/s] 21%|██        | 5536/26933 [07:15<23:21, 15.26it/s] 21%|██        | 5540/26933 [07:16<23:26, 15.21it/s] 21%|██        | 5544/26933 [07:16<23:24, 15.23it/s] 21%|██        | 5548/26933 [07:16<23:23, 15.24it/s] 21%|██        | 5552/26933 [07:16<23:20, 15.27it/s] 21%|██        | 5556/26933 [07:17<23:22, 15.25it/s] 21%|██        | 5560/26933 [07:17<23:15, 15.31it/s] 21%|██        | 5564/26933 [07:17<23:12, 15.34it/s] 21%|██        | 5568/26933 [07:17<23:06, 15.41it/s] 21%|██        | 5572/26933 [07:18<23:07, 15.39it/s] 21%|██        | 5576/26933 [07:18<23:33, 15.11it/s] 21%|██        | 5580/26933 [07:18<23:52, 14.91it/s] 21%|██        | 5584/26933 [07:19<24:05, 14.77it/s] 21%|██        | 5588/26933 [07:19<24:18, 14.64it/s] 21%|██        | 5592/26933 [07:19<24:22, 14.59it/s] 21%|██        | 5596/26933 [07:19<24:30, 14.51it/s] 21%|██        | 5600/26933 [07:20<24:33, 14.48it/s] 21%|██        | 5604/26933 [07:20<24:37, 14.44it/s] 21%|██        | 5608/26933 [07:20<24:37, 14.43it/s] 21%|██        | 5612/26933 [07:20<24:36, 14.44it/s] 21%|██        | 5616/26933 [07:21<24:36, 14.44it/s] 21%|██        | 5620/26933 [07:21<24:34, 14.45it/s] 21%|██        | 5624/26933 [07:21<24:33, 14.47it/s] 21%|██        | 5628/26933 [07:22<24:30, 14.49it/s] 21%|██        | 5632/26933 [07:22<24:28, 14.50it/s] 21%|██        | 5636/26933 [07:22<24:30, 14.48it/s] 21%|██        | 5640/26933 [07:22<24:28, 14.50it/s] 21%|██        | 5644/26933 [07:23<24:33, 14.45it/s] 21%|██        | 5648/26933 [07:23<24:29, 14.49it/s] 21%|██        | 5652/26933 [07:23<24:32, 14.45it/s] 21%|██        | 5656/26933 [07:23<24:30, 14.47it/s] 21%|██        | 5660/26933 [07:24<24:31, 14.46it/s] 21%|██        | 5664/26933 [07:24<24:29, 14.47it/s] 21%|██        | 5668/26933 [07:24<24:32, 14.44it/s] 21%|██        | 5672/26933 [07:25<24:35, 14.41it/s] 21%|██        | 5676/26933 [07:25<24:37, 14.39it/s] 21%|██        | 5680/26933 [07:25<24:34, 14.41it/s] 21%|██        | 5684/26933 [07:25<24:31, 14.44it/s] 21%|██        | 5688/26933 [07:26<24:36, 14.39it/s] 21%|██        | 5692/26933 [07:26<24:36, 14.38it/s] 21%|██        | 5696/26933 [07:26<24:35, 14.39it/s] 21%|██        | 5700/26933 [07:27<24:29, 14.45it/s] 21%|██        | 5704/26933 [07:27<24:25, 14.49it/s] 21%|██        | 5708/26933 [07:27<24:23, 14.50it/s] 21%|██        | 5712/26933 [07:27<24:21, 14.52it/s] 21%|██        | 5716/26933 [07:28<24:25, 14.48it/s] 21%|██        | 5720/26933 [07:28<24:25, 14.47it/s] 21%|██▏       | 5724/26933 [07:28<24:29, 14.43it/s] 21%|██▏       | 5728/26933 [07:28<24:28, 14.44it/s] 21%|██▏       | 5732/26933 [07:29<24:35, 14.37it/s] 21%|██▏       | 5736/26933 [07:29<24:36, 14.35it/s] 21%|██▏       | 5740/26933 [07:29<24:35, 14.37it/s] 21%|██▏       | 5744/26933 [07:30<24:38, 14.34it/s] 21%|██▏       | 5748/26933 [07:30<24:48, 14.24it/s] 21%|██▏       | 5752/26933 [07:30<25:09, 14.03it/s] 21%|██▏       | 5756/26933 [07:30<25:13, 13.99it/s] 21%|██▏       | 5760/26933 [07:31<25:14, 13.98it/s] 21%|██▏       | 5764/26933 [07:31<25:20, 13.92it/s] 21%|██▏       | 5768/26933 [07:31<25:13, 13.98it/s] 21%|██▏       | 5772/26933 [07:32<24:55, 14.15it/s] 21%|██▏       | 5776/26933 [07:32<24:27, 14.42it/s] 21%|██▏       | 5780/26933 [07:32<23:55, 14.73it/s] 21%|██▏       | 5784/26933 [07:32<23:34, 14.96it/s] 21%|██▏       | 5788/26933 [07:33<23:51, 14.77it/s] 22%|██▏       | 5792/26933 [07:33<24:11, 14.57it/s] 22%|██▏       | 5796/26933 [07:33<24:17, 14.50it/s] 22%|██▏       | 5800/26933 [07:34<24:30, 14.37it/s] 22%|██▏       | 5804/26933 [07:34<24:34, 14.33it/s] 22%|██▏       | 5808/26933 [07:34<24:29, 14.38it/s] 22%|██▏       | 5812/26933 [07:34<24:25, 14.42it/s] 22%|██▏       | 5816/26933 [07:35<23:57, 14.69it/s] 22%|██▏       | 5820/26933 [07:35<23:32, 14.94it/s] 22%|██▏       | 5824/26933 [07:35<23:14, 15.14it/s] 22%|██▏       | 5828/26933 [07:35<23:00, 15.29it/s] 22%|██▏       | 5832/26933 [07:36<22:55, 15.35it/s] 22%|██▏       | 5836/26933 [07:36<22:48, 15.41it/s] 22%|██▏       | 5840/26933 [07:36<22:50, 15.39it/s] 22%|██▏       | 5844/26933 [07:36<22:47, 15.43it/s] 22%|██▏       | 5848/26933 [07:37<22:48, 15.41it/s] 22%|██▏       | 5852/26933 [07:37<22:47, 15.42it/s] 22%|██▏       | 5856/26933 [07:37<22:50, 15.38it/s] 22%|██▏       | 5860/26933 [07:37<22:50, 15.38it/s] 22%|██▏       | 5864/26933 [07:38<22:48, 15.40it/s] 22%|██▏       | 5868/26933 [07:38<22:44, 15.44it/s] 22%|██▏       | 5872/26933 [07:38<22:43, 15.44it/s] 22%|██▏       | 5876/26933 [07:38<22:41, 15.46it/s] 22%|██▏       | 5880/26933 [07:39<22:42, 15.45it/s] 22%|██▏       | 5884/26933 [07:39<22:41, 15.46it/s] 22%|██▏       | 5888/26933 [07:39<22:41, 15.46it/s] 22%|██▏       | 5892/26933 [07:40<22:36, 15.51it/s] 22%|██▏       | 5896/26933 [07:40<22:37, 15.50it/s] 22%|██▏       | 5900/26933 [07:40<22:35, 15.52it/s] 22%|██▏       | 5904/26933 [07:40<22:32, 15.54it/s] 22%|██▏       | 5908/26933 [07:41<22:30, 15.57it/s] 22%|██▏       | 5912/26933 [07:41<22:31, 15.55it/s] 22%|██▏       | 5916/26933 [07:41<22:32, 15.54it/s] 22%|██▏       | 5920/26933 [07:41<22:30, 15.56it/s] 22%|██▏       | 5924/26933 [07:42<22:31, 15.54it/s] 22%|██▏       | 5928/26933 [07:42<23:16, 15.04it/s] 22%|██▏       | 5932/26933 [07:42<23:36, 14.82it/s] 22%|██▏       | 5936/26933 [07:42<23:41, 14.77it/s] 22%|██▏       | 5940/26933 [07:43<23:47, 14.71it/s] 22%|██▏       | 5944/26933 [07:43<23:49, 14.69it/s] 22%|██▏       | 5948/26933 [07:43<23:48, 14.69it/s] 22%|██▏       | 5952/26933 [07:44<23:54, 14.62it/s] 22%|██▏       | 5956/26933 [07:44<23:58, 14.58it/s] 22%|██▏       | 5960/26933 [07:44<24:01, 14.55it/s] 22%|██▏       | 5964/26933 [07:44<24:00, 14.56it/s] 22%|██▏       | 5968/26933 [07:45<24:00, 14.55it/s] 22%|██▏       | 5972/26933 [07:45<23:58, 14.57it/s] 22%|██▏       | 5976/26933 [07:45<23:54, 14.61it/s] 22%|██▏       | 5980/26933 [07:45<23:50, 14.64it/s] 22%|██▏       | 5984/26933 [07:46<23:52, 14.62it/s] 22%|██▏       | 5988/26933 [07:46<23:53, 14.61it/s] 22%|██▏       | 5992/26933 [07:46<23:50, 14.64it/s] 22%|██▏       | 5996/26933 [07:47<23:48, 14.66it/s] 22%|██▏       | 6000/26933 [07:47<23:49, 14.64it/s] 22%|██▏       | 6004/26933 [07:47<23:52, 14.61it/s] 22%|██▏       | 6008/26933 [07:47<23:47, 14.66it/s] 22%|██▏       | 6012/26933 [07:48<23:47, 14.66it/s] 22%|██▏       | 6016/26933 [07:48<23:44, 14.68it/s] 22%|██▏       | 6020/26933 [07:48<23:43, 14.69it/s] 22%|██▏       | 6024/26933 [07:48<23:42, 14.70it/s] 22%|██▏       | 6028/26933 [07:49<23:43, 14.68it/s] 22%|██▏       | 6032/26933 [07:49<23:43, 14.68it/s] 22%|██▏       | 6036/26933 [07:49<23:44, 14.67it/s] 22%|██▏       | 6040/26933 [07:50<23:47, 14.63it/s] 22%|██▏       | 6044/26933 [07:50<23:48, 14.62it/s] 22%|██▏       | 6048/26933 [07:50<23:44, 14.66it/s] 22%|██▏       | 6052/26933 [07:50<23:44, 14.66it/s] 22%|██▏       | 6056/26933 [07:51<23:43, 14.66it/s] 23%|██▎       | 6060/26933 [07:51<23:41, 14.69it/s] 23%|██▎       | 6064/26933 [07:51<23:35, 14.74it/s] 23%|██▎       | 6068/26933 [07:51<23:33, 14.76it/s] 23%|██▎       | 6072/26933 [07:52<23:18, 14.92it/s] 23%|██▎       | 6076/26933 [07:52<23:07, 15.03it/s] 23%|██▎       | 6080/26933 [07:52<23:04, 15.06it/s] 23%|██▎       | 6084/26933 [07:52<22:53, 15.17it/s] 23%|██▎       | 6088/26933 [07:53<22:48, 15.23it/s] 23%|██▎       | 6092/26933 [07:53<22:45, 15.26it/s] 23%|██▎       | 6096/26933 [07:53<22:39, 15.32it/s] 23%|██▎       | 6100/26933 [07:54<22:36, 15.36it/s] 23%|██▎       | 6104/26933 [07:54<22:38, 15.33it/s] 23%|██▎       | 6108/26933 [07:54<22:37, 15.34it/s] 23%|██▎       | 6112/26933 [07:54<22:35, 15.36it/s] 23%|██▎       | 6116/26933 [07:55<22:27, 15.45it/s] 23%|██▎       | 6120/26933 [07:55<22:23, 15.50it/s] 23%|██▎       | 6124/26933 [07:55<22:20, 15.53it/s] 23%|██▎       | 6128/26933 [07:55<22:20, 15.52it/s] 23%|██▎       | 6132/26933 [07:56<22:20, 15.52it/s] 23%|██▎       | 6136/26933 [07:56<22:19, 15.52it/s] 23%|██▎       | 6140/26933 [07:56<22:19, 15.52it/s] 23%|██▎       | 6144/26933 [07:56<22:22, 15.49it/s] 23%|██▎       | 6148/26933 [07:57<22:23, 15.47it/s] 23%|██▎       | 6152/26933 [07:57<22:23, 15.47it/s] 23%|██▎       | 6156/26933 [07:57<22:24, 15.45it/s] 23%|██▎       | 6160/26933 [07:57<22:22, 15.48it/s] 23%|██▎       | 6164/26933 [07:58<22:25, 15.44it/s] 23%|██▎       | 6168/26933 [07:58<22:22, 15.47it/s] 23%|██▎       | 6172/26933 [07:58<22:22, 15.47it/s] 23%|██▎       | 6176/26933 [07:58<22:14, 15.56it/s] 23%|██▎       | 6180/26933 [07:59<22:39, 15.26it/s] 23%|██▎       | 6184/26933 [07:59<23:19, 14.82it/s] 23%|██▎       | 6188/26933 [07:59<23:00, 15.03it/s] 23%|██▎       | 6192/26933 [07:59<22:43, 15.21it/s] 23%|██▎       | 6196/26933 [08:00<22:35, 15.30it/s] 23%|██▎       | 6200/26933 [08:00<22:21, 15.45it/s] 23%|██▎       | 6204/26933 [08:00<22:12, 15.56it/s] 23%|██▎       | 6208/26933 [08:01<22:14, 15.53it/s] 23%|██▎       | 6212/26933 [08:01<22:12, 15.54it/s] 23%|██▎       | 6216/26933 [08:01<22:05, 15.63it/s] 23%|██▎       | 6220/26933 [08:01<22:02, 15.66it/s] 23%|██▎       | 6224/26933 [08:02<21:58, 15.71it/s] 23%|██▎       | 6228/26933 [08:02<22:07, 15.60it/s] 23%|██▎       | 6232/26933 [08:02<22:18, 15.47it/s] 23%|██▎       | 6236/26933 [08:02<22:09, 15.57it/s] 23%|██▎       | 6240/26933 [08:03<22:08, 15.58it/s] 23%|██▎       | 6244/26933 [08:03<22:05, 15.60it/s] 23%|██▎       | 6248/26933 [08:03<22:00, 15.66it/s] 23%|██▎       | 6252/26933 [08:03<22:14, 15.49it/s] 23%|██▎       | 6256/26933 [08:04<22:40, 15.19it/s] 23%|██▎       | 6260/26933 [08:04<22:51, 15.07it/s] 23%|██▎       | 6264/26933 [08:04<23:00, 14.97it/s] 23%|██▎       | 6268/26933 [08:04<23:12, 14.84it/s] 23%|██▎       | 6272/26933 [08:05<23:16, 14.80it/s] 23%|██▎       | 6276/26933 [08:05<23:14, 14.81it/s] 23%|██▎       | 6280/26933 [08:05<23:15, 14.79it/s] 23%|██▎       | 6284/26933 [08:06<23:18, 14.76it/s] 23%|██▎       | 6288/26933 [08:06<23:19, 14.75it/s] 23%|██▎       | 6292/26933 [08:06<23:17, 14.77it/s] 23%|██▎       | 6296/26933 [08:06<23:17, 14.77it/s] 23%|██▎       | 6300/26933 [08:07<23:17, 14.77it/s] 23%|██▎       | 6304/26933 [08:07<23:12, 14.81it/s] 23%|██▎       | 6308/26933 [08:07<23:13, 14.80it/s] 23%|██▎       | 6312/26933 [08:07<23:13, 14.80it/s] 23%|██▎       | 6316/26933 [08:08<23:15, 14.77it/s] 23%|██▎       | 6320/26933 [08:08<23:12, 14.81it/s] 23%|██▎       | 6324/26933 [08:08<23:09, 14.83it/s] 23%|██▎       | 6328/26933 [08:08<23:13, 14.79it/s] 24%|██▎       | 6332/26933 [08:09<23:15, 14.77it/s] 24%|██▎       | 6336/26933 [08:09<23:09, 14.82it/s] 24%|██▎       | 6340/26933 [08:09<23:08, 14.83it/s] 24%|██▎       | 6344/26933 [08:10<23:07, 14.84it/s] 24%|██▎       | 6348/26933 [08:10<23:05, 14.85it/s] 24%|██▎       | 6352/26933 [08:10<23:04, 14.86it/s] 24%|██▎       | 6356/26933 [08:10<23:05, 14.85it/s] 24%|██▎       | 6360/26933 [08:11<23:09, 14.81it/s] 24%|██▎       | 6364/26933 [08:11<23:11, 14.79it/s] 24%|██▎       | 6368/26933 [08:11<23:10, 14.79it/s] 24%|██▎       | 6372/26933 [08:11<23:08, 14.81it/s] 24%|██▎       | 6376/26933 [08:12<23:06, 14.83it/s] 24%|██▎       | 6380/26933 [08:12<23:03, 14.86it/s] 24%|██▎       | 6384/26933 [08:12<23:04, 14.84it/s] 24%|██▎       | 6388/26933 [08:13<23:00, 14.88it/s] 24%|██▎       | 6392/26933 [08:13<23:05, 14.83it/s] 24%|██▎       | 6396/26933 [08:13<23:03, 14.85it/s] 24%|██▍       | 6400/26933 [08:13<23:04, 14.83it/s] 24%|██▍       | 6404/26933 [08:14<23:05, 14.82it/s] 24%|██▍       | 6408/26933 [08:14<23:05, 14.82it/s] 24%|██▍       | 6412/26933 [08:14<23:05, 14.81it/s] 24%|██▍       | 6416/26933 [08:14<23:02, 14.84it/s] 24%|██▍       | 6420/26933 [08:15<23:01, 14.85it/s] 24%|██▍       | 6424/26933 [08:15<23:00, 14.86it/s] 24%|██▍       | 6428/26933 [08:15<22:59, 14.86it/s] 24%|██▍       | 6432/26933 [08:15<22:58, 14.88it/s] 24%|██▍       | 6436/26933 [08:16<22:58, 14.87it/s] 24%|██▍       | 6440/26933 [08:16<22:56, 14.89it/s] 24%|██▍       | 6444/26933 [08:16<22:56, 14.88it/s] 24%|██▍       | 6448/26933 [08:17<22:55, 14.89it/s] 24%|██▍       | 6452/26933 [08:17<23:01, 14.82it/s] 24%|██▍       | 6456/26933 [08:17<23:06, 14.77it/s] 24%|██▍       | 6460/26933 [08:17<23:03, 14.80it/s] 24%|██▍       | 6464/26933 [08:18<23:09, 14.73it/s] 24%|██▍       | 6468/26933 [08:18<23:10, 14.72it/s] 24%|██▍       | 6472/26933 [08:18<23:14, 14.67it/s] 24%|██▍       | 6476/26933 [08:18<23:11, 14.70it/s] 24%|██▍       | 6480/26933 [08:19<23:16, 14.65it/s] 24%|██▍       | 6484/26933 [08:19<23:13, 14.68it/s] 24%|██▍       | 6488/26933 [08:19<23:10, 14.70it/s] 24%|██▍       | 6492/26933 [08:20<23:04, 14.76it/s] 24%|██▍       | 6496/26933 [08:20<23:02, 14.78it/s] 24%|██▍       | 6500/26933 [08:20<22:44, 14.97it/s] 24%|██▍       | 6504/26933 [08:20<22:21, 15.23it/s] 24%|██▍       | 6508/26933 [08:21<22:05, 15.40it/s] 24%|██▍       | 6512/26933 [08:21<21:54, 15.54it/s] 24%|██▍       | 6516/26933 [08:21<21:47, 15.62it/s] 24%|██▍       | 6520/26933 [08:21<21:47, 15.61it/s] 24%|██▍       | 6524/26933 [08:22<22:13, 15.31it/s] 24%|██▍       | 6528/26933 [08:22<22:25, 15.17it/s] 24%|██▍       | 6532/26933 [08:22<22:10, 15.34it/s] 24%|██▍       | 6536/26933 [08:22<21:56, 15.49it/s] 24%|██▍       | 6540/26933 [08:23<21:52, 15.54it/s] 24%|██▍       | 6544/26933 [08:23<21:45, 15.62it/s] 24%|██▍       | 6548/26933 [08:23<21:45, 15.61it/s] 24%|██▍       | 6552/26933 [08:23<21:39, 15.69it/s] 24%|██▍       | 6556/26933 [08:24<21:37, 15.71it/s] 24%|██▍       | 6560/26933 [08:24<21:39, 15.67it/s] 24%|██▍       | 6564/26933 [08:24<21:42, 15.64it/s] 24%|██▍       | 6568/26933 [08:24<21:43, 15.62it/s] 24%|██▍       | 6572/26933 [08:25<21:41, 15.64it/s] 24%|██▍       | 6576/26933 [08:25<21:38, 15.68it/s] 24%|██▍       | 6580/26933 [08:25<21:32, 15.74it/s] 24%|██▍       | 6584/26933 [08:25<21:29, 15.78it/s] 24%|██▍       | 6588/26933 [08:26<21:30, 15.76it/s] 24%|██▍       | 6592/26933 [08:26<21:28, 15.79it/s] 24%|██▍       | 6596/26933 [08:26<21:23, 15.84it/s] 25%|██▍       | 6600/26933 [08:26<21:21, 15.86it/s] 25%|██▍       | 6604/26933 [08:27<21:22, 15.85it/s] 25%|██▍       | 6608/26933 [08:27<21:21, 15.86it/s] 25%|██▍       | 6612/26933 [08:27<21:25, 15.80it/s] 25%|██▍       | 6616/26933 [08:27<21:29, 15.76it/s] 25%|██▍       | 6620/26933 [08:28<21:35, 15.68it/s] 25%|██▍       | 6624/26933 [08:28<21:52, 15.47it/s] 25%|██▍       | 6628/26933 [08:28<22:09, 15.28it/s] 25%|██▍       | 6632/26933 [08:29<22:18, 15.17it/s] 25%|██▍       | 6636/26933 [08:29<22:27, 15.06it/s] 25%|██▍       | 6640/26933 [08:29<22:33, 14.99it/s] 25%|██▍       | 6644/26933 [08:29<22:36, 14.95it/s] 25%|██▍       | 6648/26933 [08:30<22:42, 14.89it/s] 25%|██▍       | 6652/26933 [08:30<22:16, 15.18it/s] 25%|██▍       | 6656/26933 [08:30<21:59, 15.37it/s] 25%|██▍       | 6660/26933 [08:30<21:50, 15.48it/s] 25%|██▍       | 6664/26933 [08:31<21:45, 15.53it/s] 25%|██▍       | 6668/26933 [08:31<21:41, 15.57it/s] 25%|██▍       | 6672/26933 [08:31<21:36, 15.62it/s] 25%|██▍       | 6676/26933 [08:31<21:35, 15.63it/s] 25%|██▍       | 6680/26933 [08:32<21:35, 15.64it/s] 25%|██▍       | 6684/26933 [08:32<21:34, 15.64it/s] 25%|██▍       | 6688/26933 [08:32<21:33, 15.65it/s] 25%|██▍       | 6692/26933 [08:32<21:34, 15.63it/s] 25%|██▍       | 6696/26933 [08:33<21:39, 15.57it/s] 25%|██▍       | 6700/26933 [08:33<21:35, 15.61it/s] 25%|██▍       | 6704/26933 [08:33<21:33, 15.64it/s] 25%|██▍       | 6708/26933 [08:33<21:51, 15.42it/s] 25%|██▍       | 6712/26933 [08:34<22:04, 15.26it/s] 25%|██▍       | 6716/26933 [08:34<22:13, 15.16it/s] 25%|██▍       | 6720/26933 [08:34<22:17, 15.11it/s] 25%|██▍       | 6724/26933 [08:35<22:20, 15.07it/s] 25%|██▍       | 6728/26933 [08:35<22:31, 14.95it/s] 25%|██▍       | 6732/26933 [08:35<22:29, 14.97it/s] 25%|██▌       | 6736/26933 [08:35<22:28, 14.98it/s] 25%|██▌       | 6740/26933 [08:36<22:40, 14.84it/s] 25%|██▌       | 6744/26933 [08:36<22:22, 15.04it/s] 25%|██▌       | 6748/26933 [08:36<22:17, 15.09it/s] 25%|██▌       | 6752/26933 [08:36<22:21, 15.05it/s] 25%|██▌       | 6756/26933 [08:37<22:22, 15.03it/s] 25%|██▌       | 6760/26933 [08:37<22:26, 14.98it/s] 25%|██▌       | 6764/26933 [08:37<22:29, 14.94it/s] 25%|██▌       | 6768/26933 [08:37<22:30, 14.93it/s] 25%|██▌       | 6772/26933 [08:38<22:31, 14.92it/s] 25%|██▌       | 6776/26933 [08:38<22:27, 14.96it/s] 25%|██▌       | 6780/26933 [08:38<22:26, 14.97it/s] 25%|██▌       | 6784/26933 [08:39<22:25, 14.97it/s] 25%|██▌       | 6788/26933 [08:39<22:25, 14.97it/s] 25%|██▌       | 6792/26933 [08:39<22:23, 14.99it/s] 25%|██▌       | 6796/26933 [08:39<22:20, 15.02it/s] 25%|██▌       | 6800/26933 [08:40<22:23, 14.98it/s] 25%|██▌       | 6804/26933 [08:40<22:23, 14.99it/s] 25%|██▌       | 6808/26933 [08:40<22:21, 15.00it/s] 25%|██▌       | 6812/26933 [08:40<22:23, 14.97it/s] 25%|██▌       | 6816/26933 [08:41<22:24, 14.96it/s] 25%|██▌       | 6820/26933 [08:41<22:24, 14.96it/s] 25%|██▌       | 6824/26933 [08:41<22:24, 14.96it/s] 25%|██▌       | 6828/26933 [08:41<22:26, 14.93it/s] 25%|██▌       | 6832/26933 [08:42<22:30, 14.88it/s] 25%|██▌       | 6836/26933 [08:42<22:26, 14.92it/s] 25%|██▌       | 6840/26933 [08:42<22:23, 14.95it/s] 25%|██▌       | 6844/26933 [08:43<22:21, 14.98it/s] 25%|██▌       | 6848/26933 [08:43<22:18, 15.00it/s] 25%|██▌       | 6852/26933 [08:43<22:20, 14.98it/s] 25%|██▌       | 6856/26933 [08:43<22:16, 15.02it/s] 25%|██▌       | 6860/26933 [08:44<22:19, 14.98it/s] 25%|██▌       | 6864/26933 [08:44<22:27, 14.89it/s] 26%|██▌       | 6868/26933 [08:44<22:29, 14.86it/s] 26%|██▌       | 6872/26933 [08:44<22:22, 14.94it/s] 26%|██▌       | 6876/26933 [08:45<22:22, 14.94it/s] 26%|██▌       | 6880/26933 [08:45<22:18, 14.98it/s] 26%|██▌       | 6884/26933 [08:45<22:15, 15.01it/s] 26%|██▌       | 6888/26933 [08:45<22:14, 15.02it/s] 26%|██▌       | 6892/26933 [08:46<22:25, 14.89it/s] 26%|██▌       | 6896/26933 [08:46<22:22, 14.93it/s] 26%|██▌       | 6900/26933 [08:46<22:25, 14.89it/s] 26%|██▌       | 6904/26933 [08:47<22:21, 14.93it/s] 26%|██▌       | 6908/26933 [08:47<22:15, 15.00it/s] 26%|██▌       | 6912/26933 [08:47<22:12, 15.02it/s] 26%|██▌       | 6916/26933 [08:47<22:13, 15.01it/s] 26%|██▌       | 6920/26933 [08:48<22:21, 14.92it/s] 26%|██▌       | 6924/26933 [08:48<22:23, 14.90it/s] 26%|██▌       | 6928/26933 [08:48<22:18, 14.95it/s] 26%|██▌       | 6932/26933 [08:48<22:14, 14.99it/s] 26%|██▌       | 6936/26933 [08:49<22:17, 14.95it/s] 26%|██▌       | 6940/26933 [08:49<22:17, 14.95it/s] 26%|██▌       | 6944/26933 [08:49<22:35, 14.75it/s] 26%|██▌       | 6948/26933 [08:50<22:28, 14.82it/s] 26%|██▌       | 6952/26933 [08:50<22:38, 14.71it/s] 26%|██▌       | 6956/26933 [08:50<22:45, 14.63it/s] 26%|██▌       | 6960/26933 [08:50<22:47, 14.61it/s] 26%|██▌       | 6964/26933 [08:51<22:45, 14.62it/s] 26%|██▌       | 6968/26933 [08:51<22:31, 14.78it/s] 26%|██▌       | 6972/26933 [08:51<22:23, 14.86it/s] 26%|██▌       | 6976/26933 [08:51<22:15, 14.94it/s] 26%|██▌       | 6980/26933 [08:52<22:15, 14.94it/s] 26%|██▌       | 6984/26933 [08:52<22:12, 14.97it/s] 26%|██▌       | 6988/26933 [08:52<22:09, 15.00it/s] 26%|██▌       | 6992/26933 [08:52<21:51, 15.20it/s] 26%|██▌       | 6996/26933 [08:53<21:37, 15.37it/s] 26%|██▌       | 7000/26933 [08:53<21:22, 15.54it/s] 26%|██▌       | 7004/26933 [08:53<21:10, 15.68it/s] 26%|██▌       | 7008/26933 [08:53<21:03, 15.77it/s] 26%|██▌       | 7012/26933 [08:54<21:08, 15.71it/s] 26%|██▌       | 7016/26933 [08:54<21:00, 15.79it/s] 26%|██▌       | 7020/26933 [08:54<20:59, 15.81it/s] 26%|██▌       | 7024/26933 [08:54<20:55, 15.85it/s] 26%|██▌       | 7028/26933 [08:55<21:00, 15.79it/s] 26%|██▌       | 7032/26933 [08:55<21:00, 15.78it/s] 26%|██▌       | 7036/26933 [08:55<20:56, 15.84it/s] 26%|██▌       | 7040/26933 [08:55<20:52, 15.89it/s] 26%|██▌       | 7044/26933 [08:56<20:52, 15.88it/s] 26%|██▌       | 7048/26933 [08:56<20:53, 15.87it/s] 26%|██▌       | 7052/26933 [08:56<20:49, 15.91it/s] 26%|██▌       | 7056/26933 [08:56<20:48, 15.93it/s] 26%|██▌       | 7060/26933 [08:57<20:50, 15.90it/s] 26%|██▌       | 7064/26933 [08:57<20:45, 15.95it/s] 26%|██▌       | 7068/26933 [08:57<20:43, 15.97it/s] 26%|██▋       | 7072/26933 [08:57<20:43, 15.97it/s] 26%|██▋       | 7076/26933 [08:58<20:44, 15.96it/s] 26%|██▋       | 7080/26933 [08:58<20:41, 15.99it/s] 26%|██▋       | 7084/26933 [08:58<20:40, 16.01it/s] 26%|██▋       | 7088/26933 [08:58<20:37, 16.04it/s] 26%|██▋       | 7092/26933 [08:59<20:40, 15.99it/s] 26%|██▋       | 7096/26933 [08:59<20:39, 16.01it/s] 26%|██▋       | 7100/26933 [08:59<20:37, 16.02it/s] 26%|██▋       | 7104/26933 [08:59<20:37, 16.02it/s] 26%|██▋       | 7108/26933 [09:00<20:39, 16.00it/s] 26%|██▋       | 7112/26933 [09:00<20:37, 16.01it/s] 26%|██▋       | 7116/26933 [09:00<20:35, 16.03it/s] 26%|██▋       | 7120/26933 [09:00<20:39, 15.99it/s] 26%|██▋       | 7124/26933 [09:01<20:42, 15.94it/s] 26%|██▋       | 7128/26933 [09:01<20:46, 15.88it/s] 26%|██▋       | 7132/26933 [09:01<20:42, 15.93it/s] 26%|██▋       | 7136/26933 [09:02<20:42, 15.93it/s] 27%|██▋       | 7140/26933 [09:02<20:46, 15.88it/s] 27%|██▋       | 7144/26933 [09:02<20:46, 15.87it/s] 27%|██▋       | 7148/26933 [09:02<20:43, 15.91it/s] 27%|██▋       | 7152/26933 [09:03<20:45, 15.89it/s] 27%|██▋       | 7156/26933 [09:03<20:54, 15.76it/s] 27%|██▋       | 7160/26933 [09:03<20:53, 15.77it/s] 27%|██▋       | 7164/26933 [09:03<20:49, 15.83it/s] 27%|██▋       | 7168/26933 [09:04<20:48, 15.83it/s] 27%|██▋       | 7172/26933 [09:04<20:46, 15.85it/s] 27%|██▋       | 7176/26933 [09:04<20:39, 15.94it/s] 27%|██▋       | 7180/26933 [09:04<20:35, 15.99it/s] 27%|██▋       | 7184/26933 [09:05<20:31, 16.03it/s] 27%|██▋       | 7188/26933 [09:05<20:36, 15.97it/s] 27%|██▋       | 7192/26933 [09:05<20:32, 16.02it/s] 27%|██▋       | 7196/26933 [09:05<20:32, 16.02it/s] 27%|██▋       | 7200/26933 [09:06<20:32, 16.01it/s] 27%|██▋       | 7204/26933 [09:06<20:35, 15.97it/s] 27%|██▋       | 7208/26933 [09:06<20:32, 16.01it/s] 27%|██▋       | 7212/26933 [09:06<20:29, 16.03it/s] 27%|██▋       | 7216/26933 [09:07<20:28, 16.06it/s] 27%|██▋       | 7220/26933 [09:07<21:02, 15.62it/s] 27%|██▋       | 7224/26933 [09:07<21:28, 15.29it/s] 27%|██▋       | 7228/26933 [09:07<21:29, 15.28it/s] 27%|██▋       | 7232/26933 [09:08<21:32, 15.24it/s] 27%|██▋       | 7236/26933 [09:08<21:30, 15.27it/s] 27%|██▋       | 7240/26933 [09:08<21:28, 15.29it/s] 27%|██▋       | 7244/26933 [09:08<21:28, 15.28it/s] 27%|██▋       | 7248/26933 [09:09<21:32, 15.22it/s] 27%|██▋       | 7252/26933 [09:09<21:32, 15.23it/s] 27%|██▋       | 7256/26933 [09:09<21:31, 15.24it/s] 27%|██▋       | 7260/26933 [09:09<21:28, 15.27it/s] 27%|██▋       | 7264/26933 [09:10<21:31, 15.22it/s] 27%|██▋       | 7268/26933 [09:10<21:29, 15.25it/s] 27%|██▋       | 7272/26933 [09:10<21:04, 15.55it/s] 27%|██▋       | 7276/26933 [09:10<20:46, 15.77it/s] 27%|██▋       | 7280/26933 [09:11<20:40, 15.84it/s] 27%|██▋       | 7284/26933 [09:11<20:32, 15.95it/s] 27%|██▋       | 7288/26933 [09:11<20:26, 16.01it/s] 27%|██▋       | 7292/26933 [09:11<20:18, 16.11it/s] 27%|██▋       | 7296/26933 [09:12<20:20, 16.08it/s] 27%|██▋       | 7300/26933 [09:12<20:17, 16.12it/s] 27%|██▋       | 7304/26933 [09:12<20:14, 16.16it/s] 27%|██▋       | 7308/26933 [09:12<20:13, 16.18it/s] 27%|██▋       | 7312/26933 [09:13<20:11, 16.20it/s] 27%|██▋       | 7316/26933 [09:13<20:10, 16.21it/s] 27%|██▋       | 7320/26933 [09:13<20:08, 16.23it/s] 27%|██▋       | 7324/26933 [09:13<20:07, 16.24it/s] 27%|██▋       | 7328/26933 [09:14<20:09, 16.22it/s] 27%|██▋       | 7332/26933 [09:14<20:06, 16.25it/s] 27%|██▋       | 7336/26933 [09:14<20:11, 16.17it/s] 27%|██▋       | 7340/26933 [09:14<20:09, 16.20it/s] 27%|██▋       | 7344/26933 [09:15<20:10, 16.18it/s] 27%|██▋       | 7348/26933 [09:15<20:09, 16.19it/s] 27%|██▋       | 7352/26933 [09:15<20:07, 16.22it/s] 27%|██▋       | 7356/26933 [09:15<20:05, 16.24it/s] 27%|██▋       | 7360/26933 [09:16<20:05, 16.24it/s] 27%|██▋       | 7364/26933 [09:16<20:04, 16.25it/s] 27%|██▋       | 7368/26933 [09:16<20:04, 16.25it/s] 27%|██▋       | 7372/26933 [09:16<20:04, 16.24it/s] 27%|██▋       | 7376/26933 [09:17<20:56, 15.56it/s] 27%|██▋       | 7380/26933 [09:17<21:55, 14.87it/s] 27%|██▋       | 7384/26933 [09:17<21:46, 14.97it/s] 27%|██▋       | 7388/26933 [09:17<21:38, 15.05it/s] 27%|██▋       | 7392/26933 [09:18<21:28, 15.16it/s] 27%|██▋       | 7396/26933 [09:18<21:16, 15.31it/s] 27%|██▋       | 7400/26933 [09:18<21:07, 15.42it/s] 27%|██▋       | 7404/26933 [09:18<21:00, 15.49it/s] 28%|██▊       | 7408/26933 [09:19<20:55, 15.55it/s] 28%|██▊       | 7412/26933 [09:19<20:49, 15.63it/s] 28%|██▊       | 7416/26933 [09:19<20:46, 15.66it/s] 28%|██▊       | 7420/26933 [09:20<20:34, 15.81it/s] 28%|██▊       | 7424/26933 [09:20<20:29, 15.87it/s] 28%|██▊       | 7428/26933 [09:20<20:22, 15.95it/s] 28%|██▊       | 7432/26933 [09:20<20:25, 15.92it/s] 28%|██▊       | 7436/26933 [09:21<20:25, 15.92it/s] 28%|██▊       | 7440/26933 [09:21<20:21, 15.96it/s] 28%|██▊       | 7444/26933 [09:21<20:17, 16.01it/s] 28%|██▊       | 7448/26933 [09:21<20:16, 16.01it/s] 28%|██▊       | 7452/26933 [09:22<20:18, 15.98it/s] 28%|██▊       | 7456/26933 [09:22<20:24, 15.90it/s] 28%|██▊       | 7460/26933 [09:22<20:25, 15.89it/s] 28%|██▊       | 7464/26933 [09:22<20:27, 15.86it/s] 28%|██▊       | 7468/26933 [09:23<20:49, 15.58it/s] 28%|██▊       | 7472/26933 [09:23<21:03, 15.40it/s] 28%|██▊       | 7476/26933 [09:23<21:06, 15.36it/s] 28%|██▊       | 7480/26933 [09:23<21:06, 15.36it/s] 28%|██▊       | 7484/26933 [09:24<21:13, 15.27it/s] 28%|██▊       | 7488/26933 [09:24<21:08, 15.33it/s] 28%|██▊       | 7492/26933 [09:24<21:07, 15.34it/s] 28%|██▊       | 7496/26933 [09:24<20:45, 15.60it/s] 28%|██▊       | 7500/26933 [09:25<20:32, 15.77it/s] 28%|██▊       | 7504/26933 [09:25<20:18, 15.94it/s] 28%|██▊       | 7508/26933 [09:25<20:09, 16.06it/s] 28%|██▊       | 7512/26933 [09:25<20:00, 16.18it/s] 28%|██▊       | 7516/26933 [09:26<19:57, 16.21it/s] 28%|██▊       | 7520/26933 [09:26<19:54, 16.25it/s] 28%|██▊       | 7524/26933 [09:26<19:50, 16.31it/s] 28%|██▊       | 7528/26933 [09:26<19:48, 16.33it/s] 28%|██▊       | 7532/26933 [09:27<19:58, 16.19it/s] 28%|██▊       | 7536/26933 [09:27<19:54, 16.24it/s] 28%|██▊       | 7540/26933 [09:27<19:51, 16.28it/s] 28%|██▊       | 7544/26933 [09:27<19:48, 16.31it/s] 28%|██▊       | 7548/26933 [09:28<19:45, 16.35it/s] 28%|██▊       | 7552/26933 [09:28<19:46, 16.34it/s] 28%|██▊       | 7556/26933 [09:28<19:44, 16.36it/s] 28%|██▊       | 7560/26933 [09:28<19:44, 16.36it/s] 28%|██▊       | 7564/26933 [09:29<19:40, 16.40it/s] 28%|██▊       | 7568/26933 [09:29<20:15, 15.93it/s] 28%|██▊       | 7572/26933 [09:29<20:33, 15.70it/s] 28%|██▊       | 7576/26933 [09:29<20:44, 15.56it/s] 28%|██▊       | 7580/26933 [09:30<20:59, 15.36it/s] 28%|██▊       | 7584/26933 [09:30<21:12, 15.20it/s] 28%|██▊       | 7588/26933 [09:30<21:21, 15.10it/s] 28%|██▊       | 7592/26933 [09:30<21:23, 15.07it/s] 28%|██▊       | 7596/26933 [09:31<21:31, 14.97it/s] 28%|██▊       | 7600/26933 [09:31<21:31, 14.97it/s] 28%|██▊       | 7604/26933 [09:31<21:26, 15.02it/s] 28%|██▊       | 7608/26933 [09:31<21:18, 15.11it/s] 28%|██▊       | 7612/26933 [09:32<21:20, 15.09it/s] 28%|██▊       | 7616/26933 [09:32<21:12, 15.18it/s] 28%|██▊       | 7620/26933 [09:32<21:08, 15.23it/s] 28%|██▊       | 7624/26933 [09:32<21:14, 15.16it/s] 28%|██▊       | 7628/26933 [09:33<21:34, 14.91it/s] 28%|██▊       | 7632/26933 [09:33<21:29, 14.97it/s] 28%|██▊       | 7636/26933 [09:33<21:20, 15.07it/s] 28%|██▊       | 7640/26933 [09:34<21:14, 15.14it/s] 28%|██▊       | 7644/26933 [09:34<21:09, 15.19it/s] 28%|██▊       | 7648/26933 [09:34<21:09, 15.19it/s] 28%|██▊       | 7652/26933 [09:34<21:09, 15.18it/s] 28%|██▊       | 7656/26933 [09:35<21:18, 15.07it/s] 28%|██▊       | 7660/26933 [09:35<21:15, 15.11it/s] 28%|██▊       | 7664/26933 [09:35<21:13, 15.13it/s] 28%|██▊       | 7668/26933 [09:35<21:10, 15.17it/s] 28%|██▊       | 7672/26933 [09:36<21:15, 15.10it/s] 29%|██▊       | 7676/26933 [09:36<21:10, 15.15it/s] 29%|██▊       | 7680/26933 [09:36<21:09, 15.17it/s] 29%|██▊       | 7684/26933 [09:36<21:07, 15.19it/s] 29%|██▊       | 7688/26933 [09:37<21:13, 15.11it/s] 29%|██▊       | 7692/26933 [09:37<21:08, 15.17it/s] 29%|██▊       | 7696/26933 [09:37<21:06, 15.19it/s] 29%|██▊       | 7700/26933 [09:38<21:05, 15.20it/s] 29%|██▊       | 7704/26933 [09:38<21:11, 15.12it/s] 29%|██▊       | 7708/26933 [09:38<21:03, 15.22it/s] 29%|██▊       | 7712/26933 [09:38<20:58, 15.27it/s] 29%|██▊       | 7716/26933 [09:39<21:01, 15.23it/s] 29%|██▊       | 7720/26933 [09:39<20:59, 15.26it/s] 29%|██▊       | 7724/26933 [09:39<21:01, 15.23it/s] 29%|██▊       | 7728/26933 [09:39<21:07, 15.15it/s] 29%|██▊       | 7732/26933 [09:40<21:10, 15.12it/s] 29%|██▊       | 7736/26933 [09:40<21:06, 15.16it/s] 29%|██▊       | 7740/26933 [09:40<21:00, 15.23it/s] 29%|██▉       | 7744/26933 [09:40<20:59, 15.23it/s] 29%|██▉       | 7748/26933 [09:41<21:07, 15.14it/s] 29%|██▉       | 7752/26933 [09:41<21:04, 15.17it/s] 29%|██▉       | 7756/26933 [09:41<21:01, 15.20it/s] 29%|██▉       | 7760/26933 [09:41<21:01, 15.20it/s] 29%|██▉       | 7764/26933 [09:42<21:07, 15.12it/s] 29%|██▉       | 7768/26933 [09:42<21:04, 15.15it/s] 29%|██▉       | 7772/26933 [09:42<20:58, 15.23it/s] 29%|██▉       | 7776/26933 [09:43<20:54, 15.27it/s] 29%|██▉       | 7780/26933 [09:43<20:57, 15.23it/s] 29%|██▉       | 7784/26933 [09:43<20:51, 15.30it/s] 29%|██▉       | 7788/26933 [09:43<20:48, 15.34it/s] 29%|██▉       | 7792/26933 [09:44<20:45, 15.36it/s] 29%|██▉       | 7796/26933 [09:44<20:49, 15.32it/s] 29%|██▉       | 7800/26933 [09:44<20:48, 15.32it/s] 29%|██▉       | 7804/26933 [09:44<20:49, 15.31it/s] 29%|██▉       | 7808/26933 [09:45<20:54, 15.24it/s] 29%|██▉       | 7812/26933 [09:45<20:52, 15.27it/s] 29%|██▉       | 7816/26933 [09:45<20:56, 15.22it/s] 29%|██▉       | 7820/26933 [09:45<20:52, 15.26it/s] 29%|██▉       | 7824/26933 [09:46<20:57, 15.19it/s] 29%|██▉       | 7828/26933 [09:46<20:59, 15.17it/s] 29%|██▉       | 7832/26933 [09:46<20:57, 15.19it/s] 29%|██▉       | 7836/26933 [09:46<21:00, 15.15it/s] 29%|██▉       | 7840/26933 [09:47<21:02, 15.12it/s] 29%|██▉       | 7844/26933 [09:47<20:55, 15.21it/s] 29%|██▉       | 7848/26933 [09:47<21:04, 15.09it/s] 29%|██▉       | 7852/26933 [09:48<20:57, 15.18it/s] 29%|██▉       | 7856/26933 [09:48<20:58, 15.16it/s] 29%|██▉       | 7860/26933 [09:48<20:52, 15.22it/s] 29%|██▉       | 7864/26933 [09:48<20:49, 15.26it/s] 29%|██▉       | 7868/26933 [09:49<20:47, 15.28it/s] 29%|██▉       | 7872/26933 [09:49<20:42, 15.34it/s] 29%|██▉       | 7876/26933 [09:49<20:39, 15.38it/s] 29%|██▉       | 7880/26933 [09:49<20:34, 15.43it/s] 29%|██▉       | 7884/26933 [09:50<20:37, 15.39it/s] 29%|██▉       | 7888/26933 [09:50<20:35, 15.42it/s] 29%|██▉       | 7892/26933 [09:50<20:32, 15.45it/s] 29%|██▉       | 7896/26933 [09:50<20:30, 15.47it/s] 29%|██▉       | 7900/26933 [09:51<20:35, 15.40it/s] 29%|██▉       | 7904/26933 [09:51<20:35, 15.40it/s] 29%|██▉       | 7908/26933 [09:51<20:32, 15.44it/s] 29%|██▉       | 7912/26933 [09:51<20:32, 15.43it/s] 29%|██▉       | 7916/26933 [09:52<20:36, 15.38it/s] 29%|██▉       | 7920/26933 [09:52<20:36, 15.38it/s] 29%|██▉       | 7924/26933 [09:52<20:34, 15.40it/s] 29%|██▉       | 7928/26933 [09:52<20:31, 15.43it/s] 29%|██▉       | 7932/26933 [09:53<20:41, 15.30it/s] 29%|██▉       | 7936/26933 [09:53<20:37, 15.35it/s] 29%|██▉       | 7940/26933 [09:53<20:34, 15.39it/s] 29%|██▉       | 7944/26933 [09:53<20:33, 15.40it/s] 30%|██▉       | 7948/26933 [09:54<20:39, 15.32it/s] 30%|██▉       | 7952/26933 [09:54<20:38, 15.33it/s] 30%|██▉       | 7956/26933 [09:54<20:35, 15.36it/s] 30%|██▉       | 7960/26933 [09:55<20:36, 15.35it/s] 30%|██▉       | 7964/26933 [09:55<20:39, 15.30it/s] 30%|██▉       | 7968/26933 [09:55<20:40, 15.29it/s] 30%|██▉       | 7972/26933 [09:55<20:41, 15.27it/s] 30%|██▉       | 7976/26933 [09:56<20:44, 15.24it/s] 30%|██▉       | 7980/26933 [09:56<20:45, 15.21it/s] 30%|██▉       | 7984/26933 [09:56<20:43, 15.24it/s] 30%|██▉       | 7988/26933 [09:56<20:44, 15.22it/s] 30%|██▉       | 7992/26933 [09:57<20:48, 15.17it/s] 30%|██▉       | 7996/26933 [09:57<20:44, 15.22it/s] 30%|██▉       | 8000/26933 [09:57<20:41, 15.25it/s] 30%|██▉       | 8004/26933 [09:57<20:38, 15.29it/s] 30%|██▉       | 8008/26933 [09:58<20:38, 15.28it/s] 30%|██▉       | 8012/26933 [09:58<20:32, 15.36it/s] 30%|██▉       | 8016/26933 [09:58<20:28, 15.40it/s] 30%|██▉       | 8020/26933 [09:58<20:27, 15.41it/s] 30%|██▉       | 8024/26933 [09:59<20:30, 15.37it/s] 30%|██▉       | 8028/26933 [09:59<20:13, 15.58it/s] 30%|██▉       | 8032/26933 [09:59<20:13, 15.58it/s] 30%|██▉       | 8036/26933 [09:59<20:12, 15.59it/s] 30%|██▉       | 8040/26933 [10:00<19:46, 15.93it/s] 30%|██▉       | 8044/26933 [10:00<19:51, 15.85it/s] 30%|██▉       | 8048/26933 [10:00<19:58, 15.76it/s] 30%|██▉       | 8052/26933 [10:00<19:30, 16.13it/s] 30%|██▉       | 8056/26933 [10:01<19:47, 15.89it/s] 30%|██▉       | 8060/26933 [10:01<19:56, 15.77it/s] 30%|██▉       | 8064/26933 [10:01<19:30, 16.12it/s] 30%|██▉       | 8068/26933 [10:01<19:40, 15.99it/s] 30%|██▉       | 8072/26933 [10:02<19:53, 15.81it/s] 30%|██▉       | 8076/26933 [10:02<19:26, 16.17it/s] 30%|███       | 8080/26933 [10:02<19:37, 16.01it/s] 30%|███       | 8084/26933 [10:02<19:43, 15.93it/s] 30%|███       | 8088/26933 [10:03<19:24, 16.18it/s] 30%|███       | 8092/26933 [10:03<19:36, 16.02it/s] 30%|███       | 8096/26933 [10:03<19:43, 15.91it/s] 30%|███       | 8100/26933 [10:03<19:19, 16.24it/s] 30%|███       | 8104/26933 [10:04<19:53, 15.78it/s] 30%|███       | 8108/26933 [10:04<19:22, 16.19it/s] 30%|███       | 8112/26933 [10:04<18:59, 16.52it/s] 30%|███       | 8116/26933 [10:04<19:17, 16.25it/s] 30%|███       | 8120/26933 [10:05<19:01, 16.48it/s] 30%|███       | 8124/26933 [10:05<18:43, 16.74it/s] 30%|███       | 8128/26933 [10:05<18:32, 16.90it/s] 30%|███       | 8132/26933 [10:05<18:24, 17.02it/s] 30%|███       | 8136/26933 [10:06<18:23, 17.04it/s] 30%|███       | 8140/26933 [10:06<18:17, 17.12it/s] 30%|███       | 8144/26933 [10:06<18:15, 17.16it/s] 30%|███       | 8148/26933 [10:06<18:12, 17.19it/s] 30%|███       | 8152/26933 [10:07<18:16, 17.13it/s] 30%|███       | 8156/26933 [10:07<18:13, 17.17it/s] 30%|███       | 8160/26933 [10:07<18:11, 17.20it/s] 30%|███       | 8164/26933 [10:07<18:08, 17.24it/s] 30%|███       | 8168/26933 [10:07<18:06, 17.27it/s] 30%|███       | 8172/26933 [10:08<18:10, 17.20it/s] 30%|███       | 8176/26933 [10:08<18:08, 17.24it/s] 30%|███       | 8180/26933 [10:08<18:07, 17.24it/s] 30%|███       | 8184/26933 [10:08<18:04, 17.28it/s] 30%|███       | 8188/26933 [10:09<18:11, 17.17it/s] 30%|███       | 8192/26933 [10:09<18:09, 17.21it/s] 30%|███       | 8196/26933 [10:09<18:07, 17.23it/s] 30%|███       | 8200/26933 [10:09<18:06, 17.24it/s] 30%|███       | 8204/26933 [10:10<18:16, 17.08it/s] 30%|███       | 8208/26933 [10:10<18:11, 17.16it/s] 30%|███       | 8212/26933 [10:10<18:08, 17.20it/s] 31%|███       | 8216/26933 [10:10<18:07, 17.22it/s] 31%|███       | 8220/26933 [10:11<18:46, 16.61it/s] 31%|███       | 8224/26933 [10:11<18:36, 16.76it/s] 31%|███       | 8228/26933 [10:11<18:29, 16.85it/s] 31%|███       | 8232/26933 [10:11<18:35, 16.76it/s] 31%|███       | 8236/26933 [10:11<18:34, 16.78it/s] 31%|███       | 8240/26933 [10:12<18:39, 16.70it/s] 31%|███       | 8244/26933 [10:12<18:33, 16.79it/s] 31%|███       | 8248/26933 [10:12<18:29, 16.84it/s] 31%|███       | 8252/26933 [10:12<18:24, 16.91it/s] 31%|███       | 8256/26933 [10:13<18:21, 16.96it/s] 31%|███       | 8260/26933 [10:13<18:14, 17.06it/s] 31%|███       | 8264/26933 [10:13<18:11, 17.11it/s] 31%|███       | 8268/26933 [10:13<18:06, 17.18it/s] 31%|███       | 8272/26933 [10:14<18:11, 17.09it/s] 31%|███       | 8276/26933 [10:14<18:15, 17.02it/s] 31%|███       | 8280/26933 [10:14<18:18, 16.97it/s] 31%|███       | 8284/26933 [10:14<18:21, 16.92it/s] 31%|███       | 8288/26933 [10:15<18:19, 16.96it/s] 31%|███       | 8292/26933 [10:15<18:16, 17.00it/s] 31%|███       | 8296/26933 [10:15<18:08, 17.12it/s] 31%|███       | 8300/26933 [10:15<18:05, 17.16it/s] 31%|███       | 8304/26933 [10:15<18:01, 17.23it/s] 31%|███       | 8308/26933 [10:16<18:01, 17.22it/s] 31%|███       | 8312/26933 [10:16<18:00, 17.23it/s] 31%|███       | 8316/26933 [10:16<18:00, 17.23it/s] 31%|███       | 8320/26933 [10:16<17:58, 17.27it/s] 31%|███       | 8324/26933 [10:17<18:00, 17.23it/s] 31%|███       | 8328/26933 [10:17<17:58, 17.24it/s] 31%|███       | 8332/26933 [10:17<17:57, 17.27it/s] 31%|███       | 8336/26933 [10:17<17:52, 17.33it/s] 31%|███       | 8340/26933 [10:18<18:08, 17.09it/s] 31%|███       | 8344/26933 [10:18<18:11, 17.03it/s] 31%|███       | 8348/26933 [10:18<18:39, 16.59it/s] 31%|███       | 8352/26933 [10:18<19:01, 16.27it/s] 31%|███       | 8356/26933 [10:19<18:39, 16.59it/s] 31%|███       | 8360/26933 [10:19<18:30, 16.73it/s] 31%|███       | 8364/26933 [10:19<18:18, 16.90it/s] 31%|███       | 8368/26933 [10:19<18:12, 17.00it/s] 31%|███       | 8372/26933 [10:19<18:06, 17.09it/s] 31%|███       | 8376/26933 [10:20<18:05, 17.10it/s] 31%|███       | 8380/26933 [10:20<18:02, 17.13it/s] 31%|███       | 8384/26933 [10:20<17:59, 17.18it/s] 31%|███       | 8388/26933 [10:20<17:53, 17.28it/s] 31%|███       | 8392/26933 [10:21<17:47, 17.36it/s] 31%|███       | 8396/26933 [10:21<17:39, 17.49it/s] 31%|███       | 8400/26933 [10:21<17:34, 17.58it/s] 31%|███       | 8404/26933 [10:21<17:43, 17.43it/s] 31%|███       | 8408/26933 [10:22<17:49, 17.33it/s] 31%|███       | 8412/26933 [10:22<17:46, 17.37it/s] 31%|███       | 8416/26933 [10:22<17:40, 17.46it/s] 31%|███▏      | 8420/26933 [10:22<17:35, 17.53it/s] 31%|███▏      | 8424/26933 [10:22<17:31, 17.61it/s] 31%|███▏      | 8428/26933 [10:23<17:33, 17.56it/s] 31%|███▏      | 8432/26933 [10:23<17:30, 17.61it/s] 31%|███▏      | 8436/26933 [10:23<17:25, 17.70it/s] 31%|███▏      | 8440/26933 [10:23<17:22, 17.74it/s] 31%|███▏      | 8444/26933 [10:24<17:25, 17.68it/s] 31%|███▏      | 8448/26933 [10:24<17:26, 17.66it/s] 31%|███▏      | 8452/26933 [10:24<17:24, 17.70it/s] 31%|███▏      | 8456/26933 [10:24<17:24, 17.69it/s] 31%|███▏      | 8460/26933 [10:24<17:22, 17.73it/s] 31%|███▏      | 8464/26933 [10:25<17:33, 17.53it/s] 31%|███▏      | 8468/26933 [10:25<17:41, 17.40it/s] 31%|███▏      | 8472/26933 [10:25<17:37, 17.46it/s] 31%|███▏      | 8476/26933 [10:25<17:43, 17.35it/s] 31%|███▏      | 8480/26933 [10:26<17:43, 17.35it/s] 32%|███▏      | 8484/26933 [10:26<17:40, 17.39it/s] 32%|███▏      | 8488/26933 [10:26<17:37, 17.44it/s] 32%|███▏      | 8492/26933 [10:26<17:36, 17.45it/s] 32%|███▏      | 8496/26933 [10:27<17:40, 17.39it/s] 32%|███▏      | 8500/26933 [10:27<17:37, 17.43it/s] 32%|███▏      | 8504/26933 [10:27<17:33, 17.49it/s] 32%|███▏      | 8508/26933 [10:27<17:31, 17.52it/s] 32%|███▏      | 8512/26933 [10:27<17:28, 17.57it/s] 32%|███▏      | 8516/26933 [10:28<17:32, 17.49it/s] 32%|███▏      | 8520/26933 [10:28<17:31, 17.51it/s] 32%|███▏      | 8524/26933 [10:28<17:31, 17.51it/s] 32%|███▏      | 8528/26933 [10:28<17:31, 17.51it/s] 32%|███▏      | 8532/26933 [10:29<17:32, 17.48it/s] 32%|███▏      | 8536/26933 [10:29<17:31, 17.49it/s] 32%|███▏      | 8540/26933 [10:29<17:29, 17.52it/s] 32%|███▏      | 8544/26933 [10:29<17:27, 17.55it/s] 32%|███▏      | 8548/26933 [10:30<17:27, 17.55it/s] 32%|███▏      | 8552/26933 [10:30<17:33, 17.45it/s] 32%|███▏      | 8556/26933 [10:30<17:34, 17.43it/s] 32%|███▏      | 8560/26933 [10:30<17:35, 17.41it/s] 32%|███▏      | 8564/26933 [10:30<17:35, 17.41it/s] 32%|███▏      | 8568/26933 [10:31<17:36, 17.38it/s] 32%|███▏      | 8572/26933 [10:31<17:56, 17.06it/s] 32%|███▏      | 8576/26933 [10:31<18:07, 16.88it/s] 32%|███▏      | 8580/26933 [10:31<17:50, 17.15it/s] 32%|███▏      | 8584/26933 [10:32<17:39, 17.32it/s] 32%|███▏      | 8588/26933 [10:32<17:28, 17.50it/s] 32%|███▏      | 8592/26933 [10:32<17:20, 17.63it/s] 32%|███▏      | 8596/26933 [10:32<17:15, 17.71it/s] 32%|███▏      | 8600/26933 [10:32<17:10, 17.79it/s] 32%|███▏      | 8604/26933 [10:33<17:09, 17.81it/s] 32%|███▏      | 8608/26933 [10:33<17:05, 17.86it/s] 32%|███▏      | 8612/26933 [10:33<17:04, 17.88it/s] 32%|███▏      | 8616/26933 [10:33<17:02, 17.91it/s] 32%|███▏      | 8620/26933 [10:34<17:03, 17.89it/s] 32%|███▏      | 8624/26933 [10:34<17:02, 17.90it/s] 32%|███▏      | 8628/26933 [10:34<17:02, 17.90it/s] 32%|███▏      | 8632/26933 [10:34<17:03, 17.89it/s] 32%|███▏      | 8636/26933 [10:35<17:02, 17.90it/s] 32%|███▏      | 8640/26933 [10:35<17:05, 17.83it/s] 32%|███▏      | 8644/26933 [10:35<17:04, 17.86it/s] 32%|███▏      | 8648/26933 [10:35<17:03, 17.86it/s] 32%|███▏      | 8652/26933 [10:35<17:02, 17.88it/s] 32%|███▏      | 8656/26933 [10:36<17:07, 17.78it/s] 32%|███▏      | 8660/26933 [10:36<17:08, 17.77it/s] 32%|███▏      | 8664/26933 [10:36<17:06, 17.79it/s] 32%|███▏      | 8668/26933 [10:36<17:05, 17.81it/s] 32%|███▏      | 8672/26933 [10:37<17:05, 17.81it/s] 32%|███▏      | 8676/26933 [10:37<17:06, 17.78it/s] 32%|███▏      | 8680/26933 [10:37<17:05, 17.80it/s] 32%|███▏      | 8684/26933 [10:37<17:03, 17.82it/s] 32%|███▏      | 8688/26933 [10:37<17:01, 17.85it/s] 32%|███▏      | 8692/26933 [10:38<17:03, 17.82it/s] 32%|███▏      | 8696/26933 [10:38<17:02, 17.84it/s] 32%|███▏      | 8700/26933 [10:38<17:05, 17.77it/s] 32%|███▏      | 8704/26933 [10:38<17:03, 17.81it/s] 32%|███▏      | 8708/26933 [10:39<17:09, 17.71it/s] 32%|███▏      | 8712/26933 [10:39<17:10, 17.68it/s] 32%|███▏      | 8716/26933 [10:39<17:11, 17.65it/s] 32%|███▏      | 8720/26933 [10:39<17:10, 17.68it/s] 32%|███▏      | 8724/26933 [10:39<17:11, 17.65it/s] 32%|███▏      | 8728/26933 [10:40<17:13, 17.62it/s] 32%|███▏      | 8732/26933 [10:40<17:13, 17.61it/s] 32%|███▏      | 8736/26933 [10:40<17:13, 17.61it/s] 32%|███▏      | 8740/26933 [10:40<17:10, 17.65it/s] 32%|███▏      | 8744/26933 [10:41<17:13, 17.61it/s] 32%|███▏      | 8748/26933 [10:41<17:13, 17.60it/s] 32%|███▏      | 8752/26933 [10:41<17:11, 17.63it/s] 33%|███▎      | 8756/26933 [10:41<17:10, 17.64it/s] 33%|███▎      | 8760/26933 [10:41<17:09, 17.65it/s] 33%|███▎      | 8764/26933 [10:42<17:12, 17.59it/s] 33%|███▎      | 8768/26933 [10:42<17:12, 17.59it/s] 33%|███▎      | 8772/26933 [10:42<17:12, 17.59it/s] 33%|███▎      | 8776/26933 [10:42<17:13, 17.57it/s] 33%|███▎      | 8780/26933 [10:43<17:14, 17.55it/s] 33%|███▎      | 8784/26933 [10:43<17:12, 17.57it/s] 33%|███▎      | 8788/26933 [10:43<17:12, 17.57it/s] 33%|███▎      | 8792/26933 [10:43<17:09, 17.62it/s] 33%|███▎      | 8796/26933 [10:44<17:10, 17.60it/s] 33%|███▎      | 8800/26933 [10:44<17:09, 17.62it/s] 33%|███▎      | 8804/26933 [10:44<17:08, 17.63it/s] 33%|███▎      | 8808/26933 [10:44<17:08, 17.63it/s] 33%|███▎      | 8812/26933 [10:44<17:07, 17.63it/s] 33%|███▎      | 8816/26933 [10:45<17:08, 17.62it/s] 33%|███▎      | 8820/26933 [10:45<17:05, 17.66it/s] 33%|███▎      | 8824/26933 [10:45<17:05, 17.67it/s] 33%|███▎      | 8828/26933 [10:45<17:03, 17.68it/s] 33%|███▎      | 8832/26933 [10:46<17:05, 17.65it/s] 33%|███▎      | 8836/26933 [10:46<17:06, 17.63it/s] 33%|███▎      | 8840/26933 [10:46<17:06, 17.62it/s] 33%|███▎      | 8844/26933 [10:46<17:03, 17.68it/s] 33%|███▎      | 8848/26933 [10:46<17:08, 17.58it/s] 33%|███▎      | 8852/26933 [10:47<17:07, 17.59it/s] 33%|███▎      | 8856/26933 [10:47<17:06, 17.62it/s] 33%|███▎      | 8860/26933 [10:47<17:04, 17.65it/s] 33%|███▎      | 8864/26933 [10:47<17:05, 17.62it/s] 33%|███▎      | 8868/26933 [10:48<17:06, 17.60it/s] 33%|███▎      | 8872/26933 [10:48<17:06, 17.60it/s] 33%|███▎      | 8876/26933 [10:48<17:03, 17.64it/s] 33%|███▎      | 8880/26933 [10:48<17:02, 17.66it/s] 33%|███▎      | 8884/26933 [10:49<17:02, 17.66it/s] 33%|███▎      | 8888/26933 [10:49<17:06, 17.58it/s] 33%|███▎      | 8892/26933 [10:49<17:07, 17.56it/s] 33%|███▎      | 8896/26933 [10:49<17:03, 17.62it/s] 33%|███▎      | 8900/26933 [10:49<17:02, 17.63it/s] 33%|███▎      | 8904/26933 [10:50<17:05, 17.58it/s] 33%|███▎      | 8908/26933 [10:50<16:58, 17.69it/s] 33%|███▎      | 8912/26933 [10:50<16:56, 17.73it/s] 33%|███▎      | 8916/26933 [10:50<16:54, 17.76it/s] 33%|███▎      | 8920/26933 [10:51<16:56, 17.72it/s] 33%|███▎      | 8924/26933 [10:51<16:53, 17.77it/s] 33%|███▎      | 8928/26933 [10:51<16:52, 17.78it/s] 33%|███▎      | 8932/26933 [10:51<16:52, 17.78it/s] 33%|███▎      | 8936/26933 [10:51<16:49, 17.82it/s] 33%|███▎      | 8940/26933 [10:52<16:52, 17.77it/s] 33%|███▎      | 8944/26933 [10:52<16:48, 17.83it/s] 33%|███▎      | 8948/26933 [10:52<16:47, 17.84it/s] 33%|███▎      | 8952/26933 [10:52<16:47, 17.85it/s] 33%|███▎      | 8956/26933 [10:53<16:50, 17.78it/s] 33%|███▎      | 8960/26933 [10:53<16:49, 17.81it/s] 33%|███▎      | 8964/26933 [10:53<16:50, 17.79it/s] 33%|███▎      | 8968/26933 [10:53<16:47, 17.84it/s] 33%|███▎      | 8972/26933 [10:53<16:47, 17.82it/s] 33%|███▎      | 8976/26933 [10:54<16:49, 17.78it/s] 33%|███▎      | 8980/26933 [10:54<16:47, 17.82it/s] 33%|███▎      | 8984/26933 [10:54<16:51, 17.75it/s] 33%|███▎      | 8988/26933 [10:54<16:48, 17.79it/s] 33%|███▎      | 8992/26933 [10:55<16:49, 17.77it/s] 33%|███▎      | 8996/26933 [10:55<16:46, 17.82it/s] 33%|███▎      | 9000/26933 [10:55<16:45, 17.84it/s] 33%|███▎      | 9004/26933 [10:55<16:43, 17.86it/s] 33%|███▎      | 9008/26933 [10:56<16:43, 17.86it/s] 33%|███▎      | 9012/26933 [10:56<16:45, 17.82it/s] 33%|███▎      | 9016/26933 [10:56<16:44, 17.83it/s] 33%|███▎      | 9020/26933 [10:56<16:44, 17.84it/s] 34%|███▎      | 9024/26933 [10:56<16:41, 17.87it/s] 34%|███▎      | 9028/26933 [10:57<16:45, 17.81it/s] 34%|███▎      | 9032/26933 [10:57<16:43, 17.84it/s] 34%|███▎      | 9036/26933 [10:57<16:42, 17.86it/s] 34%|███▎      | 9040/26933 [10:57<16:41, 17.87it/s] 34%|███▎      | 9044/26933 [10:58<16:42, 17.85it/s] 34%|███▎      | 9048/26933 [10:58<16:44, 17.81it/s] 34%|███▎      | 9052/26933 [10:58<16:42, 17.84it/s] 34%|███▎      | 9056/26933 [10:58<16:41, 17.86it/s] 34%|███▎      | 9060/26933 [10:58<16:41, 17.85it/s] 34%|███▎      | 9064/26933 [10:59<16:42, 17.82it/s] 34%|███▎      | 9068/26933 [10:59<16:42, 17.83it/s] 34%|███▎      | 9072/26933 [10:59<16:40, 17.85it/s] 34%|███▎      | 9076/26933 [10:59<16:40, 17.86it/s] 34%|███▎      | 9080/26933 [11:00<16:42, 17.80it/s] 34%|███▎      | 9084/26933 [11:00<16:39, 17.85it/s] 34%|███▎      | 9088/26933 [11:00<16:37, 17.89it/s] 34%|███▍      | 9092/26933 [11:00<16:35, 17.92it/s] 34%|███▍      | 9096/26933 [11:00<16:35, 17.91it/s] 34%|███▍      | 9100/26933 [11:01<16:38, 17.87it/s] 34%|███▍      | 9104/26933 [11:01<16:36, 17.89it/s] 34%|███▍      | 9108/26933 [11:01<16:35, 17.91it/s] 34%|███▍      | 9112/26933 [11:01<16:34, 17.92it/s] 34%|███▍      | 9116/26933 [11:02<16:39, 17.82it/s] 34%|███▍      | 9120/26933 [11:02<16:45, 17.72it/s] 34%|███▍      | 9124/26933 [11:02<16:41, 17.79it/s] 34%|███▍      | 9128/26933 [11:02<16:37, 17.85it/s] 34%|███▍      | 9132/26933 [11:02<16:37, 17.85it/s] 34%|███▍      | 9136/26933 [11:03<16:39, 17.81it/s] 34%|███▍      | 9140/26933 [11:03<16:38, 17.82it/s] 34%|███▍      | 9144/26933 [11:03<16:37, 17.83it/s] 34%|███▍      | 9148/26933 [11:03<16:38, 17.82it/s] 34%|███▍      | 9152/26933 [11:04<16:40, 17.77it/s] 34%|███▍      | 9156/26933 [11:04<16:38, 17.80it/s] 34%|███▍      | 9160/26933 [11:04<16:37, 17.82it/s] 34%|███▍      | 9164/26933 [11:04<16:37, 17.82it/s] 34%|███▍      | 9168/26933 [11:04<16:36, 17.82it/s] 34%|███▍      | 9172/26933 [11:05<16:38, 17.78it/s] 34%|███▍      | 9176/26933 [11:05<16:36, 17.82it/s] 34%|███▍      | 9180/26933 [11:05<16:44, 17.68it/s] 34%|███▍      | 9184/26933 [11:05<16:51, 17.56it/s] 34%|███▍      | 9188/26933 [11:06<16:56, 17.46it/s] 34%|███▍      | 9192/26933 [11:06<16:57, 17.44it/s] 34%|███▍      | 9196/26933 [11:06<16:59, 17.40it/s] 34%|███▍      | 9200/26933 [11:06<17:00, 17.38it/s] 34%|███▍      | 9204/26933 [11:07<17:01, 17.35it/s] 34%|███▍      | 9208/26933 [11:07<16:57, 17.41it/s] 34%|███▍      | 9212/26933 [11:07<16:55, 17.44it/s] 34%|███▍      | 9216/26933 [11:07<16:56, 17.43it/s] 34%|███▍      | 9220/26933 [11:07<16:57, 17.40it/s] 34%|███▍      | 9224/26933 [11:08<16:59, 17.38it/s] 34%|███▍      | 9228/26933 [11:08<16:54, 17.45it/s] 34%|███▍      | 9232/26933 [11:08<16:50, 17.52it/s] 34%|███▍      | 9236/26933 [11:08<16:48, 17.55it/s] 34%|███▍      | 9240/26933 [11:09<16:48, 17.54it/s] 34%|███▍      | 9244/26933 [11:09<16:47, 17.56it/s] 34%|███▍      | 9248/26933 [11:09<16:44, 17.61it/s] 34%|███▍      | 9252/26933 [11:09<16:45, 17.58it/s] 34%|███▍      | 9256/26933 [11:10<16:44, 17.60it/s] 34%|███▍      | 9260/26933 [11:10<16:44, 17.59it/s] 34%|███▍      | 9264/26933 [11:10<16:39, 17.68it/s] 34%|███▍      | 9268/26933 [11:10<16:39, 17.67it/s] 34%|███▍      | 9272/26933 [11:10<16:40, 17.65it/s] 34%|███▍      | 9276/26933 [11:11<16:45, 17.57it/s] 34%|███▍      | 9280/26933 [11:11<16:42, 17.61it/s] 34%|███▍      | 9284/26933 [11:11<16:41, 17.62it/s] 34%|███▍      | 9288/26933 [11:11<16:38, 17.66it/s] 35%|███▍      | 9292/26933 [11:12<16:42, 17.60it/s] 35%|███▍      | 9296/26933 [11:12<16:39, 17.65it/s] 35%|███▍      | 9300/26933 [11:12<16:36, 17.70it/s] 35%|███▍      | 9304/26933 [11:12<16:37, 17.68it/s] 35%|███▍      | 9308/26933 [11:12<16:35, 17.70it/s] 35%|███▍      | 9312/26933 [11:13<16:38, 17.66it/s] 35%|███▍      | 9316/26933 [11:13<16:36, 17.68it/s] 35%|███▍      | 9320/26933 [11:13<16:35, 17.69it/s] 35%|███▍      | 9324/26933 [11:13<16:34, 17.70it/s] 35%|███▍      | 9328/26933 [11:14<16:37, 17.65it/s] 35%|███▍      | 9332/26933 [11:14<16:34, 17.70it/s] 35%|███▍      | 9336/26933 [11:14<16:33, 17.71it/s] 35%|███▍      | 9340/26933 [11:14<16:33, 17.71it/s] 35%|███▍      | 9344/26933 [11:14<16:32, 17.73it/s] 35%|███▍      | 9348/26933 [11:15<16:35, 17.67it/s] 35%|███▍      | 9352/26933 [11:15<16:32, 17.71it/s] 35%|███▍      | 9356/26933 [11:15<16:34, 17.67it/s] 35%|███▍      | 9360/26933 [11:15<16:32, 17.70it/s] 35%|███▍      | 9364/26933 [11:16<16:36, 17.63it/s] 35%|███▍      | 9368/26933 [11:16<16:33, 17.68it/s] 35%|███▍      | 9372/26933 [11:16<16:32, 17.69it/s] 35%|███▍      | 9376/26933 [11:16<16:32, 17.70it/s] 35%|███▍      | 9380/26933 [11:17<16:30, 17.73it/s] 35%|███▍      | 9384/26933 [11:17<16:33, 17.66it/s] 35%|███▍      | 9388/26933 [11:17<16:31, 17.70it/s] 35%|███▍      | 9392/26933 [11:17<16:36, 17.60it/s] 35%|███▍      | 9396/26933 [11:17<16:30, 17.70it/s] 35%|███▍      | 9400/26933 [11:18<16:32, 17.66it/s] 35%|███▍      | 9404/26933 [11:18<16:30, 17.70it/s] 35%|███▍      | 9408/26933 [11:18<16:27, 17.75it/s] 35%|███▍      | 9412/26933 [11:18<16:26, 17.76it/s] 35%|███▍      | 9416/26933 [11:19<16:34, 17.62it/s] 35%|███▍      | 9420/26933 [11:19<16:36, 17.57it/s] 35%|███▍      | 9424/26933 [11:19<16:32, 17.65it/s] 35%|███▌      | 9428/26933 [11:19<16:24, 17.77it/s] 35%|███▌      | 9432/26933 [11:19<16:19, 17.87it/s] 35%|███▌      | 9436/26933 [11:20<16:20, 17.85it/s] 35%|███▌      | 9440/26933 [11:20<16:16, 17.92it/s] 35%|███▌      | 9444/26933 [11:20<16:12, 17.98it/s] 35%|███▌      | 9448/26933 [11:20<16:09, 18.03it/s] 35%|███▌      | 9452/26933 [11:21<16:14, 17.94it/s] 35%|███▌      | 9456/26933 [11:21<16:13, 17.94it/s] 35%|███▌      | 9460/26933 [11:21<16:13, 17.96it/s] 35%|███▌      | 9464/26933 [11:21<16:13, 17.94it/s] 35%|███▌      | 9468/26933 [11:21<16:12, 17.95it/s] 35%|███▌      | 9472/26933 [11:22<16:14, 17.92it/s] 35%|███▌      | 9476/26933 [11:22<16:12, 17.95it/s] 35%|███▌      | 9480/26933 [11:22<16:11, 17.97it/s] 35%|███▌      | 9484/26933 [11:22<16:09, 17.99it/s] 35%|███▌      | 9488/26933 [11:23<16:12, 17.94it/s] 35%|███▌      | 9492/26933 [11:23<16:12, 17.93it/s] 35%|███▌      | 9496/26933 [11:23<16:12, 17.92it/s] 35%|███▌      | 9500/26933 [11:23<16:09, 17.98it/s] 35%|███▌      | 9504/26933 [11:23<16:06, 18.02it/s] 35%|███▌      | 9508/26933 [11:24<16:12, 17.93it/s] 35%|███▌      | 9512/26933 [11:24<16:10, 17.96it/s] 35%|███▌      | 9516/26933 [11:24<16:10, 17.95it/s] 35%|███▌      | 9520/26933 [11:24<16:09, 17.96it/s] 35%|███▌      | 9524/26933 [11:25<16:18, 17.79it/s] 35%|███▌      | 9528/26933 [11:25<16:22, 17.72it/s] 35%|███▌      | 9532/26933 [11:25<16:26, 17.64it/s] 35%|███▌      | 9536/26933 [11:25<16:30, 17.56it/s] 35%|███▌      | 9540/26933 [11:26<16:34, 17.48it/s] 35%|███▌      | 9544/26933 [11:26<16:41, 17.37it/s] 35%|███▌      | 9548/26933 [11:26<16:43, 17.32it/s] 35%|███▌      | 9552/26933 [11:26<16:42, 17.34it/s] 35%|███▌      | 9556/26933 [11:26<16:41, 17.35it/s] 35%|███▌      | 9560/26933 [11:27<16:41, 17.34it/s] 36%|███▌      | 9564/26933 [11:27<16:37, 17.41it/s] 36%|███▌      | 9568/26933 [11:27<16:40, 17.36it/s] 36%|███▌      | 9572/26933 [11:27<16:39, 17.37it/s] 36%|███▌      | 9576/26933 [11:28<16:45, 17.26it/s] 36%|███▌      | 9580/26933 [11:28<16:40, 17.34it/s] 36%|███▌      | 9584/26933 [11:28<16:40, 17.33it/s] 36%|███▌      | 9588/26933 [11:28<16:38, 17.37it/s] 36%|███▌      | 9592/26933 [11:29<16:39, 17.36it/s] 36%|███▌      | 9596/26933 [11:29<16:38, 17.36it/s] 36%|███▌      | 9600/26933 [11:29<16:38, 17.36it/s] 36%|███▌      | 9604/26933 [11:29<16:36, 17.39it/s] 36%|███▌      | 9608/26933 [11:29<16:36, 17.39it/s] 36%|███▌      | 9612/26933 [11:30<16:36, 17.38it/s] 36%|███▌      | 9616/26933 [11:30<16:34, 17.42it/s] 36%|███▌      | 9620/26933 [11:30<16:32, 17.45it/s] 36%|███▌      | 9624/26933 [11:30<16:31, 17.46it/s] 36%|███▌      | 9628/26933 [11:31<16:31, 17.45it/s] 36%|███▌      | 9632/26933 [11:31<16:29, 17.48it/s] 36%|███▌      | 9636/26933 [11:31<16:28, 17.50it/s] 36%|███▌      | 9640/26933 [11:31<16:30, 17.47it/s] 36%|███▌      | 9644/26933 [11:31<16:28, 17.49it/s] 36%|███▌      | 9648/26933 [11:32<16:29, 17.47it/s] 36%|███▌      | 9652/26933 [11:32<16:28, 17.48it/s] 36%|███▌      | 9656/26933 [11:32<16:28, 17.48it/s] 36%|███▌      | 9660/26933 [11:32<16:25, 17.52it/s] 36%|███▌      | 9664/26933 [11:33<16:25, 17.52it/s] 36%|███▌      | 9668/26933 [11:33<16:25, 17.51it/s] 36%|███▌      | 9672/26933 [11:33<16:25, 17.51it/s] 36%|███▌      | 9676/26933 [11:33<16:22, 17.56it/s] 36%|███▌      | 9680/26933 [11:34<16:20, 17.60it/s] 36%|███▌      | 9684/26933 [11:34<16:17, 17.64it/s] 36%|███▌      | 9688/26933 [11:34<16:15, 17.68it/s] 36%|███▌      | 9692/26933 [11:34<16:12, 17.72it/s] 36%|███▌      | 9696/26933 [11:34<16:10, 17.76it/s] 36%|███▌      | 9700/26933 [11:35<16:13, 17.71it/s] 36%|███▌      | 9704/26933 [11:35<16:13, 17.69it/s] 36%|███▌      | 9708/26933 [11:35<16:13, 17.69it/s] 36%|███▌      | 9712/26933 [11:35<16:14, 17.67it/s] 36%|███▌      | 9716/26933 [11:36<16:16, 17.64it/s] 36%|███▌      | 9720/26933 [11:36<16:15, 17.65it/s] 36%|███▌      | 9724/26933 [11:36<16:16, 17.62it/s] 36%|███▌      | 9728/26933 [11:36<16:19, 17.56it/s] 36%|███▌      | 9732/26933 [11:36<16:20, 17.55it/s] 36%|███▌      | 9736/26933 [11:37<16:21, 17.52it/s] 36%|███▌      | 9740/26933 [11:37<16:20, 17.54it/s] 36%|███▌      | 9744/26933 [11:37<16:20, 17.52it/s] 36%|███▌      | 9748/26933 [11:37<16:20, 17.53it/s] 36%|███▌      | 9752/26933 [11:38<16:21, 17.50it/s] 36%|███▌      | 9756/26933 [11:38<16:33, 17.29it/s] 36%|███▌      | 9760/26933 [11:38<16:29, 17.36it/s] 36%|███▋      | 9764/26933 [11:38<16:27, 17.38it/s] 36%|███▋      | 9768/26933 [11:39<16:29, 17.34it/s] 36%|███▋      | 9772/26933 [11:39<16:27, 17.38it/s] 36%|███▋      | 9776/26933 [11:39<16:23, 17.44it/s] 36%|███▋      | 9780/26933 [11:39<16:23, 17.45it/s] 36%|███▋      | 9784/26933 [11:39<16:21, 17.48it/s] 36%|███▋      | 9788/26933 [11:40<16:23, 17.44it/s] 36%|███▋      | 9792/26933 [11:40<16:21, 17.47it/s] 36%|███▋      | 9796/26933 [11:40<16:20, 17.48it/s] 36%|███▋      | 9800/26933 [11:40<16:18, 17.51it/s] 36%|███▋      | 9804/26933 [11:41<16:20, 17.48it/s] 36%|███▋      | 9808/26933 [11:41<16:18, 17.50it/s] 36%|███▋      | 9812/26933 [11:41<16:16, 17.53it/s] 36%|███▋      | 9816/26933 [11:41<16:15, 17.54it/s] 36%|███▋      | 9820/26933 [11:42<16:17, 17.50it/s] 36%|███▋      | 9824/26933 [11:42<16:22, 17.41it/s] 36%|███▋      | 9828/26933 [11:42<16:20, 17.45it/s] 37%|███▋      | 9832/26933 [11:42<16:21, 17.43it/s] 37%|███▋      | 9836/26933 [11:42<16:22, 17.41it/s] 37%|███▋      | 9840/26933 [11:43<16:23, 17.39it/s] 37%|███▋      | 9844/26933 [11:43<16:22, 17.39it/s] 37%|███▋      | 9848/26933 [11:43<16:20, 17.42it/s] 37%|███▋      | 9852/26933 [11:43<16:18, 17.45it/s] 37%|███▋      | 9856/26933 [11:44<16:18, 17.45it/s] 37%|███▋      | 9860/26933 [11:44<16:16, 17.48it/s] 37%|███▋      | 9864/26933 [11:44<16:14, 17.51it/s] 37%|███▋      | 9868/26933 [11:44<16:14, 17.51it/s] 37%|███▋      | 9872/26933 [11:45<16:14, 17.51it/s] 37%|███▋      | 9876/26933 [11:45<16:14, 17.51it/s] 37%|███▋      | 9880/26933 [11:45<16:13, 17.52it/s] 37%|███▋      | 9884/26933 [11:45<16:12, 17.52it/s] 37%|███▋      | 9888/26933 [11:45<16:12, 17.53it/s] 37%|███▋      | 9892/26933 [11:46<16:13, 17.50it/s] 37%|███▋      | 9896/26933 [11:46<16:13, 17.50it/s] 37%|███▋      | 9900/26933 [11:46<16:12, 17.52it/s] 37%|███▋      | 9904/26933 [11:46<16:11, 17.53it/s] 37%|███▋      | 9908/26933 [11:47<16:12, 17.51it/s] 37%|███▋      | 9912/26933 [11:47<16:11, 17.52it/s] 37%|███▋      | 9916/26933 [11:47<16:11, 17.52it/s] 37%|███▋      | 9920/26933 [11:47<16:11, 17.52it/s] 37%|███▋      | 9924/26933 [11:47<16:10, 17.53it/s] 37%|███▋      | 9928/26933 [11:48<16:10, 17.52it/s] 37%|███▋      | 9932/26933 [11:48<16:16, 17.42it/s] 37%|███▋      | 9936/26933 [11:48<16:17, 17.39it/s] 37%|███▋      | 9940/26933 [11:48<16:14, 17.45it/s] 37%|███▋      | 9944/26933 [11:49<16:13, 17.45it/s] 37%|███▋      | 9948/26933 [11:49<16:11, 17.48it/s] 37%|███▋      | 9952/26933 [11:49<16:10, 17.50it/s] 37%|███▋      | 9956/26933 [11:49<16:07, 17.55it/s] 37%|███▋      | 9960/26933 [11:50<16:03, 17.61it/s] 37%|███▋      | 9964/26933 [11:50<16:04, 17.59it/s] 37%|███▋      | 9968/26933 [11:50<16:04, 17.59it/s] 37%|███▋      | 9972/26933 [11:50<16:05, 17.57it/s] 37%|███▋      | 9976/26933 [11:50<16:04, 17.58it/s] 37%|███▋      | 9980/26933 [11:51<16:06, 17.55it/s] 37%|███▋      | 9984/26933 [11:51<16:05, 17.55it/s] 37%|███▋      | 9988/26933 [11:51<16:07, 17.52it/s] 37%|███▋      | 9992/26933 [11:51<16:05, 17.55it/s] 37%|███▋      | 9996/26933 [11:52<16:07, 17.51it/s] 37%|███▋      | 10000/26933 [11:52<16:07, 17.50it/s] 37%|███▋      | 10004/26933 [11:52<16:06, 17.52it/s] 37%|███▋      | 10008/26933 [11:52<16:03, 17.56it/s] 37%|███▋      | 10012/26933 [11:52<16:02, 17.57it/s] 37%|███▋      | 10016/26933 [11:53<16:05, 17.52it/s] 37%|███▋      | 10020/26933 [11:53<16:04, 17.54it/s] 37%|███▋      | 10024/26933 [11:53<16:03, 17.56it/s] 37%|███▋      | 10028/26933 [11:53<16:01, 17.58it/s] 37%|███▋      | 10032/26933 [11:54<16:04, 17.52it/s] 37%|███▋      | 10036/26933 [11:54<16:04, 17.52it/s] 37%|███▋      | 10040/26933 [11:54<16:03, 17.53it/s] 37%|███▋      | 10044/26933 [11:54<16:01, 17.56it/s] 37%|███▋      | 10048/26933 [11:55<16:04, 17.51it/s] 37%|███▋      | 10052/26933 [11:55<16:03, 17.52it/s] 37%|███▋      | 10056/26933 [11:55<16:04, 17.50it/s] 37%|███▋      | 10060/26933 [11:55<16:03, 17.52it/s] 37%|███▋      | 10064/26933 [11:55<16:02, 17.52it/s] 37%|███▋      | 10068/26933 [11:56<16:04, 17.48it/s] 37%|███▋      | 10072/26933 [11:56<16:01, 17.53it/s] 37%|███▋      | 10076/26933 [11:56<16:03, 17.50it/s] 37%|███▋      | 10080/26933 [11:56<16:00, 17.55it/s] 37%|███▋      | 10084/26933 [11:57<16:00, 17.55it/s] 37%|███▋      | 10088/26933 [11:57<15:58, 17.57it/s] 37%|███▋      | 10092/26933 [11:57<15:57, 17.58it/s] 37%|███▋      | 10096/26933 [11:57<15:58, 17.57it/s] 38%|███▊      | 10100/26933 [11:58<15:59, 17.55it/s] 38%|███▊      | 10104/26933 [11:58<16:00, 17.53it/s] 38%|███▊      | 10108/26933 [11:58<15:59, 17.54it/s] 38%|███▊      | 10112/26933 [11:58<15:58, 17.54it/s] 38%|███▊      | 10116/26933 [11:58<15:57, 17.57it/s] 38%|███▊      | 10120/26933 [11:59<16:00, 17.51it/s] 38%|███▊      | 10124/26933 [11:59<16:00, 17.50it/s] 38%|███▊      | 10128/26933 [11:59<16:02, 17.46it/s] 38%|███▊      | 10132/26933 [11:59<16:00, 17.50it/s] 38%|███▊      | 10136/26933 [12:00<16:00, 17.48it/s] 38%|███▊      | 10140/26933 [12:00<15:58, 17.52it/s] 38%|███▊      | 10144/26933 [12:00<15:55, 17.57it/s] 38%|███▊      | 10148/26933 [12:00<15:54, 17.58it/s] 38%|███▊      | 10152/26933 [12:00<15:54, 17.58it/s] 38%|███▊      | 10156/26933 [12:01<15:57, 17.52it/s] 38%|███▊      | 10160/26933 [12:01<15:54, 17.57it/s] 38%|███▊      | 10164/26933 [12:01<15:51, 17.62it/s] 38%|███▊      | 10168/26933 [12:01<15:51, 17.63it/s] 38%|███▊      | 10172/26933 [12:02<15:51, 17.61it/s] 38%|███▊      | 10176/26933 [12:02<15:51, 17.61it/s] 38%|███▊      | 10180/26933 [12:02<15:50, 17.62it/s] 38%|███▊      | 10184/26933 [12:02<15:50, 17.61it/s] 38%|███▊      | 10188/26933 [12:03<15:51, 17.61it/s] 38%|███▊      | 10192/26933 [12:03<15:53, 17.56it/s] 38%|███▊      | 10196/26933 [12:03<15:52, 17.58it/s] 38%|███▊      | 10200/26933 [12:03<15:54, 17.54it/s] 38%|███▊      | 10204/26933 [12:03<15:51, 17.57it/s] 38%|███▊      | 10208/26933 [12:04<15:52, 17.56it/s] 38%|███▊      | 10212/26933 [12:04<15:51, 17.57it/s] 38%|███▊      | 10216/26933 [12:04<15:50, 17.58it/s] 38%|███▊      | 10220/26933 [12:04<15:48, 17.62it/s] 38%|███▊      | 10224/26933 [12:05<15:50, 17.59it/s] 38%|███▊      | 10228/26933 [12:05<15:48, 17.62it/s] 38%|███▊      | 10232/26933 [12:05<15:48, 17.60it/s] 38%|███▊      | 10236/26933 [12:05<15:50, 17.56it/s] 38%|███▊      | 10240/26933 [12:05<15:50, 17.57it/s] 38%|███▊      | 10244/26933 [12:06<15:51, 17.53it/s] 38%|███▊      | 10248/26933 [12:06<15:48, 17.60it/s] 38%|███▊      | 10252/26933 [12:06<15:44, 17.66it/s] 38%|███▊      | 10256/26933 [12:06<15:43, 17.68it/s] 38%|███▊      | 10260/26933 [12:07<15:45, 17.64it/s] 38%|███▊      | 10264/26933 [12:07<15:42, 17.68it/s] 38%|███▊      | 10268/26933 [12:07<15:41, 17.70it/s] 38%|███▊      | 10272/26933 [12:07<15:40, 17.72it/s] 38%|███▊      | 10276/26933 [12:08<15:39, 17.73it/s] 38%|███▊      | 10280/26933 [12:08<15:46, 17.59it/s] 38%|███▊      | 10284/26933 [12:08<15:47, 17.58it/s] 38%|███▊      | 10288/26933 [12:08<15:46, 17.58it/s] 38%|███▊      | 10292/26933 [12:08<15:48, 17.54it/s] 38%|███▊      | 10296/26933 [12:09<15:51, 17.48it/s] 38%|███▊      | 10300/26933 [12:09<15:51, 17.48it/s] 38%|███▊      | 10304/26933 [12:09<15:49, 17.51it/s] 38%|███▊      | 10308/26933 [12:09<15:50, 17.49it/s] 38%|███▊      | 10312/26933 [12:10<15:44, 17.60it/s] 38%|███▊      | 10316/26933 [12:10<15:37, 17.72it/s] 38%|███▊      | 10320/26933 [12:10<15:32, 17.82it/s] 38%|███▊      | 10324/26933 [12:10<15:27, 17.91it/s] 38%|███▊      | 10328/26933 [12:10<15:24, 17.97it/s] 38%|███▊      | 10332/26933 [12:11<15:29, 17.86it/s] 38%|███▊      | 10336/26933 [12:11<15:26, 17.92it/s] 38%|███▊      | 10340/26933 [12:11<15:21, 18.00it/s] 38%|███▊      | 10344/26933 [12:11<15:19, 18.05it/s] 38%|███▊      | 10348/26933 [12:12<15:18, 18.05it/s] 38%|███▊      | 10352/26933 [12:12<15:19, 18.04it/s] 38%|███▊      | 10356/26933 [12:12<15:18, 18.05it/s] 38%|███▊      | 10360/26933 [12:12<15:18, 18.04it/s] 38%|███▊      | 10364/26933 [12:12<15:18, 18.03it/s] 38%|███▊      | 10368/26933 [12:13<15:21, 17.98it/s] 39%|███▊      | 10372/26933 [12:13<15:19, 18.01it/s] 39%|███▊      | 10376/26933 [12:13<15:17, 18.05it/s] 39%|███▊      | 10380/26933 [12:13<15:15, 18.08it/s] 39%|███▊      | 10384/26933 [12:14<15:25, 17.87it/s] 39%|███▊      | 10388/26933 [12:14<15:31, 17.76it/s] 39%|███▊      | 10392/26933 [12:14<15:35, 17.67it/s] 39%|███▊      | 10396/26933 [12:14<15:36, 17.66it/s] 39%|███▊      | 10400/26933 [12:14<15:35, 17.67it/s] 39%|███▊      | 10404/26933 [12:15<15:36, 17.65it/s] 39%|███▊      | 10408/26933 [12:15<15:37, 17.63it/s] 39%|███▊      | 10412/26933 [12:15<15:39, 17.59it/s] 39%|███▊      | 10416/26933 [12:15<15:39, 17.58it/s] 39%|███▊      | 10420/26933 [12:16<15:43, 17.51it/s] 39%|███▊      | 10424/26933 [12:16<15:42, 17.52it/s] 39%|███▊      | 10428/26933 [12:16<15:40, 17.56it/s] 39%|███▊      | 10432/26933 [12:16<15:38, 17.58it/s] 39%|███▊      | 10436/26933 [12:17<15:36, 17.62it/s] 39%|███▉      | 10440/26933 [12:17<15:36, 17.61it/s] 39%|███▉      | 10444/26933 [12:17<15:34, 17.64it/s] 39%|███▉      | 10448/26933 [12:17<15:35, 17.63it/s] 39%|███▉      | 10452/26933 [12:17<15:36, 17.59it/s] 39%|███▉      | 10456/26933 [12:18<15:39, 17.54it/s] 39%|███▉      | 10460/26933 [12:18<15:38, 17.56it/s] 39%|███▉      | 10464/26933 [12:18<15:37, 17.56it/s] 39%|███▉      | 10468/26933 [12:18<15:36, 17.58it/s] 39%|███▉      | 10472/26933 [12:19<15:37, 17.56it/s] 39%|███▉      | 10476/26933 [12:19<15:35, 17.59it/s] 39%|███▉      | 10480/26933 [12:19<15:35, 17.59it/s] 39%|███▉      | 10484/26933 [12:19<15:35, 17.58it/s] 39%|███▉      | 10488/26933 [12:19<15:35, 17.58it/s] 39%|███▉      | 10492/26933 [12:20<15:37, 17.54it/s] 39%|███▉      | 10496/26933 [12:20<15:36, 17.55it/s] 39%|███▉      | 10500/26933 [12:20<15:37, 17.54it/s] 39%|███▉      | 10504/26933 [12:20<15:36, 17.54it/s] 39%|███▉      | 10508/26933 [12:21<15:38, 17.51it/s] 39%|███▉      | 10512/26933 [12:21<15:37, 17.51it/s] 39%|███▉      | 10516/26933 [12:21<15:37, 17.52it/s] 39%|███▉      | 10520/26933 [12:21<15:35, 17.54it/s] 39%|███▉      | 10524/26933 [12:22<15:37, 17.50it/s] 39%|███▉      | 10528/26933 [12:22<15:35, 17.53it/s] 39%|███▉      | 10532/26933 [12:22<15:34, 17.54it/s] 39%|███▉      | 10536/26933 [12:22<15:34, 17.55it/s] 39%|███▉      | 10540/26933 [12:22<15:33, 17.56it/s] 39%|███▉      | 10544/26933 [12:23<15:33, 17.55it/s] 39%|███▉      | 10548/26933 [12:23<15:34, 17.54it/s] 39%|███▉      | 10552/26933 [12:23<15:33, 17.55it/s] 39%|███▉      | 10556/26933 [12:23<15:31, 17.57it/s] 39%|███▉      | 10560/26933 [12:24<15:33, 17.54it/s] 39%|███▉      | 10564/26933 [12:24<15:44, 17.34it/s] 39%|███▉      | 10568/26933 [12:24<15:39, 17.42it/s] 39%|███▉      | 10572/26933 [12:24<15:34, 17.51it/s] 39%|███▉      | 10576/26933 [12:25<15:31, 17.56it/s] 39%|███▉      | 10580/26933 [12:25<15:32, 17.54it/s] 39%|███▉      | 10584/26933 [12:25<15:31, 17.56it/s] 39%|███▉      | 10588/26933 [12:25<15:28, 17.60it/s] 39%|███▉      | 10592/26933 [12:25<15:29, 17.59it/s] 39%|███▉      | 10596/26933 [12:26<15:30, 17.56it/s] 39%|███▉      | 10600/26933 [12:26<15:28, 17.60it/s] 39%|███▉      | 10604/26933 [12:26<15:26, 17.62it/s] 39%|███▉      | 10608/26933 [12:26<15:23, 17.68it/s] 39%|███▉      | 10612/26933 [12:27<15:24, 17.65it/s] 39%|███▉      | 10616/26933 [12:27<15:22, 17.69it/s] 39%|███▉      | 10620/26933 [12:27<15:22, 17.68it/s] 39%|███▉      | 10624/26933 [12:27<15:21, 17.70it/s] 39%|███▉      | 10628/26933 [12:27<15:21, 17.69it/s] 39%|███▉      | 10632/26933 [12:28<15:25, 17.62it/s] 39%|███▉      | 10636/26933 [12:28<15:25, 17.60it/s] 40%|███▉      | 10640/26933 [12:28<15:26, 17.59it/s] 40%|███▉      | 10644/26933 [12:28<15:27, 17.56it/s] 40%|███▉      | 10648/26933 [12:29<15:30, 17.51it/s] 40%|███▉      | 10652/26933 [12:29<15:28, 17.53it/s] 40%|███▉      | 10656/26933 [12:29<15:26, 17.56it/s] 40%|███▉      | 10660/26933 [12:29<15:27, 17.54it/s] 40%|███▉      | 10664/26933 [12:30<15:25, 17.57it/s] 40%|███▉      | 10668/26933 [12:30<15:26, 17.56it/s] 40%|███▉      | 10672/26933 [12:30<15:23, 17.60it/s] 40%|███▉      | 10676/26933 [12:30<15:23, 17.61it/s] 40%|███▉      | 10680/26933 [12:30<15:20, 17.66it/s] 40%|███▉      | 10684/26933 [12:31<15:21, 17.63it/s] 40%|███▉      | 10688/26933 [12:31<15:18, 17.69it/s] 40%|███▉      | 10692/26933 [12:31<15:19, 17.67it/s] 40%|███▉      | 10696/26933 [12:31<15:20, 17.64it/s] 40%|███▉      | 10700/26933 [12:32<15:21, 17.61it/s] 40%|███▉      | 10704/26933 [12:32<15:20, 17.62it/s] 40%|███▉      | 10708/26933 [12:32<15:21, 17.61it/s] 40%|███▉      | 10712/26933 [12:32<15:22, 17.58it/s] 40%|███▉      | 10716/26933 [12:32<15:23, 17.56it/s] 40%|███▉      | 10720/26933 [12:33<15:24, 17.55it/s] 40%|███▉      | 10724/26933 [12:33<15:23, 17.55it/s] 40%|███▉      | 10728/26933 [12:33<15:22, 17.56it/s] 40%|███▉      | 10732/26933 [12:33<15:22, 17.57it/s] 40%|███▉      | 10736/26933 [12:34<15:24, 17.52it/s] 40%|███▉      | 10740/26933 [12:34<15:23, 17.53it/s] 40%|███▉      | 10744/26933 [12:34<15:23, 17.53it/s] 40%|███▉      | 10748/26933 [12:34<15:24, 17.51it/s] 40%|███▉      | 10752/26933 [12:35<15:23, 17.51it/s] 40%|███▉      | 10756/26933 [12:35<15:26, 17.47it/s] 40%|███▉      | 10760/26933 [12:35<15:23, 17.51it/s] 40%|███▉      | 10764/26933 [12:35<15:21, 17.55it/s] 40%|███▉      | 10768/26933 [12:35<15:20, 17.55it/s] 40%|███▉      | 10772/26933 [12:36<15:22, 17.52it/s] 40%|████      | 10776/26933 [12:36<15:21, 17.53it/s] 40%|████      | 10780/26933 [12:36<15:19, 17.57it/s] 40%|████      | 10784/26933 [12:36<15:20, 17.54it/s] 40%|████      | 10788/26933 [12:37<15:28, 17.39it/s] 40%|████      | 10792/26933 [12:37<15:23, 17.48it/s] 40%|████      | 10796/26933 [12:37<15:18, 17.57it/s] 40%|████      | 10800/26933 [12:37<15:16, 17.61it/s] 40%|████      | 10804/26933 [12:37<15:14, 17.63it/s] 40%|████      | 10808/26933 [12:38<15:17, 17.57it/s] 40%|████      | 10812/26933 [12:38<15:14, 17.64it/s] 40%|████      | 10816/26933 [12:38<15:14, 17.63it/s] 40%|████      | 10820/26933 [12:38<15:13, 17.63it/s] 40%|████      | 10824/26933 [12:39<15:16, 17.58it/s] 40%|████      | 10828/26933 [12:39<15:16, 17.57it/s] 40%|████      | 10832/26933 [12:39<15:18, 17.53it/s] 40%|████      | 10836/26933 [12:39<15:18, 17.53it/s] 40%|████      | 10840/26933 [12:40<15:18, 17.52it/s] 40%|████      | 10844/26933 [12:40<15:21, 17.46it/s] 40%|████      | 10848/26933 [12:40<15:18, 17.50it/s] 40%|████      | 10852/26933 [12:40<15:19, 17.50it/s] 40%|████      | 10856/26933 [12:40<15:18, 17.51it/s] 40%|████      | 10860/26933 [12:41<15:20, 17.47it/s] 40%|████      | 10864/26933 [12:41<15:17, 17.52it/s] 40%|████      | 10868/26933 [12:41<15:15, 17.55it/s] 40%|████      | 10872/26933 [12:41<15:15, 17.54it/s] 40%|████      | 10876/26933 [12:42<15:16, 17.52it/s] 40%|████      | 10880/26933 [12:42<15:15, 17.54it/s] 40%|████      | 10884/26933 [12:42<15:15, 17.53it/s] 40%|████      | 10888/26933 [12:42<15:12, 17.57it/s] 40%|████      | 10892/26933 [12:42<15:13, 17.57it/s] 40%|████      | 10896/26933 [12:43<15:12, 17.58it/s] 40%|████      | 10900/26933 [12:43<15:10, 17.61it/s] 40%|████      | 10904/26933 [12:43<15:09, 17.63it/s] 41%|████      | 10908/26933 [12:43<15:07, 17.65it/s] 41%|████      | 10912/26933 [12:44<15:08, 17.64it/s] 41%|████      | 10916/26933 [12:44<15:06, 17.68it/s] 41%|████      | 10920/26933 [12:44<15:03, 17.72it/s] 41%|████      | 10924/26933 [12:44<15:05, 17.69it/s] 41%|████      | 10928/26933 [12:45<15:05, 17.68it/s] 41%|████      | 10932/26933 [12:45<15:06, 17.66it/s] 41%|████      | 10936/26933 [12:45<15:06, 17.65it/s] 41%|████      | 10940/26933 [12:45<15:05, 17.67it/s] 41%|████      | 10944/26933 [12:45<15:05, 17.67it/s] 41%|████      | 10948/26933 [12:46<15:04, 17.67it/s] 41%|████      | 10952/26933 [12:46<15:02, 17.71it/s] 41%|████      | 10956/26933 [12:46<15:03, 17.69it/s] 41%|████      | 10960/26933 [12:46<15:01, 17.72it/s] 41%|████      | 10964/26933 [12:47<15:01, 17.71it/s] 41%|████      | 10968/26933 [12:47<15:00, 17.74it/s] 41%|████      | 10972/26933 [12:47<15:01, 17.70it/s] 41%|████      | 10976/26933 [12:47<15:02, 17.69it/s] 41%|████      | 10980/26933 [12:47<15:01, 17.70it/s] 41%|████      | 10984/26933 [12:48<15:06, 17.60it/s] 41%|████      | 10988/26933 [12:48<15:07, 17.58it/s] 41%|████      | 10992/26933 [12:48<15:06, 17.58it/s] 41%|████      | 10996/26933 [12:48<15:06, 17.58it/s] 41%|████      | 11000/26933 [12:49<15:07, 17.56it/s] 41%|████      | 11004/26933 [12:49<15:07, 17.55it/s] 41%|████      | 11008/26933 [12:49<15:06, 17.56it/s] 41%|████      | 11012/26933 [12:49<15:05, 17.58it/s] 41%|████      | 11016/26933 [12:50<15:07, 17.54it/s] 41%|████      | 11020/26933 [12:50<15:06, 17.56it/s] 41%|████      | 11024/26933 [12:50<15:03, 17.61it/s] 41%|████      | 11028/26933 [12:50<15:00, 17.66it/s] 41%|████      | 11032/26933 [12:50<14:57, 17.71it/s] 41%|████      | 11036/26933 [12:51<15:00, 17.65it/s] 41%|████      | 11040/26933 [12:51<15:00, 17.65it/s] 41%|████      | 11044/26933 [12:51<14:58, 17.69it/s] 41%|████      | 11048/26933 [12:51<14:56, 17.72it/s] 41%|████      | 11052/26933 [12:52<15:02, 17.60it/s] 41%|████      | 11056/26933 [12:52<15:01, 17.61it/s] 41%|████      | 11060/26933 [12:52<15:01, 17.60it/s] 41%|████      | 11064/26933 [12:52<15:02, 17.59it/s] 41%|████      | 11068/26933 [12:52<15:01, 17.60it/s] 41%|████      | 11072/26933 [12:53<15:01, 17.59it/s] 41%|████      | 11076/26933 [12:53<14:59, 17.63it/s] 41%|████      | 11080/26933 [12:53<15:00, 17.61it/s] 41%|████      | 11084/26933 [12:53<15:00, 17.60it/s] 41%|████      | 11088/26933 [12:54<15:02, 17.56it/s] 41%|████      | 11092/26933 [12:54<15:02, 17.56it/s] 41%|████      | 11096/26933 [12:54<15:03, 17.53it/s] 41%|████      | 11100/26933 [12:54<15:00, 17.59it/s] 41%|████      | 11104/26933 [12:55<14:58, 17.62it/s] 41%|████      | 11108/26933 [12:55<14:59, 17.59it/s] 41%|████▏     | 11112/26933 [12:55<15:00, 17.57it/s] 41%|████▏     | 11116/26933 [12:55<15:00, 17.57it/s] 41%|████▏     | 11120/26933 [12:55<15:02, 17.52it/s] 41%|████▏     | 11124/26933 [12:56<15:03, 17.50it/s] 41%|████▏     | 11128/26933 [12:56<15:03, 17.48it/s] 41%|████▏     | 11132/26933 [12:56<15:02, 17.51it/s] 41%|████▏     | 11136/26933 [12:56<14:59, 17.56it/s] 41%|████▏     | 11140/26933 [12:57<15:00, 17.54it/s] 41%|████▏     | 11144/26933 [12:57<14:59, 17.56it/s] 41%|████▏     | 11148/26933 [12:57<14:59, 17.55it/s] 41%|████▏     | 11152/26933 [12:57<14:57, 17.59it/s] 41%|████▏     | 11156/26933 [12:57<14:55, 17.61it/s] 41%|████▏     | 11160/26933 [12:58<14:56, 17.59it/s] 41%|████▏     | 11164/26933 [12:58<14:56, 17.59it/s] 41%|████▏     | 11168/26933 [12:58<14:55, 17.61it/s] 41%|████▏     | 11172/26933 [12:58<14:54, 17.62it/s] 41%|████▏     | 11176/26933 [12:59<14:55, 17.60it/s] 42%|████▏     | 11180/26933 [12:59<14:54, 17.61it/s] 42%|████▏     | 11184/26933 [12:59<14:54, 17.60it/s] 42%|████▏     | 11188/26933 [12:59<14:55, 17.58it/s] 42%|████▏     | 11192/26933 [13:00<14:56, 17.57it/s] 42%|████▏     | 11196/26933 [13:00<14:57, 17.54it/s] 42%|████▏     | 11200/26933 [13:00<14:56, 17.55it/s] 42%|████▏     | 11204/26933 [13:00<14:59, 17.49it/s] 42%|████▏     | 11208/26933 [13:00<14:58, 17.51it/s] 42%|████▏     | 11212/26933 [13:01<14:58, 17.49it/s] 42%|████▏     | 11216/26933 [13:01<14:58, 17.49it/s] 42%|████▏     | 11220/26933 [13:01<14:57, 17.51it/s] 42%|████▏     | 11224/26933 [13:01<14:56, 17.52it/s] 42%|████▏     | 11228/26933 [13:02<14:56, 17.52it/s] 42%|████▏     | 11232/26933 [13:02<14:54, 17.55it/s] 42%|████▏     | 11236/26933 [13:02<14:52, 17.58it/s] 42%|████▏     | 11240/26933 [13:02<14:51, 17.60it/s] 42%|████▏     | 11244/26933 [13:02<14:50, 17.62it/s] 42%|████▏     | 11248/26933 [13:03<14:54, 17.54it/s] 42%|████▏     | 11252/26933 [13:03<14:51, 17.59it/s] 42%|████▏     | 11256/26933 [13:03<14:49, 17.63it/s] 42%|████▏     | 11260/26933 [13:03<14:49, 17.61it/s] 42%|████▏     | 11264/26933 [13:04<14:49, 17.62it/s] 42%|████▏     | 11268/26933 [13:04<14:48, 17.62it/s] 42%|████▏     | 11272/26933 [13:04<14:48, 17.63it/s] 42%|████▏     | 11276/26933 [13:04<14:47, 17.65it/s] 42%|████▏     | 11280/26933 [13:05<14:46, 17.65it/s] 42%|████▏     | 11284/26933 [13:05<14:47, 17.64it/s] 42%|████▏     | 11288/26933 [13:05<14:47, 17.63it/s] 42%|████▏     | 11292/26933 [13:05<14:48, 17.61it/s] 42%|████▏     | 11296/26933 [13:05<14:49, 17.58it/s] 42%|████▏     | 11300/26933 [13:06<14:51, 17.54it/s] 42%|████▏     | 11304/26933 [13:06<14:51, 17.54it/s] 42%|████▏     | 11308/26933 [13:06<14:49, 17.57it/s] 42%|████▏     | 11312/26933 [13:06<14:49, 17.56it/s] 42%|████▏     | 11316/26933 [13:07<14:54, 17.45it/s] 42%|████▏     | 11320/26933 [13:07<14:52, 17.49it/s] 42%|████▏     | 11324/26933 [13:07<14:52, 17.49it/s] 42%|████▏     | 11328/26933 [13:07<14:54, 17.44it/s] 42%|████▏     | 11332/26933 [13:08<14:54, 17.45it/s] 42%|████▏     | 11336/26933 [13:08<14:49, 17.54it/s] 42%|████▏     | 11340/26933 [13:08<14:44, 17.64it/s] 42%|████▏     | 11344/26933 [13:08<14:41, 17.69it/s] 42%|████▏     | 11348/26933 [13:08<14:38, 17.74it/s] 42%|████▏     | 11352/26933 [13:09<14:38, 17.73it/s] 42%|████▏     | 11356/26933 [13:09<14:36, 17.77it/s] 42%|████▏     | 11360/26933 [13:09<14:34, 17.80it/s] 42%|████▏     | 11364/26933 [13:09<14:32, 17.85it/s] 42%|████▏     | 11368/26933 [13:10<14:30, 17.87it/s] 42%|████▏     | 11372/26933 [13:10<14:32, 17.83it/s] 42%|████▏     | 11376/26933 [13:10<14:32, 17.83it/s] 42%|████▏     | 11380/26933 [13:10<14:33, 17.80it/s] 42%|████▏     | 11384/26933 [13:10<14:34, 17.78it/s] 42%|████▏     | 11388/26933 [13:11<14:35, 17.75it/s] 42%|████▏     | 11392/26933 [13:11<14:35, 17.75it/s] 42%|████▏     | 11396/26933 [13:11<14:36, 17.73it/s] 42%|████▏     | 11400/26933 [13:11<14:34, 17.75it/s] 42%|████▏     | 11404/26933 [13:12<14:36, 17.72it/s] 42%|████▏     | 11408/26933 [13:12<14:35, 17.73it/s] 42%|████▏     | 11412/26933 [13:12<14:35, 17.74it/s] 42%|████▏     | 11416/26933 [13:12<14:34, 17.74it/s] 42%|████▏     | 11420/26933 [13:12<14:38, 17.65it/s] 42%|████▏     | 11424/26933 [13:13<14:43, 17.55it/s] 42%|████▏     | 11428/26933 [13:13<14:44, 17.52it/s] 42%|████▏     | 11432/26933 [13:13<14:44, 17.52it/s] 42%|████▏     | 11436/26933 [13:13<14:43, 17.54it/s] 42%|████▏     | 11440/26933 [13:14<14:47, 17.47it/s] 42%|████▏     | 11444/26933 [13:14<14:47, 17.45it/s] 43%|████▎     | 11448/26933 [13:14<14:45, 17.48it/s] 43%|████▎     | 11452/26933 [13:14<14:44, 17.50it/s] 43%|████▎     | 11456/26933 [13:15<14:44, 17.49it/s] 43%|████▎     | 11460/26933 [13:15<14:47, 17.44it/s] 43%|████▎     | 11464/26933 [13:15<14:46, 17.45it/s] 43%|████▎     | 11468/26933 [13:15<14:46, 17.44it/s] 43%|████▎     | 11472/26933 [13:15<14:45, 17.46it/s] 43%|████▎     | 11476/26933 [13:16<14:47, 17.41it/s] 43%|████▎     | 11480/26933 [13:16<14:45, 17.44it/s] 43%|████▎     | 11484/26933 [13:16<14:45, 17.45it/s] 43%|████▎     | 11488/26933 [13:16<14:42, 17.51it/s] 43%|████▎     | 11492/26933 [13:17<14:43, 17.49it/s] 43%|████▎     | 11496/26933 [13:17<14:38, 17.57it/s] 43%|████▎     | 11500/26933 [13:17<14:38, 17.56it/s] 43%|████▎     | 11504/26933 [13:17<14:40, 17.52it/s] 43%|████▎     | 11508/26933 [13:17<14:41, 17.50it/s] 43%|████▎     | 11512/26933 [13:18<14:44, 17.44it/s] 43%|████▎     | 11516/26933 [13:18<14:43, 17.46it/s] 43%|████▎     | 11520/26933 [13:18<14:42, 17.46it/s] 43%|████▎     | 11524/26933 [13:18<14:42, 17.47it/s] 43%|████▎     | 11528/26933 [13:19<14:42, 17.45it/s] 43%|████▎     | 11532/26933 [13:19<14:42, 17.45it/s] 43%|████▎     | 11536/26933 [13:19<14:38, 17.52it/s] 43%|████▎     | 11540/26933 [13:19<14:50, 17.29it/s] 43%|████▎     | 11544/26933 [13:20<14:47, 17.33it/s] 43%|████▎     | 11548/26933 [13:20<14:43, 17.42it/s] 43%|████▎     | 11552/26933 [13:20<14:39, 17.50it/s] 43%|████▎     | 11556/26933 [13:20<14:37, 17.52it/s] 43%|████▎     | 11560/26933 [13:20<14:35, 17.56it/s] 43%|████▎     | 11564/26933 [13:21<14:35, 17.55it/s] 43%|████▎     | 11568/26933 [13:21<14:34, 17.57it/s] 43%|████▎     | 11572/26933 [13:21<14:36, 17.53it/s] 43%|████▎     | 11576/26933 [13:21<14:39, 17.47it/s] 43%|████▎     | 11580/26933 [13:22<14:42, 17.41it/s] 43%|████▎     | 11584/26933 [13:22<14:45, 17.34it/s] 43%|████▎     | 11588/26933 [13:22<14:44, 17.35it/s] 43%|████▎     | 11592/26933 [13:22<14:39, 17.44it/s] 43%|████▎     | 11596/26933 [13:23<14:36, 17.49it/s] 43%|████▎     | 11600/26933 [13:23<14:35, 17.51it/s] 43%|████▎     | 11604/26933 [13:23<14:32, 17.57it/s] 43%|████▎     | 11608/26933 [13:23<14:29, 17.62it/s] 43%|████▎     | 11612/26933 [13:23<14:26, 17.67it/s] 43%|████▎     | 11616/26933 [13:24<14:26, 17.68it/s] 43%|████▎     | 11620/26933 [13:24<14:23, 17.73it/s] 43%|████▎     | 11624/26933 [13:24<14:22, 17.76it/s] 43%|████▎     | 11628/26933 [13:24<14:21, 17.76it/s] 43%|████▎     | 11632/26933 [13:25<14:24, 17.71it/s] 43%|████▎     | 11636/26933 [13:25<14:24, 17.70it/s] 43%|████▎     | 11640/26933 [13:25<14:23, 17.71it/s] 43%|████▎     | 11644/26933 [13:25<14:24, 17.69it/s] 43%|████▎     | 11648/26933 [13:25<14:25, 17.65it/s] 43%|████▎     | 11652/26933 [13:26<14:29, 17.58it/s] 43%|████▎     | 11656/26933 [13:26<14:28, 17.58it/s] 43%|████▎     | 11660/26933 [13:26<14:27, 17.60it/s] 43%|████▎     | 11664/26933 [13:26<14:28, 17.58it/s] 43%|████▎     | 11668/26933 [13:27<14:31, 17.52it/s] 43%|████▎     | 11672/26933 [13:27<14:29, 17.55it/s] 43%|████▎     | 11676/26933 [13:27<14:29, 17.55it/s] 43%|████▎     | 11680/26933 [13:27<14:30, 17.52it/s] 43%|████▎     | 11684/26933 [13:28<14:28, 17.55it/s] 43%|████▎     | 11688/26933 [13:28<14:28, 17.54it/s] 43%|████▎     | 11692/26933 [13:28<14:27, 17.57it/s] 43%|████▎     | 11696/26933 [13:28<14:28, 17.55it/s] 43%|████▎     | 11700/26933 [13:28<14:28, 17.55it/s] 43%|████▎     | 11704/26933 [13:29<14:29, 17.52it/s] 43%|████▎     | 11708/26933 [13:29<14:29, 17.52it/s] 43%|████▎     | 11712/26933 [13:29<14:26, 17.56it/s] 44%|████▎     | 11716/26933 [13:29<14:26, 17.55it/s] 44%|████▎     | 11720/26933 [13:30<14:26, 17.55it/s] 44%|████▎     | 11724/26933 [13:30<14:25, 17.57it/s] 44%|████▎     | 11728/26933 [13:30<14:26, 17.55it/s] 44%|████▎     | 11732/26933 [13:30<14:28, 17.51it/s] 44%|████▎     | 11736/26933 [13:30<14:27, 17.53it/s] 44%|████▎     | 11740/26933 [13:31<14:27, 17.51it/s] 44%|████▎     | 11744/26933 [13:31<14:28, 17.49it/s] 44%|████▎     | 11748/26933 [13:31<14:29, 17.46it/s] 44%|████▎     | 11752/26933 [13:31<14:30, 17.44it/s] 44%|████▎     | 11756/26933 [13:32<14:31, 17.41it/s] 44%|████▎     | 11760/26933 [13:32<14:30, 17.43it/s] 44%|████▎     | 11764/26933 [13:32<14:27, 17.49it/s] 44%|████▎     | 11768/26933 [13:32<14:25, 17.51it/s] 44%|████▎     | 11772/26933 [13:33<14:29, 17.45it/s] 44%|████▎     | 11776/26933 [13:33<14:27, 17.48it/s] 44%|████▎     | 11780/26933 [13:33<14:26, 17.50it/s] 44%|████▍     | 11784/26933 [13:33<14:27, 17.47it/s] 44%|████▍     | 11788/26933 [13:33<14:26, 17.47it/s] 44%|████▍     | 11792/26933 [13:34<14:30, 17.39it/s] 44%|████▍     | 11796/26933 [13:34<14:27, 17.46it/s] 44%|████▍     | 11800/26933 [13:34<14:27, 17.45it/s] 44%|████▍     | 11804/26933 [13:34<14:24, 17.49it/s] 44%|████▍     | 11808/26933 [13:35<14:27, 17.44it/s] 44%|████▍     | 11812/26933 [13:35<14:24, 17.49it/s] 44%|████▍     | 11816/26933 [13:35<14:23, 17.50it/s] 44%|████▍     | 11820/26933 [13:35<14:22, 17.53it/s] 44%|████▍     | 11824/26933 [13:36<14:23, 17.49it/s] 44%|████▍     | 11828/26933 [13:36<14:23, 17.49it/s] 44%|████▍     | 11832/26933 [13:36<14:23, 17.50it/s] 44%|████▍     | 11836/26933 [13:36<14:22, 17.51it/s] 44%|████▍     | 11840/26933 [13:36<14:22, 17.50it/s] 44%|████▍     | 11844/26933 [13:37<14:23, 17.48it/s] 44%|████▍     | 11848/26933 [13:37<14:19, 17.55it/s] 44%|████▍     | 11852/26933 [13:37<14:18, 17.57it/s] 44%|████▍     | 11856/26933 [13:37<14:17, 17.57it/s] 44%|████▍     | 11860/26933 [13:38<14:20, 17.52it/s] 44%|████▍     | 11864/26933 [13:38<14:17, 17.58it/s] 44%|████▍     | 11868/26933 [13:38<14:15, 17.61it/s] 44%|████▍     | 11872/26933 [13:38<14:15, 17.60it/s] 44%|████▍     | 11876/26933 [13:38<14:17, 17.56it/s] 44%|████▍     | 11880/26933 [13:39<14:18, 17.53it/s] 44%|████▍     | 11884/26933 [13:39<14:17, 17.54it/s] 44%|████▍     | 11888/26933 [13:39<14:16, 17.57it/s] 44%|████▍     | 11892/26933 [13:39<14:15, 17.57it/s] 44%|████▍     | 11896/26933 [13:40<14:18, 17.52it/s] 44%|████▍     | 11900/26933 [13:40<14:16, 17.56it/s] 44%|████▍     | 11904/26933 [13:40<14:15, 17.58it/s] 44%|████▍     | 11908/26933 [13:40<14:14, 17.58it/s] 44%|████▍     | 11912/26933 [13:41<14:14, 17.57it/s] 44%|████▍     | 11916/26933 [13:41<14:18, 17.50it/s] 44%|████▍     | 11920/26933 [13:41<14:16, 17.53it/s] 44%|████▍     | 11924/26933 [13:41<14:15, 17.54it/s] 44%|████▍     | 11928/26933 [13:41<14:17, 17.50it/s] 44%|████▍     | 11932/26933 [13:42<14:16, 17.51it/s] 44%|████▍     | 11936/26933 [13:42<14:15, 17.53it/s] 44%|████▍     | 11940/26933 [13:42<14:13, 17.56it/s] 44%|████▍     | 11944/26933 [13:42<14:18, 17.47it/s] 44%|████▍     | 11948/26933 [13:43<14:19, 17.43it/s] 44%|████▍     | 11952/26933 [13:43<14:19, 17.43it/s] 44%|████▍     | 11956/26933 [13:43<14:19, 17.43it/s] 44%|████▍     | 11960/26933 [13:43<14:17, 17.45it/s] 44%|████▍     | 11964/26933 [13:44<14:17, 17.45it/s] 44%|████▍     | 11968/26933 [13:44<14:19, 17.42it/s] 44%|████▍     | 11972/26933 [13:44<14:14, 17.50it/s] 44%|████▍     | 11976/26933 [13:44<14:12, 17.54it/s] 44%|████▍     | 11980/26933 [13:44<14:10, 17.58it/s] 44%|████▍     | 11984/26933 [13:45<14:10, 17.58it/s] 45%|████▍     | 11988/26933 [13:45<14:08, 17.62it/s] 45%|████▍     | 11992/26933 [13:45<14:06, 17.66it/s] 45%|████▍     | 11996/26933 [13:45<13:59, 17.78it/s] 45%|████▍     | 12000/26933 [13:46<13:57, 17.84it/s] 45%|████▍     | 12004/26933 [13:46<13:52, 17.93it/s] 45%|████▍     | 12008/26933 [13:46<13:48, 18.01it/s] 45%|████▍     | 12012/26933 [13:46<13:48, 18.01it/s] 45%|████▍     | 12016/26933 [13:46<13:46, 18.05it/s] 45%|████▍     | 12020/26933 [13:47<13:46, 18.05it/s] 45%|████▍     | 12024/26933 [13:47<13:43, 18.09it/s] 45%|████▍     | 12028/26933 [13:47<13:42, 18.11it/s] 45%|████▍     | 12032/26933 [13:47<13:41, 18.13it/s] 45%|████▍     | 12036/26933 [13:48<13:41, 18.13it/s] 45%|████▍     | 12040/26933 [13:48<13:45, 18.05it/s] 45%|████▍     | 12044/26933 [13:48<13:41, 18.11it/s] 45%|████▍     | 12048/26933 [13:48<13:42, 18.09it/s] 45%|████▍     | 12052/26933 [13:48<13:45, 18.02it/s] 45%|████▍     | 12056/26933 [13:49<13:46, 18.01it/s] 45%|████▍     | 12060/26933 [13:49<13:43, 18.05it/s] 45%|████▍     | 12064/26933 [13:49<13:42, 18.08it/s] 45%|████▍     | 12068/26933 [13:49<13:41, 18.09it/s] 45%|████▍     | 12072/26933 [13:50<13:39, 18.12it/s] 45%|████▍     | 12076/26933 [13:50<13:41, 18.09it/s] 45%|████▍     | 12080/26933 [13:50<13:43, 18.04it/s] 45%|████▍     | 12084/26933 [13:50<13:49, 17.91it/s] 45%|████▍     | 12088/26933 [13:50<13:52, 17.83it/s] 45%|████▍     | 12092/26933 [13:51<13:55, 17.76it/s] 45%|████▍     | 12096/26933 [13:51<13:55, 17.76it/s] 45%|████▍     | 12100/26933 [13:51<13:54, 17.78it/s] 45%|████▍     | 12104/26933 [13:51<13:51, 17.83it/s] 45%|████▍     | 12108/26933 [13:52<13:54, 17.76it/s] 45%|████▍     | 12112/26933 [13:52<13:55, 17.74it/s] 45%|████▍     | 12116/26933 [13:52<13:55, 17.74it/s] 45%|████▌     | 12120/26933 [13:52<13:55, 17.73it/s] 45%|████▌     | 12124/26933 [13:52<13:55, 17.73it/s] 45%|████▌     | 12128/26933 [13:53<13:57, 17.67it/s] 45%|████▌     | 12132/26933 [13:53<13:57, 17.68it/s] 45%|████▌     | 12136/26933 [13:53<13:55, 17.71it/s] 45%|████▌     | 12140/26933 [13:53<13:55, 17.72it/s] 45%|████▌     | 12144/26933 [13:54<13:57, 17.66it/s] 45%|████▌     | 12148/26933 [13:54<13:56, 17.68it/s] 45%|████▌     | 12152/26933 [13:54<13:55, 17.70it/s] 45%|████▌     | 12156/26933 [13:54<13:54, 17.70it/s] 45%|████▌     | 12160/26933 [13:54<13:54, 17.70it/s] 45%|████▌     | 12164/26933 [13:55<13:57, 17.63it/s] 45%|████▌     | 12168/26933 [13:55<13:59, 17.59it/s] 45%|████▌     | 12172/26933 [13:55<13:59, 17.57it/s] 45%|████▌     | 12176/26933 [13:55<13:58, 17.61it/s] 45%|████▌     | 12180/26933 [13:56<13:59, 17.57it/s] 45%|████▌     | 12184/26933 [13:56<13:58, 17.60it/s] 45%|████▌     | 12188/26933 [13:56<13:55, 17.64it/s] 45%|████▌     | 12192/26933 [13:56<13:54, 17.67it/s] 45%|████▌     | 12196/26933 [13:57<13:54, 17.67it/s] 45%|████▌     | 12200/26933 [13:57<13:55, 17.64it/s] 45%|████▌     | 12204/26933 [13:57<13:53, 17.67it/s] 45%|████▌     | 12208/26933 [13:57<13:54, 17.65it/s] 45%|████▌     | 12212/26933 [13:57<13:56, 17.60it/s] 45%|████▌     | 12216/26933 [13:58<14:02, 17.47it/s] 45%|████▌     | 12220/26933 [13:58<14:01, 17.49it/s] 45%|████▌     | 12224/26933 [13:58<14:02, 17.46it/s] 45%|████▌     | 12228/26933 [13:58<14:02, 17.45it/s] 45%|████▌     | 12232/26933 [13:59<14:01, 17.46it/s] 45%|████▌     | 12236/26933 [13:59<14:00, 17.49it/s] 45%|████▌     | 12240/26933 [13:59<13:59, 17.51it/s] 45%|████▌     | 12244/26933 [13:59<13:59, 17.50it/s] 45%|████▌     | 12248/26933 [14:00<13:57, 17.54it/s] 45%|████▌     | 12252/26933 [14:00<13:58, 17.50it/s] 46%|████▌     | 12256/26933 [14:00<13:55, 17.58it/s] 46%|████▌     | 12260/26933 [14:00<13:52, 17.63it/s] 46%|████▌     | 12264/26933 [14:00<13:50, 17.66it/s] 46%|████▌     | 12268/26933 [14:01<13:51, 17.63it/s] 46%|████▌     | 12272/26933 [14:01<13:50, 17.65it/s] 46%|████▌     | 12276/26933 [14:01<13:52, 17.60it/s] 46%|████▌     | 12280/26933 [14:01<13:52, 17.60it/s] 46%|████▌     | 12284/26933 [14:02<13:55, 17.54it/s] 46%|████▌     | 12288/26933 [14:02<13:52, 17.59it/s] 46%|████▌     | 12292/26933 [14:02<13:50, 17.64it/s] 46%|████▌     | 12296/26933 [14:02<13:47, 17.68it/s] 46%|████▌     | 12300/26933 [14:02<13:46, 17.69it/s] 46%|████▌     | 12304/26933 [14:03<13:47, 17.68it/s] 46%|████▌     | 12308/26933 [14:03<13:46, 17.69it/s] 46%|████▌     | 12312/26933 [14:03<13:45, 17.71it/s] 46%|████▌     | 12316/26933 [14:03<13:46, 17.69it/s] 46%|████▌     | 12320/26933 [14:04<13:46, 17.67it/s] 46%|████▌     | 12324/26933 [14:04<13:44, 17.72it/s] 46%|████▌     | 12328/26933 [14:04<13:43, 17.73it/s] 46%|████▌     | 12332/26933 [14:04<13:43, 17.72it/s] 46%|████▌     | 12336/26933 [14:04<13:44, 17.70it/s] 46%|████▌     | 12340/26933 [14:05<13:47, 17.63it/s] 46%|████▌     | 12344/26933 [14:05<13:46, 17.65it/s] 46%|████▌     | 12348/26933 [14:05<13:47, 17.63it/s] 46%|████▌     | 12352/26933 [14:05<13:52, 17.51it/s] 46%|████▌     | 12356/26933 [14:06<13:56, 17.43it/s] 46%|████▌     | 12360/26933 [14:06<13:55, 17.45it/s] 46%|████▌     | 12364/26933 [14:06<13:54, 17.45it/s] 46%|████▌     | 12368/26933 [14:06<13:52, 17.50it/s] 46%|████▌     | 12372/26933 [14:07<13:53, 17.46it/s] 46%|████▌     | 12376/26933 [14:07<13:52, 17.48it/s] 46%|████▌     | 12380/26933 [14:07<13:51, 17.49it/s] 46%|████▌     | 12384/26933 [14:07<13:50, 17.51it/s] 46%|████▌     | 12388/26933 [14:07<13:50, 17.52it/s] 46%|████▌     | 12392/26933 [14:08<13:52, 17.47it/s] 46%|████▌     | 12396/26933 [14:08<13:50, 17.51it/s] 46%|████▌     | 12400/26933 [14:08<13:49, 17.53it/s] 46%|████▌     | 12404/26933 [14:08<13:48, 17.54it/s] 46%|████▌     | 12408/26933 [14:09<13:50, 17.48it/s] 46%|████▌     | 12412/26933 [14:09<13:49, 17.51it/s] 46%|████▌     | 12416/26933 [14:09<13:47, 17.53it/s] 46%|████▌     | 12420/26933 [14:09<13:46, 17.57it/s] 46%|████▌     | 12424/26933 [14:10<13:46, 17.55it/s] 46%|████▌     | 12428/26933 [14:10<13:48, 17.51it/s] 46%|████▌     | 12432/26933 [14:10<13:47, 17.52it/s] 46%|████▌     | 12436/26933 [14:10<13:48, 17.51it/s] 46%|████▌     | 12440/26933 [14:10<13:48, 17.50it/s] 46%|████▌     | 12444/26933 [14:11<13:46, 17.54it/s] 46%|████▌     | 12448/26933 [14:11<13:43, 17.60it/s] 46%|████▌     | 12452/26933 [14:11<13:41, 17.63it/s] 46%|████▌     | 12456/26933 [14:11<13:40, 17.65it/s] 46%|████▋     | 12460/26933 [14:12<13:40, 17.64it/s] 46%|████▋     | 12464/26933 [14:12<13:37, 17.70it/s] 46%|████▋     | 12468/26933 [14:12<13:38, 17.68it/s] 46%|████▋     | 12472/26933 [14:12<13:39, 17.65it/s] 46%|████▋     | 12476/26933 [14:12<13:40, 17.63it/s] 46%|████▋     | 12480/26933 [14:13<13:41, 17.59it/s] 46%|████▋     | 12484/26933 [14:13<13:42, 17.57it/s] 46%|████▋     | 12488/26933 [14:13<13:43, 17.54it/s] 46%|████▋     | 12492/26933 [14:13<13:43, 17.53it/s] 46%|████▋     | 12496/26933 [14:14<13:43, 17.53it/s] 46%|████▋     | 12500/26933 [14:14<13:42, 17.55it/s] 46%|████▋     | 12504/26933 [14:14<13:41, 17.55it/s] 46%|████▋     | 12508/26933 [14:14<13:40, 17.58it/s] 46%|████▋     | 12512/26933 [14:15<13:38, 17.63it/s] 46%|████▋     | 12516/26933 [14:15<13:41, 17.55it/s] 46%|████▋     | 12520/26933 [14:15<13:40, 17.56it/s] 47%|████▋     | 12524/26933 [14:15<13:43, 17.51it/s] 47%|████▋     | 12528/26933 [14:15<13:45, 17.46it/s] 47%|████▋     | 12532/26933 [14:16<13:46, 17.42it/s] 47%|████▋     | 12536/26933 [14:16<13:44, 17.46it/s] 47%|████▋     | 12540/26933 [14:16<13:42, 17.49it/s] 47%|████▋     | 12544/26933 [14:16<13:43, 17.47it/s] 47%|████▋     | 12548/26933 [14:17<13:43, 17.46it/s] 47%|████▋     | 12552/26933 [14:17<13:44, 17.45it/s] 47%|████▋     | 12556/26933 [14:17<13:43, 17.45it/s] 47%|████▋     | 12560/26933 [14:17<13:42, 17.47it/s] 47%|████▋     | 12564/26933 [14:18<13:40, 17.50it/s] 47%|████▋     | 12568/26933 [14:18<13:41, 17.49it/s] 47%|████▋     | 12572/26933 [14:18<13:40, 17.51it/s] 47%|████▋     | 12576/26933 [14:18<13:39, 17.53it/s] 47%|████▋     | 12580/26933 [14:18<13:38, 17.54it/s] 47%|████▋     | 12584/26933 [14:19<13:39, 17.52it/s] 47%|████▋     | 12588/26933 [14:19<13:38, 17.52it/s] 47%|████▋     | 12592/26933 [14:19<13:36, 17.56it/s] 47%|████▋     | 12596/26933 [14:19<13:34, 17.60it/s] 47%|████▋     | 12600/26933 [14:20<13:35, 17.58it/s] 47%|████▋     | 12604/26933 [14:20<13:35, 17.58it/s] 47%|████▋     | 12608/26933 [14:20<13:34, 17.58it/s] 47%|████▋     | 12612/26933 [14:20<13:35, 17.55it/s] 47%|████▋     | 12616/26933 [14:20<13:35, 17.56it/s] 47%|████▋     | 12620/26933 [14:21<13:43, 17.37it/s] 47%|████▋     | 12624/26933 [14:21<13:42, 17.40it/s] 47%|████▋     | 12628/26933 [14:21<13:40, 17.43it/s] 47%|████▋     | 12632/26933 [14:21<13:38, 17.47it/s] 47%|████▋     | 12636/26933 [14:22<13:41, 17.41it/s] 47%|████▋     | 12640/26933 [14:22<13:39, 17.44it/s] 47%|████▋     | 12644/26933 [14:22<13:38, 17.46it/s] 47%|████▋     | 12648/26933 [14:22<13:52, 17.17it/s] 47%|████▋     | 12652/26933 [14:23<13:46, 17.29it/s] 47%|████▋     | 12656/26933 [14:23<13:43, 17.33it/s] 47%|████▋     | 12660/26933 [14:23<13:40, 17.39it/s] 47%|████▋     | 12664/26933 [14:23<13:39, 17.41it/s] 47%|████▋     | 12668/26933 [14:23<13:39, 17.41it/s] 47%|████▋     | 12672/26933 [14:24<13:39, 17.41it/s] 47%|████▋     | 12676/26933 [14:24<13:35, 17.49it/s] 47%|████▋     | 12680/26933 [14:24<13:33, 17.51it/s] 47%|████▋     | 12684/26933 [14:24<13:32, 17.54it/s] 47%|████▋     | 12688/26933 [14:25<13:33, 17.52it/s] 47%|████▋     | 12692/26933 [14:25<13:30, 17.57it/s] 47%|████▋     | 12696/26933 [14:25<13:28, 17.61it/s] 47%|████▋     | 12700/26933 [14:25<13:29, 17.57it/s] 47%|████▋     | 12704/26933 [14:26<13:34, 17.47it/s] 47%|████▋     | 12708/26933 [14:26<13:35, 17.44it/s] 47%|████▋     | 12712/26933 [14:26<13:39, 17.35it/s] 47%|████▋     | 12716/26933 [14:26<13:50, 17.13it/s] 47%|████▋     | 12720/26933 [14:26<13:43, 17.26it/s] 47%|████▋     | 12724/26933 [14:27<13:39, 17.33it/s] 47%|████▋     | 12728/26933 [14:27<13:34, 17.44it/s] 47%|████▋     | 12732/26933 [14:27<13:39, 17.33it/s] 47%|████▋     | 12736/26933 [14:27<13:35, 17.41it/s] 47%|████▋     | 12740/26933 [14:28<13:33, 17.45it/s] 47%|████▋     | 12744/26933 [14:28<13:29, 17.53it/s] 47%|████▋     | 12748/26933 [14:28<13:26, 17.59it/s] 47%|████▋     | 12752/26933 [14:28<13:25, 17.60it/s] 47%|████▋     | 12756/26933 [14:28<13:25, 17.61it/s] 47%|████▋     | 12760/26933 [14:29<13:25, 17.60it/s] 47%|████▋     | 12764/26933 [14:29<13:21, 17.67it/s] 47%|████▋     | 12768/26933 [14:29<13:22, 17.65it/s] 47%|████▋     | 12772/26933 [14:29<13:21, 17.67it/s] 47%|████▋     | 12776/26933 [14:30<13:23, 17.63it/s] 47%|████▋     | 12780/26933 [14:30<13:20, 17.68it/s] 47%|████▋     | 12784/26933 [14:30<13:21, 17.66it/s] 47%|████▋     | 12788/26933 [14:30<13:20, 17.66it/s] 47%|████▋     | 12792/26933 [14:31<13:20, 17.67it/s] 48%|████▊     | 12796/26933 [14:31<13:21, 17.63it/s] 48%|████▊     | 12800/26933 [14:31<13:18, 17.69it/s] 48%|████▊     | 12804/26933 [14:31<13:20, 17.66it/s] 48%|████▊     | 12808/26933 [14:31<13:20, 17.65it/s] 48%|████▊     | 12812/26933 [14:32<13:21, 17.62it/s] 48%|████▊     | 12816/26933 [14:32<13:17, 17.69it/s] 48%|████▊     | 12820/26933 [14:32<13:16, 17.72it/s] 48%|████▊     | 12824/26933 [14:32<13:15, 17.74it/s] 48%|████▊     | 12828/26933 [14:33<13:17, 17.69it/s] 48%|████▊     | 12832/26933 [14:33<13:15, 17.73it/s] 48%|████▊     | 12836/26933 [14:33<13:14, 17.75it/s] 48%|████▊     | 12840/26933 [14:33<13:14, 17.75it/s] 48%|████▊     | 12844/26933 [14:33<13:13, 17.75it/s] 48%|████▊     | 12848/26933 [14:34<13:14, 17.73it/s] 48%|████▊     | 12852/26933 [14:34<13:12, 17.77it/s] 48%|████▊     | 12856/26933 [14:34<13:10, 17.80it/s] 48%|████▊     | 12860/26933 [14:34<13:09, 17.83it/s] 48%|████▊     | 12864/26933 [14:35<13:10, 17.81it/s] 48%|████▊     | 12868/26933 [14:35<13:10, 17.79it/s] 48%|████▊     | 12872/26933 [14:35<13:09, 17.80it/s] 48%|████▊     | 12876/26933 [14:35<13:11, 17.77it/s] 48%|████▊     | 12880/26933 [14:35<13:10, 17.77it/s] 48%|████▊     | 12884/26933 [14:36<13:12, 17.72it/s] 48%|████▊     | 12888/26933 [14:36<13:10, 17.77it/s] 48%|████▊     | 12892/26933 [14:36<13:09, 17.78it/s] 48%|████▊     | 12896/26933 [14:36<13:09, 17.79it/s] 48%|████▊     | 12900/26933 [14:37<13:10, 17.75it/s] 48%|████▊     | 12904/26933 [14:37<13:10, 17.75it/s] 48%|████▊     | 12908/26933 [14:37<13:09, 17.76it/s] 48%|████▊     | 12912/26933 [14:37<13:09, 17.76it/s] 48%|████▊     | 12916/26933 [14:38<13:09, 17.76it/s] 48%|████▊     | 12920/26933 [14:38<13:11, 17.71it/s] 48%|████▊     | 12924/26933 [14:38<13:10, 17.72it/s] 48%|████▊     | 12928/26933 [14:38<13:08, 17.75it/s] 48%|████▊     | 12932/26933 [14:38<13:07, 17.78it/s] 48%|████▊     | 12936/26933 [14:39<13:09, 17.73it/s] 48%|████▊     | 12940/26933 [14:39<13:08, 17.75it/s] 48%|████▊     | 12944/26933 [14:39<13:06, 17.79it/s] 48%|████▊     | 12948/26933 [14:39<13:05, 17.80it/s] 48%|████▊     | 12952/26933 [14:40<13:14, 17.59it/s] 48%|████▊     | 12956/26933 [14:40<13:10, 17.67it/s] 48%|████▊     | 12960/26933 [14:40<13:09, 17.69it/s] 48%|████▊     | 12964/26933 [14:40<13:11, 17.66it/s] 48%|████▊     | 12968/26933 [14:40<13:10, 17.66it/s] 48%|████▊     | 12972/26933 [14:41<13:12, 17.61it/s] 48%|████▊     | 12976/26933 [14:41<13:11, 17.63it/s] 48%|████▊     | 12980/26933 [14:41<13:10, 17.66it/s] 48%|████▊     | 12984/26933 [14:41<13:10, 17.64it/s] 48%|████▊     | 12988/26933 [14:42<13:11, 17.62it/s] 48%|████▊     | 12992/26933 [14:42<13:08, 17.69it/s] 48%|████▊     | 12996/26933 [14:42<13:03, 17.78it/s] 48%|████▊     | 13000/26933 [14:42<13:02, 17.81it/s] 48%|████▊     | 13004/26933 [14:42<13:01, 17.83it/s] 48%|████▊     | 13008/26933 [14:43<13:02, 17.80it/s] 48%|████▊     | 13012/26933 [14:43<13:00, 17.83it/s] 48%|████▊     | 13016/26933 [14:43<13:00, 17.83it/s] 48%|████▊     | 13020/26933 [14:43<12:59, 17.86it/s] 48%|████▊     | 13024/26933 [14:44<13:03, 17.75it/s] 48%|████▊     | 13028/26933 [14:44<13:01, 17.80it/s] 48%|████▊     | 13032/26933 [14:44<13:00, 17.82it/s] 48%|████▊     | 13036/26933 [14:44<13:01, 17.79it/s] 48%|████▊     | 13040/26933 [14:45<13:01, 17.78it/s] 48%|████▊     | 13044/26933 [14:45<13:13, 17.51it/s] 48%|████▊     | 13048/26933 [14:45<13:10, 17.56it/s] 48%|████▊     | 13052/26933 [14:45<13:08, 17.61it/s] 48%|████▊     | 13056/26933 [14:45<13:07, 17.61it/s] 48%|████▊     | 13060/26933 [14:46<13:07, 17.61it/s] 49%|████▊     | 13064/26933 [14:46<13:06, 17.62it/s] 49%|████▊     | 13068/26933 [14:46<13:06, 17.64it/s] 49%|████▊     | 13072/26933 [14:46<13:04, 17.68it/s] 49%|████▊     | 13076/26933 [14:47<13:05, 17.64it/s] 49%|████▊     | 13080/26933 [14:47<13:04, 17.66it/s] 49%|████▊     | 13084/26933 [14:47<13:02, 17.70it/s] 49%|████▊     | 13088/26933 [14:47<13:00, 17.74it/s] 49%|████▊     | 13092/26933 [14:47<12:58, 17.77it/s] 49%|████▊     | 13096/26933 [14:48<13:01, 17.70it/s] 49%|████▊     | 13100/26933 [14:48<12:58, 17.77it/s] 49%|████▊     | 13104/26933 [14:48<12:56, 17.80it/s] 49%|████▊     | 13108/26933 [14:48<12:56, 17.81it/s] 49%|████▊     | 13112/26933 [14:49<12:59, 17.73it/s] 49%|████▊     | 13116/26933 [14:49<12:59, 17.73it/s] 49%|████▊     | 13120/26933 [14:49<12:58, 17.75it/s] 49%|████▊     | 13124/26933 [14:49<12:58, 17.74it/s] 49%|████▊     | 13128/26933 [14:49<12:58, 17.73it/s] 49%|████▉     | 13132/26933 [14:50<13:00, 17.69it/s] 49%|████▉     | 13136/26933 [14:50<12:57, 17.74it/s] 49%|████▉     | 13140/26933 [14:50<12:55, 17.78it/s] 49%|████▉     | 13144/26933 [14:50<12:54, 17.80it/s] 49%|████▉     | 13148/26933 [14:51<12:56, 17.76it/s] 49%|████▉     | 13152/26933 [14:51<12:55, 17.78it/s] 49%|████▉     | 13156/26933 [14:51<12:52, 17.83it/s] 49%|████▉     | 13160/26933 [14:51<12:52, 17.84it/s] 49%|████▉     | 13164/26933 [14:52<12:51, 17.84it/s] 49%|████▉     | 13168/26933 [14:52<12:53, 17.79it/s] 49%|████▉     | 13172/26933 [14:52<12:50, 17.85it/s] 49%|████▉     | 13176/26933 [14:52<12:52, 17.82it/s] 49%|████▉     | 13180/26933 [14:52<12:55, 17.74it/s] 49%|████▉     | 13184/26933 [14:53<12:59, 17.65it/s] 49%|████▉     | 13188/26933 [14:53<12:58, 17.65it/s] 49%|████▉     | 13192/26933 [14:53<12:57, 17.67it/s] 49%|████▉     | 13196/26933 [14:53<12:57, 17.67it/s] 49%|████▉     | 13200/26933 [14:54<12:57, 17.67it/s] 49%|████▉     | 13204/26933 [14:54<12:59, 17.61it/s] 49%|████▉     | 13208/26933 [14:54<12:58, 17.63it/s] 49%|████▉     | 13212/26933 [14:54<12:56, 17.66it/s] 49%|████▉     | 13216/26933 [14:54<12:55, 17.69it/s] 49%|████▉     | 13220/26933 [14:55<12:53, 17.72it/s] 49%|████▉     | 13224/26933 [14:55<12:53, 17.72it/s] 49%|████▉     | 13228/26933 [14:55<12:51, 17.76it/s] 49%|████▉     | 13232/26933 [14:55<12:53, 17.71it/s] 49%|████▉     | 13236/26933 [14:56<12:57, 17.61it/s] 49%|████▉     | 13240/26933 [14:56<12:57, 17.62it/s] 49%|████▉     | 13244/26933 [14:56<12:55, 17.65it/s] 49%|████▉     | 13248/26933 [14:56<12:54, 17.66it/s] 49%|████▉     | 13252/26933 [14:56<12:53, 17.69it/s] 49%|████▉     | 13256/26933 [14:57<12:53, 17.68it/s] 49%|████▉     | 13260/26933 [14:57<12:49, 17.77it/s] 49%|████▉     | 13264/26933 [14:57<12:54, 17.64it/s] 49%|████▉     | 13268/26933 [14:57<12:51, 17.71it/s] 49%|████▉     | 13272/26933 [14:58<12:53, 17.66it/s] 49%|████▉     | 13276/26933 [14:58<12:52, 17.68it/s] 49%|████▉     | 13280/26933 [14:58<12:52, 17.68it/s] 49%|████▉     | 13284/26933 [14:58<12:51, 17.69it/s] 49%|████▉     | 13288/26933 [14:59<12:50, 17.70it/s] 49%|████▉     | 13292/26933 [14:59<12:53, 17.64it/s] 49%|████▉     | 13296/26933 [14:59<12:50, 17.69it/s] 49%|████▉     | 13300/26933 [14:59<12:47, 17.75it/s] 49%|████▉     | 13304/26933 [14:59<12:47, 17.75it/s] 49%|████▉     | 13308/26933 [15:00<12:48, 17.73it/s] 49%|████▉     | 13312/26933 [15:00<12:48, 17.73it/s] 49%|████▉     | 13316/26933 [15:00<12:47, 17.74it/s] 49%|████▉     | 13320/26933 [15:00<12:47, 17.73it/s] 49%|████▉     | 13324/26933 [15:01<12:50, 17.67it/s] 49%|████▉     | 13328/26933 [15:01<12:49, 17.69it/s] 50%|████▉     | 13332/26933 [15:01<12:48, 17.70it/s] 50%|████▉     | 13336/26933 [15:01<12:49, 17.66it/s] 50%|████▉     | 13340/26933 [15:01<12:49, 17.68it/s] 50%|████▉     | 13344/26933 [15:02<12:51, 17.61it/s] 50%|████▉     | 13348/26933 [15:02<12:49, 17.64it/s] 50%|████▉     | 13352/26933 [15:02<12:48, 17.67it/s] 50%|████▉     | 13356/26933 [15:02<12:47, 17.68it/s] 50%|████▉     | 13360/26933 [15:03<12:50, 17.61it/s] 50%|████▉     | 13364/26933 [15:03<12:50, 17.60it/s] 50%|████▉     | 13368/26933 [15:03<12:50, 17.61it/s] 50%|████▉     | 13372/26933 [15:03<12:49, 17.62it/s] 50%|████▉     | 13376/26933 [15:04<12:49, 17.62it/s] 50%|████▉     | 13380/26933 [15:04<12:49, 17.62it/s] 50%|████▉     | 13384/26933 [15:04<12:47, 17.64it/s] 50%|████▉     | 13388/26933 [15:04<12:46, 17.66it/s] 50%|████▉     | 13392/26933 [15:04<12:45, 17.68it/s] 50%|████▉     | 13396/26933 [15:05<12:44, 17.70it/s] 50%|████▉     | 13400/26933 [15:05<12:42, 17.75it/s] 50%|████▉     | 13404/26933 [15:05<12:41, 17.77it/s] 50%|████▉     | 13408/26933 [15:05<12:42, 17.73it/s] 50%|████▉     | 13412/26933 [15:06<12:43, 17.71it/s] 50%|████▉     | 13416/26933 [15:06<12:46, 17.64it/s] 50%|████▉     | 13420/26933 [15:06<12:46, 17.63it/s] 50%|████▉     | 13424/26933 [15:06<12:47, 17.59it/s] 50%|████▉     | 13428/26933 [15:06<12:47, 17.61it/s] 50%|████▉     | 13432/26933 [15:07<12:47, 17.60it/s] 50%|████▉     | 13436/26933 [15:07<12:44, 17.65it/s] 50%|████▉     | 13440/26933 [15:07<12:45, 17.64it/s] 50%|████▉     | 13444/26933 [15:07<12:45, 17.63it/s] 50%|████▉     | 13448/26933 [15:08<12:48, 17.55it/s] 50%|████▉     | 13452/26933 [15:08<12:46, 17.59it/s] 50%|████▉     | 13456/26933 [15:08<12:45, 17.61it/s] 50%|████▉     | 13460/26933 [15:08<12:43, 17.64it/s] 50%|████▉     | 13464/26933 [15:08<12:42, 17.66it/s] 50%|█████     | 13468/26933 [15:09<12:42, 17.65it/s] 50%|█████     | 13472/26933 [15:09<12:44, 17.62it/s] 50%|█████     | 13476/26933 [15:09<12:42, 17.64it/s] 50%|█████     | 13480/26933 [15:09<12:41, 17.68it/s] 50%|█████     | 13484/26933 [15:10<12:42, 17.63it/s] 50%|█████     | 13488/26933 [15:10<12:42, 17.63it/s] 50%|█████     | 13492/26933 [15:10<12:42, 17.64it/s] 50%|█████     | 13496/26933 [15:10<12:41, 17.64it/s] 50%|█████     | 13500/26933 [15:11<12:40, 17.66it/s] 50%|█████     | 13504/26933 [15:11<12:43, 17.60it/s] 50%|█████     | 13508/26933 [15:11<12:41, 17.64it/s] 50%|█████     | 13512/26933 [15:11<12:40, 17.65it/s] 50%|█████     | 13516/26933 [15:11<12:40, 17.64it/s] 50%|█████     | 13520/26933 [15:12<12:43, 17.57it/s] 50%|█████     | 13524/26933 [15:12<12:41, 17.61it/s] 50%|█████     | 13528/26933 [15:12<12:41, 17.61it/s] 50%|█████     | 13532/26933 [15:12<12:39, 17.64it/s] 50%|█████     | 13536/26933 [15:13<12:41, 17.59it/s] 50%|█████     | 13540/26933 [15:13<12:39, 17.63it/s] 50%|█████     | 13544/26933 [15:13<12:38, 17.66it/s] 50%|█████     | 13548/26933 [15:13<12:37, 17.67it/s] 50%|█████     | 13552/26933 [15:13<12:38, 17.64it/s] 50%|█████     | 13556/26933 [15:14<12:39, 17.62it/s] 50%|█████     | 13560/26933 [15:14<12:36, 17.68it/s] 50%|█████     | 13564/26933 [15:14<12:36, 17.66it/s] 50%|█████     | 13568/26933 [15:14<12:34, 17.70it/s] 50%|█████     | 13572/26933 [15:15<12:36, 17.66it/s] 50%|█████     | 13576/26933 [15:15<12:35, 17.67it/s] 50%|█████     | 13580/26933 [15:15<12:35, 17.68it/s] 50%|█████     | 13584/26933 [15:15<12:36, 17.65it/s] 50%|█████     | 13588/26933 [15:16<12:35, 17.66it/s] 50%|█████     | 13592/26933 [15:16<12:37, 17.62it/s] 50%|█████     | 13596/26933 [15:16<12:36, 17.62it/s] 50%|█████     | 13600/26933 [15:16<12:37, 17.60it/s] 51%|█████     | 13604/26933 [15:16<12:36, 17.61it/s] 51%|█████     | 13608/26933 [15:17<12:41, 17.50it/s] 51%|█████     | 13612/26933 [15:17<12:38, 17.56it/s] 51%|█████     | 13616/26933 [15:17<12:35, 17.62it/s] 51%|█████     | 13620/26933 [15:17<12:40, 17.51it/s] 51%|█████     | 13624/26933 [15:18<12:40, 17.49it/s] 51%|█████     | 13628/26933 [15:18<12:39, 17.52it/s] 51%|█████     | 13632/26933 [15:18<12:36, 17.59it/s] 51%|█████     | 13636/26933 [15:18<12:35, 17.59it/s] 51%|█████     | 13640/26933 [15:18<12:33, 17.64it/s] 51%|█████     | 13644/26933 [15:19<12:33, 17.64it/s] 51%|█████     | 13648/26933 [15:19<12:31, 17.69it/s] 51%|█████     | 13652/26933 [15:19<12:28, 17.74it/s] 51%|█████     | 13656/26933 [15:19<12:27, 17.76it/s] 51%|█████     | 13660/26933 [15:20<12:32, 17.64it/s] 51%|█████     | 13664/26933 [15:20<12:30, 17.69it/s] 51%|█████     | 13668/26933 [15:20<12:29, 17.71it/s] 51%|█████     | 13672/26933 [15:20<12:28, 17.72it/s] 51%|█████     | 13676/26933 [15:21<12:28, 17.72it/s] 51%|█████     | 13680/26933 [15:21<12:36, 17.53it/s] 51%|█████     | 13684/26933 [15:21<12:34, 17.57it/s] 51%|█████     | 13688/26933 [15:21<12:33, 17.58it/s] 51%|█████     | 13692/26933 [15:21<12:33, 17.57it/s] 51%|█████     | 13696/26933 [15:22<12:37, 17.47it/s] 51%|█████     | 13700/26933 [15:22<12:34, 17.54it/s] 51%|█████     | 13704/26933 [15:22<12:32, 17.58it/s] 51%|█████     | 13708/26933 [15:22<12:30, 17.61it/s] 51%|█████     | 13712/26933 [15:23<12:34, 17.53it/s] 51%|█████     | 13716/26933 [15:23<12:31, 17.59it/s] 51%|█████     | 13720/26933 [15:23<12:29, 17.62it/s] 51%|█████     | 13724/26933 [15:23<12:28, 17.64it/s] 51%|█████     | 13728/26933 [15:23<12:27, 17.67it/s] 51%|█████     | 13732/26933 [15:24<12:31, 17.56it/s] 51%|█████     | 13736/26933 [15:24<12:28, 17.62it/s] 51%|█████     | 13740/26933 [15:24<12:28, 17.63it/s] 51%|█████     | 13744/26933 [15:24<12:26, 17.66it/s] 51%|█████     | 13748/26933 [15:25<12:30, 17.56it/s] 51%|█████     | 13752/26933 [15:25<12:28, 17.61it/s] 51%|█████     | 13756/26933 [15:25<12:27, 17.63it/s] 51%|█████     | 13760/26933 [15:25<12:26, 17.65it/s] 51%|█████     | 13764/26933 [15:26<12:24, 17.68it/s] 51%|█████     | 13768/26933 [15:26<12:30, 17.55it/s] 51%|█████     | 13772/26933 [15:26<12:29, 17.57it/s] 51%|█████     | 13776/26933 [15:26<12:27, 17.60it/s] 51%|█████     | 13780/26933 [15:26<12:26, 17.61it/s] 51%|█████     | 13784/26933 [15:27<12:30, 17.52it/s] 51%|█████     | 13788/26933 [15:27<12:30, 17.51it/s] 51%|█████     | 13792/26933 [15:27<12:29, 17.53it/s] 51%|█████     | 13796/26933 [15:27<12:29, 17.52it/s] 51%|█████     | 13800/26933 [15:28<12:33, 17.44it/s] 51%|█████▏    | 13804/26933 [15:28<12:29, 17.52it/s] 51%|█████▏    | 13808/26933 [15:28<12:26, 17.57it/s] 51%|█████▏    | 13812/26933 [15:28<12:26, 17.58it/s] 51%|█████▏    | 13816/26933 [15:28<12:24, 17.61it/s] 51%|█████▏    | 13820/26933 [15:29<12:28, 17.52it/s] 51%|█████▏    | 13824/26933 [15:29<12:25, 17.58it/s] 51%|█████▏    | 13828/26933 [15:29<12:24, 17.60it/s] 51%|█████▏    | 13832/26933 [15:29<12:22, 17.64it/s] 51%|█████▏    | 13836/26933 [15:30<12:25, 17.56it/s] 51%|█████▏    | 13840/26933 [15:30<12:21, 17.66it/s] 51%|█████▏    | 13844/26933 [15:30<12:18, 17.72it/s] 51%|█████▏    | 13848/26933 [15:30<12:16, 17.76it/s] 51%|█████▏    | 13852/26933 [15:31<12:16, 17.76it/s] 51%|█████▏    | 13856/26933 [15:31<12:18, 17.71it/s] 51%|█████▏    | 13860/26933 [15:31<12:15, 17.77it/s] 51%|█████▏    | 13864/26933 [15:31<12:14, 17.80it/s] 51%|█████▏    | 13868/26933 [15:31<12:12, 17.83it/s] 52%|█████▏    | 13872/26933 [15:32<12:17, 17.72it/s] 52%|█████▏    | 13876/26933 [15:32<12:14, 17.77it/s] 52%|█████▏    | 13880/26933 [15:32<12:13, 17.80it/s] 52%|█████▏    | 13884/26933 [15:32<12:13, 17.79it/s] 52%|█████▏    | 13888/26933 [15:33<12:15, 17.73it/s] 52%|█████▏    | 13892/26933 [15:33<12:20, 17.61it/s] 52%|█████▏    | 13896/26933 [15:33<12:19, 17.63it/s] 52%|█████▏    | 13900/26933 [15:33<12:21, 17.59it/s] 52%|█████▏    | 13904/26933 [15:33<12:20, 17.59it/s] 52%|█████▏    | 13908/26933 [15:34<12:24, 17.50it/s] 52%|█████▏    | 13912/26933 [15:34<12:22, 17.53it/s] 52%|█████▏    | 13916/26933 [15:34<12:20, 17.58it/s] 52%|█████▏    | 13920/26933 [15:34<12:21, 17.54it/s] 52%|█████▏    | 13924/26933 [15:35<12:26, 17.42it/s] 52%|█████▏    | 13928/26933 [15:35<12:25, 17.45it/s] 52%|█████▏    | 13932/26933 [15:35<12:24, 17.46it/s] 52%|█████▏    | 13936/26933 [15:35<12:23, 17.48it/s] 52%|█████▏    | 13940/26933 [15:36<12:22, 17.49it/s] 52%|█████▏    | 13944/26933 [15:36<12:27, 17.38it/s] 52%|█████▏    | 13948/26933 [15:36<12:25, 17.41it/s] 52%|█████▏    | 13952/26933 [15:36<12:25, 17.41it/s] 52%|█████▏    | 13956/26933 [15:36<12:26, 17.39it/s] 52%|█████▏    | 13960/26933 [15:37<12:27, 17.36it/s] 52%|█████▏    | 13964/26933 [15:37<12:24, 17.42it/s] 52%|█████▏    | 13968/26933 [15:37<12:20, 17.52it/s] 52%|█████▏    | 13972/26933 [15:37<12:17, 17.56it/s] 52%|█████▏    | 13976/26933 [15:38<12:22, 17.45it/s] 52%|█████▏    | 13980/26933 [15:38<12:18, 17.54it/s] 52%|█████▏    | 13984/26933 [15:38<12:21, 17.47it/s] 52%|█████▏    | 13988/26933 [15:38<12:17, 17.54it/s] 52%|█████▏    | 13992/26933 [15:38<12:14, 17.63it/s] 52%|█████▏    | 13996/26933 [15:39<12:14, 17.60it/s] 52%|█████▏    | 14000/26933 [15:39<12:13, 17.63it/s] 52%|█████▏    | 14004/26933 [15:39<12:13, 17.63it/s] 52%|█████▏    | 14008/26933 [15:39<12:11, 17.66it/s] 52%|█████▏    | 14012/26933 [15:40<12:14, 17.59it/s] 52%|█████▏    | 14016/26933 [15:40<12:10, 17.67it/s] 52%|█████▏    | 14020/26933 [15:40<12:07, 17.75it/s] 52%|█████▏    | 14024/26933 [15:40<12:04, 17.81it/s] 52%|█████▏    | 14028/26933 [15:41<12:04, 17.80it/s] 52%|█████▏    | 14032/26933 [15:41<12:08, 17.71it/s] 52%|█████▏    | 14036/26933 [15:41<12:07, 17.72it/s] 52%|█████▏    | 14040/26933 [15:41<12:04, 17.78it/s] 52%|█████▏    | 14044/26933 [15:41<12:05, 17.77it/s] 52%|█████▏    | 14048/26933 [15:42<12:08, 17.68it/s] 52%|█████▏    | 14052/26933 [15:42<12:06, 17.74it/s] 52%|█████▏    | 14056/26933 [15:42<12:02, 17.81it/s] 52%|█████▏    | 14060/26933 [15:42<12:03, 17.80it/s] 52%|█████▏    | 14064/26933 [15:43<12:02, 17.82it/s] 52%|█████▏    | 14068/26933 [15:43<12:04, 17.75it/s] 52%|█████▏    | 14072/26933 [15:43<12:03, 17.78it/s] 52%|█████▏    | 14076/26933 [15:43<12:02, 17.79it/s] 52%|█████▏    | 14080/26933 [15:43<12:00, 17.84it/s] 52%|█████▏    | 14084/26933 [15:44<12:05, 17.70it/s] 52%|█████▏    | 14088/26933 [15:44<12:02, 17.77it/s] 52%|█████▏    | 14092/26933 [15:44<12:00, 17.83it/s] 52%|█████▏    | 14096/26933 [15:44<11:58, 17.86it/s] 52%|█████▏    | 14100/26933 [15:45<12:03, 17.75it/s] 52%|█████▏    | 14104/26933 [15:45<11:59, 17.82it/s] 52%|█████▏    | 14108/26933 [15:45<12:00, 17.80it/s] 52%|█████▏    | 14112/26933 [15:45<11:59, 17.81it/s] 52%|█████▏    | 14116/26933 [15:45<11:59, 17.82it/s] 52%|█████▏    | 14120/26933 [15:46<12:01, 17.75it/s] 52%|█████▏    | 14124/26933 [15:46<11:59, 17.80it/s] 52%|█████▏    | 14128/26933 [15:46<11:59, 17.81it/s] 52%|█████▏    | 14132/26933 [15:46<11:57, 17.84it/s] 52%|█████▏    | 14136/26933 [15:47<12:00, 17.76it/s] 53%|█████▎    | 14140/26933 [15:47<12:01, 17.72it/s] 53%|█████▎    | 14144/26933 [15:47<12:01, 17.73it/s] 53%|█████▎    | 14148/26933 [15:47<12:00, 17.75it/s] 53%|█████▎    | 14152/26933 [15:47<12:00, 17.74it/s] 53%|█████▎    | 14156/26933 [15:48<12:05, 17.62it/s] 53%|█████▎    | 14160/26933 [15:48<12:03, 17.65it/s] 53%|█████▎    | 14164/26933 [15:48<12:03, 17.66it/s] 53%|█████▎    | 14168/26933 [15:48<12:00, 17.71it/s] 53%|█████▎    | 14172/26933 [15:49<12:04, 17.62it/s] 53%|█████▎    | 14176/26933 [15:49<12:01, 17.69it/s] 53%|█████▎    | 14180/26933 [15:49<11:58, 17.75it/s] 53%|█████▎    | 14184/26933 [15:49<11:56, 17.80it/s] 53%|█████▎    | 14188/26933 [15:50<11:54, 17.84it/s] 53%|█████▎    | 14192/26933 [15:50<12:00, 17.70it/s] 53%|█████▎    | 14196/26933 [15:50<12:00, 17.68it/s] 53%|█████▎    | 14200/26933 [15:50<12:00, 17.68it/s] 53%|█████▎    | 14204/26933 [15:50<12:00, 17.68it/s] 53%|█████▎    | 14208/26933 [15:51<12:02, 17.62it/s] 53%|█████▎    | 14212/26933 [15:51<12:01, 17.64it/s] 53%|█████▎    | 14216/26933 [15:51<12:01, 17.64it/s] 53%|█████▎    | 14220/26933 [15:51<12:01, 17.63it/s] 53%|█████▎    | 14224/26933 [15:52<12:04, 17.55it/s] 53%|█████▎    | 14228/26933 [15:52<12:02, 17.58it/s] 53%|█████▎    | 14232/26933 [15:52<12:02, 17.59it/s] 53%|█████▎    | 14236/26933 [15:52<12:02, 17.57it/s] 53%|█████▎    | 14240/26933 [15:52<12:00, 17.61it/s] 53%|█████▎    | 14244/26933 [15:53<12:03, 17.54it/s] 53%|█████▎    | 14248/26933 [15:53<12:01, 17.58it/s] 53%|█████▎    | 14252/26933 [15:53<12:01, 17.58it/s] 53%|█████▎    | 14256/26933 [15:53<12:00, 17.60it/s] 53%|█████▎    | 14260/26933 [15:54<12:03, 17.53it/s] 53%|█████▎    | 14264/26933 [15:54<12:01, 17.56it/s] 53%|█████▎    | 14268/26933 [15:54<11:59, 17.59it/s] 53%|█████▎    | 14272/26933 [15:54<11:57, 17.64it/s] 53%|█████▎    | 14276/26933 [15:55<11:57, 17.65it/s] 53%|█████▎    | 14280/26933 [15:55<11:59, 17.59it/s] 53%|█████▎    | 14284/26933 [15:55<11:58, 17.60it/s] 53%|█████▎    | 14288/26933 [15:55<11:57, 17.63it/s] 53%|█████▎    | 14292/26933 [15:55<11:55, 17.67it/s] 53%|█████▎    | 14296/26933 [15:56<11:59, 17.57it/s] 53%|█████▎    | 14300/26933 [15:56<11:58, 17.59it/s] 53%|█████▎    | 14304/26933 [15:56<11:56, 17.63it/s] 53%|█████▎    | 14308/26933 [15:56<11:55, 17.64it/s] 53%|█████▎    | 14312/26933 [15:57<11:58, 17.56it/s] 53%|█████▎    | 14316/26933 [15:57<11:55, 17.64it/s] 53%|█████▎    | 14320/26933 [15:57<11:52, 17.70it/s] 53%|█████▎    | 14324/26933 [15:57<11:49, 17.77it/s] 53%|█████▎    | 14328/26933 [15:57<11:47, 17.81it/s] 53%|█████▎    | 14332/26933 [15:58<11:51, 17.71it/s] 53%|█████▎    | 14336/26933 [15:58<11:49, 17.76it/s] 53%|█████▎    | 14340/26933 [15:58<11:47, 17.79it/s] 53%|█████▎    | 14344/26933 [15:58<11:47, 17.81it/s] 53%|█████▎    | 14348/26933 [15:59<11:50, 17.72it/s] 53%|█████▎    | 14352/26933 [15:59<11:47, 17.77it/s] 53%|█████▎    | 14356/26933 [15:59<11:46, 17.79it/s] 53%|█████▎    | 14360/26933 [15:59<11:48, 17.75it/s] 53%|█████▎    | 14364/26933 [15:59<11:45, 17.82it/s] 53%|█████▎    | 14368/26933 [16:00<11:47, 17.75it/s] 53%|█████▎    | 14372/26933 [16:00<11:43, 17.84it/s] 53%|█████▎    | 14376/26933 [16:00<11:42, 17.87it/s] 53%|█████▎    | 14380/26933 [16:00<11:41, 17.90it/s] 53%|█████▎    | 14384/26933 [16:01<11:44, 17.81it/s] 53%|█████▎    | 14388/26933 [16:01<11:42, 17.86it/s] 53%|█████▎    | 14392/26933 [16:01<11:43, 17.83it/s] 53%|█████▎    | 14396/26933 [16:01<11:42, 17.84it/s] 53%|█████▎    | 14400/26933 [16:02<11:42, 17.83it/s] 53%|█████▎    | 14404/26933 [16:02<11:46, 17.73it/s] 53%|█████▎    | 14408/26933 [16:02<11:45, 17.76it/s] 54%|█████▎    | 14412/26933 [16:02<11:44, 17.76it/s] 54%|█████▎    | 14416/26933 [16:02<11:44, 17.76it/s] 54%|█████▎    | 14420/26933 [16:03<11:48, 17.65it/s] 54%|█████▎    | 14424/26933 [16:03<11:47, 17.67it/s] 54%|█████▎    | 14428/26933 [16:03<11:45, 17.72it/s] 54%|█████▎    | 14432/26933 [16:03<11:44, 17.73it/s] 54%|█████▎    | 14436/26933 [16:04<11:45, 17.72it/s] 54%|█████▎    | 14440/26933 [16:04<11:47, 17.67it/s] 54%|█████▎    | 14444/26933 [16:04<11:44, 17.73it/s] 54%|█████▎    | 14448/26933 [16:04<11:42, 17.77it/s] 54%|█████▎    | 14452/26933 [16:04<11:41, 17.79it/s] 54%|█████▎    | 14456/26933 [16:05<11:42, 17.75it/s] 54%|█████▎    | 14460/26933 [16:05<11:40, 17.81it/s] 54%|█████▎    | 14464/26933 [16:05<11:38, 17.84it/s] 54%|█████▎    | 14468/26933 [16:05<11:39, 17.81it/s] 54%|█████▎    | 14472/26933 [16:06<11:44, 17.69it/s] 54%|█████▎    | 14476/26933 [16:06<11:40, 17.78it/s] 54%|█████▍    | 14480/26933 [16:06<11:38, 17.83it/s] 54%|█████▍    | 14484/26933 [16:06<11:35, 17.89it/s] 54%|█████▍    | 14488/26933 [16:06<11:34, 17.91it/s] 54%|█████▍    | 14492/26933 [16:07<11:35, 17.88it/s] 54%|█████▍    | 14496/26933 [16:07<11:33, 17.93it/s] 54%|█████▍    | 14500/26933 [16:07<11:32, 17.96it/s] 54%|█████▍    | 14504/26933 [16:07<11:33, 17.91it/s] 54%|█████▍    | 14508/26933 [16:08<11:40, 17.73it/s] 54%|█████▍    | 14512/26933 [16:08<11:36, 17.83it/s] 54%|█████▍    | 14516/26933 [16:08<11:33, 17.89it/s] 54%|█████▍    | 14520/26933 [16:08<11:32, 17.92it/s] 54%|█████▍    | 14524/26933 [16:08<11:31, 17.94it/s] 54%|█████▍    | 14528/26933 [16:09<11:32, 17.91it/s] 54%|█████▍    | 14532/26933 [16:09<11:30, 17.97it/s] 54%|█████▍    | 14536/26933 [16:09<11:29, 17.97it/s] 54%|█████▍    | 14540/26933 [16:09<11:27, 18.03it/s] 54%|█████▍    | 14544/26933 [16:10<11:30, 17.95it/s] 54%|█████▍    | 14548/26933 [16:10<11:29, 17.97it/s] 54%|█████▍    | 14552/26933 [16:10<11:28, 17.99it/s] 54%|█████▍    | 14556/26933 [16:10<11:27, 18.01it/s] 54%|█████▍    | 14560/26933 [16:10<11:26, 18.01it/s] 54%|█████▍    | 14564/26933 [16:11<11:29, 17.95it/s] 54%|█████▍    | 14568/26933 [16:11<11:26, 18.00it/s] 54%|█████▍    | 14572/26933 [16:11<11:26, 18.01it/s] 54%|█████▍    | 14576/26933 [16:11<11:25, 18.03it/s] 54%|█████▍    | 14580/26933 [16:12<11:27, 17.96it/s] 54%|█████▍    | 14584/26933 [16:12<11:26, 17.98it/s] 54%|█████▍    | 14588/26933 [16:12<11:25, 18.01it/s] 54%|█████▍    | 14592/26933 [16:12<11:25, 18.01it/s] 54%|█████▍    | 14596/26933 [16:12<11:24, 18.02it/s] 54%|█████▍    | 14600/26933 [16:13<11:26, 17.96it/s] 54%|█████▍    | 14604/26933 [16:13<11:25, 18.00it/s] 54%|█████▍    | 14608/26933 [16:13<11:24, 18.00it/s] 54%|█████▍    | 14612/26933 [16:13<11:24, 18.01it/s] 54%|█████▍    | 14616/26933 [16:14<11:26, 17.94it/s] 54%|█████▍    | 14620/26933 [16:14<11:26, 17.95it/s] 54%|█████▍    | 14624/26933 [16:14<11:25, 17.96it/s] 54%|█████▍    | 14628/26933 [16:14<11:23, 17.99it/s] 54%|█████▍    | 14632/26933 [16:14<11:22, 18.01it/s] 54%|█████▍    | 14636/26933 [16:15<11:25, 17.94it/s] 54%|█████▍    | 14640/26933 [16:15<11:23, 17.99it/s] 54%|█████▍    | 14644/26933 [16:15<11:22, 18.00it/s] 54%|█████▍    | 14648/26933 [16:15<11:21, 18.01it/s] 54%|█████▍    | 14652/26933 [16:16<11:24, 17.95it/s] 54%|█████▍    | 14656/26933 [16:16<11:22, 17.99it/s] 54%|█████▍    | 14660/26933 [16:16<11:21, 18.01it/s] 54%|█████▍    | 14664/26933 [16:16<11:19, 18.05it/s] 54%|█████▍    | 14668/26933 [16:16<11:18, 18.08it/s] 54%|█████▍    | 14672/26933 [16:17<11:21, 17.99it/s] 54%|█████▍    | 14676/26933 [16:17<11:19, 18.04it/s] 55%|█████▍    | 14680/26933 [16:17<11:18, 18.05it/s] 55%|█████▍    | 14684/26933 [16:17<11:18, 18.06it/s] 55%|█████▍    | 14688/26933 [16:18<11:19, 18.01it/s] 55%|█████▍    | 14692/26933 [16:18<11:19, 18.02it/s] 55%|█████▍    | 14696/26933 [16:18<11:19, 18.01it/s] 55%|█████▍    | 14700/26933 [16:18<11:18, 18.02it/s] 55%|█████▍    | 14704/26933 [16:18<11:16, 18.07it/s] 55%|█████▍    | 14708/26933 [16:19<11:17, 18.05it/s] 55%|█████▍    | 14712/26933 [16:19<11:16, 18.06it/s] 55%|█████▍    | 14716/26933 [16:19<11:14, 18.11it/s] 55%|█████▍    | 14720/26933 [16:19<11:14, 18.09it/s] 55%|█████▍    | 14724/26933 [16:20<11:18, 18.00it/s] 55%|█████▍    | 14728/26933 [16:20<11:17, 18.00it/s] 55%|█████▍    | 14732/26933 [16:20<11:16, 18.05it/s] 55%|█████▍    | 14736/26933 [16:20<11:14, 18.08it/s] 55%|█████▍    | 14740/26933 [16:20<11:14, 18.07it/s] 55%|█████▍    | 14744/26933 [16:21<11:14, 18.06it/s] 55%|█████▍    | 14748/26933 [16:21<11:14, 18.05it/s] 55%|█████▍    | 14752/26933 [16:21<11:16, 18.00it/s] 55%|█████▍    | 14756/26933 [16:21<11:18, 17.95it/s] 55%|█████▍    | 14760/26933 [16:22<11:21, 17.87it/s] 55%|█████▍    | 14764/26933 [16:22<11:20, 17.87it/s] 55%|█████▍    | 14768/26933 [16:22<11:20, 17.89it/s] 55%|█████▍    | 14772/26933 [16:22<11:19, 17.91it/s] 55%|█████▍    | 14776/26933 [16:22<11:18, 17.91it/s] 55%|█████▍    | 14780/26933 [16:23<11:21, 17.84it/s] 55%|█████▍    | 14784/26933 [16:23<11:19, 17.88it/s] 55%|█████▍    | 14788/26933 [16:23<11:20, 17.85it/s] 55%|█████▍    | 14792/26933 [16:23<11:25, 17.70it/s] 55%|█████▍    | 14796/26933 [16:24<11:24, 17.73it/s] 55%|█████▍    | 14800/26933 [16:24<11:23, 17.75it/s] 55%|█████▍    | 14804/26933 [16:24<11:21, 17.80it/s] 55%|█████▍    | 14808/26933 [16:24<11:20, 17.82it/s] 55%|█████▍    | 14812/26933 [16:25<11:19, 17.85it/s] 55%|█████▌    | 14816/26933 [16:25<11:20, 17.81it/s] 55%|█████▌    | 14820/26933 [16:25<11:17, 17.87it/s] 55%|█████▌    | 14824/26933 [16:25<11:18, 17.86it/s] 55%|█████▌    | 14828/26933 [16:25<11:18, 17.83it/s] 55%|█████▌    | 14832/26933 [16:26<11:21, 17.76it/s] 55%|█████▌    | 14836/26933 [16:26<11:22, 17.71it/s] 55%|█████▌    | 14840/26933 [16:26<11:23, 17.70it/s] 55%|█████▌    | 14844/26933 [16:26<11:23, 17.70it/s] 55%|█████▌    | 14848/26933 [16:27<11:21, 17.73it/s] 55%|█████▌    | 14852/26933 [16:27<11:23, 17.68it/s] 55%|█████▌    | 14856/26933 [16:27<11:22, 17.69it/s] 55%|█████▌    | 14860/26933 [16:27<11:22, 17.69it/s] 55%|█████▌    | 14864/26933 [16:27<11:23, 17.65it/s] 55%|█████▌    | 14868/26933 [16:28<11:25, 17.59it/s] 55%|█████▌    | 14872/26933 [16:28<11:25, 17.59it/s] 55%|█████▌    | 14876/26933 [16:28<11:24, 17.61it/s] 55%|█████▌    | 14880/26933 [16:28<11:22, 17.66it/s] 55%|█████▌    | 14884/26933 [16:29<11:23, 17.63it/s] 55%|█████▌    | 14888/26933 [16:29<11:22, 17.65it/s] 55%|█████▌    | 14892/26933 [16:29<11:21, 17.67it/s] 55%|█████▌    | 14896/26933 [16:29<11:22, 17.64it/s] 55%|█████▌    | 14900/26933 [16:29<11:21, 17.65it/s] 55%|█████▌    | 14904/26933 [16:30<11:24, 17.57it/s] 55%|█████▌    | 14908/26933 [16:30<11:22, 17.62it/s] 55%|█████▌    | 14912/26933 [16:30<11:21, 17.65it/s] 55%|█████▌    | 14916/26933 [16:30<11:19, 17.69it/s] 55%|█████▌    | 14920/26933 [16:31<11:19, 17.68it/s] 55%|█████▌    | 14924/26933 [16:31<11:19, 17.68it/s] 55%|█████▌    | 14928/26933 [16:31<11:19, 17.68it/s] 55%|█████▌    | 14932/26933 [16:31<11:18, 17.68it/s] 55%|█████▌    | 14936/26933 [16:32<11:16, 17.72it/s] 55%|█████▌    | 14940/26933 [16:32<11:18, 17.68it/s] 55%|█████▌    | 14944/26933 [16:32<11:15, 17.75it/s] 56%|█████▌    | 14948/26933 [16:32<11:13, 17.79it/s] 56%|█████▌    | 14952/26933 [16:32<11:12, 17.82it/s] 56%|█████▌    | 14956/26933 [16:33<11:12, 17.81it/s] 56%|█████▌    | 14960/26933 [16:33<11:10, 17.85it/s] 56%|█████▌    | 14964/26933 [16:33<11:10, 17.85it/s] 56%|█████▌    | 14968/26933 [16:33<11:09, 17.88it/s] 56%|█████▌    | 14972/26933 [16:34<11:09, 17.87it/s] 56%|█████▌    | 14976/26933 [16:34<11:12, 17.77it/s] 56%|█████▌    | 14980/26933 [16:34<11:11, 17.80it/s] 56%|█████▌    | 14984/26933 [16:34<11:11, 17.79it/s] 56%|█████▌    | 14988/26933 [16:34<11:11, 17.78it/s] 56%|█████▌    | 14992/26933 [16:35<11:14, 17.69it/s] 56%|█████▌    | 14996/26933 [16:35<11:14, 17.70it/s] 56%|█████▌    | 15000/26933 [16:35<11:14, 17.70it/s] 56%|█████▌    | 15004/26933 [16:35<11:14, 17.70it/s] 56%|█████▌    | 15008/26933 [16:36<11:15, 17.64it/s] 56%|█████▌    | 15012/26933 [16:36<11:14, 17.67it/s] 56%|█████▌    | 15016/26933 [16:36<11:13, 17.68it/s] 56%|█████▌    | 15020/26933 [16:36<11:14, 17.67it/s] 56%|█████▌    | 15024/26933 [16:36<11:14, 17.66it/s] 56%|█████▌    | 15028/26933 [16:37<11:17, 17.58it/s] 56%|█████▌    | 15032/26933 [16:37<11:17, 17.58it/s] 56%|█████▌    | 15036/26933 [16:37<11:15, 17.61it/s] 56%|█████▌    | 15040/26933 [16:37<11:13, 17.65it/s] 56%|█████▌    | 15044/26933 [16:38<11:13, 17.66it/s] 56%|█████▌    | 15048/26933 [16:38<11:11, 17.70it/s] 56%|█████▌    | 15052/26933 [16:38<11:10, 17.73it/s] 56%|█████▌    | 15056/26933 [16:38<11:10, 17.71it/s] 56%|█████▌    | 15060/26933 [16:39<11:10, 17.72it/s] 56%|█████▌    | 15064/26933 [16:39<11:12, 17.64it/s] 56%|█████▌    | 15068/26933 [16:39<11:12, 17.63it/s] 56%|█████▌    | 15072/26933 [16:39<11:13, 17.61it/s] 56%|█████▌    | 15076/26933 [16:39<11:12, 17.64it/s] 56%|█████▌    | 15080/26933 [16:40<11:14, 17.58it/s] 56%|█████▌    | 15084/26933 [16:40<11:14, 17.57it/s] 56%|█████▌    | 15088/26933 [16:40<11:15, 17.54it/s] 56%|█████▌    | 15092/26933 [16:40<11:13, 17.58it/s] 56%|█████▌    | 15096/26933 [16:41<11:15, 17.54it/s] 56%|█████▌    | 15100/26933 [16:41<11:12, 17.58it/s] 56%|█████▌    | 15104/26933 [16:41<11:11, 17.61it/s] 56%|█████▌    | 15108/26933 [16:41<11:10, 17.63it/s] 56%|█████▌    | 15112/26933 [16:41<11:10, 17.63it/s] 56%|█████▌    | 15116/26933 [16:42<11:10, 17.62it/s] 56%|█████▌    | 15120/26933 [16:42<11:09, 17.64it/s] 56%|█████▌    | 15124/26933 [16:42<11:08, 17.67it/s] 56%|█████▌    | 15128/26933 [16:42<11:08, 17.66it/s] 56%|█████▌    | 15132/26933 [16:43<11:09, 17.62it/s] 56%|█████▌    | 15136/26933 [16:43<11:10, 17.61it/s] 56%|█████▌    | 15140/26933 [16:43<11:22, 17.27it/s] 56%|█████▌    | 15144/26933 [16:43<11:18, 17.37it/s] 56%|█████▌    | 15148/26933 [16:44<11:13, 17.49it/s] 56%|█████▋    | 15152/26933 [16:44<11:13, 17.48it/s] 56%|█████▋    | 15156/26933 [16:44<11:53, 16.51it/s] 56%|█████▋    | 15160/26933 [16:44<12:46, 15.37it/s] 56%|█████▋    | 15164/26933 [16:45<12:18, 15.95it/s] 56%|█████▋    | 15168/26933 [16:45<11:55, 16.44it/s] 56%|█████▋    | 15172/26933 [16:45<11:41, 16.77it/s] 56%|█████▋    | 15176/26933 [16:45<11:30, 17.03it/s] 56%|█████▋    | 15180/26933 [16:45<11:21, 17.24it/s] 56%|█████▋    | 15184/26933 [16:46<11:16, 17.36it/s] 56%|█████▋    | 15188/26933 [16:46<11:16, 17.35it/s] 56%|█████▋    | 15192/26933 [16:46<11:19, 17.27it/s] 56%|█████▋    | 15196/26933 [16:46<11:15, 17.38it/s] 56%|█████▋    | 15200/26933 [16:47<11:14, 17.39it/s] 56%|█████▋    | 15204/26933 [16:47<11:16, 17.34it/s] 56%|█████▋    | 15208/26933 [16:47<11:28, 17.03it/s] 56%|█████▋    | 15212/26933 [16:47<11:30, 16.97it/s] 56%|█████▋    | 15216/26933 [16:48<11:28, 17.01it/s] 57%|█████▋    | 15220/26933 [16:48<11:21, 17.20it/s] 57%|█████▋    | 15224/26933 [16:48<11:13, 17.40it/s] 57%|█████▋    | 15228/26933 [16:48<11:10, 17.47it/s] 57%|█████▋    | 15232/26933 [16:48<11:06, 17.56it/s] 57%|█████▋    | 15236/26933 [16:49<11:04, 17.61it/s] 57%|█████▋    | 15240/26933 [16:49<11:18, 17.23it/s] 57%|█████▋    | 15244/26933 [16:49<11:13, 17.36it/s] 57%|█████▋    | 15248/26933 [16:49<11:11, 17.41it/s] 57%|█████▋    | 15252/26933 [16:50<11:08, 17.46it/s] 57%|█████▋    | 15256/26933 [16:50<11:05, 17.54it/s] 57%|█████▋    | 15260/26933 [16:50<11:03, 17.59it/s] 57%|█████▋    | 15264/26933 [16:50<11:00, 17.66it/s] 57%|█████▋    | 15268/26933 [16:51<10:58, 17.73it/s] 57%|█████▋    | 15272/26933 [16:51<10:58, 17.71it/s] 57%|█████▋    | 15276/26933 [16:51<10:56, 17.75it/s] 57%|█████▋    | 15280/26933 [16:51<10:55, 17.77it/s] 57%|█████▋    | 15284/26933 [16:51<10:55, 17.78it/s] 57%|█████▋    | 15288/26933 [16:52<10:56, 17.73it/s] 57%|█████▋    | 15292/26933 [16:52<10:56, 17.73it/s] 57%|█████▋    | 15296/26933 [16:52<10:57, 17.71it/s] 57%|█████▋    | 15300/26933 [16:52<10:56, 17.71it/s] 57%|█████▋    | 15304/26933 [16:53<10:57, 17.69it/s] 57%|█████▋    | 15308/26933 [16:53<10:58, 17.64it/s] 57%|█████▋    | 15312/26933 [16:53<10:59, 17.62it/s] 57%|█████▋    | 15316/26933 [16:53<10:59, 17.62it/s] 57%|█████▋    | 15320/26933 [16:53<10:58, 17.62it/s] 57%|█████▋    | 15324/26933 [16:54<10:59, 17.60it/s] 57%|█████▋    | 15328/26933 [16:54<10:58, 17.62it/s] 57%|█████▋    | 15332/26933 [16:54<10:56, 17.66it/s] 57%|█████▋    | 15336/26933 [16:54<10:56, 17.67it/s] 57%|█████▋    | 15340/26933 [16:55<10:56, 17.65it/s] 57%|█████▋    | 15344/26933 [16:55<10:55, 17.67it/s] 57%|█████▋    | 15348/26933 [16:55<10:55, 17.68it/s] 57%|█████▋    | 15352/26933 [16:55<10:54, 17.70it/s] 57%|█████▋    | 15356/26933 [16:55<10:55, 17.66it/s] 57%|█████▋    | 15360/26933 [16:56<10:56, 17.62it/s] 57%|█████▋    | 15364/26933 [16:56<10:57, 17.61it/s] 57%|█████▋    | 15368/26933 [16:56<10:56, 17.61it/s] 57%|█████▋    | 15372/26933 [16:56<10:58, 17.56it/s] 57%|█████▋    | 15376/26933 [16:57<11:00, 17.50it/s] 57%|█████▋    | 15380/26933 [16:57<10:58, 17.56it/s] 57%|█████▋    | 15384/26933 [16:57<10:55, 17.62it/s] 57%|█████▋    | 15388/26933 [16:57<10:52, 17.69it/s] 57%|█████▋    | 15392/26933 [16:58<10:51, 17.72it/s] 57%|█████▋    | 15396/26933 [16:58<10:51, 17.71it/s] 57%|█████▋    | 15400/26933 [16:58<10:49, 17.77it/s] 57%|█████▋    | 15404/26933 [16:58<10:48, 17.77it/s] 57%|█████▋    | 15408/26933 [16:58<10:48, 17.77it/s] 57%|█████▋    | 15412/26933 [16:59<10:48, 17.76it/s] 57%|█████▋    | 15416/26933 [16:59<10:47, 17.78it/s] 57%|█████▋    | 15420/26933 [16:59<10:46, 17.81it/s] 57%|█████▋    | 15424/26933 [16:59<10:47, 17.78it/s] 57%|█████▋    | 15428/26933 [17:00<10:49, 17.71it/s] 57%|█████▋    | 15432/26933 [17:00<10:48, 17.73it/s] 57%|█████▋    | 15436/26933 [17:00<10:47, 17.75it/s] 57%|█████▋    | 15440/26933 [17:00<10:47, 17.76it/s] 57%|█████▋    | 15444/26933 [17:00<10:46, 17.77it/s] 57%|█████▋    | 15448/26933 [17:01<10:47, 17.73it/s] 57%|█████▋    | 15452/26933 [17:01<10:46, 17.75it/s] 57%|█████▋    | 15456/26933 [17:01<10:45, 17.77it/s] 57%|█████▋    | 15460/26933 [17:01<10:43, 17.82it/s] 57%|█████▋    | 15464/26933 [17:02<10:45, 17.78it/s] 57%|█████▋    | 15468/26933 [17:02<10:42, 17.84it/s] 57%|█████▋    | 15472/26933 [17:02<10:41, 17.87it/s] 57%|█████▋    | 15476/26933 [17:02<10:41, 17.85it/s] 57%|█████▋    | 15480/26933 [17:02<10:41, 17.86it/s] 57%|█████▋    | 15484/26933 [17:03<10:43, 17.78it/s] 58%|█████▊    | 15488/26933 [17:03<10:41, 17.83it/s] 58%|█████▊    | 15492/26933 [17:03<10:41, 17.83it/s] 58%|█████▊    | 15496/26933 [17:03<10:42, 17.81it/s] 58%|█████▊    | 15500/26933 [17:04<10:44, 17.75it/s] 58%|█████▊    | 15504/26933 [17:04<10:42, 17.78it/s] 58%|█████▊    | 15508/26933 [17:04<10:43, 17.75it/s] 58%|█████▊    | 15512/26933 [17:04<10:43, 17.74it/s] 58%|█████▊    | 15516/26933 [17:05<10:43, 17.74it/s] 58%|█████▊    | 15520/26933 [17:05<10:46, 17.65it/s] 58%|█████▊    | 15524/26933 [17:05<10:46, 17.66it/s] 58%|█████▊    | 15528/26933 [17:05<10:44, 17.68it/s] 58%|█████▊    | 15532/26933 [17:05<10:45, 17.66it/s] 58%|█████▊    | 15536/26933 [17:06<10:47, 17.59it/s] 58%|█████▊    | 15540/26933 [17:06<10:46, 17.62it/s] 58%|█████▊    | 15544/26933 [17:06<10:45, 17.64it/s] 58%|█████▊    | 15548/26933 [17:06<10:44, 17.66it/s] 58%|█████▊    | 15552/26933 [17:07<10:44, 17.65it/s] 58%|█████▊    | 15556/26933 [17:07<10:44, 17.65it/s] 58%|█████▊    | 15560/26933 [17:07<10:44, 17.65it/s] 58%|█████▊    | 15564/26933 [17:07<10:43, 17.67it/s] 58%|█████▊    | 15568/26933 [17:07<10:43, 17.67it/s] 58%|█████▊    | 15572/26933 [17:08<10:45, 17.60it/s] 58%|█████▊    | 15576/26933 [17:08<10:45, 17.60it/s] 58%|█████▊    | 15580/26933 [17:08<10:45, 17.60it/s] 58%|█████▊    | 15584/26933 [17:08<10:42, 17.66it/s] 58%|█████▊    | 15588/26933 [17:09<10:41, 17.70it/s] 58%|█████▊    | 15592/26933 [17:09<10:40, 17.71it/s] 58%|█████▊    | 15596/26933 [17:09<10:38, 17.74it/s] 58%|█████▊    | 15600/26933 [17:09<10:37, 17.77it/s] 58%|█████▊    | 15604/26933 [17:09<10:40, 17.69it/s] 58%|█████▊    | 15608/26933 [17:10<10:41, 17.67it/s] 58%|█████▊    | 15612/26933 [17:10<10:38, 17.73it/s] 58%|█████▊    | 15616/26933 [17:10<10:36, 17.77it/s] 58%|█████▊    | 15620/26933 [17:10<10:34, 17.83it/s] 58%|█████▊    | 15624/26933 [17:11<10:35, 17.79it/s] 58%|█████▊    | 15628/26933 [17:11<10:34, 17.81it/s] 58%|█████▊    | 15632/26933 [17:11<10:35, 17.77it/s] 58%|█████▊    | 15636/26933 [17:11<10:35, 17.78it/s] 58%|█████▊    | 15640/26933 [17:12<10:34, 17.81it/s] 58%|█████▊    | 15644/26933 [17:12<10:33, 17.82it/s] 58%|█████▊    | 15648/26933 [17:12<10:31, 17.87it/s] 58%|█████▊    | 15652/26933 [17:12<10:29, 17.93it/s] 58%|█████▊    | 15656/26933 [17:12<10:28, 17.93it/s] 58%|█████▊    | 15660/26933 [17:13<10:29, 17.91it/s] 58%|█████▊    | 15664/26933 [17:13<10:29, 17.91it/s] 58%|█████▊    | 15668/26933 [17:13<10:28, 17.94it/s] 58%|█████▊    | 15672/26933 [17:13<10:26, 17.96it/s] 58%|█████▊    | 15676/26933 [17:14<10:28, 17.92it/s] 58%|█████▊    | 15680/26933 [17:14<10:39, 17.59it/s] 58%|█████▊    | 15684/26933 [17:14<10:33, 17.75it/s] 58%|█████▊    | 15688/26933 [17:14<10:30, 17.82it/s] 58%|█████▊    | 15692/26933 [17:14<10:28, 17.88it/s] 58%|█████▊    | 15696/26933 [17:15<10:28, 17.87it/s] 58%|█████▊    | 15700/26933 [17:15<10:27, 17.90it/s] 58%|█████▊    | 15704/26933 [17:15<10:25, 17.96it/s] 58%|█████▊    | 15708/26933 [17:15<10:24, 17.97it/s] 58%|█████▊    | 15712/26933 [17:16<10:23, 17.98it/s] 58%|█████▊    | 15716/26933 [17:16<10:25, 17.93it/s] 58%|█████▊    | 15720/26933 [17:16<10:24, 17.96it/s] 58%|█████▊    | 15724/26933 [17:16<10:23, 17.97it/s] 58%|█████▊    | 15728/26933 [17:16<10:24, 17.95it/s] 58%|█████▊    | 15732/26933 [17:17<10:24, 17.93it/s] 58%|█████▊    | 15736/26933 [17:17<10:24, 17.93it/s] 58%|█████▊    | 15740/26933 [17:17<10:23, 17.95it/s] 58%|█████▊    | 15744/26933 [17:17<10:23, 17.95it/s] 58%|█████▊    | 15748/26933 [17:18<10:23, 17.95it/s] 58%|█████▊    | 15752/26933 [17:18<10:24, 17.90it/s] 59%|█████▊    | 15756/26933 [17:18<10:23, 17.92it/s] 59%|█████▊    | 15760/26933 [17:18<10:23, 17.92it/s] 59%|█████▊    | 15764/26933 [17:18<10:22, 17.94it/s] 59%|█████▊    | 15768/26933 [17:19<10:23, 17.91it/s] 59%|█████▊    | 15772/26933 [17:19<10:23, 17.91it/s] 59%|█████▊    | 15776/26933 [17:19<10:22, 17.91it/s] 59%|█████▊    | 15780/26933 [17:19<10:22, 17.93it/s] 59%|█████▊    | 15784/26933 [17:20<10:22, 17.91it/s] 59%|█████▊    | 15788/26933 [17:20<10:20, 17.95it/s] 59%|█████▊    | 15792/26933 [17:20<10:21, 17.93it/s] 59%|█████▊    | 15796/26933 [17:20<10:20, 17.96it/s] 59%|█████▊    | 15800/26933 [17:20<10:18, 17.99it/s] 59%|█████▊    | 15804/26933 [17:21<10:20, 17.93it/s] 59%|█████▊    | 15808/26933 [17:21<10:20, 17.94it/s] 59%|█████▊    | 15812/26933 [17:21<10:20, 17.93it/s] 59%|█████▊    | 15816/26933 [17:21<10:20, 17.91it/s] 59%|█████▊    | 15820/26933 [17:22<10:22, 17.85it/s] 59%|█████▉    | 15824/26933 [17:22<10:21, 17.87it/s] 59%|█████▉    | 15828/26933 [17:22<10:21, 17.87it/s] 59%|█████▉    | 15832/26933 [17:22<10:20, 17.88it/s] 59%|█████▉    | 15836/26933 [17:22<10:20, 17.89it/s] 59%|█████▉    | 15840/26933 [17:23<10:20, 17.87it/s] 59%|█████▉    | 15844/26933 [17:23<10:19, 17.89it/s] 59%|█████▉    | 15848/26933 [17:23<10:19, 17.90it/s] 59%|█████▉    | 15852/26933 [17:23<10:19, 17.90it/s] 59%|█████▉    | 15856/26933 [17:24<10:19, 17.88it/s] 59%|█████▉    | 15860/26933 [17:24<10:17, 17.93it/s] 59%|█████▉    | 15864/26933 [17:24<10:17, 17.92it/s] 59%|█████▉    | 15868/26933 [17:24<10:16, 17.93it/s] 59%|█████▉    | 15872/26933 [17:24<10:16, 17.95it/s] 59%|█████▉    | 15876/26933 [17:25<10:16, 17.94it/s] 59%|█████▉    | 15880/26933 [17:25<10:15, 17.96it/s] 59%|█████▉    | 15884/26933 [17:25<10:14, 17.97it/s] 59%|█████▉    | 15888/26933 [17:25<10:15, 17.94it/s] 59%|█████▉    | 15892/26933 [17:26<10:18, 17.84it/s] 59%|█████▉    | 15896/26933 [17:26<10:19, 17.83it/s] 59%|█████▉    | 15900/26933 [17:26<10:19, 17.80it/s] 59%|█████▉    | 15904/26933 [17:26<10:18, 17.84it/s] 59%|█████▉    | 15908/26933 [17:26<10:16, 17.88it/s] 59%|█████▉    | 15912/26933 [17:27<10:16, 17.89it/s] 59%|█████▉    | 15916/26933 [17:27<10:15, 17.91it/s] 59%|█████▉    | 15920/26933 [17:27<10:14, 17.91it/s] 59%|█████▉    | 15924/26933 [17:27<10:16, 17.85it/s] 59%|█████▉    | 15928/26933 [17:28<10:19, 17.77it/s] 59%|█████▉    | 15932/26933 [17:28<10:19, 17.77it/s] 59%|█████▉    | 15936/26933 [17:28<10:19, 17.76it/s] 59%|█████▉    | 15940/26933 [17:28<10:20, 17.71it/s] 59%|█████▉    | 15944/26933 [17:29<10:19, 17.73it/s] 59%|█████▉    | 15948/26933 [17:29<10:20, 17.69it/s] 59%|█████▉    | 15952/26933 [17:29<10:20, 17.68it/s] 59%|█████▉    | 15956/26933 [17:29<10:20, 17.70it/s] 59%|█████▉    | 15960/26933 [17:29<10:19, 17.71it/s] 59%|█████▉    | 15964/26933 [17:30<10:20, 17.67it/s] 59%|█████▉    | 15968/26933 [17:30<10:20, 17.68it/s] 59%|█████▉    | 15972/26933 [17:30<10:20, 17.67it/s] 59%|█████▉    | 15976/26933 [17:30<10:19, 17.68it/s] 59%|█████▉    | 15980/26933 [17:31<10:19, 17.68it/s] 59%|█████▉    | 15984/26933 [17:31<10:20, 17.65it/s] 59%|█████▉    | 15988/26933 [17:31<10:19, 17.67it/s] 59%|█████▉    | 15992/26933 [17:31<10:18, 17.68it/s] 59%|█████▉    | 15996/26933 [17:31<10:18, 17.68it/s] 59%|█████▉    | 16000/26933 [17:32<10:18, 17.67it/s] 59%|█████▉    | 16004/26933 [17:32<10:15, 17.75it/s] 59%|█████▉    | 16008/26933 [17:32<10:14, 17.78it/s] 59%|█████▉    | 16012/26933 [17:32<10:11, 17.85it/s] 59%|█████▉    | 16016/26933 [17:33<10:12, 17.82it/s] 59%|█████▉    | 16020/26933 [17:33<10:11, 17.84it/s] 59%|█████▉    | 16024/26933 [17:33<10:10, 17.87it/s] 60%|█████▉    | 16028/26933 [17:33<10:09, 17.89it/s] 60%|█████▉    | 16032/26933 [17:33<10:09, 17.89it/s] 60%|█████▉    | 16036/26933 [17:34<10:10, 17.84it/s] 60%|█████▉    | 16040/26933 [17:34<10:11, 17.83it/s] 60%|█████▉    | 16044/26933 [17:34<10:10, 17.83it/s] 60%|█████▉    | 16048/26933 [17:34<10:10, 17.83it/s] 60%|█████▉    | 16052/26933 [17:35<10:12, 17.76it/s] 60%|█████▉    | 16056/26933 [17:35<10:11, 17.79it/s] 60%|█████▉    | 16060/26933 [17:35<10:12, 17.76it/s] 60%|█████▉    | 16064/26933 [17:35<10:11, 17.77it/s] 60%|█████▉    | 16068/26933 [17:35<10:12, 17.74it/s] 60%|█████▉    | 16072/26933 [17:36<10:14, 17.68it/s] 60%|█████▉    | 16076/26933 [17:36<10:13, 17.71it/s] 60%|█████▉    | 16080/26933 [17:36<10:11, 17.75it/s] 60%|█████▉    | 16084/26933 [17:36<10:10, 17.76it/s] 60%|█████▉    | 16088/26933 [17:37<10:11, 17.72it/s] 60%|█████▉    | 16092/26933 [17:37<10:10, 17.75it/s] 60%|█████▉    | 16096/26933 [17:37<10:09, 17.78it/s] 60%|█████▉    | 16100/26933 [17:37<10:07, 17.84it/s] 60%|█████▉    | 16104/26933 [17:38<10:07, 17.82it/s] 60%|█████▉    | 16108/26933 [17:38<10:07, 17.82it/s] 60%|█████▉    | 16112/26933 [17:38<10:08, 17.79it/s] 60%|█████▉    | 16116/26933 [17:38<10:07, 17.80it/s] 60%|█████▉    | 16120/26933 [17:38<10:06, 17.84it/s] 60%|█████▉    | 16124/26933 [17:39<10:10, 17.71it/s] 60%|█████▉    | 16128/26933 [17:39<10:08, 17.76it/s] 60%|█████▉    | 16132/26933 [17:39<10:06, 17.80it/s] 60%|█████▉    | 16136/26933 [17:39<10:04, 17.86it/s] 60%|█████▉    | 16140/26933 [17:40<10:05, 17.83it/s] 60%|█████▉    | 16144/26933 [17:40<10:04, 17.86it/s] 60%|█████▉    | 16148/26933 [17:40<10:02, 17.91it/s] 60%|█████▉    | 16152/26933 [17:40<10:04, 17.83it/s] 60%|█████▉    | 16156/26933 [17:40<10:25, 17.22it/s] 60%|██████    | 16160/26933 [17:41<10:21, 17.33it/s] 60%|██████    | 16164/26933 [17:41<10:18, 17.42it/s] 60%|██████    | 16168/26933 [17:41<10:15, 17.49it/s] 60%|██████    | 16172/26933 [17:41<10:12, 17.56it/s] 60%|██████    | 16176/26933 [17:42<10:12, 17.55it/s] 60%|██████    | 16180/26933 [17:42<10:09, 17.63it/s] 60%|██████    | 16184/26933 [17:42<10:07, 17.70it/s] 60%|██████    | 16188/26933 [17:42<10:05, 17.75it/s] 60%|██████    | 16192/26933 [17:42<10:03, 17.79it/s] 60%|██████    | 16196/26933 [17:43<10:04, 17.77it/s] 60%|██████    | 16200/26933 [17:43<10:04, 17.77it/s] 60%|██████    | 16204/26933 [17:43<10:07, 17.67it/s] 60%|██████    | 16208/26933 [17:43<10:04, 17.74it/s] 60%|██████    | 16212/26933 [17:44<10:05, 17.71it/s] 60%|██████    | 16216/26933 [17:44<10:03, 17.77it/s] 60%|██████    | 16220/26933 [17:44<10:01, 17.80it/s] 60%|██████    | 16224/26933 [17:44<09:59, 17.88it/s] 60%|██████    | 16228/26933 [17:45<09:57, 17.92it/s] 60%|██████    | 16232/26933 [17:45<09:58, 17.88it/s] 60%|██████    | 16236/26933 [17:45<09:56, 17.92it/s] 60%|██████    | 16240/26933 [17:45<09:57, 17.90it/s] 60%|██████    | 16244/26933 [17:45<09:56, 17.92it/s] 60%|██████    | 16248/26933 [17:46<09:58, 17.86it/s] 60%|██████    | 16252/26933 [17:46<09:58, 17.86it/s] 60%|██████    | 16256/26933 [17:46<09:57, 17.86it/s] 60%|██████    | 16260/26933 [17:46<09:57, 17.85it/s] 60%|██████    | 16264/26933 [17:47<09:57, 17.85it/s] 60%|██████    | 16268/26933 [17:47<09:59, 17.78it/s] 60%|██████    | 16272/26933 [17:47<09:59, 17.79it/s] 60%|██████    | 16276/26933 [17:47<09:59, 17.77it/s] 60%|██████    | 16280/26933 [17:47<09:59, 17.78it/s] 60%|██████    | 16284/26933 [17:48<10:05, 17.57it/s] 60%|██████    | 16288/26933 [17:48<10:04, 17.62it/s] 60%|██████    | 16292/26933 [17:48<10:02, 17.66it/s] 61%|██████    | 16296/26933 [17:48<10:00, 17.72it/s] 61%|██████    | 16300/26933 [17:49<10:01, 17.67it/s] 61%|██████    | 16304/26933 [17:49<10:01, 17.66it/s] 61%|██████    | 16308/26933 [17:49<10:00, 17.70it/s] 61%|██████    | 16312/26933 [17:49<09:59, 17.73it/s] 61%|██████    | 16316/26933 [17:49<09:57, 17.76it/s] 61%|██████    | 16320/26933 [17:50<09:58, 17.74it/s] 61%|██████    | 16324/26933 [17:50<09:57, 17.76it/s] 61%|██████    | 16328/26933 [17:50<09:56, 17.79it/s] 61%|██████    | 16332/26933 [17:50<09:55, 17.81it/s] 61%|██████    | 16336/26933 [17:51<09:56, 17.76it/s] 61%|██████    | 16340/26933 [17:51<09:55, 17.78it/s] 61%|██████    | 16344/26933 [17:51<09:55, 17.77it/s] 61%|██████    | 16348/26933 [17:51<09:55, 17.77it/s] 61%|██████    | 16352/26933 [17:51<09:54, 17.78it/s] 61%|██████    | 16356/26933 [17:52<09:57, 17.69it/s] 61%|██████    | 16360/26933 [17:52<09:59, 17.64it/s] 61%|██████    | 16364/26933 [17:52<09:59, 17.63it/s] 61%|██████    | 16368/26933 [17:52<10:00, 17.59it/s] 61%|██████    | 16372/26933 [17:53<10:01, 17.55it/s] 61%|██████    | 16376/26933 [17:53<09:59, 17.60it/s] 61%|██████    | 16380/26933 [17:53<09:58, 17.62it/s] 61%|██████    | 16384/26933 [17:53<09:58, 17.63it/s] 61%|██████    | 16388/26933 [17:54<09:57, 17.64it/s] 61%|██████    | 16392/26933 [17:54<09:57, 17.64it/s] 61%|██████    | 16396/26933 [17:54<09:56, 17.65it/s] 61%|██████    | 16400/26933 [17:54<09:55, 17.68it/s] 61%|██████    | 16404/26933 [17:54<09:55, 17.69it/s] 61%|██████    | 16408/26933 [17:55<09:56, 17.64it/s] 61%|██████    | 16412/26933 [17:55<09:57, 17.62it/s] 61%|██████    | 16416/26933 [17:55<09:57, 17.61it/s] 61%|██████    | 16420/26933 [17:55<09:57, 17.60it/s] 61%|██████    | 16424/26933 [17:56<09:58, 17.57it/s] 61%|██████    | 16428/26933 [17:56<09:56, 17.63it/s] 61%|██████    | 16432/26933 [17:56<09:55, 17.64it/s] 61%|██████    | 16436/26933 [17:56<09:54, 17.66it/s] 61%|██████    | 16440/26933 [17:56<09:53, 17.68it/s] 61%|██████    | 16444/26933 [17:57<09:54, 17.65it/s] 61%|██████    | 16448/26933 [17:57<09:53, 17.66it/s] 61%|██████    | 16452/26933 [17:57<09:54, 17.64it/s] 61%|██████    | 16456/26933 [17:57<09:53, 17.64it/s] 61%|██████    | 16460/26933 [17:58<09:56, 17.56it/s] 61%|██████    | 16464/26933 [17:58<09:54, 17.60it/s] 61%|██████    | 16468/26933 [17:58<09:54, 17.60it/s] 61%|██████    | 16472/26933 [17:58<09:54, 17.60it/s] 61%|██████    | 16476/26933 [17:59<09:52, 17.64it/s] 61%|██████    | 16480/26933 [17:59<09:53, 17.61it/s] 61%|██████    | 16484/26933 [17:59<09:52, 17.63it/s] 61%|██████    | 16488/26933 [17:59<09:53, 17.61it/s] 61%|██████    | 16492/26933 [17:59<09:53, 17.60it/s] 61%|██████    | 16496/26933 [18:00<09:53, 17.58it/s] 61%|██████▏   | 16500/26933 [18:00<09:51, 17.63it/s] 61%|██████▏   | 16504/26933 [18:00<09:50, 17.65it/s] 61%|██████▏   | 16508/26933 [18:00<09:50, 17.65it/s] 61%|██████▏   | 16512/26933 [18:01<09:54, 17.52it/s] 61%|██████▏   | 16516/26933 [18:01<09:52, 17.58it/s] 61%|██████▏   | 16520/26933 [18:01<09:53, 17.55it/s] 61%|██████▏   | 16524/26933 [18:01<09:52, 17.56it/s] 61%|██████▏   | 16528/26933 [18:01<09:51, 17.59it/s] 61%|██████▏   | 16532/26933 [18:02<09:51, 17.59it/s] 61%|██████▏   | 16536/26933 [18:02<09:50, 17.62it/s] 61%|██████▏   | 16540/26933 [18:02<09:49, 17.64it/s] 61%|██████▏   | 16544/26933 [18:02<09:49, 17.64it/s] 61%|██████▏   | 16548/26933 [18:03<09:49, 17.61it/s] 61%|██████▏   | 16552/26933 [18:03<09:47, 17.67it/s] 61%|██████▏   | 16556/26933 [18:03<09:47, 17.67it/s] 61%|██████▏   | 16560/26933 [18:03<09:47, 17.65it/s] 62%|██████▏   | 16564/26933 [18:04<09:47, 17.64it/s] 62%|██████▏   | 16568/26933 [18:04<09:49, 17.59it/s] 62%|██████▏   | 16572/26933 [18:04<09:48, 17.61it/s] 62%|██████▏   | 16576/26933 [18:04<09:50, 17.55it/s] 62%|██████▏   | 16580/26933 [18:04<09:49, 17.56it/s] 62%|██████▏   | 16584/26933 [18:05<09:50, 17.51it/s] 62%|██████▏   | 16588/26933 [18:05<09:50, 17.52it/s] 62%|██████▏   | 16592/26933 [18:05<09:49, 17.53it/s] 62%|██████▏   | 16596/26933 [18:05<09:47, 17.61it/s] 62%|██████▏   | 16600/26933 [18:06<09:47, 17.59it/s] 62%|██████▏   | 16604/26933 [18:06<09:45, 17.63it/s] 62%|██████▏   | 16608/26933 [18:06<09:46, 17.62it/s] 62%|██████▏   | 16612/26933 [18:06<09:44, 17.64it/s] 62%|██████▏   | 16616/26933 [18:06<09:43, 17.67it/s] 62%|██████▏   | 16620/26933 [18:07<09:45, 17.62it/s] 62%|██████▏   | 16624/26933 [18:07<09:45, 17.61it/s] 62%|██████▏   | 16628/26933 [18:07<09:45, 17.61it/s] 62%|██████▏   | 16632/26933 [18:07<09:43, 17.64it/s] 62%|██████▏   | 16636/26933 [18:08<09:44, 17.62it/s] 62%|██████▏   | 16640/26933 [18:08<09:42, 17.66it/s] 62%|██████▏   | 16644/26933 [18:08<09:47, 17.51it/s] 62%|██████▏   | 16648/26933 [18:08<09:45, 17.58it/s] 62%|██████▏   | 16652/26933 [18:09<09:43, 17.62it/s] 62%|██████▏   | 16656/26933 [18:09<09:44, 17.60it/s] 62%|██████▏   | 16660/26933 [18:09<09:42, 17.64it/s] 62%|██████▏   | 16664/26933 [18:09<09:42, 17.63it/s] 62%|██████▏   | 16668/26933 [18:09<09:41, 17.66it/s] 62%|██████▏   | 16672/26933 [18:10<09:43, 17.60it/s] 62%|██████▏   | 16676/26933 [18:10<09:41, 17.65it/s] 62%|██████▏   | 16680/26933 [18:10<09:40, 17.66it/s] 62%|██████▏   | 16684/26933 [18:10<09:41, 17.63it/s] 62%|██████▏   | 16688/26933 [18:11<09:42, 17.59it/s] 62%|██████▏   | 16692/26933 [18:11<09:40, 17.64it/s] 62%|██████▏   | 16696/26933 [18:11<09:40, 17.64it/s] 62%|██████▏   | 16700/26933 [18:11<09:41, 17.61it/s] 62%|██████▏   | 16704/26933 [18:11<09:40, 17.63it/s] 62%|██████▏   | 16708/26933 [18:12<09:43, 17.53it/s] 62%|██████▏   | 16712/26933 [18:12<09:43, 17.50it/s] 62%|██████▏   | 16716/26933 [18:12<09:42, 17.54it/s] 62%|██████▏   | 16720/26933 [18:12<09:42, 17.55it/s] 62%|██████▏   | 16724/26933 [18:13<09:43, 17.51it/s] 62%|██████▏   | 16728/26933 [18:13<09:41, 17.54it/s] 62%|██████▏   | 16732/26933 [18:13<09:41, 17.55it/s] 62%|██████▏   | 16736/26933 [18:13<09:39, 17.59it/s] 62%|██████▏   | 16740/26933 [18:14<09:40, 17.57it/s] 62%|██████▏   | 16744/26933 [18:14<09:39, 17.58it/s] 62%|██████▏   | 16748/26933 [18:14<09:38, 17.61it/s] 62%|██████▏   | 16752/26933 [18:14<09:37, 17.62it/s] 62%|██████▏   | 16756/26933 [18:14<09:37, 17.63it/s] 62%|██████▏   | 16760/26933 [18:15<09:37, 17.62it/s] 62%|██████▏   | 16764/26933 [18:15<09:36, 17.64it/s] 62%|██████▏   | 16768/26933 [18:15<09:35, 17.65it/s] 62%|██████▏   | 16772/26933 [18:15<09:35, 17.67it/s] 62%|██████▏   | 16776/26933 [18:16<09:37, 17.59it/s] 62%|██████▏   | 16780/26933 [18:16<09:39, 17.53it/s] 62%|██████▏   | 16784/26933 [18:16<09:41, 17.46it/s] 62%|██████▏   | 16788/26933 [18:16<09:45, 17.33it/s] 62%|██████▏   | 16792/26933 [18:17<09:46, 17.28it/s] 62%|██████▏   | 16796/26933 [18:17<09:46, 17.29it/s] 62%|██████▏   | 16800/26933 [18:17<09:43, 17.38it/s] 62%|██████▏   | 16804/26933 [18:17<09:40, 17.44it/s] 62%|██████▏   | 16808/26933 [18:17<09:38, 17.50it/s] 62%|██████▏   | 16812/26933 [18:18<09:38, 17.49it/s] 62%|██████▏   | 16816/26933 [18:18<09:37, 17.53it/s] 62%|██████▏   | 16820/26933 [18:18<09:35, 17.57it/s] 62%|██████▏   | 16824/26933 [18:18<09:35, 17.57it/s] 62%|██████▏   | 16828/26933 [18:19<09:34, 17.59it/s] 62%|██████▏   | 16832/26933 [18:19<09:34, 17.59it/s] 63%|██████▎   | 16836/26933 [18:19<09:33, 17.59it/s] 63%|██████▎   | 16840/26933 [18:19<09:33, 17.60it/s] 63%|██████▎   | 16844/26933 [18:19<09:32, 17.61it/s] 63%|██████▎   | 16848/26933 [18:20<09:33, 17.59it/s] 63%|██████▎   | 16852/26933 [18:20<09:31, 17.65it/s] 63%|██████▎   | 16856/26933 [18:20<09:31, 17.63it/s] 63%|██████▎   | 16860/26933 [18:20<09:30, 17.67it/s] 63%|██████▎   | 16864/26933 [18:21<09:30, 17.65it/s] 63%|██████▎   | 16868/26933 [18:21<09:30, 17.65it/s] 63%|██████▎   | 16872/26933 [18:21<09:29, 17.66it/s] 63%|██████▎   | 16876/26933 [18:21<09:28, 17.69it/s] 63%|██████▎   | 16880/26933 [18:21<09:28, 17.68it/s] 63%|██████▎   | 16884/26933 [18:22<09:30, 17.63it/s] 63%|██████▎   | 16888/26933 [18:22<09:29, 17.65it/s] 63%|██████▎   | 16892/26933 [18:22<09:28, 17.67it/s] 63%|██████▎   | 16896/26933 [18:22<09:27, 17.69it/s] 63%|██████▎   | 16900/26933 [18:23<09:28, 17.65it/s] 63%|██████▎   | 16904/26933 [18:23<09:28, 17.64it/s] 63%|██████▎   | 16908/26933 [18:23<09:27, 17.65it/s] 63%|██████▎   | 16912/26933 [18:23<09:27, 17.67it/s] 63%|██████▎   | 16916/26933 [18:24<09:27, 17.66it/s] 63%|██████▎   | 16920/26933 [18:24<09:28, 17.62it/s] 63%|██████▎   | 16924/26933 [18:24<09:27, 17.65it/s] 63%|██████▎   | 16928/26933 [18:24<09:25, 17.68it/s] 63%|██████▎   | 16932/26933 [18:24<09:25, 17.68it/s] 63%|██████▎   | 16936/26933 [18:25<09:26, 17.65it/s] 63%|██████▎   | 16940/26933 [18:25<09:25, 17.67it/s] 63%|██████▎   | 16944/26933 [18:25<09:25, 17.67it/s] 63%|██████▎   | 16948/26933 [18:25<09:25, 17.65it/s] 63%|██████▎   | 16952/26933 [18:26<09:26, 17.63it/s] 63%|██████▎   | 16956/26933 [18:26<09:25, 17.66it/s] 63%|██████▎   | 16960/26933 [18:26<09:25, 17.65it/s] 63%|██████▎   | 16964/26933 [18:26<09:24, 17.65it/s] 63%|██████▎   | 16968/26933 [18:26<09:23, 17.68it/s] 63%|██████▎   | 16972/26933 [18:27<09:24, 17.65it/s] 63%|██████▎   | 16976/26933 [18:27<09:23, 17.67it/s] 63%|██████▎   | 16980/26933 [18:27<09:22, 17.69it/s] 63%|██████▎   | 16984/26933 [18:27<09:22, 17.69it/s] 63%|██████▎   | 16988/26933 [18:28<09:23, 17.63it/s] 63%|██████▎   | 16992/26933 [18:28<09:23, 17.63it/s] 63%|██████▎   | 16996/26933 [18:28<09:22, 17.67it/s] 63%|██████▎   | 17000/26933 [18:28<09:22, 17.67it/s] 63%|██████▎   | 17004/26933 [18:29<09:21, 17.68it/s] 63%|██████▎   | 17008/26933 [18:29<09:22, 17.65it/s] 63%|██████▎   | 17012/26933 [18:29<09:22, 17.64it/s] 63%|██████▎   | 17016/26933 [18:29<09:22, 17.62it/s] 63%|██████▎   | 17020/26933 [18:29<09:22, 17.64it/s] 63%|██████▎   | 17024/26933 [18:30<09:23, 17.58it/s] 63%|██████▎   | 17028/26933 [18:30<09:22, 17.62it/s] 63%|██████▎   | 17032/26933 [18:30<09:21, 17.65it/s] 63%|██████▎   | 17036/26933 [18:30<09:21, 17.63it/s] 63%|██████▎   | 17040/26933 [18:31<09:22, 17.59it/s] 63%|██████▎   | 17044/26933 [18:31<09:22, 17.57it/s] 63%|██████▎   | 17048/26933 [18:31<09:22, 17.59it/s] 63%|██████▎   | 17052/26933 [18:31<09:21, 17.59it/s] 63%|██████▎   | 17056/26933 [18:31<09:21, 17.60it/s] 63%|██████▎   | 17060/26933 [18:32<09:22, 17.57it/s] 63%|██████▎   | 17064/26933 [18:32<09:20, 17.60it/s] 63%|██████▎   | 17068/26933 [18:32<09:20, 17.61it/s] 63%|██████▎   | 17072/26933 [18:32<09:19, 17.62it/s] 63%|██████▎   | 17076/26933 [18:33<09:20, 17.58it/s] 63%|██████▎   | 17080/26933 [18:33<09:20, 17.59it/s] 63%|██████▎   | 17084/26933 [18:33<09:18, 17.63it/s] 63%|██████▎   | 17088/26933 [18:33<09:18, 17.63it/s] 63%|██████▎   | 17092/26933 [18:34<09:18, 17.64it/s] 63%|██████▎   | 17096/26933 [18:34<09:18, 17.61it/s] 63%|██████▎   | 17100/26933 [18:34<09:18, 17.62it/s] 64%|██████▎   | 17104/26933 [18:34<09:17, 17.62it/s] 64%|██████▎   | 17108/26933 [18:34<09:17, 17.64it/s] 64%|██████▎   | 17112/26933 [18:35<09:17, 17.63it/s] 64%|██████▎   | 17116/26933 [18:35<09:16, 17.66it/s] 64%|██████▎   | 17120/26933 [18:35<09:15, 17.68it/s] 64%|██████▎   | 17124/26933 [18:35<09:15, 17.67it/s] 64%|██████▎   | 17128/26933 [18:36<09:14, 17.69it/s] 64%|██████▎   | 17132/26933 [18:36<09:14, 17.68it/s] 64%|██████▎   | 17136/26933 [18:36<09:14, 17.68it/s] 64%|██████▎   | 17140/26933 [18:36<09:14, 17.67it/s] 64%|██████▎   | 17144/26933 [18:36<09:14, 17.65it/s] 64%|██████▎   | 17148/26933 [18:37<09:14, 17.64it/s] 64%|██████▎   | 17152/26933 [18:37<09:13, 17.66it/s] 64%|██████▎   | 17156/26933 [18:37<09:13, 17.67it/s] 64%|██████▎   | 17160/26933 [18:37<09:12, 17.69it/s] 64%|██████▎   | 17164/26933 [18:38<09:12, 17.68it/s] 64%|██████▎   | 17168/26933 [18:38<09:11, 17.69it/s] 64%|██████▍   | 17172/26933 [18:38<09:10, 17.72it/s] 64%|██████▍   | 17176/26933 [18:38<09:10, 17.72it/s] 64%|██████▍   | 17180/26933 [18:38<09:09, 17.73it/s] 64%|██████▍   | 17184/26933 [18:39<09:11, 17.69it/s] 64%|██████▍   | 17188/26933 [18:39<09:10, 17.69it/s] 64%|██████▍   | 17192/26933 [18:39<09:11, 17.67it/s] 64%|██████▍   | 17196/26933 [18:39<09:11, 17.67it/s] 64%|██████▍   | 17200/26933 [18:40<09:12, 17.62it/s] 64%|██████▍   | 17204/26933 [18:40<09:11, 17.63it/s] 64%|██████▍   | 17208/26933 [18:40<09:11, 17.63it/s] 64%|██████▍   | 17212/26933 [18:40<09:10, 17.65it/s] 64%|██████▍   | 17216/26933 [18:41<09:10, 17.64it/s] 64%|██████▍   | 17220/26933 [18:41<09:11, 17.61it/s] 64%|██████▍   | 17224/26933 [18:41<09:10, 17.64it/s] 64%|██████▍   | 17228/26933 [18:41<09:10, 17.64it/s] 64%|██████▍   | 17232/26933 [18:41<09:09, 17.66it/s] 64%|██████▍   | 17236/26933 [18:42<09:10, 17.62it/s] 64%|██████▍   | 17240/26933 [18:42<09:09, 17.64it/s] 64%|██████▍   | 17244/26933 [18:42<09:09, 17.65it/s] 64%|██████▍   | 17248/26933 [18:42<09:08, 17.65it/s] 64%|██████▍   | 17252/26933 [18:43<09:09, 17.60it/s] 64%|██████▍   | 17256/26933 [18:43<09:09, 17.62it/s] 64%|██████▍   | 17260/26933 [18:43<09:11, 17.52it/s] 64%|██████▍   | 17264/26933 [18:43<09:10, 17.57it/s] 64%|██████▍   | 17268/26933 [18:43<09:09, 17.58it/s] 64%|██████▍   | 17272/26933 [18:44<09:09, 17.57it/s] 64%|██████▍   | 17276/26933 [18:44<09:07, 17.63it/s] 64%|██████▍   | 17280/26933 [18:44<09:07, 17.64it/s] 64%|██████▍   | 17284/26933 [18:44<09:07, 17.63it/s] 64%|██████▍   | 17288/26933 [18:45<09:07, 17.61it/s] 64%|██████▍   | 17292/26933 [18:45<09:06, 17.64it/s] 64%|██████▍   | 17296/26933 [18:45<09:05, 17.66it/s] 64%|██████▍   | 17300/26933 [18:45<09:06, 17.64it/s] 64%|██████▍   | 17304/26933 [18:46<09:06, 17.62it/s] 64%|██████▍   | 17308/26933 [18:46<09:06, 17.61it/s] 64%|██████▍   | 17312/26933 [18:46<09:04, 17.65it/s] 64%|██████▍   | 17316/26933 [18:46<09:04, 17.67it/s] 64%|██████▍   | 17320/26933 [18:46<09:03, 17.68it/s] 64%|██████▍   | 17324/26933 [18:47<09:04, 17.64it/s] 64%|██████▍   | 17328/26933 [18:47<09:03, 17.68it/s] 64%|██████▍   | 17332/26933 [18:47<09:03, 17.67it/s] 64%|██████▍   | 17336/26933 [18:47<09:04, 17.64it/s] 64%|██████▍   | 17340/26933 [18:48<09:05, 17.59it/s] 64%|██████▍   | 17344/26933 [18:48<09:03, 17.63it/s] 64%|██████▍   | 17348/26933 [18:48<09:03, 17.65it/s] 64%|██████▍   | 17352/26933 [18:48<09:02, 17.67it/s] 64%|██████▍   | 17356/26933 [18:48<09:01, 17.70it/s] 64%|██████▍   | 17360/26933 [18:49<09:01, 17.68it/s] 64%|██████▍   | 17364/26933 [18:49<09:01, 17.67it/s] 64%|██████▍   | 17368/26933 [18:49<09:00, 17.68it/s] 65%|██████▍   | 17372/26933 [18:49<09:00, 17.68it/s] 65%|██████▍   | 17376/26933 [18:50<09:02, 17.60it/s] 65%|██████▍   | 17380/26933 [18:50<09:02, 17.59it/s] 65%|██████▍   | 17384/26933 [18:50<09:01, 17.64it/s] 65%|██████▍   | 17388/26933 [18:50<09:00, 17.66it/s] 65%|██████▍   | 17392/26933 [18:51<08:59, 17.70it/s] 65%|██████▍   | 17396/26933 [18:51<09:00, 17.64it/s] 65%|██████▍   | 17400/26933 [18:51<09:01, 17.62it/s] 65%|██████▍   | 17404/26933 [18:51<09:00, 17.62it/s] 65%|██████▍   | 17408/26933 [18:51<08:59, 17.64it/s] 65%|██████▍   | 17412/26933 [18:52<09:01, 17.59it/s] 65%|██████▍   | 17416/26933 [18:52<09:00, 17.62it/s] 65%|██████▍   | 17420/26933 [18:52<08:58, 17.66it/s] 65%|██████▍   | 17424/26933 [18:52<08:58, 17.66it/s] 65%|██████▍   | 17428/26933 [18:53<09:00, 17.60it/s] 65%|██████▍   | 17432/26933 [18:53<08:58, 17.64it/s] 65%|██████▍   | 17436/26933 [18:53<08:58, 17.64it/s] 65%|██████▍   | 17440/26933 [18:53<08:57, 17.66it/s] 65%|██████▍   | 17444/26933 [18:53<08:57, 17.67it/s] 65%|██████▍   | 17448/26933 [18:54<08:57, 17.66it/s] 65%|██████▍   | 17452/26933 [18:54<08:54, 17.74it/s] 65%|██████▍   | 17456/26933 [18:54<08:53, 17.76it/s] 65%|██████▍   | 17460/26933 [18:54<08:52, 17.80it/s] 65%|██████▍   | 17464/26933 [18:55<08:52, 17.77it/s] 65%|██████▍   | 17468/26933 [18:55<08:53, 17.74it/s] 65%|██████▍   | 17472/26933 [18:55<08:53, 17.74it/s] 65%|██████▍   | 17476/26933 [18:55<09:00, 17.51it/s] 65%|██████▍   | 17480/26933 [18:56<08:57, 17.58it/s] 65%|██████▍   | 17484/26933 [18:56<08:56, 17.62it/s] 65%|██████▍   | 17488/26933 [18:56<08:53, 17.71it/s] 65%|██████▍   | 17492/26933 [18:56<08:52, 17.74it/s] 65%|██████▍   | 17496/26933 [18:56<08:52, 17.71it/s] 65%|██████▍   | 17500/26933 [18:57<08:53, 17.68it/s] 65%|██████▍   | 17504/26933 [18:57<08:52, 17.70it/s] 65%|██████▌   | 17508/26933 [18:57<09:17, 16.92it/s] 65%|██████▌   | 17512/26933 [18:57<09:14, 16.98it/s] 65%|██████▌   | 17516/26933 [18:58<09:29, 16.53it/s] 65%|██████▌   | 17520/26933 [18:58<09:22, 16.73it/s] 65%|██████▌   | 17524/26933 [18:58<09:13, 16.99it/s] 65%|██████▌   | 17528/26933 [18:58<09:08, 17.16it/s] 65%|██████▌   | 17532/26933 [18:59<09:03, 17.31it/s] 65%|██████▌   | 17536/26933 [18:59<09:03, 17.29it/s] 65%|██████▌   | 17540/26933 [18:59<09:00, 17.38it/s] 65%|██████▌   | 17544/26933 [18:59<08:58, 17.42it/s] 65%|██████▌   | 17548/26933 [18:59<08:56, 17.49it/s] 65%|██████▌   | 17552/26933 [19:00<08:57, 17.46it/s] 65%|██████▌   | 17556/26933 [19:00<08:56, 17.47it/s] 65%|██████▌   | 17560/26933 [19:00<08:57, 17.44it/s] 65%|██████▌   | 17564/26933 [19:00<08:56, 17.46it/s] 65%|██████▌   | 17568/26933 [19:01<08:55, 17.47it/s] 65%|██████▌   | 17572/26933 [19:01<08:50, 17.63it/s] 65%|██████▌   | 17576/26933 [19:01<08:46, 17.76it/s] 65%|██████▌   | 17580/26933 [19:01<08:45, 17.81it/s] 65%|██████▌   | 17584/26933 [19:01<08:43, 17.87it/s] 65%|██████▌   | 17588/26933 [19:02<08:41, 17.91it/s] 65%|██████▌   | 17592/26933 [19:02<08:40, 17.96it/s] 65%|██████▌   | 17596/26933 [19:02<08:39, 17.97it/s] 65%|██████▌   | 17600/26933 [19:02<08:38, 17.99it/s] 65%|██████▌   | 17604/26933 [19:03<08:39, 17.95it/s] 65%|██████▌   | 17608/26933 [19:03<08:38, 17.99it/s] 65%|██████▌   | 17612/26933 [19:03<08:37, 18.03it/s] 65%|██████▌   | 17616/26933 [19:03<08:37, 18.02it/s] 65%|██████▌   | 17620/26933 [19:03<08:36, 18.02it/s] 65%|██████▌   | 17624/26933 [19:04<08:37, 17.99it/s] 65%|██████▌   | 17628/26933 [19:04<08:36, 18.01it/s] 65%|██████▌   | 17632/26933 [19:04<08:36, 18.01it/s] 65%|██████▌   | 17636/26933 [19:04<08:35, 18.02it/s] 65%|██████▌   | 17640/26933 [19:05<08:35, 18.01it/s] 66%|██████▌   | 17644/26933 [19:05<08:35, 18.03it/s] 66%|██████▌   | 17648/26933 [19:05<08:34, 18.05it/s] 66%|██████▌   | 17652/26933 [19:05<08:34, 18.04it/s] 66%|██████▌   | 17656/26933 [19:05<08:35, 18.00it/s] 66%|██████▌   | 17660/26933 [19:06<08:36, 17.97it/s] 66%|██████▌   | 17664/26933 [19:06<08:35, 17.97it/s] 66%|██████▌   | 17668/26933 [19:06<08:35, 17.97it/s] 66%|██████▌   | 17672/26933 [19:06<08:35, 17.97it/s] 66%|██████▌   | 17676/26933 [19:07<08:35, 17.97it/s] 66%|██████▌   | 17680/26933 [19:07<08:34, 18.00it/s] 66%|██████▌   | 17684/26933 [19:07<08:32, 18.05it/s] 66%|██████▌   | 17688/26933 [19:07<08:32, 18.03it/s] 66%|██████▌   | 17692/26933 [19:07<08:33, 18.01it/s] 66%|██████▌   | 17696/26933 [19:08<08:34, 17.97it/s] 66%|██████▌   | 17700/26933 [19:08<08:33, 18.00it/s] 66%|██████▌   | 17704/26933 [19:08<08:32, 18.00it/s] 66%|██████▌   | 17708/26933 [19:08<08:32, 18.02it/s] 66%|██████▌   | 17712/26933 [19:09<08:33, 17.96it/s] 66%|██████▌   | 17716/26933 [19:09<08:31, 18.01it/s] 66%|██████▌   | 17720/26933 [19:09<08:29, 18.07it/s] 66%|██████▌   | 17724/26933 [19:09<08:30, 18.06it/s] 66%|██████▌   | 17728/26933 [19:09<08:30, 18.04it/s] 66%|██████▌   | 17732/26933 [19:10<08:31, 17.98it/s] 66%|██████▌   | 17736/26933 [19:10<08:31, 18.00it/s] 66%|██████▌   | 17740/26933 [19:10<08:30, 18.02it/s] 66%|██████▌   | 17744/26933 [19:10<08:30, 18.00it/s] 66%|██████▌   | 17748/26933 [19:11<08:31, 17.97it/s] 66%|██████▌   | 17752/26933 [19:11<08:30, 17.97it/s] 66%|██████▌   | 17756/26933 [19:11<08:30, 17.99it/s] 66%|██████▌   | 17760/26933 [19:11<08:29, 17.99it/s] 66%|██████▌   | 17764/26933 [19:11<08:29, 17.99it/s] 66%|██████▌   | 17768/26933 [19:12<08:29, 17.97it/s] 66%|██████▌   | 17772/26933 [19:12<08:29, 17.97it/s] 66%|██████▌   | 17776/26933 [19:12<08:28, 17.99it/s] 66%|██████▌   | 17780/26933 [19:12<08:27, 18.02it/s] 66%|██████▌   | 17784/26933 [19:13<08:28, 18.00it/s] 66%|██████▌   | 17788/26933 [19:13<08:26, 18.04it/s] 66%|██████▌   | 17792/26933 [19:13<08:25, 18.07it/s] 66%|██████▌   | 17796/26933 [19:13<08:25, 18.06it/s] 66%|██████▌   | 17800/26933 [19:13<08:25, 18.05it/s] 66%|██████▌   | 17804/26933 [19:14<08:26, 18.02it/s] 66%|██████▌   | 17808/26933 [19:14<08:26, 18.03it/s] 66%|██████▌   | 17812/26933 [19:14<08:27, 17.98it/s] 66%|██████▌   | 17816/26933 [19:14<08:28, 17.94it/s] 66%|██████▌   | 17820/26933 [19:15<08:29, 17.88it/s] 66%|██████▌   | 17824/26933 [19:15<08:28, 17.92it/s] 66%|██████▌   | 17828/26933 [19:15<08:27, 17.94it/s] 66%|██████▌   | 17832/26933 [19:15<08:27, 17.93it/s] 66%|██████▌   | 17836/26933 [19:15<08:26, 17.95it/s] 66%|██████▌   | 17840/26933 [19:16<08:27, 17.93it/s] 66%|██████▋   | 17844/26933 [19:16<08:26, 17.95it/s] 66%|██████▋   | 17848/26933 [19:16<08:25, 17.96it/s] 66%|██████▋   | 17852/26933 [19:16<08:25, 17.96it/s] 66%|██████▋   | 17856/26933 [19:17<08:27, 17.90it/s] 66%|██████▋   | 17860/26933 [19:17<08:26, 17.91it/s] 66%|██████▋   | 17864/26933 [19:17<08:25, 17.96it/s] 66%|██████▋   | 17868/26933 [19:17<08:24, 17.98it/s] 66%|██████▋   | 17872/26933 [19:17<08:23, 17.99it/s] 66%|██████▋   | 17876/26933 [19:18<08:24, 17.96it/s] 66%|██████▋   | 17880/26933 [19:18<08:24, 17.95it/s] 66%|██████▋   | 17884/26933 [19:18<08:24, 17.94it/s] 66%|██████▋   | 17888/26933 [19:18<08:33, 17.62it/s] 66%|██████▋   | 17892/26933 [19:19<08:31, 17.68it/s] 66%|██████▋   | 17896/26933 [19:19<08:29, 17.75it/s] 66%|██████▋   | 17900/26933 [19:19<08:27, 17.80it/s] 66%|██████▋   | 17904/26933 [19:19<08:27, 17.80it/s] 66%|██████▋   | 17908/26933 [19:20<08:27, 17.79it/s] 67%|██████▋   | 17912/26933 [19:20<08:28, 17.75it/s] 67%|██████▋   | 17916/26933 [19:20<08:25, 17.83it/s] 67%|██████▋   | 17920/26933 [19:20<08:25, 17.85it/s] 67%|██████▋   | 17924/26933 [19:20<08:24, 17.87it/s] 67%|██████▋   | 17928/26933 [19:21<08:23, 17.87it/s] 67%|██████▋   | 17932/26933 [19:21<08:22, 17.90it/s] 67%|██████▋   | 17936/26933 [19:21<08:21, 17.94it/s] 67%|██████▋   | 17940/26933 [19:21<08:21, 17.95it/s] 67%|██████▋   | 17944/26933 [19:22<08:21, 17.94it/s] 67%|██████▋   | 17948/26933 [19:22<08:22, 17.89it/s] 67%|██████▋   | 17952/26933 [19:22<08:21, 17.91it/s] 67%|██████▋   | 17956/26933 [19:22<08:21, 17.89it/s] 67%|██████▋   | 17960/26933 [19:22<08:21, 17.88it/s] 67%|██████▋   | 17964/26933 [19:23<08:22, 17.83it/s] 67%|██████▋   | 17968/26933 [19:23<08:22, 17.85it/s] 67%|██████▋   | 17972/26933 [19:23<08:21, 17.87it/s] 67%|██████▋   | 17976/26933 [19:23<08:19, 17.92it/s] 67%|██████▋   | 17980/26933 [19:24<08:19, 17.94it/s] 67%|██████▋   | 17984/26933 [19:24<08:20, 17.88it/s] 67%|██████▋   | 17988/26933 [19:24<08:19, 17.91it/s] 67%|██████▋   | 17992/26933 [19:24<08:20, 17.87it/s] 67%|██████▋   | 17996/26933 [19:24<08:19, 17.88it/s] 67%|██████▋   | 18000/26933 [19:25<08:20, 17.84it/s] 67%|██████▋   | 18004/26933 [19:25<08:19, 17.87it/s] 67%|██████▋   | 18008/26933 [19:25<08:18, 17.92it/s] 67%|██████▋   | 18012/26933 [19:25<08:18, 17.90it/s] 67%|██████▋   | 18016/26933 [19:26<08:18, 17.90it/s] 67%|██████▋   | 18020/26933 [19:26<08:18, 17.89it/s] 67%|██████▋   | 18024/26933 [19:26<08:16, 17.93it/s] 67%|██████▋   | 18028/26933 [19:26<08:17, 17.92it/s] 67%|██████▋   | 18032/26933 [19:26<08:16, 17.94it/s] 67%|██████▋   | 18036/26933 [19:27<08:15, 17.94it/s] 67%|██████▋   | 18040/26933 [19:27<08:14, 17.98it/s] 67%|██████▋   | 18044/26933 [19:27<08:14, 17.97it/s] 67%|██████▋   | 18048/26933 [19:27<08:15, 17.94it/s] 67%|██████▋   | 18052/26933 [19:28<08:15, 17.93it/s] 67%|██████▋   | 18056/26933 [19:28<08:16, 17.87it/s] 67%|██████▋   | 18060/26933 [19:28<08:15, 17.90it/s] 67%|██████▋   | 18064/26933 [19:28<08:15, 17.89it/s] 67%|██████▋   | 18068/26933 [19:28<08:14, 17.91it/s] 67%|██████▋   | 18072/26933 [19:29<08:15, 17.87it/s] 67%|██████▋   | 18076/26933 [19:29<08:15, 17.89it/s] 67%|██████▋   | 18080/26933 [19:29<08:15, 17.88it/s] 67%|██████▋   | 18084/26933 [19:29<08:14, 17.88it/s] 67%|██████▋   | 18088/26933 [19:30<08:14, 17.88it/s] 67%|██████▋   | 18092/26933 [19:30<08:15, 17.84it/s] 67%|██████▋   | 18096/26933 [19:30<08:13, 17.90it/s] 67%|██████▋   | 18100/26933 [19:30<08:13, 17.89it/s] 67%|██████▋   | 18104/26933 [19:30<08:15, 17.83it/s] 67%|██████▋   | 18108/26933 [19:31<08:18, 17.71it/s] 67%|██████▋   | 18112/26933 [19:31<08:18, 17.70it/s] 67%|██████▋   | 18116/26933 [19:31<08:17, 17.71it/s] 67%|██████▋   | 18120/26933 [19:31<08:18, 17.69it/s] 67%|██████▋   | 18124/26933 [19:32<08:18, 17.67it/s] 67%|██████▋   | 18128/26933 [19:32<08:17, 17.68it/s] 67%|██████▋   | 18132/26933 [19:32<08:17, 17.69it/s] 67%|██████▋   | 18136/26933 [19:32<08:17, 17.67it/s] 67%|██████▋   | 18140/26933 [19:32<08:17, 17.67it/s] 67%|██████▋   | 18144/26933 [19:33<08:17, 17.65it/s] 67%|██████▋   | 18148/26933 [19:33<08:17, 17.67it/s] 67%|██████▋   | 18152/26933 [19:33<08:16, 17.69it/s] 67%|██████▋   | 18156/26933 [19:33<08:16, 17.69it/s] 67%|██████▋   | 18160/26933 [19:34<08:16, 17.66it/s] 67%|██████▋   | 18164/26933 [19:34<08:16, 17.66it/s] 67%|██████▋   | 18168/26933 [19:34<08:16, 17.67it/s] 67%|██████▋   | 18172/26933 [19:34<08:15, 17.70it/s] 67%|██████▋   | 18176/26933 [19:35<08:14, 17.71it/s] 68%|██████▊   | 18180/26933 [19:35<08:15, 17.67it/s] 68%|██████▊   | 18184/26933 [19:35<08:14, 17.68it/s] 68%|██████▊   | 18188/26933 [19:35<08:14, 17.70it/s] 68%|██████▊   | 18192/26933 [19:35<08:13, 17.72it/s] 68%|██████▊   | 18196/26933 [19:36<08:13, 17.71it/s] 68%|██████▊   | 18200/26933 [19:36<08:13, 17.70it/s] 68%|██████▊   | 18204/26933 [19:36<08:12, 17.73it/s] 68%|██████▊   | 18208/26933 [19:36<08:11, 17.74it/s] 68%|██████▊   | 18212/26933 [19:37<08:10, 17.79it/s] 68%|██████▊   | 18216/26933 [19:37<08:12, 17.70it/s] 68%|██████▊   | 18220/26933 [19:37<08:10, 17.75it/s] 68%|██████▊   | 18224/26933 [19:37<08:10, 17.74it/s] 68%|██████▊   | 18228/26933 [19:37<08:10, 17.74it/s] 68%|██████▊   | 18232/26933 [19:38<08:11, 17.72it/s] 68%|██████▊   | 18236/26933 [19:38<08:08, 17.79it/s] 68%|██████▊   | 18240/26933 [19:38<08:07, 17.82it/s] 68%|██████▊   | 18244/26933 [19:38<08:06, 17.85it/s] 68%|██████▊   | 18248/26933 [19:39<08:07, 17.83it/s] 68%|██████▊   | 18252/26933 [19:39<08:05, 17.87it/s] 68%|██████▊   | 18256/26933 [19:39<08:04, 17.92it/s] 68%|██████▊   | 18260/26933 [19:39<08:03, 17.95it/s] 68%|██████▊   | 18264/26933 [19:39<08:03, 17.94it/s] 68%|██████▊   | 18268/26933 [19:40<08:05, 17.83it/s] 68%|██████▊   | 18272/26933 [19:40<08:05, 17.85it/s] 68%|██████▊   | 18276/26933 [19:40<08:05, 17.83it/s] 68%|██████▊   | 18280/26933 [19:40<08:05, 17.82it/s] 68%|██████▊   | 18284/26933 [19:41<08:06, 17.77it/s] 68%|██████▊   | 18288/26933 [19:41<08:05, 17.80it/s] 68%|██████▊   | 18292/26933 [19:41<08:05, 17.80it/s] 68%|██████▊   | 18296/26933 [19:41<08:04, 17.81it/s] 68%|██████▊   | 18300/26933 [19:41<08:04, 17.83it/s] 68%|██████▊   | 18304/26933 [19:42<08:05, 17.78it/s] 68%|██████▊   | 18308/26933 [19:42<08:03, 17.85it/s] 68%|██████▊   | 18312/26933 [19:42<08:02, 17.86it/s] 68%|██████▊   | 18316/26933 [19:42<08:02, 17.86it/s] 68%|██████▊   | 18320/26933 [19:43<08:02, 17.85it/s] 68%|██████▊   | 18324/26933 [19:43<08:02, 17.84it/s] 68%|██████▊   | 18328/26933 [19:43<08:00, 17.90it/s] 68%|██████▊   | 18332/26933 [19:43<07:59, 17.93it/s] 68%|██████▊   | 18336/26933 [19:44<07:57, 18.01it/s] 68%|██████▊   | 18340/26933 [19:44<07:57, 17.99it/s] 68%|██████▊   | 18344/26933 [19:44<07:56, 18.01it/s] 68%|██████▊   | 18348/26933 [19:44<07:55, 18.05it/s] 68%|██████▊   | 18352/26933 [19:44<07:54, 18.08it/s] 68%|██████▊   | 18356/26933 [19:45<07:55, 18.05it/s] 68%|██████▊   | 18360/26933 [19:45<07:55, 18.05it/s] 68%|██████▊   | 18364/26933 [19:45<07:54, 18.05it/s] 68%|██████▊   | 18368/26933 [19:45<07:54, 18.04it/s] 68%|██████▊   | 18372/26933 [19:46<07:55, 18.01it/s] 68%|██████▊   | 18376/26933 [19:46<07:55, 18.01it/s] 68%|██████▊   | 18380/26933 [19:46<07:54, 18.03it/s] 68%|██████▊   | 18384/26933 [19:46<07:54, 18.03it/s] 68%|██████▊   | 18388/26933 [19:46<07:53, 18.04it/s] 68%|██████▊   | 18392/26933 [19:47<07:53, 18.02it/s] 68%|██████▊   | 18396/26933 [19:47<07:53, 18.04it/s] 68%|██████▊   | 18400/26933 [19:47<07:53, 18.02it/s] 68%|██████▊   | 18404/26933 [19:47<07:52, 18.05it/s] 68%|██████▊   | 18408/26933 [19:47<07:52, 18.05it/s] 68%|██████▊   | 18412/26933 [19:48<07:53, 18.00it/s] 68%|██████▊   | 18416/26933 [19:48<07:53, 18.01it/s] 68%|██████▊   | 18420/26933 [19:48<07:52, 18.00it/s] 68%|██████▊   | 18424/26933 [19:48<07:52, 18.01it/s] 68%|██████▊   | 18428/26933 [19:49<07:53, 17.98it/s] 68%|██████▊   | 18432/26933 [19:49<07:52, 18.00it/s] 68%|██████▊   | 18436/26933 [19:49<07:51, 18.02it/s] 68%|██████▊   | 18440/26933 [19:49<07:50, 18.05it/s] 68%|██████▊   | 18444/26933 [19:49<07:49, 18.07it/s] 68%|██████▊   | 18448/26933 [19:50<07:50, 18.03it/s] 69%|██████▊   | 18452/26933 [19:50<07:50, 18.02it/s] 69%|██████▊   | 18456/26933 [19:50<07:49, 18.05it/s] 69%|██████▊   | 18460/26933 [19:50<07:49, 18.04it/s] 69%|██████▊   | 18464/26933 [19:51<07:49, 18.02it/s] 69%|██████▊   | 18468/26933 [19:51<07:48, 18.05it/s] 69%|██████▊   | 18472/26933 [19:51<07:48, 18.05it/s] 69%|██████▊   | 18476/26933 [19:51<07:47, 18.09it/s] 69%|██████▊   | 18480/26933 [19:51<07:47, 18.08it/s] 69%|██████▊   | 18484/26933 [19:52<07:48, 18.03it/s] 69%|██████▊   | 18488/26933 [19:52<07:48, 18.03it/s] 69%|██████▊   | 18492/26933 [19:52<07:48, 18.04it/s] 69%|██████▊   | 18496/26933 [19:52<07:47, 18.06it/s] 69%|██████▊   | 18500/26933 [19:53<07:48, 18.02it/s] 69%|██████▊   | 18504/26933 [19:53<07:46, 18.06it/s] 69%|██████▊   | 18508/26933 [19:53<07:46, 18.06it/s] 69%|██████▊   | 18512/26933 [19:53<07:46, 18.07it/s] 69%|██████▊   | 18516/26933 [19:53<07:45, 18.08it/s] 69%|██████▉   | 18520/26933 [19:54<07:46, 18.02it/s] 69%|██████▉   | 18524/26933 [19:54<07:46, 18.03it/s] 69%|██████▉   | 18528/26933 [19:54<07:45, 18.05it/s] 69%|██████▉   | 18532/26933 [19:54<07:45, 18.07it/s] 69%|██████▉   | 18536/26933 [19:55<07:44, 18.06it/s] 69%|██████▉   | 18540/26933 [19:55<07:44, 18.07it/s] 69%|██████▉   | 18544/26933 [19:55<07:43, 18.10it/s] 69%|██████▉   | 18548/26933 [19:55<07:42, 18.15it/s] 69%|██████▉   | 18552/26933 [19:55<07:41, 18.14it/s] 69%|██████▉   | 18556/26933 [19:56<07:43, 18.08it/s] 69%|██████▉   | 18560/26933 [19:56<07:43, 18.08it/s] 69%|██████▉   | 18564/26933 [19:56<07:43, 18.07it/s] 69%|██████▉   | 18568/26933 [19:56<07:42, 18.09it/s] 69%|██████▉   | 18572/26933 [19:57<07:44, 18.01it/s] 69%|██████▉   | 18576/26933 [19:57<07:42, 18.05it/s] 69%|██████▉   | 18580/26933 [19:57<07:42, 18.05it/s] 69%|██████▉   | 18584/26933 [19:57<07:42, 18.05it/s] 69%|██████▉   | 18588/26933 [19:57<07:42, 18.03it/s] 69%|██████▉   | 18592/26933 [19:58<07:46, 17.89it/s] 69%|██████▉   | 18596/26933 [19:58<07:47, 17.83it/s] 69%|██████▉   | 18600/26933 [19:58<07:48, 17.80it/s] 69%|██████▉   | 18604/26933 [19:58<07:50, 17.70it/s] 69%|██████▉   | 18608/26933 [19:59<07:50, 17.68it/s] 69%|██████▉   | 18612/26933 [19:59<07:48, 17.75it/s] 69%|██████▉   | 18616/26933 [19:59<07:46, 17.81it/s] 69%|██████▉   | 18620/26933 [19:59<07:47, 17.79it/s] 69%|██████▉   | 18624/26933 [20:00<07:47, 17.79it/s] 69%|██████▉   | 18628/26933 [20:00<07:47, 17.77it/s] 69%|██████▉   | 18632/26933 [20:00<07:48, 17.74it/s] 69%|██████▉   | 18636/26933 [20:00<07:48, 17.72it/s] 69%|██████▉   | 18640/26933 [20:00<07:48, 17.71it/s] 69%|██████▉   | 18644/26933 [20:01<07:49, 17.66it/s] 69%|██████▉   | 18648/26933 [20:01<07:48, 17.69it/s] 69%|██████▉   | 18652/26933 [20:01<07:47, 17.71it/s] 69%|██████▉   | 18656/26933 [20:01<07:47, 17.72it/s] 69%|██████▉   | 18660/26933 [20:02<07:47, 17.71it/s] 69%|██████▉   | 18664/26933 [20:02<07:47, 17.67it/s] 69%|██████▉   | 18668/26933 [20:02<07:47, 17.68it/s] 69%|██████▉   | 18672/26933 [20:02<07:48, 17.63it/s] 69%|██████▉   | 18676/26933 [20:02<07:48, 17.63it/s] 69%|██████▉   | 18680/26933 [20:03<07:47, 17.65it/s] 69%|██████▉   | 18684/26933 [20:03<07:44, 17.75it/s] 69%|██████▉   | 18688/26933 [20:03<07:43, 17.80it/s] 69%|██████▉   | 18692/26933 [20:03<07:41, 17.86it/s] 69%|██████▉   | 18696/26933 [20:04<07:42, 17.82it/s] 69%|██████▉   | 18700/26933 [20:04<07:41, 17.84it/s] 69%|██████▉   | 18704/26933 [20:04<07:40, 17.89it/s] 69%|██████▉   | 18708/26933 [20:04<07:39, 17.89it/s] 69%|██████▉   | 18712/26933 [20:04<07:40, 17.84it/s] 69%|██████▉   | 18716/26933 [20:05<07:43, 17.73it/s] 70%|██████▉   | 18720/26933 [20:05<07:43, 17.72it/s] 70%|██████▉   | 18724/26933 [20:05<07:43, 17.70it/s] 70%|██████▉   | 18728/26933 [20:05<07:44, 17.68it/s] 70%|██████▉   | 18732/26933 [20:06<07:45, 17.63it/s] 70%|██████▉   | 18736/26933 [20:06<07:44, 17.66it/s] 70%|██████▉   | 18740/26933 [20:06<07:40, 17.79it/s] 70%|██████▉   | 18744/26933 [20:06<07:36, 17.94it/s] 70%|██████▉   | 18748/26933 [20:06<07:33, 18.04it/s] 70%|██████▉   | 18752/26933 [20:07<07:33, 18.04it/s] 70%|██████▉   | 18756/26933 [20:07<07:31, 18.13it/s] 70%|██████▉   | 18760/26933 [20:07<07:30, 18.15it/s] 70%|██████▉   | 18764/26933 [20:07<07:29, 18.18it/s] 70%|██████▉   | 18768/26933 [20:08<07:29, 18.15it/s] 70%|██████▉   | 18772/26933 [20:08<07:29, 18.15it/s] 70%|██████▉   | 18776/26933 [20:08<07:27, 18.22it/s] 70%|██████▉   | 18780/26933 [20:08<07:27, 18.21it/s] 70%|██████▉   | 18784/26933 [20:08<07:26, 18.24it/s] 70%|██████▉   | 18788/26933 [20:09<07:27, 18.21it/s] 70%|██████▉   | 18792/26933 [20:09<07:26, 18.23it/s] 70%|██████▉   | 18796/26933 [20:09<07:25, 18.25it/s] 70%|██████▉   | 18800/26933 [20:09<07:25, 18.26it/s] 70%|██████▉   | 18804/26933 [20:10<07:25, 18.25it/s] 70%|██████▉   | 18808/26933 [20:10<07:26, 18.19it/s] 70%|██████▉   | 18812/26933 [20:10<07:26, 18.21it/s] 70%|██████▉   | 18816/26933 [20:10<07:26, 18.19it/s] 70%|██████▉   | 18820/26933 [20:10<07:26, 18.19it/s] 70%|██████▉   | 18824/26933 [20:11<07:26, 18.18it/s] 70%|██████▉   | 18828/26933 [20:11<07:24, 18.22it/s] 70%|██████▉   | 18832/26933 [20:11<07:24, 18.21it/s] 70%|██████▉   | 18836/26933 [20:11<07:24, 18.21it/s] 70%|██████▉   | 18840/26933 [20:12<07:26, 18.14it/s] 70%|██████▉   | 18844/26933 [20:12<07:26, 18.13it/s] 70%|██████▉   | 18848/26933 [20:12<07:32, 17.89it/s] 70%|██████▉   | 18852/26933 [20:12<07:37, 17.67it/s] 70%|███████   | 18856/26933 [20:12<07:52, 17.09it/s] 70%|███████   | 18860/26933 [20:13<07:48, 17.23it/s] 70%|███████   | 18864/26933 [20:13<07:44, 17.36it/s] 70%|███████   | 18868/26933 [20:13<07:44, 17.36it/s] 70%|███████   | 18872/26933 [20:13<07:45, 17.31it/s] 70%|███████   | 18876/26933 [20:14<07:44, 17.35it/s] 70%|███████   | 18880/26933 [20:14<07:42, 17.41it/s] 70%|███████   | 18884/26933 [20:14<07:40, 17.48it/s] 70%|███████   | 18888/26933 [20:14<07:38, 17.54it/s] 70%|███████   | 18892/26933 [20:15<07:37, 17.58it/s] 70%|███████   | 18896/26933 [20:15<07:37, 17.56it/s] 70%|███████   | 18900/26933 [20:15<07:36, 17.61it/s] 70%|███████   | 18904/26933 [20:15<07:34, 17.66it/s] 70%|███████   | 18908/26933 [20:15<07:33, 17.70it/s] 70%|███████   | 18912/26933 [20:16<07:34, 17.66it/s] 70%|███████   | 18916/26933 [20:16<07:33, 17.68it/s] 70%|███████   | 18920/26933 [20:16<07:32, 17.70it/s] 70%|███████   | 18924/26933 [20:16<07:32, 17.71it/s] 70%|███████   | 18928/26933 [20:17<07:32, 17.69it/s] 70%|███████   | 18932/26933 [20:17<07:32, 17.67it/s] 70%|███████   | 18936/26933 [20:17<07:31, 17.70it/s] 70%|███████   | 18940/26933 [20:17<07:30, 17.72it/s] 70%|███████   | 18944/26933 [20:17<07:30, 17.74it/s] 70%|███████   | 18948/26933 [20:18<07:30, 17.72it/s] 70%|███████   | 18952/26933 [20:18<07:30, 17.73it/s] 70%|███████   | 18956/26933 [20:18<07:30, 17.71it/s] 70%|███████   | 18960/26933 [20:18<07:29, 17.72it/s] 70%|███████   | 18964/26933 [20:19<07:30, 17.68it/s] 70%|███████   | 18968/26933 [20:19<07:29, 17.71it/s] 70%|███████   | 18972/26933 [20:19<07:28, 17.74it/s] 70%|███████   | 18976/26933 [20:19<07:28, 17.75it/s] 70%|███████   | 18980/26933 [20:19<07:28, 17.75it/s] 70%|███████   | 18984/26933 [20:20<07:28, 17.71it/s] 71%|███████   | 18988/26933 [20:20<07:29, 17.69it/s] 71%|███████   | 18992/26933 [20:20<07:28, 17.70it/s] 71%|███████   | 18996/26933 [20:20<07:26, 17.76it/s] 71%|███████   | 19000/26933 [20:21<07:26, 17.77it/s] 71%|███████   | 19004/26933 [20:21<07:26, 17.75it/s] 71%|███████   | 19008/26933 [20:21<07:26, 17.75it/s] 71%|███████   | 19012/26933 [20:21<07:26, 17.75it/s] 71%|███████   | 19016/26933 [20:22<07:26, 17.74it/s] 71%|███████   | 19020/26933 [20:22<07:26, 17.74it/s] 71%|███████   | 19024/26933 [20:22<07:23, 17.82it/s] 71%|███████   | 19028/26933 [20:22<07:21, 17.89it/s] 71%|███████   | 19032/26933 [20:22<07:20, 17.92it/s] 71%|███████   | 19036/26933 [20:23<07:20, 17.93it/s] 71%|███████   | 19040/26933 [20:23<07:19, 17.97it/s] 71%|███████   | 19044/26933 [20:23<07:17, 18.03it/s] 71%|███████   | 19048/26933 [20:23<07:15, 18.09it/s] 71%|███████   | 19052/26933 [20:24<07:16, 18.07it/s] 71%|███████   | 19056/26933 [20:24<07:17, 18.02it/s] 71%|███████   | 19060/26933 [20:24<07:16, 18.04it/s] 71%|███████   | 19064/26933 [20:24<07:15, 18.06it/s] 71%|███████   | 19068/26933 [20:24<07:15, 18.08it/s] 71%|███████   | 19072/26933 [20:25<07:15, 18.05it/s] 71%|███████   | 19076/26933 [20:25<07:15, 18.06it/s] 71%|███████   | 19080/26933 [20:25<07:15, 18.05it/s] 71%|███████   | 19084/26933 [20:25<07:13, 18.09it/s] 71%|███████   | 19088/26933 [20:26<07:13, 18.08it/s] 71%|███████   | 19092/26933 [20:26<07:14, 18.03it/s] 71%|███████   | 19096/26933 [20:26<07:18, 17.86it/s] 71%|███████   | 19100/26933 [20:26<07:21, 17.73it/s] 71%|███████   | 19104/26933 [20:26<07:22, 17.71it/s] 71%|███████   | 19108/26933 [20:27<07:25, 17.55it/s] 71%|███████   | 19112/26933 [20:27<07:25, 17.56it/s] 71%|███████   | 19116/26933 [20:27<07:27, 17.49it/s] 71%|███████   | 19120/26933 [20:27<07:24, 17.58it/s] 71%|███████   | 19124/26933 [20:28<07:21, 17.70it/s] 71%|███████   | 19128/26933 [20:28<07:19, 17.78it/s] 71%|███████   | 19132/26933 [20:28<07:16, 17.86it/s] 71%|███████   | 19136/26933 [20:28<07:14, 17.94it/s] 71%|███████   | 19140/26933 [20:28<07:14, 17.95it/s] 71%|███████   | 19144/26933 [20:29<07:13, 17.97it/s] 71%|███████   | 19148/26933 [20:29<07:13, 17.96it/s] 71%|███████   | 19152/26933 [20:29<07:12, 18.01it/s] 71%|███████   | 19156/26933 [20:29<07:10, 18.05it/s] 71%|███████   | 19160/26933 [20:30<07:09, 18.08it/s] 71%|███████   | 19164/26933 [20:30<07:10, 18.06it/s] 71%|███████   | 19168/26933 [20:30<07:09, 18.06it/s] 71%|███████   | 19172/26933 [20:30<07:09, 18.09it/s] 71%|███████   | 19176/26933 [20:30<07:08, 18.10it/s] 71%|███████   | 19180/26933 [20:31<07:09, 18.04it/s] 71%|███████   | 19184/26933 [20:31<07:09, 18.03it/s] 71%|███████   | 19188/26933 [20:31<07:09, 18.03it/s] 71%|███████▏  | 19192/26933 [20:31<07:09, 18.01it/s] 71%|███████▏  | 19196/26933 [20:32<07:09, 17.99it/s] 71%|███████▏  | 19200/26933 [20:32<07:10, 17.96it/s] 71%|███████▏  | 19204/26933 [20:32<07:12, 17.87it/s] 71%|███████▏  | 19208/26933 [20:32<07:13, 17.83it/s] 71%|███████▏  | 19212/26933 [20:32<07:13, 17.80it/s] 71%|███████▏  | 19216/26933 [20:33<07:15, 17.71it/s] 71%|███████▏  | 19220/26933 [20:33<07:16, 17.68it/s] 71%|███████▏  | 19224/26933 [20:33<07:16, 17.65it/s] 71%|███████▏  | 19228/26933 [20:33<07:16, 17.67it/s] 71%|███████▏  | 19232/26933 [20:34<07:17, 17.60it/s] 71%|███████▏  | 19236/26933 [20:34<07:16, 17.64it/s] 71%|███████▏  | 19240/26933 [20:34<07:15, 17.67it/s] 71%|███████▏  | 19244/26933 [20:34<07:16, 17.60it/s] 71%|███████▏  | 19248/26933 [20:34<07:16, 17.61it/s] 71%|███████▏  | 19252/26933 [20:35<07:17, 17.56it/s] 71%|███████▏  | 19256/26933 [20:35<07:15, 17.63it/s] 72%|███████▏  | 19260/26933 [20:35<07:13, 17.68it/s] 72%|███████▏  | 19264/26933 [20:35<07:13, 17.70it/s] 72%|███████▏  | 19268/26933 [20:36<07:13, 17.68it/s] 72%|███████▏  | 19272/26933 [20:36<07:13, 17.69it/s] 72%|███████▏  | 19276/26933 [20:36<07:12, 17.69it/s] 72%|███████▏  | 19280/26933 [20:36<07:13, 17.67it/s] 72%|███████▏  | 19284/26933 [20:37<07:12, 17.68it/s] 72%|███████▏  | 19288/26933 [20:37<07:14, 17.61it/s] 72%|███████▏  | 19292/26933 [20:37<07:12, 17.65it/s] 72%|███████▏  | 19296/26933 [20:37<07:13, 17.62it/s] 72%|███████▏  | 19300/26933 [20:37<07:13, 17.61it/s] 72%|███████▏  | 19304/26933 [20:38<07:14, 17.58it/s] 72%|███████▏  | 19308/26933 [20:38<07:12, 17.63it/s] 72%|███████▏  | 19312/26933 [20:38<07:13, 17.59it/s] 72%|███████▏  | 19316/26933 [20:38<07:12, 17.62it/s] 72%|███████▏  | 19320/26933 [20:39<07:13, 17.55it/s] 72%|███████▏  | 19324/26933 [20:39<07:12, 17.58it/s] 72%|███████▏  | 19328/26933 [20:39<07:11, 17.61it/s] 72%|███████▏  | 19332/26933 [20:39<07:10, 17.68it/s] 72%|███████▏  | 19336/26933 [20:39<07:08, 17.72it/s] 72%|███████▏  | 19340/26933 [20:40<07:10, 17.64it/s] 72%|███████▏  | 19344/26933 [20:40<07:09, 17.67it/s] 72%|███████▏  | 19348/26933 [20:40<07:08, 17.72it/s] 72%|███████▏  | 19352/26933 [20:40<07:07, 17.74it/s] 72%|███████▏  | 19356/26933 [20:41<07:07, 17.71it/s] 72%|███████▏  | 19360/26933 [20:41<07:06, 17.74it/s] 72%|███████▏  | 19364/26933 [20:41<07:06, 17.75it/s] 72%|███████▏  | 19368/26933 [20:41<07:05, 17.77it/s] 72%|███████▏  | 19372/26933 [20:42<07:05, 17.79it/s] 72%|███████▏  | 19376/26933 [20:42<07:13, 17.45it/s] 72%|███████▏  | 19380/26933 [20:42<07:10, 17.54it/s] 72%|███████▏  | 19384/26933 [20:42<07:09, 17.59it/s] 72%|███████▏  | 19388/26933 [20:42<07:08, 17.61it/s] 72%|███████▏  | 19392/26933 [20:43<07:09, 17.54it/s] 72%|███████▏  | 19396/26933 [20:43<07:08, 17.61it/s] 72%|███████▏  | 19400/26933 [20:43<07:07, 17.64it/s] 72%|███████▏  | 19404/26933 [20:43<07:05, 17.68it/s] 72%|███████▏  | 19408/26933 [20:44<07:07, 17.61it/s] 72%|███████▏  | 19412/26933 [20:44<07:05, 17.67it/s] 72%|███████▏  | 19416/26933 [20:44<07:03, 17.73it/s] 72%|███████▏  | 19420/26933 [20:44<07:02, 17.77it/s] 72%|███████▏  | 19424/26933 [20:44<07:01, 17.80it/s] 72%|███████▏  | 19428/26933 [20:45<07:02, 17.77it/s] 72%|███████▏  | 19432/26933 [20:45<07:00, 17.85it/s] 72%|███████▏  | 19436/26933 [20:45<07:00, 17.84it/s] 72%|███████▏  | 19440/26933 [20:45<07:00, 17.84it/s] 72%|███████▏  | 19444/26933 [20:46<07:07, 17.54it/s] 72%|███████▏  | 19448/26933 [20:46<07:05, 17.60it/s] 72%|███████▏  | 19452/26933 [20:46<07:05, 17.58it/s] 72%|███████▏  | 19456/26933 [20:46<07:05, 17.57it/s] 72%|███████▏  | 19460/26933 [20:47<07:05, 17.54it/s] 72%|███████▏  | 19464/26933 [20:47<07:06, 17.49it/s] 72%|███████▏  | 19468/26933 [20:47<07:07, 17.47it/s] 72%|███████▏  | 19472/26933 [20:47<07:06, 17.50it/s] 72%|███████▏  | 19476/26933 [20:47<07:04, 17.56it/s] 72%|███████▏  | 19480/26933 [20:48<07:04, 17.57it/s] 72%|███████▏  | 19484/26933 [20:48<07:02, 17.63it/s] 72%|███████▏  | 19488/26933 [20:48<07:02, 17.63it/s] 72%|███████▏  | 19492/26933 [20:48<07:01, 17.65it/s] 72%|███████▏  | 19496/26933 [20:49<07:00, 17.69it/s] 72%|███████▏  | 19500/26933 [20:49<07:01, 17.65it/s] 72%|███████▏  | 19504/26933 [20:49<07:00, 17.66it/s] 72%|███████▏  | 19508/26933 [20:49<07:00, 17.65it/s] 72%|███████▏  | 19512/26933 [20:49<06:59, 17.69it/s] 72%|███████▏  | 19516/26933 [20:50<06:59, 17.67it/s] 72%|███████▏  | 19520/26933 [20:50<06:59, 17.65it/s] 72%|███████▏  | 19524/26933 [20:50<06:59, 17.67it/s] 73%|███████▎  | 19528/26933 [20:50<06:58, 17.71it/s] 73%|███████▎  | 19532/26933 [20:51<06:58, 17.68it/s] 73%|███████▎  | 19536/26933 [20:51<06:56, 17.75it/s] 73%|███████▎  | 19540/26933 [20:51<06:56, 17.75it/s] 73%|███████▎  | 19544/26933 [20:51<06:55, 17.79it/s] 73%|███████▎  | 19548/26933 [20:51<06:55, 17.78it/s] 73%|███████▎  | 19552/26933 [20:52<06:56, 17.74it/s] 73%|███████▎  | 19556/26933 [20:52<06:56, 17.73it/s] 73%|███████▎  | 19560/26933 [20:52<06:55, 17.75it/s] 73%|███████▎  | 19564/26933 [20:52<06:55, 17.72it/s] 73%|███████▎  | 19568/26933 [20:53<06:57, 17.66it/s] 73%|███████▎  | 19572/26933 [20:53<06:56, 17.67it/s] 73%|███████▎  | 19576/26933 [20:53<06:57, 17.64it/s] 73%|███████▎  | 19580/26933 [20:53<06:56, 17.65it/s] 73%|███████▎  | 19584/26933 [20:54<06:57, 17.62it/s] 73%|███████▎  | 19588/26933 [20:54<06:57, 17.60it/s] 73%|███████▎  | 19592/26933 [20:54<06:55, 17.65it/s] 73%|███████▎  | 19596/26933 [20:54<06:55, 17.65it/s] 73%|███████▎  | 19600/26933 [20:54<06:55, 17.64it/s] 73%|███████▎  | 19604/26933 [20:55<06:55, 17.63it/s] 73%|███████▎  | 19608/26933 [20:55<06:54, 17.66it/s] 73%|███████▎  | 19612/26933 [20:55<06:54, 17.67it/s] 73%|███████▎  | 19616/26933 [20:55<06:53, 17.68it/s] 73%|███████▎  | 19620/26933 [20:56<06:53, 17.70it/s] 73%|███████▎  | 19624/26933 [20:56<06:52, 17.71it/s] 73%|███████▎  | 19628/26933 [20:56<06:52, 17.73it/s] 73%|███████▎  | 19632/26933 [20:56<06:51, 17.72it/s] 73%|███████▎  | 19636/26933 [20:56<06:52, 17.71it/s] 73%|███████▎  | 19640/26933 [20:57<06:52, 17.66it/s] 73%|███████▎  | 19644/26933 [20:57<06:51, 17.69it/s] 73%|███████▎  | 19648/26933 [20:57<06:51, 17.70it/s] 73%|███████▎  | 19652/26933 [20:57<06:51, 17.70it/s] 73%|███████▎  | 19656/26933 [20:58<06:52, 17.66it/s] 73%|███████▎  | 19660/26933 [20:58<06:51, 17.68it/s] 73%|███████▎  | 19664/26933 [20:58<06:50, 17.69it/s] 73%|███████▎  | 19668/26933 [20:58<06:51, 17.67it/s] 73%|███████▎  | 19672/26933 [20:59<06:50, 17.69it/s] 73%|███████▎  | 19676/26933 [20:59<06:50, 17.68it/s] 73%|███████▎  | 19680/26933 [20:59<06:49, 17.71it/s] 73%|███████▎  | 19684/26933 [20:59<06:49, 17.69it/s] 73%|███████▎  | 19688/26933 [20:59<06:49, 17.71it/s] 73%|███████▎  | 19692/26933 [21:00<06:50, 17.65it/s] 73%|███████▎  | 19696/26933 [21:00<06:48, 17.70it/s] 73%|███████▎  | 19700/26933 [21:00<06:49, 17.67it/s] 73%|███████▎  | 19704/26933 [21:00<06:47, 17.72it/s] 73%|███████▎  | 19708/26933 [21:01<06:47, 17.75it/s] 73%|███████▎  | 19712/26933 [21:01<06:46, 17.76it/s] 73%|███████▎  | 19716/26933 [21:01<06:46, 17.77it/s] 73%|███████▎  | 19720/26933 [21:01<06:46, 17.76it/s] 73%|███████▎  | 19724/26933 [21:01<06:46, 17.72it/s] 73%|███████▎  | 19728/26933 [21:02<06:47, 17.66it/s] 73%|███████▎  | 19732/26933 [21:02<06:47, 17.67it/s] 73%|███████▎  | 19736/26933 [21:02<06:47, 17.67it/s] 73%|███████▎  | 19740/26933 [21:02<06:46, 17.67it/s] 73%|███████▎  | 19744/26933 [21:03<06:47, 17.65it/s] 73%|███████▎  | 19748/26933 [21:03<06:47, 17.62it/s] 73%|███████▎  | 19752/26933 [21:03<06:46, 17.65it/s] 73%|███████▎  | 19756/26933 [21:03<06:46, 17.64it/s] 73%|███████▎  | 19760/26933 [21:03<06:46, 17.66it/s] 73%|███████▎  | 19764/26933 [21:04<06:46, 17.64it/s] 73%|███████▎  | 19768/26933 [21:04<06:45, 17.66it/s] 73%|███████▎  | 19772/26933 [21:04<06:45, 17.67it/s] 73%|███████▎  | 19776/26933 [21:04<06:44, 17.67it/s] 73%|███████▎  | 19780/26933 [21:05<06:45, 17.64it/s] 73%|███████▎  | 19784/26933 [21:05<06:45, 17.64it/s] 73%|███████▎  | 19788/26933 [21:05<06:45, 17.63it/s] 73%|███████▎  | 19792/26933 [21:05<06:44, 17.64it/s] 74%|███████▎  | 19796/26933 [21:06<06:44, 17.64it/s] 74%|███████▎  | 19800/26933 [21:06<06:45, 17.58it/s] 74%|███████▎  | 19804/26933 [21:06<06:45, 17.58it/s] 74%|███████▎  | 19808/26933 [21:06<06:43, 17.64it/s] 74%|███████▎  | 19812/26933 [21:06<06:43, 17.67it/s] 74%|███████▎  | 19816/26933 [21:07<06:42, 17.67it/s] 74%|███████▎  | 19820/26933 [21:07<06:40, 17.75it/s] 74%|███████▎  | 19824/26933 [21:07<06:39, 17.81it/s] 74%|███████▎  | 19828/26933 [21:07<06:37, 17.85it/s] 74%|███████▎  | 19832/26933 [21:08<06:37, 17.86it/s] 74%|███████▎  | 19836/26933 [21:08<06:38, 17.81it/s] 74%|███████▎  | 19840/26933 [21:08<06:38, 17.80it/s] 74%|███████▎  | 19844/26933 [21:08<06:38, 17.78it/s] 74%|███████▎  | 19848/26933 [21:08<06:38, 17.77it/s] 74%|███████▎  | 19852/26933 [21:09<06:39, 17.74it/s] 74%|███████▎  | 19856/26933 [21:09<06:38, 17.77it/s] 74%|███████▎  | 19860/26933 [21:09<06:37, 17.78it/s] 74%|███████▍  | 19864/26933 [21:09<06:37, 17.77it/s] 74%|███████▍  | 19868/26933 [21:10<06:38, 17.74it/s] 74%|███████▍  | 19872/26933 [21:10<06:37, 17.74it/s] 74%|███████▍  | 19876/26933 [21:10<06:37, 17.75it/s] 74%|███████▍  | 19880/26933 [21:10<06:37, 17.73it/s] 74%|███████▍  | 19884/26933 [21:10<06:37, 17.73it/s] 74%|███████▍  | 19888/26933 [21:11<06:38, 17.69it/s] 74%|███████▍  | 19892/26933 [21:11<06:37, 17.71it/s] 74%|███████▍  | 19896/26933 [21:11<06:37, 17.71it/s] 74%|███████▍  | 19900/26933 [21:11<06:36, 17.74it/s] 74%|███████▍  | 19904/26933 [21:12<06:36, 17.71it/s] 74%|███████▍  | 19908/26933 [21:12<06:36, 17.72it/s] 74%|███████▍  | 19912/26933 [21:12<06:35, 17.74it/s] 74%|███████▍  | 19916/26933 [21:12<06:35, 17.73it/s] 74%|███████▍  | 19920/26933 [21:13<06:35, 17.73it/s] 74%|███████▍  | 19924/26933 [21:13<06:35, 17.71it/s] 74%|███████▍  | 19928/26933 [21:13<06:34, 17.74it/s] 74%|███████▍  | 19932/26933 [21:13<06:34, 17.74it/s] 74%|███████▍  | 19936/26933 [21:13<06:33, 17.77it/s] 74%|███████▍  | 19940/26933 [21:14<06:34, 17.72it/s] 74%|███████▍  | 19944/26933 [21:14<06:33, 17.77it/s] 74%|███████▍  | 19948/26933 [21:14<06:33, 17.75it/s] 74%|███████▍  | 19952/26933 [21:14<06:33, 17.72it/s] 74%|███████▍  | 19956/26933 [21:15<06:33, 17.72it/s] 74%|███████▍  | 19960/26933 [21:15<06:34, 17.67it/s] 74%|███████▍  | 19964/26933 [21:15<06:33, 17.70it/s] 74%|███████▍  | 19968/26933 [21:15<06:33, 17.71it/s] 74%|███████▍  | 19972/26933 [21:15<06:32, 17.75it/s] 74%|███████▍  | 19976/26933 [21:16<06:32, 17.71it/s] 74%|███████▍  | 19980/26933 [21:16<06:33, 17.69it/s] 74%|███████▍  | 19984/26933 [21:16<06:33, 17.67it/s] 74%|███████▍  | 19988/26933 [21:16<06:32, 17.69it/s] 74%|███████▍  | 19992/26933 [21:17<06:34, 17.60it/s] 74%|███████▍  | 19996/26933 [21:17<06:32, 17.68it/s] 74%|███████▍  | 20000/26933 [21:17<06:31, 17.72it/s] 74%|███████▍  | 20004/26933 [21:17<06:30, 17.74it/s] 74%|███████▍  | 20008/26933 [21:17<06:30, 17.72it/s] 74%|███████▍  | 20012/26933 [21:18<06:33, 17.61it/s] 74%|███████▍  | 20016/26933 [21:18<06:31, 17.66it/s] 74%|███████▍  | 20020/26933 [21:18<06:31, 17.67it/s] 74%|███████▍  | 20024/26933 [21:18<06:31, 17.64it/s] 74%|███████▍  | 20028/26933 [21:19<06:34, 17.49it/s] 74%|███████▍  | 20032/26933 [21:19<06:33, 17.55it/s] 74%|███████▍  | 20036/26933 [21:19<06:31, 17.60it/s] 74%|███████▍  | 20040/26933 [21:19<06:30, 17.65it/s] 74%|███████▍  | 20044/26933 [21:20<06:30, 17.66it/s] 74%|███████▍  | 20048/26933 [21:20<06:31, 17.57it/s] 74%|███████▍  | 20052/26933 [21:20<06:30, 17.62it/s] 74%|███████▍  | 20056/26933 [21:20<06:29, 17.67it/s] 74%|███████▍  | 20060/26933 [21:20<06:28, 17.68it/s] 74%|███████▍  | 20064/26933 [21:21<06:30, 17.58it/s] 75%|███████▍  | 20068/26933 [21:21<06:29, 17.63it/s] 75%|███████▍  | 20072/26933 [21:21<06:27, 17.69it/s] 75%|███████▍  | 20076/26933 [21:21<06:27, 17.68it/s] 75%|███████▍  | 20080/26933 [21:22<06:27, 17.70it/s] 75%|███████▍  | 20084/26933 [21:22<06:29, 17.60it/s] 75%|███████▍  | 20088/26933 [21:22<06:27, 17.67it/s] 75%|███████▍  | 20092/26933 [21:22<06:26, 17.68it/s] 75%|███████▍  | 20096/26933 [21:22<06:25, 17.75it/s] 75%|███████▍  | 20100/26933 [21:23<06:26, 17.66it/s] 75%|███████▍  | 20104/26933 [21:23<06:25, 17.71it/s] 75%|███████▍  | 20108/26933 [21:23<06:24, 17.75it/s] 75%|███████▍  | 20112/26933 [21:23<06:23, 17.77it/s] 75%|███████▍  | 20116/26933 [21:24<06:25, 17.70it/s] 75%|███████▍  | 20120/26933 [21:24<06:23, 17.75it/s] 75%|███████▍  | 20124/26933 [21:24<06:22, 17.79it/s] 75%|███████▍  | 20128/26933 [21:24<06:22, 17.81it/s] 75%|███████▍  | 20132/26933 [21:24<06:22, 17.79it/s] 75%|███████▍  | 20136/26933 [21:25<06:24, 17.66it/s] 75%|███████▍  | 20140/26933 [21:25<06:26, 17.57it/s] 75%|███████▍  | 20144/26933 [21:25<06:25, 17.60it/s] 75%|███████▍  | 20148/26933 [21:25<06:25, 17.62it/s] 75%|███████▍  | 20152/26933 [21:26<06:26, 17.54it/s] 75%|███████▍  | 20156/26933 [21:26<06:25, 17.59it/s] 75%|███████▍  | 20160/26933 [21:26<06:24, 17.60it/s] 75%|███████▍  | 20164/26933 [21:26<06:25, 17.58it/s] 75%|███████▍  | 20168/26933 [21:27<06:23, 17.64it/s] 75%|███████▍  | 20172/26933 [21:27<06:25, 17.56it/s] 75%|███████▍  | 20176/26933 [21:27<06:23, 17.62it/s] 75%|███████▍  | 20180/26933 [21:27<06:22, 17.66it/s] 75%|███████▍  | 20184/26933 [21:27<06:21, 17.71it/s] 75%|███████▍  | 20188/26933 [21:28<06:23, 17.60it/s] 75%|███████▍  | 20192/26933 [21:28<06:21, 17.65it/s] 75%|███████▍  | 20196/26933 [21:28<06:21, 17.64it/s] 75%|███████▌  | 20200/26933 [21:28<06:20, 17.68it/s] 75%|███████▌  | 20204/26933 [21:29<06:22, 17.61it/s] 75%|███████▌  | 20208/26933 [21:29<06:20, 17.66it/s] 75%|███████▌  | 20212/26933 [21:29<06:20, 17.66it/s] 75%|███████▌  | 20216/26933 [21:29<06:19, 17.70it/s] 75%|███████▌  | 20220/26933 [21:29<06:20, 17.65it/s] 75%|███████▌  | 20224/26933 [21:30<06:22, 17.55it/s] 75%|███████▌  | 20228/26933 [21:30<06:20, 17.61it/s] 75%|███████▌  | 20232/26933 [21:30<06:19, 17.64it/s] 75%|███████▌  | 20236/26933 [21:30<06:19, 17.64it/s] 75%|███████▌  | 20240/26933 [21:31<06:21, 17.56it/s] 75%|███████▌  | 20244/26933 [21:31<06:20, 17.59it/s] 75%|███████▌  | 20248/26933 [21:31<06:19, 17.63it/s] 75%|███████▌  | 20252/26933 [21:31<06:18, 17.65it/s] 75%|███████▌  | 20256/26933 [21:32<06:17, 17.69it/s] 75%|███████▌  | 20260/26933 [21:32<06:19, 17.60it/s] 75%|███████▌  | 20264/26933 [21:32<06:17, 17.66it/s] 75%|███████▌  | 20268/26933 [21:32<06:17, 17.66it/s] 75%|███████▌  | 20272/26933 [21:32<06:16, 17.69it/s] 75%|███████▌  | 20276/26933 [21:33<06:17, 17.62it/s] 75%|███████▌  | 20280/26933 [21:33<06:16, 17.68it/s] 75%|███████▌  | 20284/26933 [21:33<06:15, 17.70it/s] 75%|███████▌  | 20288/26933 [21:33<06:13, 17.79it/s] 75%|███████▌  | 20292/26933 [21:34<06:12, 17.82it/s] 75%|███████▌  | 20296/26933 [21:34<06:13, 17.76it/s] 75%|███████▌  | 20300/26933 [21:34<06:13, 17.78it/s] 75%|███████▌  | 20304/26933 [21:34<06:12, 17.78it/s] 75%|███████▌  | 20308/26933 [21:34<06:11, 17.82it/s] 75%|███████▌  | 20312/26933 [21:35<06:13, 17.73it/s] 75%|███████▌  | 20316/26933 [21:35<06:11, 17.81it/s] 75%|███████▌  | 20320/26933 [21:35<06:10, 17.84it/s] 75%|███████▌  | 20324/26933 [21:35<06:09, 17.87it/s] 75%|███████▌  | 20328/26933 [21:36<06:11, 17.77it/s] 75%|███████▌  | 20332/26933 [21:36<06:11, 17.78it/s] 76%|███████▌  | 20336/26933 [21:36<06:10, 17.78it/s] 76%|███████▌  | 20340/26933 [21:36<06:10, 17.79it/s] 76%|███████▌  | 20344/26933 [21:36<06:09, 17.81it/s] 76%|███████▌  | 20348/26933 [21:37<06:12, 17.67it/s] 76%|███████▌  | 20352/26933 [21:37<06:12, 17.68it/s] 76%|███████▌  | 20356/26933 [21:37<06:11, 17.69it/s] 76%|███████▌  | 20360/26933 [21:37<06:11, 17.71it/s] 76%|███████▌  | 20364/26933 [21:38<06:12, 17.64it/s] 76%|███████▌  | 20368/26933 [21:38<06:11, 17.65it/s] 76%|███████▌  | 20372/26933 [21:38<06:11, 17.65it/s] 76%|███████▌  | 20376/26933 [21:38<06:10, 17.70it/s] 76%|███████▌  | 20380/26933 [21:39<06:09, 17.72it/s] 76%|███████▌  | 20384/26933 [21:39<06:11, 17.63it/s] 76%|███████▌  | 20388/26933 [21:39<06:10, 17.68it/s] 76%|███████▌  | 20392/26933 [21:39<06:09, 17.69it/s] 76%|███████▌  | 20396/26933 [21:39<06:09, 17.70it/s] 76%|███████▌  | 20400/26933 [21:40<06:11, 17.58it/s] 76%|███████▌  | 20404/26933 [21:40<06:10, 17.63it/s] 76%|███████▌  | 20408/26933 [21:40<06:09, 17.65it/s] 76%|███████▌  | 20412/26933 [21:40<06:08, 17.71it/s] 76%|███████▌  | 20416/26933 [21:41<06:07, 17.73it/s] 76%|███████▌  | 20420/26933 [21:41<06:08, 17.67it/s] 76%|███████▌  | 20424/26933 [21:41<06:07, 17.71it/s] 76%|███████▌  | 20428/26933 [21:41<06:06, 17.73it/s] 76%|███████▌  | 20432/26933 [21:41<06:06, 17.73it/s] 76%|███████▌  | 20436/26933 [21:42<06:08, 17.63it/s] 76%|███████▌  | 20440/26933 [21:42<06:07, 17.66it/s] 76%|███████▌  | 20444/26933 [21:42<06:07, 17.67it/s] 76%|███████▌  | 20448/26933 [21:42<06:06, 17.68it/s] 76%|███████▌  | 20452/26933 [21:43<06:07, 17.61it/s] 76%|███████▌  | 20456/26933 [21:43<06:07, 17.64it/s] 76%|███████▌  | 20460/26933 [21:43<06:05, 17.71it/s] 76%|███████▌  | 20464/26933 [21:43<06:04, 17.74it/s] 76%|███████▌  | 20468/26933 [21:43<06:03, 17.78it/s] 76%|███████▌  | 20472/26933 [21:44<06:04, 17.71it/s] 76%|███████▌  | 20476/26933 [21:44<06:03, 17.75it/s] 76%|███████▌  | 20480/26933 [21:44<06:01, 17.83it/s] 76%|███████▌  | 20484/26933 [21:44<06:02, 17.78it/s] 76%|███████▌  | 20488/26933 [21:45<06:03, 17.74it/s] 76%|███████▌  | 20492/26933 [21:45<06:02, 17.78it/s] 76%|███████▌  | 20496/26933 [21:45<06:01, 17.79it/s] 76%|███████▌  | 20500/26933 [21:45<06:01, 17.78it/s] 76%|███████▌  | 20504/26933 [21:46<06:01, 17.77it/s] 76%|███████▌  | 20508/26933 [21:46<06:02, 17.71it/s] 76%|███████▌  | 20512/26933 [21:46<06:01, 17.76it/s] 76%|███████▌  | 20516/26933 [21:46<06:01, 17.77it/s] 76%|███████▌  | 20520/26933 [21:46<06:00, 17.78it/s] 76%|███████▌  | 20524/26933 [21:47<06:01, 17.73it/s] 76%|███████▌  | 20528/26933 [21:47<06:00, 17.78it/s] 76%|███████▌  | 20532/26933 [21:47<05:59, 17.81it/s] 76%|███████▌  | 20536/26933 [21:47<05:58, 17.83it/s] 76%|███████▋  | 20540/26933 [21:48<05:58, 17.86it/s] 76%|███████▋  | 20544/26933 [21:48<05:58, 17.82it/s] 76%|███████▋  | 20548/26933 [21:48<05:57, 17.88it/s] 76%|███████▋  | 20552/26933 [21:48<05:56, 17.88it/s] 76%|███████▋  | 20556/26933 [21:48<05:56, 17.89it/s] 76%|███████▋  | 20560/26933 [21:49<05:58, 17.78it/s] 76%|███████▋  | 20564/26933 [21:49<05:58, 17.77it/s] 76%|███████▋  | 20568/26933 [21:49<05:57, 17.78it/s] 76%|███████▋  | 20572/26933 [21:49<05:58, 17.76it/s] 76%|███████▋  | 20576/26933 [21:50<05:59, 17.68it/s] 76%|███████▋  | 20580/26933 [21:50<05:58, 17.73it/s] 76%|███████▋  | 20584/26933 [21:50<05:58, 17.73it/s] 76%|███████▋  | 20588/26933 [21:50<05:58, 17.71it/s] 76%|███████▋  | 20592/26933 [21:50<05:57, 17.73it/s] 76%|███████▋  | 20596/26933 [21:51<05:58, 17.70it/s] 76%|███████▋  | 20600/26933 [21:51<05:57, 17.73it/s] 77%|███████▋  | 20604/26933 [21:51<05:56, 17.76it/s] 77%|███████▋  | 20608/26933 [21:51<05:56, 17.74it/s] 77%|███████▋  | 20612/26933 [21:52<05:58, 17.65it/s] 77%|███████▋  | 20616/26933 [21:52<05:57, 17.66it/s] 77%|███████▋  | 20620/26933 [21:52<05:57, 17.67it/s] 77%|███████▋  | 20624/26933 [21:52<05:56, 17.69it/s] 77%|███████▋  | 20628/26933 [21:53<05:56, 17.71it/s] 77%|███████▋  | 20632/26933 [21:53<05:57, 17.62it/s] 77%|███████▋  | 20636/26933 [21:53<05:56, 17.64it/s] 77%|███████▋  | 20640/26933 [21:53<05:55, 17.68it/s] 77%|███████▋  | 20644/26933 [21:53<05:55, 17.71it/s] 77%|███████▋  | 20648/26933 [21:54<05:55, 17.66it/s] 77%|███████▋  | 20652/26933 [21:54<05:54, 17.70it/s] 77%|███████▋  | 20656/26933 [21:54<05:54, 17.71it/s] 77%|███████▋  | 20660/26933 [21:54<05:53, 17.76it/s] 77%|███████▋  | 20664/26933 [21:55<05:53, 17.75it/s] 77%|███████▋  | 20668/26933 [21:55<05:54, 17.66it/s] 77%|███████▋  | 20672/26933 [21:55<05:54, 17.66it/s] 77%|███████▋  | 20676/26933 [21:55<05:54, 17.67it/s] 77%|███████▋  | 20680/26933 [21:55<05:52, 17.72it/s] 77%|███████▋  | 20684/26933 [21:56<05:52, 17.71it/s] 77%|███████▋  | 20688/26933 [21:56<05:50, 17.82it/s] 77%|███████▋  | 20692/26933 [21:56<05:48, 17.92it/s] 77%|███████▋  | 20696/26933 [21:56<05:47, 17.96it/s] 77%|███████▋  | 20700/26933 [21:57<05:45, 18.04it/s] 77%|███████▋  | 20704/26933 [21:57<05:46, 17.99it/s] 77%|███████▋  | 20708/26933 [21:57<05:44, 18.07it/s] 77%|███████▋  | 20712/26933 [21:57<05:43, 18.12it/s] 77%|███████▋  | 20716/26933 [21:57<05:42, 18.17it/s] 77%|███████▋  | 20720/26933 [21:58<05:42, 18.13it/s] 77%|███████▋  | 20724/26933 [21:58<05:41, 18.19it/s] 77%|███████▋  | 20728/26933 [21:58<05:41, 18.17it/s] 77%|███████▋  | 20732/26933 [21:58<05:41, 18.17it/s] 77%|███████▋  | 20736/26933 [21:59<05:40, 18.18it/s] 77%|███████▋  | 20740/26933 [21:59<05:40, 18.17it/s] 77%|███████▋  | 20744/26933 [21:59<05:39, 18.20it/s] 77%|███████▋  | 20748/26933 [21:59<05:41, 18.14it/s] 77%|███████▋  | 20752/26933 [21:59<05:40, 18.16it/s] 77%|███████▋  | 20756/26933 [22:00<05:42, 18.06it/s] 77%|███████▋  | 20760/26933 [22:00<05:41, 18.06it/s] 77%|███████▋  | 20764/26933 [22:00<05:42, 18.02it/s] 77%|███████▋  | 20768/26933 [22:00<05:40, 18.08it/s] 77%|███████▋  | 20772/26933 [22:01<05:40, 18.12it/s] 77%|███████▋  | 20776/26933 [22:01<05:40, 18.08it/s] 77%|███████▋  | 20780/26933 [22:01<05:39, 18.14it/s] 77%|███████▋  | 20784/26933 [22:01<05:38, 18.17it/s] 77%|███████▋  | 20788/26933 [22:01<05:36, 18.25it/s] 77%|███████▋  | 20792/26933 [22:02<05:37, 18.18it/s] 77%|███████▋  | 20796/26933 [22:02<05:36, 18.23it/s] 77%|███████▋  | 20800/26933 [22:02<05:35, 18.25it/s] 77%|███████▋  | 20804/26933 [22:02<05:35, 18.28it/s] 77%|███████▋  | 20808/26933 [22:03<05:35, 18.28it/s] 77%|███████▋  | 20812/26933 [22:03<05:35, 18.26it/s] 77%|███████▋  | 20816/26933 [22:03<05:34, 18.28it/s] 77%|███████▋  | 20820/26933 [22:03<05:34, 18.29it/s] 77%|███████▋  | 20824/26933 [22:03<05:34, 18.28it/s] 77%|███████▋  | 20828/26933 [22:04<05:34, 18.23it/s] 77%|███████▋  | 20832/26933 [22:04<05:34, 18.23it/s] 77%|███████▋  | 20836/26933 [22:04<05:34, 18.24it/s] 77%|███████▋  | 20840/26933 [22:04<05:33, 18.28it/s] 77%|███████▋  | 20844/26933 [22:04<05:33, 18.28it/s] 77%|███████▋  | 20848/26933 [22:05<05:33, 18.25it/s] 77%|███████▋  | 20852/26933 [22:05<05:33, 18.26it/s] 77%|███████▋  | 20856/26933 [22:05<05:32, 18.25it/s] 77%|███████▋  | 20860/26933 [22:05<05:33, 18.22it/s] 77%|███████▋  | 20864/26933 [22:06<05:33, 18.20it/s] 77%|███████▋  | 20868/26933 [22:06<05:33, 18.20it/s] 77%|███████▋  | 20872/26933 [22:06<05:32, 18.25it/s] 78%|███████▊  | 20876/26933 [22:06<05:31, 18.26it/s] 78%|███████▊  | 20880/26933 [22:06<05:31, 18.28it/s] 78%|███████▊  | 20884/26933 [22:07<05:32, 18.21it/s] 78%|███████▊  | 20888/26933 [22:07<05:31, 18.25it/s] 78%|███████▊  | 20892/26933 [22:07<05:30, 18.28it/s] 78%|███████▊  | 20896/26933 [22:07<05:31, 18.20it/s] 78%|███████▊  | 20900/26933 [22:08<05:33, 18.08it/s] 78%|███████▊  | 20904/26933 [22:08<05:37, 17.84it/s] 78%|███████▊  | 20908/26933 [22:08<05:38, 17.78it/s] 78%|███████▊  | 20912/26933 [22:08<05:40, 17.69it/s] 78%|███████▊  | 20916/26933 [22:08<05:39, 17.71it/s] 78%|███████▊  | 20920/26933 [22:09<05:40, 17.68it/s] 78%|███████▊  | 20924/26933 [22:09<05:39, 17.71it/s] 78%|███████▊  | 20928/26933 [22:09<05:40, 17.61it/s] 78%|███████▊  | 20932/26933 [22:09<05:39, 17.67it/s] 78%|███████▊  | 20936/26933 [22:10<05:45, 17.36it/s] 78%|███████▊  | 20940/26933 [22:10<05:43, 17.45it/s] 78%|███████▊  | 20944/26933 [22:10<05:40, 17.59it/s] 78%|███████▊  | 20948/26933 [22:10<05:42, 17.49it/s] 78%|███████▊  | 20952/26933 [22:11<05:40, 17.56it/s] 78%|███████▊  | 20956/26933 [22:11<05:40, 17.57it/s] 78%|███████▊  | 20960/26933 [22:11<05:38, 17.67it/s] 78%|███████▊  | 20964/26933 [22:11<05:37, 17.71it/s] 78%|███████▊  | 20968/26933 [22:11<05:34, 17.81it/s] 78%|███████▊  | 20972/26933 [22:12<05:32, 17.90it/s] 78%|███████▊  | 20976/26933 [22:12<05:30, 18.00it/s] 78%|███████▊  | 20980/26933 [22:12<05:29, 18.07it/s] 78%|███████▊  | 20984/26933 [22:12<05:27, 18.15it/s] 78%|███████▊  | 20988/26933 [22:13<05:26, 18.18it/s] 78%|███████▊  | 20992/26933 [22:13<05:26, 18.17it/s] 78%|███████▊  | 20996/26933 [22:13<05:25, 18.22it/s] 78%|███████▊  | 21000/26933 [22:13<05:25, 18.23it/s] 78%|███████▊  | 21004/26933 [22:13<05:24, 18.25it/s] 78%|███████▊  | 21008/26933 [22:14<05:25, 18.22it/s] 78%|███████▊  | 21012/26933 [22:14<05:24, 18.27it/s] 78%|███████▊  | 21016/26933 [22:14<05:23, 18.28it/s] 78%|███████▊  | 21020/26933 [22:14<05:23, 18.28it/s] 78%|███████▊  | 21024/26933 [22:14<05:22, 18.31it/s] 78%|███████▊  | 21028/26933 [22:15<05:23, 18.24it/s] 78%|███████▊  | 21032/26933 [22:15<05:23, 18.24it/s] 78%|███████▊  | 21036/26933 [22:15<05:22, 18.27it/s] 78%|███████▊  | 21040/26933 [22:15<05:22, 18.25it/s] 78%|███████▊  | 21044/26933 [22:16<05:23, 18.21it/s] 78%|███████▊  | 21048/26933 [22:16<05:22, 18.25it/s] 78%|███████▊  | 21052/26933 [22:16<05:21, 18.28it/s] 78%|███████▊  | 21056/26933 [22:16<05:21, 18.26it/s] 78%|███████▊  | 21060/26933 [22:16<05:21, 18.24it/s] 78%|███████▊  | 21064/26933 [22:17<05:29, 17.81it/s] 78%|███████▊  | 21068/26933 [22:17<05:32, 17.65it/s] 78%|███████▊  | 21072/26933 [22:17<05:35, 17.47it/s] 78%|███████▊  | 21076/26933 [22:17<05:38, 17.30it/s] 78%|███████▊  | 21080/26933 [22:18<05:39, 17.23it/s] 78%|███████▊  | 21084/26933 [22:18<05:40, 17.18it/s] 78%|███████▊  | 21088/26933 [22:18<05:41, 17.10it/s] 78%|███████▊  | 21092/26933 [22:18<05:41, 17.12it/s] 78%|███████▊  | 21096/26933 [22:19<05:36, 17.32it/s] 78%|███████▊  | 21100/26933 [22:19<05:33, 17.47it/s] 78%|███████▊  | 21104/26933 [22:19<05:30, 17.64it/s] 78%|███████▊  | 21108/26933 [22:19<05:28, 17.75it/s] 78%|███████▊  | 21112/26933 [22:19<05:26, 17.83it/s] 78%|███████▊  | 21116/26933 [22:20<05:26, 17.84it/s] 78%|███████▊  | 21120/26933 [22:20<05:25, 17.86it/s] 78%|███████▊  | 21124/26933 [22:20<05:24, 17.92it/s] 78%|███████▊  | 21128/26933 [22:20<05:23, 17.96it/s] 78%|███████▊  | 21132/26933 [22:21<05:22, 17.97it/s] 78%|███████▊  | 21136/26933 [22:21<05:23, 17.95it/s] 78%|███████▊  | 21140/26933 [22:21<05:22, 17.96it/s] 79%|███████▊  | 21144/26933 [22:21<05:21, 18.00it/s] 79%|███████▊  | 21148/26933 [22:21<05:20, 18.04it/s] 79%|███████▊  | 21152/26933 [22:22<05:24, 17.82it/s] 79%|███████▊  | 21156/26933 [22:22<05:25, 17.77it/s] 79%|███████▊  | 21160/26933 [22:22<05:25, 17.72it/s] 79%|███████▊  | 21164/26933 [22:22<05:26, 17.69it/s] 79%|███████▊  | 21168/26933 [22:23<05:27, 17.62it/s] 79%|███████▊  | 21172/26933 [22:23<05:26, 17.63it/s] 79%|███████▊  | 21176/26933 [22:23<05:25, 17.66it/s] 79%|███████▊  | 21180/26933 [22:23<05:25, 17.69it/s] 79%|███████▊  | 21184/26933 [22:23<05:25, 17.68it/s] 79%|███████▊  | 21188/26933 [22:24<05:25, 17.65it/s] 79%|███████▊  | 21192/26933 [22:24<05:24, 17.67it/s] 79%|███████▊  | 21196/26933 [22:24<05:23, 17.71it/s] 79%|███████▊  | 21200/26933 [22:24<05:24, 17.68it/s] 79%|███████▊  | 21204/26933 [22:25<05:24, 17.63it/s] 79%|███████▊  | 21208/26933 [22:25<05:24, 17.65it/s] 79%|███████▉  | 21212/26933 [22:25<05:23, 17.67it/s] 79%|███████▉  | 21216/26933 [22:25<05:23, 17.67it/s] 79%|███████▉  | 21220/26933 [22:26<05:23, 17.68it/s] 79%|███████▉  | 21224/26933 [22:26<05:23, 17.65it/s] 79%|███████▉  | 21228/26933 [22:26<05:22, 17.68it/s] 79%|███████▉  | 21232/26933 [22:26<05:22, 17.66it/s] 79%|███████▉  | 21236/26933 [22:26<05:22, 17.69it/s] 79%|███████▉  | 21240/26933 [22:27<05:22, 17.68it/s] 79%|███████▉  | 21244/26933 [22:27<05:21, 17.69it/s] 79%|███████▉  | 21248/26933 [22:27<05:21, 17.69it/s] 79%|███████▉  | 21252/26933 [22:27<05:20, 17.70it/s] 79%|███████▉  | 21256/26933 [22:28<05:20, 17.71it/s] 79%|███████▉  | 21260/26933 [22:28<05:20, 17.68it/s] 79%|███████▉  | 21264/26933 [22:28<05:20, 17.67it/s] 79%|███████▉  | 21268/26933 [22:28<05:20, 17.66it/s] 79%|███████▉  | 21272/26933 [22:28<05:20, 17.68it/s] 79%|███████▉  | 21276/26933 [22:29<05:20, 17.65it/s] 79%|███████▉  | 21280/26933 [22:29<05:20, 17.66it/s] 79%|███████▉  | 21284/26933 [22:29<05:19, 17.66it/s] 79%|███████▉  | 21288/26933 [22:29<05:18, 17.70it/s] 79%|███████▉  | 21292/26933 [22:30<05:19, 17.67it/s] 79%|███████▉  | 21296/26933 [22:30<05:18, 17.67it/s] 79%|███████▉  | 21300/26933 [22:30<05:17, 17.71it/s] 79%|███████▉  | 21304/26933 [22:30<05:17, 17.73it/s] 79%|███████▉  | 21308/26933 [22:31<05:17, 17.74it/s] 79%|███████▉  | 21312/26933 [22:31<05:18, 17.67it/s] 79%|███████▉  | 21316/26933 [22:31<05:18, 17.65it/s] 79%|███████▉  | 21320/26933 [22:31<05:17, 17.65it/s] 79%|███████▉  | 21324/26933 [22:31<05:17, 17.69it/s] 79%|███████▉  | 21328/26933 [22:32<05:17, 17.65it/s] 79%|███████▉  | 21332/26933 [22:32<05:15, 17.73it/s] 79%|███████▉  | 21336/26933 [22:32<05:13, 17.83it/s] 79%|███████▉  | 21340/26933 [22:32<05:12, 17.87it/s] 79%|███████▉  | 21344/26933 [22:33<05:12, 17.87it/s] 79%|███████▉  | 21348/26933 [22:33<05:12, 17.89it/s] 79%|███████▉  | 21352/26933 [22:33<05:11, 17.93it/s] 79%|███████▉  | 21356/26933 [22:33<05:11, 17.91it/s] 79%|███████▉  | 21360/26933 [22:33<05:10, 17.92it/s] 79%|███████▉  | 21364/26933 [22:34<05:11, 17.89it/s] 79%|███████▉  | 21368/26933 [22:34<05:10, 17.90it/s] 79%|███████▉  | 21372/26933 [22:34<05:11, 17.88it/s] 79%|███████▉  | 21376/26933 [22:34<05:10, 17.87it/s] 79%|███████▉  | 21380/26933 [22:35<05:10, 17.90it/s] 79%|███████▉  | 21384/26933 [22:35<05:10, 17.87it/s] 79%|███████▉  | 21388/26933 [22:35<05:09, 17.92it/s] 79%|███████▉  | 21392/26933 [22:35<05:09, 17.91it/s] 79%|███████▉  | 21396/26933 [22:35<05:09, 17.91it/s] 79%|███████▉  | 21400/26933 [22:36<05:08, 17.93it/s] 79%|███████▉  | 21404/26933 [22:36<05:07, 17.97it/s] 79%|███████▉  | 21408/26933 [22:36<05:06, 18.02it/s] 80%|███████▉  | 21412/26933 [22:36<05:05, 18.07it/s] 80%|███████▉  | 21416/26933 [22:37<05:05, 18.06it/s] 80%|███████▉  | 21420/26933 [22:37<05:05, 18.05it/s] 80%|███████▉  | 21424/26933 [22:37<05:05, 18.06it/s] 80%|███████▉  | 21428/26933 [22:37<05:04, 18.08it/s] 80%|███████▉  | 21432/26933 [22:37<05:03, 18.12it/s] 80%|███████▉  | 21436/26933 [22:38<05:06, 17.96it/s] 80%|███████▉  | 21440/26933 [22:38<05:06, 17.90it/s] 80%|███████▉  | 21444/26933 [22:38<05:07, 17.82it/s] 80%|███████▉  | 21448/26933 [22:38<05:08, 17.80it/s] 80%|███████▉  | 21452/26933 [22:39<05:08, 17.76it/s] 80%|███████▉  | 21456/26933 [22:39<05:09, 17.70it/s] 80%|███████▉  | 21460/26933 [22:39<05:09, 17.67it/s] 80%|███████▉  | 21464/26933 [22:39<05:09, 17.66it/s] 80%|███████▉  | 21468/26933 [22:39<05:08, 17.70it/s] 80%|███████▉  | 21472/26933 [22:40<05:08, 17.70it/s] 80%|███████▉  | 21476/26933 [22:40<05:06, 17.83it/s] 80%|███████▉  | 21480/26933 [22:40<05:03, 17.94it/s] 80%|███████▉  | 21484/26933 [22:40<05:02, 18.00it/s] 80%|███████▉  | 21488/26933 [22:41<05:02, 18.00it/s] 80%|███████▉  | 21492/26933 [22:41<05:01, 18.03it/s] 80%|███████▉  | 21496/26933 [22:41<05:01, 18.03it/s] 80%|███████▉  | 21500/26933 [22:41<05:01, 18.04it/s] 80%|███████▉  | 21504/26933 [22:41<05:00, 18.09it/s] 80%|███████▉  | 21508/26933 [22:42<05:00, 18.06it/s] 80%|███████▉  | 21512/26933 [22:42<04:59, 18.08it/s] 80%|███████▉  | 21516/26933 [22:42<04:59, 18.09it/s] 80%|███████▉  | 21520/26933 [22:42<04:58, 18.11it/s] 80%|███████▉  | 21524/26933 [22:43<04:58, 18.13it/s] 80%|███████▉  | 21528/26933 [22:43<04:58, 18.10it/s] 80%|███████▉  | 21532/26933 [22:43<04:58, 18.09it/s] 80%|███████▉  | 21536/26933 [22:43<04:57, 18.11it/s] 80%|███████▉  | 21540/26933 [22:43<04:57, 18.14it/s] 80%|███████▉  | 21544/26933 [22:44<04:57, 18.12it/s] 80%|████████  | 21548/26933 [22:44<04:56, 18.13it/s] 80%|████████  | 21552/26933 [22:44<04:56, 18.16it/s] 80%|████████  | 21556/26933 [22:44<04:55, 18.17it/s] 80%|████████  | 21560/26933 [22:45<04:56, 18.13it/s] 80%|████████  | 21564/26933 [22:45<04:56, 18.08it/s] 80%|████████  | 21568/26933 [22:45<04:56, 18.10it/s] 80%|████████  | 21572/26933 [22:45<04:55, 18.13it/s] 80%|████████  | 21576/26933 [22:45<04:55, 18.14it/s] 80%|████████  | 21580/26933 [22:46<04:56, 18.07it/s] 80%|████████  | 21584/26933 [22:46<04:55, 18.11it/s] 80%|████████  | 21588/26933 [22:46<04:54, 18.12it/s] 80%|████████  | 21592/26933 [22:46<04:54, 18.13it/s] 80%|████████  | 21596/26933 [22:47<04:54, 18.12it/s] 80%|████████  | 21600/26933 [22:47<04:55, 18.07it/s] 80%|████████  | 21604/26933 [22:47<04:54, 18.08it/s] 80%|████████  | 21608/26933 [22:47<04:54, 18.08it/s] 80%|████████  | 21612/26933 [22:47<04:54, 18.06it/s] 80%|████████  | 21616/26933 [22:48<04:55, 18.01it/s] 80%|████████  | 21620/26933 [22:48<04:55, 17.96it/s] 80%|████████  | 21624/26933 [22:48<04:56, 17.94it/s] 80%|████████  | 21628/26933 [22:48<04:56, 17.92it/s] 80%|████████  | 21632/26933 [22:49<04:56, 17.89it/s] 80%|████████  | 21636/26933 [22:49<04:56, 17.84it/s] 80%|████████  | 21640/26933 [22:49<04:56, 17.85it/s] 80%|████████  | 21644/26933 [22:49<04:55, 17.87it/s] 80%|████████  | 21648/26933 [22:49<04:55, 17.86it/s] 80%|████████  | 21652/26933 [22:50<04:56, 17.81it/s] 80%|████████  | 21656/26933 [22:50<04:56, 17.80it/s] 80%|████████  | 21660/26933 [22:50<04:55, 17.85it/s] 80%|████████  | 21664/26933 [22:50<04:54, 17.87it/s] 80%|████████  | 21668/26933 [22:51<04:54, 17.90it/s] 80%|████████  | 21672/26933 [22:51<04:54, 17.85it/s] 80%|████████  | 21676/26933 [22:51<04:54, 17.88it/s] 80%|████████  | 21680/26933 [22:51<04:53, 17.93it/s] 81%|████████  | 21684/26933 [22:51<04:52, 17.96it/s] 81%|████████  | 21688/26933 [22:52<04:51, 17.98it/s] 81%|████████  | 21692/26933 [22:52<04:51, 17.99it/s] 81%|████████  | 21696/26933 [22:52<04:51, 17.99it/s] 81%|████████  | 21700/26933 [22:52<04:50, 18.02it/s] 81%|████████  | 21704/26933 [22:53<04:49, 18.04it/s] 81%|████████  | 21708/26933 [22:53<04:49, 18.02it/s] 81%|████████  | 21712/26933 [22:53<04:49, 18.05it/s] 81%|████████  | 21716/26933 [22:53<04:48, 18.08it/s] 81%|████████  | 21720/26933 [22:53<04:47, 18.10it/s] 81%|████████  | 21724/26933 [22:54<04:47, 18.09it/s] 81%|████████  | 21728/26933 [22:54<04:47, 18.11it/s] 81%|████████  | 21732/26933 [22:54<04:47, 18.08it/s] 81%|████████  | 21736/26933 [22:54<04:47, 18.10it/s] 81%|████████  | 21740/26933 [22:55<04:47, 18.09it/s] 81%|████████  | 21744/26933 [22:55<04:47, 18.05it/s] 81%|████████  | 21748/26933 [22:55<04:46, 18.07it/s] 81%|████████  | 21752/26933 [22:55<04:47, 18.03it/s] 81%|████████  | 21756/26933 [22:55<04:46, 18.09it/s] 81%|████████  | 21760/26933 [22:56<04:46, 18.05it/s] 81%|████████  | 21764/26933 [22:56<04:46, 18.05it/s] 81%|████████  | 21768/26933 [22:56<04:47, 17.95it/s] 81%|████████  | 21772/26933 [22:56<04:48, 17.88it/s] 81%|████████  | 21776/26933 [22:57<04:48, 17.85it/s] 81%|████████  | 21780/26933 [22:57<04:49, 17.83it/s] 81%|████████  | 21784/26933 [22:57<04:48, 17.85it/s] 81%|████████  | 21788/26933 [22:57<04:48, 17.84it/s] 81%|████████  | 21792/26933 [22:57<04:50, 17.69it/s] 81%|████████  | 21796/26933 [22:58<04:49, 17.74it/s] 81%|████████  | 21800/26933 [22:58<04:48, 17.80it/s] 81%|████████  | 21804/26933 [22:58<04:47, 17.82it/s] 81%|████████  | 21808/26933 [22:58<04:47, 17.85it/s] 81%|████████  | 21812/26933 [22:59<04:47, 17.81it/s] 81%|████████  | 21816/26933 [22:59<04:47, 17.83it/s] 81%|████████  | 21820/26933 [22:59<04:46, 17.85it/s] 81%|████████  | 21824/26933 [22:59<04:46, 17.86it/s] 81%|████████  | 21828/26933 [22:59<04:45, 17.87it/s] 81%|████████  | 21832/26933 [23:00<04:46, 17.83it/s] 81%|████████  | 21836/26933 [23:00<04:45, 17.87it/s] 81%|████████  | 21840/26933 [23:00<04:45, 17.86it/s] 81%|████████  | 21844/26933 [23:00<04:44, 17.90it/s] 81%|████████  | 21848/26933 [23:01<04:44, 17.86it/s] 81%|████████  | 21852/26933 [23:01<04:43, 17.89it/s] 81%|████████  | 21856/26933 [23:01<04:44, 17.86it/s] 81%|████████  | 21860/26933 [23:01<04:44, 17.83it/s] 81%|████████  | 21864/26933 [23:01<04:44, 17.84it/s] 81%|████████  | 21868/26933 [23:02<04:44, 17.81it/s] 81%|████████  | 21872/26933 [23:02<04:43, 17.83it/s] 81%|████████  | 21876/26933 [23:02<04:43, 17.81it/s] 81%|████████  | 21880/26933 [23:02<04:44, 17.79it/s] 81%|████████▏ | 21884/26933 [23:03<04:43, 17.78it/s] 81%|████████▏ | 21888/26933 [23:03<04:42, 17.83it/s] 81%|████████▏ | 21892/26933 [23:03<04:42, 17.86it/s] 81%|████████▏ | 21896/26933 [23:03<04:42, 17.85it/s] 81%|████████▏ | 21900/26933 [23:04<04:41, 17.88it/s] 81%|████████▏ | 21904/26933 [23:04<04:41, 17.88it/s] 81%|████████▏ | 21908/26933 [23:04<04:40, 17.91it/s] 81%|████████▏ | 21912/26933 [23:04<04:40, 17.90it/s] 81%|████████▏ | 21916/26933 [23:04<04:39, 17.92it/s] 81%|████████▏ | 21920/26933 [23:05<04:40, 17.89it/s] 81%|████████▏ | 21924/26933 [23:05<04:39, 17.91it/s] 81%|████████▏ | 21928/26933 [23:05<04:39, 17.93it/s] 81%|████████▏ | 21932/26933 [23:05<04:39, 17.86it/s] 81%|████████▏ | 21936/26933 [23:06<04:40, 17.84it/s] 81%|████████▏ | 21940/26933 [23:06<04:40, 17.78it/s] 81%|████████▏ | 21944/26933 [23:06<04:40, 17.82it/s] 81%|████████▏ | 21948/26933 [23:06<04:39, 17.83it/s] 82%|████████▏ | 21952/26933 [23:06<04:38, 17.87it/s] 82%|████████▏ | 21956/26933 [23:07<04:38, 17.88it/s] 82%|████████▏ | 21960/26933 [23:07<04:38, 17.89it/s] 82%|████████▏ | 21964/26933 [23:07<04:37, 17.90it/s] 82%|████████▏ | 21968/26933 [23:07<04:37, 17.87it/s] 82%|████████▏ | 21972/26933 [23:08<04:38, 17.83it/s] 82%|████████▏ | 21976/26933 [23:08<04:39, 17.75it/s] 82%|████████▏ | 21980/26933 [23:08<04:39, 17.73it/s] 82%|████████▏ | 21984/26933 [23:08<04:40, 17.65it/s] 82%|████████▏ | 21988/26933 [23:08<04:39, 17.66it/s] 82%|████████▏ | 21992/26933 [23:09<04:39, 17.66it/s] 82%|████████▏ | 21996/26933 [23:09<04:40, 17.62it/s] 82%|████████▏ | 22000/26933 [23:09<04:41, 17.53it/s] 82%|████████▏ | 22004/26933 [23:09<04:40, 17.60it/s] 82%|████████▏ | 22008/26933 [23:10<04:40, 17.58it/s] 82%|████████▏ | 22012/26933 [23:10<04:38, 17.65it/s] 82%|████████▏ | 22016/26933 [23:10<04:38, 17.67it/s] 82%|████████▏ | 22020/26933 [23:10<04:37, 17.68it/s] 82%|████████▏ | 22024/26933 [23:10<04:37, 17.67it/s] 82%|████████▏ | 22028/26933 [23:11<04:37, 17.64it/s] 82%|████████▏ | 22032/26933 [23:11<04:37, 17.66it/s] 82%|████████▏ | 22036/26933 [23:11<04:37, 17.66it/s] 82%|████████▏ | 22040/26933 [23:11<04:37, 17.66it/s] 82%|████████▏ | 22044/26933 [23:12<04:37, 17.64it/s] 82%|████████▏ | 22048/26933 [23:12<04:36, 17.66it/s] 82%|████████▏ | 22052/26933 [23:12<04:35, 17.69it/s] 82%|████████▏ | 22056/26933 [23:12<04:35, 17.70it/s] 82%|████████▏ | 22060/26933 [23:13<04:35, 17.71it/s] 82%|████████▏ | 22064/26933 [23:13<04:35, 17.69it/s] 82%|████████▏ | 22068/26933 [23:13<04:34, 17.70it/s] 82%|████████▏ | 22072/26933 [23:13<04:35, 17.66it/s] 82%|████████▏ | 22076/26933 [23:13<04:34, 17.68it/s] 82%|████████▏ | 22080/26933 [23:14<04:34, 17.65it/s] 82%|████████▏ | 22084/26933 [23:14<04:34, 17.69it/s] 82%|████████▏ | 22088/26933 [23:14<04:34, 17.67it/s] 82%|████████▏ | 22092/26933 [23:14<04:33, 17.69it/s] 82%|████████▏ | 22096/26933 [23:15<04:33, 17.69it/s] 82%|████████▏ | 22100/26933 [23:15<04:33, 17.68it/s] 82%|████████▏ | 22104/26933 [23:15<04:32, 17.70it/s] 82%|████████▏ | 22108/26933 [23:15<04:32, 17.70it/s] 82%|████████▏ | 22112/26933 [23:15<04:32, 17.69it/s] 82%|████████▏ | 22116/26933 [23:16<04:32, 17.68it/s] 82%|████████▏ | 22120/26933 [23:16<04:31, 17.72it/s] 82%|████████▏ | 22124/26933 [23:16<04:30, 17.75it/s] 82%|████████▏ | 22128/26933 [23:16<04:30, 17.73it/s] 82%|████████▏ | 22132/26933 [23:17<04:31, 17.70it/s] 82%|████████▏ | 22136/26933 [23:17<04:30, 17.75it/s] 82%|████████▏ | 22140/26933 [23:17<04:29, 17.78it/s] 82%|████████▏ | 22144/26933 [23:17<04:29, 17.76it/s] 82%|████████▏ | 22148/26933 [23:17<04:29, 17.77it/s] 82%|████████▏ | 22152/26933 [23:18<04:29, 17.71it/s] 82%|████████▏ | 22156/26933 [23:18<04:28, 17.77it/s] 82%|████████▏ | 22160/26933 [23:18<04:27, 17.81it/s] 82%|████████▏ | 22164/26933 [23:18<04:27, 17.82it/s] 82%|████████▏ | 22168/26933 [23:19<04:27, 17.80it/s] 82%|████████▏ | 22172/26933 [23:19<04:26, 17.83it/s] 82%|████████▏ | 22176/26933 [23:19<04:26, 17.88it/s] 82%|████████▏ | 22180/26933 [23:19<04:25, 17.88it/s] 82%|████████▏ | 22184/26933 [23:20<04:25, 17.89it/s] 82%|████████▏ | 22188/26933 [23:20<04:25, 17.87it/s] 82%|████████▏ | 22192/26933 [23:20<04:25, 17.86it/s] 82%|████████▏ | 22196/26933 [23:20<04:25, 17.83it/s] 82%|████████▏ | 22200/26933 [23:20<04:25, 17.81it/s] 82%|████████▏ | 22204/26933 [23:21<04:26, 17.76it/s] 82%|████████▏ | 22208/26933 [23:21<04:25, 17.79it/s] 82%|████████▏ | 22212/26933 [23:21<04:25, 17.78it/s] 82%|████████▏ | 22216/26933 [23:21<04:25, 17.79it/s] 83%|████████▎ | 22220/26933 [23:22<04:25, 17.76it/s] 83%|████████▎ | 22224/26933 [23:22<04:25, 17.73it/s] 83%|████████▎ | 22228/26933 [23:22<04:24, 17.76it/s] 83%|████████▎ | 22232/26933 [23:22<04:25, 17.74it/s] 83%|████████▎ | 22236/26933 [23:22<04:24, 17.75it/s] 83%|████████▎ | 22240/26933 [23:23<04:24, 17.73it/s] 83%|████████▎ | 22244/26933 [23:23<04:24, 17.76it/s] 83%|████████▎ | 22248/26933 [23:23<04:24, 17.74it/s] 83%|████████▎ | 22252/26933 [23:23<04:24, 17.72it/s] 83%|████████▎ | 22256/26933 [23:24<04:23, 17.72it/s] 83%|████████▎ | 22260/26933 [23:24<04:24, 17.70it/s] 83%|████████▎ | 22264/26933 [23:24<04:23, 17.74it/s] 83%|████████▎ | 22268/26933 [23:24<04:22, 17.77it/s] 83%|████████▎ | 22272/26933 [23:24<04:21, 17.80it/s] 83%|████████▎ | 22276/26933 [23:25<04:21, 17.78it/s] 83%|████████▎ | 22280/26933 [23:25<04:21, 17.81it/s] 83%|████████▎ | 22284/26933 [23:25<04:21, 17.80it/s] 83%|████████▎ | 22288/26933 [23:25<04:20, 17.81it/s] 83%|████████▎ | 22292/26933 [23:26<04:20, 17.80it/s] 83%|████████▎ | 22296/26933 [23:26<04:19, 17.86it/s] 83%|████████▎ | 22300/26933 [23:26<04:18, 17.90it/s] 83%|████████▎ | 22304/26933 [23:26<04:18, 17.92it/s] 83%|████████▎ | 22308/26933 [23:26<04:17, 17.95it/s] 83%|████████▎ | 22312/26933 [23:27<04:17, 17.94it/s] 83%|████████▎ | 22316/26933 [23:27<04:16, 17.98it/s] 83%|████████▎ | 22320/26933 [23:27<04:16, 17.96it/s] 83%|████████▎ | 22324/26933 [23:27<04:16, 17.98it/s] 83%|████████▎ | 22328/26933 [23:28<04:16, 17.95it/s] 83%|████████▎ | 22332/26933 [23:28<04:16, 17.97it/s] 83%|████████▎ | 22336/26933 [23:28<04:16, 17.94it/s] 83%|████████▎ | 22340/26933 [23:28<04:15, 17.95it/s] 83%|████████▎ | 22344/26933 [23:28<04:15, 17.93it/s] 83%|████████▎ | 22348/26933 [23:29<04:16, 17.87it/s] 83%|████████▎ | 22352/26933 [23:29<04:16, 17.89it/s] 83%|████████▎ | 22356/26933 [23:29<04:15, 17.88it/s] 83%|████████▎ | 22360/26933 [23:29<04:15, 17.93it/s] 83%|████████▎ | 22364/26933 [23:30<04:15, 17.90it/s] 83%|████████▎ | 22368/26933 [23:30<04:14, 17.95it/s] 83%|████████▎ | 22372/26933 [23:30<04:13, 17.97it/s] 83%|████████▎ | 22376/26933 [23:30<04:13, 17.97it/s] 83%|████████▎ | 22380/26933 [23:30<04:12, 18.00it/s] 83%|████████▎ | 22384/26933 [23:31<04:13, 17.95it/s] 83%|████████▎ | 22388/26933 [23:31<04:12, 17.97it/s] 83%|████████▎ | 22392/26933 [23:31<04:12, 18.00it/s] 83%|████████▎ | 22396/26933 [23:31<04:11, 18.01it/s] 83%|████████▎ | 22400/26933 [23:32<04:11, 17.99it/s] 83%|████████▎ | 22404/26933 [23:32<04:12, 17.93it/s] 83%|████████▎ | 22408/26933 [23:32<04:13, 17.85it/s] 83%|████████▎ | 22412/26933 [23:32<04:15, 17.72it/s] 83%|████████▎ | 22416/26933 [23:33<04:15, 17.69it/s] 83%|████████▎ | 22420/26933 [23:33<04:15, 17.67it/s] 83%|████████▎ | 22424/26933 [23:33<04:14, 17.74it/s] 83%|████████▎ | 22428/26933 [23:33<04:13, 17.79it/s] 83%|████████▎ | 22432/26933 [23:33<04:12, 17.81it/s] 83%|████████▎ | 22436/26933 [23:34<04:12, 17.83it/s] 83%|████████▎ | 22440/26933 [23:34<04:11, 17.89it/s] 83%|████████▎ | 22444/26933 [23:34<04:11, 17.84it/s] 83%|████████▎ | 22448/26933 [23:34<04:11, 17.85it/s] 83%|████████▎ | 22452/26933 [23:35<04:10, 17.87it/s] 83%|████████▎ | 22456/26933 [23:35<04:10, 17.84it/s] 83%|████████▎ | 22460/26933 [23:35<04:10, 17.86it/s] 83%|████████▎ | 22464/26933 [23:35<04:10, 17.83it/s] 83%|████████▎ | 22468/26933 [23:35<04:10, 17.86it/s] 83%|████████▎ | 22472/26933 [23:36<04:10, 17.82it/s] 83%|████████▎ | 22476/26933 [23:36<04:09, 17.84it/s] 83%|████████▎ | 22480/26933 [23:36<04:09, 17.86it/s] 83%|████████▎ | 22484/26933 [23:36<04:09, 17.81it/s] 83%|████████▎ | 22488/26933 [23:37<04:09, 17.81it/s] 84%|████████▎ | 22492/26933 [23:37<04:09, 17.78it/s] 84%|████████▎ | 22496/26933 [23:37<04:08, 17.88it/s] 84%|████████▎ | 22500/26933 [23:37<04:07, 17.93it/s] 84%|████████▎ | 22504/26933 [23:37<04:06, 17.97it/s] 84%|████████▎ | 22508/26933 [23:38<04:06, 17.98it/s] 84%|████████▎ | 22512/26933 [23:38<04:05, 18.01it/s] 84%|████████▎ | 22516/26933 [23:38<04:04, 18.06it/s] 84%|████████▎ | 22520/26933 [23:38<04:04, 18.06it/s] 84%|████████▎ | 22524/26933 [23:39<04:03, 18.10it/s] 84%|████████▎ | 22528/26933 [23:39<04:03, 18.09it/s] 84%|████████▎ | 22532/26933 [23:39<04:02, 18.15it/s] 84%|████████▎ | 22536/26933 [23:39<04:02, 18.14it/s] 84%|████████▎ | 22540/26933 [23:39<04:02, 18.09it/s] 84%|████████▎ | 22544/26933 [23:40<04:03, 18.04it/s] 84%|████████▎ | 22548/26933 [23:40<04:02, 18.08it/s] 84%|████████▎ | 22552/26933 [23:40<04:02, 18.06it/s] 84%|████████▎ | 22556/26933 [23:40<04:02, 18.08it/s] 84%|████████▍ | 22560/26933 [23:41<04:01, 18.09it/s] 84%|████████▍ | 22564/26933 [23:41<04:02, 18.05it/s] 84%|████████▍ | 22568/26933 [23:41<04:01, 18.07it/s] 84%|████████▍ | 22572/26933 [23:41<04:00, 18.13it/s] 84%|████████▍ | 22576/26933 [23:41<04:00, 18.13it/s] 84%|████████▍ | 22580/26933 [23:42<04:00, 18.08it/s] 84%|████████▍ | 22584/26933 [23:42<04:00, 18.11it/s] 84%|████████▍ | 22588/26933 [23:42<03:59, 18.15it/s] 84%|████████▍ | 22592/26933 [23:42<03:59, 18.16it/s] 84%|████████▍ | 22596/26933 [23:43<03:58, 18.15it/s] 84%|████████▍ | 22600/26933 [23:43<03:58, 18.15it/s] 84%|████████▍ | 22604/26933 [23:43<03:58, 18.13it/s] 84%|████████▍ | 22608/26933 [23:43<03:58, 18.13it/s] 84%|████████▍ | 22612/26933 [23:43<03:58, 18.13it/s] 84%|████████▍ | 22616/26933 [23:44<03:58, 18.10it/s] 84%|████████▍ | 22620/26933 [23:44<03:58, 18.11it/s] 84%|████████▍ | 22624/26933 [23:44<03:57, 18.13it/s] 84%|████████▍ | 22628/26933 [23:44<03:57, 18.13it/s] 84%|████████▍ | 22632/26933 [23:44<03:57, 18.10it/s] 84%|████████▍ | 22636/26933 [23:45<03:57, 18.07it/s] 84%|████████▍ | 22640/26933 [23:45<04:00, 17.86it/s] 84%|████████▍ | 22644/26933 [23:45<03:59, 17.91it/s] 84%|████████▍ | 22648/26933 [23:45<03:58, 17.98it/s] 84%|████████▍ | 22652/26933 [23:46<03:57, 17.99it/s] 84%|████████▍ | 22656/26933 [23:46<03:57, 18.04it/s] 84%|████████▍ | 22660/26933 [23:46<03:56, 18.07it/s] 84%|████████▍ | 22664/26933 [23:46<03:56, 18.09it/s] 84%|████████▍ | 22668/26933 [23:46<03:55, 18.13it/s] 84%|████████▍ | 22672/26933 [23:47<03:55, 18.11it/s] 84%|████████▍ | 22676/26933 [23:47<03:57, 17.94it/s] 84%|████████▍ | 22680/26933 [23:47<03:56, 17.98it/s] 84%|████████▍ | 22684/26933 [23:47<03:55, 18.01it/s] 84%|████████▍ | 22688/26933 [23:48<03:55, 17.99it/s] 84%|████████▍ | 22692/26933 [23:48<03:55, 18.03it/s] 84%|████████▍ | 22696/26933 [23:48<03:54, 18.04it/s] 84%|████████▍ | 22700/26933 [23:48<03:56, 17.89it/s] 84%|████████▍ | 22704/26933 [23:49<03:57, 17.81it/s] 84%|████████▍ | 22708/26933 [23:49<03:58, 17.74it/s] 84%|████████▍ | 22712/26933 [23:49<03:57, 17.74it/s] 84%|████████▍ | 22716/26933 [23:49<03:57, 17.75it/s] 84%|████████▍ | 22720/26933 [23:49<03:56, 17.84it/s] 84%|████████▍ | 22724/26933 [23:50<03:55, 17.86it/s] 84%|████████▍ | 22728/26933 [23:50<03:54, 17.93it/s] 84%|████████▍ | 22732/26933 [23:50<03:53, 17.97it/s] 84%|████████▍ | 22736/26933 [23:50<03:53, 18.01it/s] 84%|████████▍ | 22740/26933 [23:51<03:53, 17.99it/s] 84%|████████▍ | 22744/26933 [23:51<03:52, 17.98it/s] 84%|████████▍ | 22748/26933 [23:51<03:52, 18.02it/s] 84%|████████▍ | 22752/26933 [23:51<03:51, 18.03it/s] 84%|████████▍ | 22756/26933 [23:51<03:51, 18.05it/s] 85%|████████▍ | 22760/26933 [23:52<03:52, 17.97it/s] 85%|████████▍ | 22764/26933 [23:52<03:51, 18.02it/s] 85%|████████▍ | 22768/26933 [23:52<03:50, 18.07it/s] 85%|████████▍ | 22772/26933 [23:52<03:50, 18.07it/s] 85%|████████▍ | 22776/26933 [23:53<03:49, 18.09it/s] 85%|████████▍ | 22780/26933 [23:53<03:49, 18.07it/s] 85%|████████▍ | 22784/26933 [23:53<03:49, 18.11it/s] 85%|████████▍ | 22788/26933 [23:53<03:48, 18.12it/s] 85%|████████▍ | 22792/26933 [23:53<03:48, 18.12it/s] 85%|████████▍ | 22796/26933 [23:54<03:49, 18.06it/s] 85%|████████▍ | 22800/26933 [23:54<03:48, 18.09it/s] 85%|████████▍ | 22804/26933 [23:54<03:47, 18.11it/s] 85%|████████▍ | 22808/26933 [23:54<03:47, 18.14it/s] 85%|████████▍ | 22812/26933 [23:54<03:46, 18.16it/s] 85%|████████▍ | 22816/26933 [23:55<03:46, 18.16it/s] 85%|████████▍ | 22820/26933 [23:55<03:46, 18.16it/s] 85%|████████▍ | 22824/26933 [23:55<03:46, 18.17it/s] 85%|████████▍ | 22828/26933 [23:55<03:46, 18.15it/s] 85%|████████▍ | 22832/26933 [23:56<03:46, 18.08it/s] 85%|████████▍ | 22836/26933 [23:56<03:46, 18.11it/s] 85%|████████▍ | 22840/26933 [23:56<03:45, 18.13it/s] 85%|████████▍ | 22844/26933 [23:56<03:45, 18.15it/s] 85%|████████▍ | 22848/26933 [23:56<03:44, 18.17it/s] 85%|████████▍ | 22852/26933 [23:57<03:44, 18.15it/s] 85%|████████▍ | 22856/26933 [23:57<03:44, 18.20it/s] 85%|████████▍ | 22860/26933 [23:57<03:43, 18.24it/s] 85%|████████▍ | 22864/26933 [23:57<03:43, 18.23it/s] 85%|████████▍ | 22868/26933 [23:58<03:43, 18.20it/s] 85%|████████▍ | 22872/26933 [23:58<03:42, 18.21it/s] 85%|████████▍ | 22876/26933 [23:58<03:42, 18.21it/s] 85%|████████▍ | 22880/26933 [23:58<03:43, 18.17it/s] 85%|████████▍ | 22884/26933 [23:58<03:42, 18.16it/s] 85%|████████▍ | 22888/26933 [23:59<03:43, 18.13it/s] 85%|████████▍ | 22892/26933 [23:59<03:43, 18.08it/s] 85%|████████▌ | 22896/26933 [23:59<03:42, 18.11it/s] 85%|████████▌ | 22900/26933 [23:59<03:42, 18.16it/s] 85%|████████▌ | 22904/26933 [24:00<03:41, 18.17it/s] 85%|████████▌ | 22908/26933 [24:00<03:42, 18.08it/s] 85%|████████▌ | 22912/26933 [24:00<03:42, 18.09it/s] 85%|████████▌ | 22916/26933 [24:00<03:41, 18.10it/s] 85%|████████▌ | 22920/26933 [24:00<03:41, 18.13it/s] 85%|████████▌ | 22924/26933 [24:01<03:41, 18.07it/s] 85%|████████▌ | 22928/26933 [24:01<03:41, 18.10it/s] 85%|████████▌ | 22932/26933 [24:01<03:41, 18.10it/s] 85%|████████▌ | 22936/26933 [24:01<03:40, 18.14it/s] 85%|████████▌ | 22940/26933 [24:02<03:40, 18.14it/s] 85%|████████▌ | 22944/26933 [24:02<03:40, 18.09it/s] 85%|████████▌ | 22948/26933 [24:02<03:40, 18.10it/s] 85%|████████▌ | 22952/26933 [24:02<03:39, 18.14it/s] 85%|████████▌ | 22956/26933 [24:02<03:39, 18.14it/s] 85%|████████▌ | 22960/26933 [24:03<03:39, 18.13it/s] 85%|████████▌ | 22964/26933 [24:03<03:38, 18.15it/s] 85%|████████▌ | 22968/26933 [24:03<03:38, 18.16it/s] 85%|████████▌ | 22972/26933 [24:03<03:37, 18.19it/s] 85%|████████▌ | 22976/26933 [24:04<03:37, 18.18it/s] 85%|████████▌ | 22980/26933 [24:04<03:38, 18.13it/s] 85%|████████▌ | 22984/26933 [24:04<03:37, 18.16it/s] 85%|████████▌ | 22988/26933 [24:04<03:37, 18.15it/s] 85%|████████▌ | 22992/26933 [24:04<03:37, 18.12it/s] 85%|████████▌ | 22996/26933 [24:05<03:37, 18.09it/s] 85%|████████▌ | 23000/26933 [24:05<03:36, 18.14it/s] 85%|████████▌ | 23004/26933 [24:05<03:39, 17.92it/s] 85%|████████▌ | 23008/26933 [24:05<03:38, 17.99it/s] 85%|████████▌ | 23012/26933 [24:06<03:37, 18.04it/s] 85%|████████▌ | 23016/26933 [24:06<03:37, 18.03it/s] 85%|████████▌ | 23020/26933 [24:06<03:36, 18.05it/s] 85%|████████▌ | 23024/26933 [24:06<03:35, 18.11it/s] 86%|████████▌ | 23028/26933 [24:06<03:35, 18.13it/s] 86%|████████▌ | 23032/26933 [24:07<03:35, 18.08it/s] 86%|████████▌ | 23036/26933 [24:07<03:35, 18.11it/s] 86%|████████▌ | 23040/26933 [24:07<03:34, 18.13it/s] 86%|████████▌ | 23044/26933 [24:07<03:34, 18.16it/s] 86%|████████▌ | 23048/26933 [24:08<03:33, 18.17it/s] 86%|████████▌ | 23052/26933 [24:08<03:34, 18.13it/s] 86%|████████▌ | 23056/26933 [24:08<03:33, 18.14it/s] 86%|████████▌ | 23060/26933 [24:08<03:33, 18.15it/s] 86%|████████▌ | 23064/26933 [24:08<03:32, 18.18it/s] 86%|████████▌ | 23068/26933 [24:09<03:32, 18.16it/s] 86%|████████▌ | 23072/26933 [24:09<03:32, 18.17it/s] 86%|████████▌ | 23076/26933 [24:09<03:32, 18.19it/s] 86%|████████▌ | 23080/26933 [24:09<03:31, 18.21it/s] 86%|████████▌ | 23084/26933 [24:09<03:31, 18.18it/s] 86%|████████▌ | 23088/26933 [24:10<03:31, 18.16it/s] 86%|████████▌ | 23092/26933 [24:10<03:31, 18.19it/s] 86%|████████▌ | 23096/26933 [24:10<03:30, 18.19it/s] 86%|████████▌ | 23100/26933 [24:10<03:30, 18.20it/s] 86%|████████▌ | 23104/26933 [24:11<03:30, 18.15it/s] 86%|████████▌ | 23108/26933 [24:11<03:30, 18.17it/s] 86%|████████▌ | 23112/26933 [24:11<03:30, 18.19it/s] 86%|████████▌ | 23116/26933 [24:11<03:30, 18.17it/s] 86%|████████▌ | 23120/26933 [24:11<03:29, 18.19it/s] 86%|████████▌ | 23124/26933 [24:12<03:30, 18.14it/s] 86%|████████▌ | 23128/26933 [24:12<03:29, 18.17it/s] 86%|████████▌ | 23132/26933 [24:12<03:29, 18.17it/s] 86%|████████▌ | 23136/26933 [24:12<03:30, 18.04it/s] 86%|████████▌ | 23140/26933 [24:13<03:31, 17.95it/s] 86%|████████▌ | 23144/26933 [24:13<03:31, 17.93it/s] 86%|████████▌ | 23148/26933 [24:13<03:30, 17.95it/s] 86%|████████▌ | 23152/26933 [24:13<03:30, 17.95it/s] 86%|████████▌ | 23156/26933 [24:13<03:30, 17.93it/s] 86%|████████▌ | 23160/26933 [24:14<03:32, 17.75it/s] 86%|████████▌ | 23164/26933 [24:14<03:31, 17.78it/s] 86%|████████▌ | 23168/26933 [24:14<03:31, 17.78it/s] 86%|████████▌ | 23172/26933 [24:14<03:31, 17.75it/s] 86%|████████▌ | 23176/26933 [24:15<03:32, 17.70it/s] 86%|████████▌ | 23180/26933 [24:15<03:31, 17.76it/s] 86%|████████▌ | 23184/26933 [24:15<03:30, 17.84it/s] 86%|████████▌ | 23188/26933 [24:15<03:29, 17.90it/s] 86%|████████▌ | 23192/26933 [24:16<03:28, 17.94it/s] 86%|████████▌ | 23196/26933 [24:16<03:28, 17.93it/s] 86%|████████▌ | 23200/26933 [24:16<03:27, 17.96it/s] 86%|████████▌ | 23204/26933 [24:16<03:27, 17.96it/s] 86%|████████▌ | 23208/26933 [24:16<03:27, 17.99it/s] 86%|████████▌ | 23212/26933 [24:17<03:27, 17.93it/s] 86%|████████▌ | 23216/26933 [24:17<03:27, 17.92it/s] 86%|████████▌ | 23220/26933 [24:17<03:26, 17.94it/s] 86%|████████▌ | 23224/26933 [24:17<03:26, 17.97it/s] 86%|████████▌ | 23228/26933 [24:18<03:26, 17.98it/s] 86%|████████▋ | 23232/26933 [24:18<03:26, 17.88it/s] 86%|████████▋ | 23236/26933 [24:18<03:26, 17.86it/s] 86%|████████▋ | 23240/26933 [24:18<03:26, 17.86it/s] 86%|████████▋ | 23244/26933 [24:18<03:26, 17.85it/s] 86%|████████▋ | 23248/26933 [24:19<03:26, 17.81it/s] 86%|████████▋ | 23252/26933 [24:19<03:26, 17.83it/s] 86%|████████▋ | 23256/26933 [24:19<03:25, 17.85it/s] 86%|████████▋ | 23260/26933 [24:19<03:25, 17.84it/s] 86%|████████▋ | 23264/26933 [24:20<03:25, 17.82it/s] 86%|████████▋ | 23268/26933 [24:20<03:26, 17.72it/s] 86%|████████▋ | 23272/26933 [24:20<03:27, 17.68it/s] 86%|████████▋ | 23276/26933 [24:20<03:26, 17.68it/s] 86%|████████▋ | 23280/26933 [24:20<03:26, 17.73it/s] 86%|████████▋ | 23284/26933 [24:21<03:25, 17.74it/s] 86%|████████▋ | 23288/26933 [24:21<03:25, 17.75it/s] 86%|████████▋ | 23292/26933 [24:21<03:25, 17.73it/s] 86%|████████▋ | 23296/26933 [24:21<03:25, 17.73it/s] 87%|████████▋ | 23300/26933 [24:22<03:24, 17.73it/s] 87%|████████▋ | 23304/26933 [24:22<03:24, 17.75it/s] 87%|████████▋ | 23308/26933 [24:22<03:23, 17.78it/s] 87%|████████▋ | 23312/26933 [24:22<03:23, 17.77it/s] 87%|████████▋ | 23316/26933 [24:22<03:23, 17.76it/s] 87%|████████▋ | 23320/26933 [24:23<03:23, 17.74it/s] 87%|████████▋ | 23324/26933 [24:23<03:22, 17.78it/s] 87%|████████▋ | 23328/26933 [24:23<03:22, 17.80it/s] 87%|████████▋ | 23332/26933 [24:23<03:22, 17.82it/s] 87%|████████▋ | 23336/26933 [24:24<03:21, 17.83it/s] 87%|████████▋ | 23340/26933 [24:24<03:21, 17.85it/s] 87%|████████▋ | 23344/26933 [24:24<03:20, 17.86it/s] 87%|████████▋ | 23348/26933 [24:24<03:21, 17.83it/s] 87%|████████▋ | 23352/26933 [24:24<03:21, 17.80it/s] 87%|████████▋ | 23356/26933 [24:25<03:21, 17.76it/s] 87%|████████▋ | 23360/26933 [24:25<03:21, 17.76it/s] 87%|████████▋ | 23364/26933 [24:25<03:21, 17.75it/s] 87%|████████▋ | 23368/26933 [24:25<03:20, 17.79it/s] 87%|████████▋ | 23372/26933 [24:26<03:20, 17.77it/s] 87%|████████▋ | 23376/26933 [24:26<03:20, 17.77it/s] 87%|████████▋ | 23380/26933 [24:26<03:19, 17.78it/s] 87%|████████▋ | 23384/26933 [24:26<03:19, 17.79it/s] 87%|████████▋ | 23388/26933 [24:27<03:19, 17.80it/s] 87%|████████▋ | 23392/26933 [24:27<03:19, 17.78it/s] 87%|████████▋ | 23396/26933 [24:27<03:18, 17.80it/s] 87%|████████▋ | 23400/26933 [24:27<03:18, 17.79it/s] 87%|████████▋ | 23404/26933 [24:27<03:18, 17.81it/s] 87%|████████▋ | 23408/26933 [24:28<03:18, 17.77it/s] 87%|████████▋ | 23412/26933 [24:28<03:17, 17.81it/s] 87%|████████▋ | 23416/26933 [24:28<03:17, 17.81it/s] 87%|████████▋ | 23420/26933 [24:28<03:17, 17.83it/s] 87%|████████▋ | 23424/26933 [24:29<03:16, 17.82it/s] 87%|████████▋ | 23428/26933 [24:29<03:17, 17.78it/s] 87%|████████▋ | 23432/26933 [24:29<03:17, 17.77it/s] 87%|████████▋ | 23436/26933 [24:29<03:16, 17.78it/s] 87%|████████▋ | 23440/26933 [24:29<03:16, 17.81it/s] 87%|████████▋ | 23444/26933 [24:30<03:16, 17.79it/s] 87%|████████▋ | 23448/26933 [24:30<03:15, 17.80it/s] 87%|████████▋ | 23452/26933 [24:30<03:16, 17.73it/s] 87%|████████▋ | 23456/26933 [24:30<03:16, 17.73it/s] 87%|████████▋ | 23460/26933 [24:31<03:15, 17.75it/s] 87%|████████▋ | 23464/26933 [24:31<03:15, 17.79it/s] 87%|████████▋ | 23468/26933 [24:31<03:14, 17.79it/s] 87%|████████▋ | 23472/26933 [24:31<03:14, 17.81it/s] 87%|████████▋ | 23476/26933 [24:31<03:13, 17.83it/s] 87%|████████▋ | 23480/26933 [24:32<03:13, 17.81it/s] 87%|████████▋ | 23484/26933 [24:32<03:13, 17.79it/s] 87%|████████▋ | 23488/26933 [24:32<03:13, 17.81it/s] 87%|████████▋ | 23492/26933 [24:32<03:13, 17.79it/s] 87%|████████▋ | 23496/26933 [24:33<03:13, 17.74it/s] 87%|████████▋ | 23500/26933 [24:33<03:13, 17.74it/s] 87%|████████▋ | 23504/26933 [24:33<03:12, 17.78it/s] 87%|████████▋ | 23508/26933 [24:33<03:12, 17.79it/s] 87%|████████▋ | 23512/26933 [24:33<03:12, 17.81it/s] 87%|████████▋ | 23516/26933 [24:34<03:12, 17.76it/s] 87%|████████▋ | 23520/26933 [24:34<03:12, 17.75it/s] 87%|████████▋ | 23524/26933 [24:34<03:11, 17.82it/s] 87%|████████▋ | 23528/26933 [24:34<03:10, 17.85it/s] 87%|████████▋ | 23532/26933 [24:35<03:10, 17.84it/s] 87%|████████▋ | 23536/26933 [24:35<03:10, 17.81it/s] 87%|████████▋ | 23540/26933 [24:35<03:10, 17.80it/s] 87%|████████▋ | 23544/26933 [24:35<03:10, 17.83it/s] 87%|████████▋ | 23548/26933 [24:36<03:10, 17.81it/s] 87%|████████▋ | 23552/26933 [24:36<03:10, 17.75it/s] 87%|████████▋ | 23556/26933 [24:36<03:09, 17.78it/s] 87%|████████▋ | 23560/26933 [24:36<03:09, 17.77it/s] 87%|████████▋ | 23564/26933 [24:36<03:09, 17.79it/s] 88%|████████▊ | 23568/26933 [24:37<03:09, 17.75it/s] 88%|████████▊ | 23572/26933 [24:37<03:09, 17.76it/s] 88%|████████▊ | 23576/26933 [24:37<03:09, 17.74it/s] 88%|████████▊ | 23580/26933 [24:37<03:08, 17.79it/s] 88%|████████▊ | 23584/26933 [24:38<03:08, 17.75it/s] 88%|████████▊ | 23588/26933 [24:38<03:09, 17.67it/s] 88%|████████▊ | 23592/26933 [24:38<03:08, 17.70it/s] 88%|████████▊ | 23596/26933 [24:38<03:08, 17.72it/s] 88%|████████▊ | 23600/26933 [24:38<03:07, 17.75it/s] 88%|████████▊ | 23604/26933 [24:39<03:07, 17.73it/s] 88%|████████▊ | 23608/26933 [24:39<03:06, 17.78it/s] 88%|████████▊ | 23612/26933 [24:39<03:06, 17.84it/s] 88%|████████▊ | 23616/26933 [24:39<03:05, 17.85it/s] 88%|████████▊ | 23620/26933 [24:40<03:05, 17.86it/s] 88%|████████▊ | 23624/26933 [24:40<03:05, 17.83it/s] 88%|████████▊ | 23628/26933 [24:40<03:05, 17.85it/s] 88%|████████▊ | 23632/26933 [24:40<03:04, 17.86it/s] 88%|████████▊ | 23636/26933 [24:40<03:04, 17.85it/s] 88%|████████▊ | 23640/26933 [24:41<03:04, 17.83it/s] 88%|████████▊ | 23644/26933 [24:41<03:04, 17.86it/s] 88%|████████▊ | 23648/26933 [24:41<03:03, 17.89it/s] 88%|████████▊ | 23652/26933 [24:41<03:02, 17.94it/s] 88%|████████▊ | 23656/26933 [24:42<03:02, 17.92it/s] 88%|████████▊ | 23660/26933 [24:42<03:03, 17.87it/s] 88%|████████▊ | 23664/26933 [24:42<03:02, 17.89it/s] 88%|████████▊ | 23668/26933 [24:42<03:02, 17.87it/s] 88%|████████▊ | 23672/26933 [24:42<03:02, 17.88it/s] 88%|████████▊ | 23676/26933 [24:43<03:02, 17.83it/s] 88%|████████▊ | 23680/26933 [24:43<03:02, 17.87it/s] 88%|████████▊ | 23684/26933 [24:43<03:01, 17.89it/s] 88%|████████▊ | 23688/26933 [24:43<03:01, 17.88it/s] 88%|████████▊ | 23692/26933 [24:44<03:02, 17.79it/s] 88%|████████▊ | 23696/26933 [24:44<03:01, 17.80it/s] 88%|████████▊ | 23700/26933 [24:44<03:01, 17.79it/s] 88%|████████▊ | 23704/26933 [24:44<03:01, 17.81it/s] 88%|████████▊ | 23708/26933 [24:44<03:01, 17.78it/s] 88%|████████▊ | 23712/26933 [24:45<03:01, 17.70it/s] 88%|████████▊ | 23716/26933 [24:45<03:01, 17.73it/s] 88%|████████▊ | 23720/26933 [24:45<03:00, 17.77it/s] 88%|████████▊ | 23724/26933 [24:45<03:00, 17.76it/s] 88%|████████▊ | 23728/26933 [24:46<03:00, 17.71it/s] 88%|████████▊ | 23732/26933 [24:46<03:00, 17.73it/s] 88%|████████▊ | 23736/26933 [24:46<03:00, 17.76it/s] 88%|████████▊ | 23740/26933 [24:46<02:59, 17.79it/s] 88%|████████▊ | 23744/26933 [24:47<02:59, 17.76it/s] 88%|████████▊ | 23748/26933 [24:47<02:59, 17.76it/s] 88%|████████▊ | 23752/26933 [24:47<02:58, 17.79it/s] 88%|████████▊ | 23756/26933 [24:47<02:58, 17.79it/s] 88%|████████▊ | 23760/26933 [24:47<02:58, 17.82it/s] 88%|████████▊ | 23764/26933 [24:48<02:58, 17.75it/s] 88%|████████▊ | 23768/26933 [24:48<02:58, 17.77it/s] 88%|████████▊ | 23772/26933 [24:48<02:57, 17.81it/s] 88%|████████▊ | 23776/26933 [24:48<02:56, 17.85it/s] 88%|████████▊ | 23780/26933 [24:49<02:56, 17.87it/s] 88%|████████▊ | 23784/26933 [24:49<02:56, 17.83it/s] 88%|████████▊ | 23788/26933 [24:49<02:55, 17.88it/s] 88%|████████▊ | 23792/26933 [24:49<02:55, 17.90it/s] 88%|████████▊ | 23796/26933 [24:49<02:54, 17.94it/s] 88%|████████▊ | 23800/26933 [24:50<02:54, 17.91it/s] 88%|████████▊ | 23804/26933 [24:50<02:54, 17.92it/s] 88%|████████▊ | 23808/26933 [24:50<02:54, 17.89it/s] 88%|████████▊ | 23812/26933 [24:50<02:54, 17.93it/s] 88%|████████▊ | 23816/26933 [24:51<02:53, 17.92it/s] 88%|████████▊ | 23820/26933 [24:51<02:54, 17.87it/s] 88%|████████▊ | 23824/26933 [24:51<02:53, 17.92it/s] 88%|████████▊ | 23828/26933 [24:51<02:53, 17.91it/s] 88%|████████▊ | 23832/26933 [24:51<02:52, 17.93it/s] 89%|████████▊ | 23836/26933 [24:52<02:53, 17.88it/s] 89%|████████▊ | 23840/26933 [24:52<02:52, 17.93it/s] 89%|████████▊ | 23844/26933 [24:52<02:52, 17.95it/s] 89%|████████▊ | 23848/26933 [24:52<02:51, 17.99it/s] 89%|████████▊ | 23852/26933 [24:53<02:51, 18.01it/s] 89%|████████▊ | 23856/26933 [24:53<02:51, 17.97it/s] 89%|████████▊ | 23860/26933 [24:53<02:50, 18.00it/s] 89%|████████▊ | 23864/26933 [24:53<02:50, 18.02it/s] 89%|████████▊ | 23868/26933 [24:53<02:50, 18.01it/s] 89%|████████▊ | 23872/26933 [24:54<02:50, 17.95it/s] 89%|████████▊ | 23876/26933 [24:54<02:50, 17.97it/s] 89%|████████▊ | 23880/26933 [24:54<02:49, 17.97it/s] 89%|████████▊ | 23884/26933 [24:54<02:49, 18.00it/s] 89%|████████▊ | 23888/26933 [24:55<02:49, 18.00it/s] 89%|████████▊ | 23892/26933 [24:55<02:49, 17.93it/s] 89%|████████▊ | 23896/26933 [24:55<02:49, 17.93it/s] 89%|████████▊ | 23900/26933 [24:55<02:48, 17.96it/s] 89%|████████▉ | 23904/26933 [24:55<02:48, 17.99it/s] 89%|████████▉ | 23908/26933 [24:56<02:48, 17.92it/s] 89%|████████▉ | 23912/26933 [24:56<02:48, 17.95it/s] 89%|████████▉ | 23916/26933 [24:56<02:47, 17.97it/s] 89%|████████▉ | 23920/26933 [24:56<02:47, 17.96it/s] 89%|████████▉ | 23924/26933 [24:57<02:47, 17.94it/s] 89%|████████▉ | 23928/26933 [24:57<02:48, 17.88it/s] 89%|████████▉ | 23932/26933 [24:57<02:47, 17.91it/s] 89%|████████▉ | 23936/26933 [24:57<02:47, 17.93it/s] 89%|████████▉ | 23940/26933 [24:57<02:46, 17.93it/s] 89%|████████▉ | 23944/26933 [24:58<02:47, 17.90it/s] 89%|████████▉ | 23948/26933 [24:58<02:46, 17.95it/s] 89%|████████▉ | 23952/26933 [24:58<02:46, 17.93it/s] 89%|████████▉ | 23956/26933 [24:58<02:46, 17.86it/s] 89%|████████▉ | 23960/26933 [24:59<02:46, 17.83it/s] 89%|████████▉ | 23964/26933 [24:59<02:47, 17.75it/s] 89%|████████▉ | 23968/26933 [24:59<02:46, 17.78it/s] 89%|████████▉ | 23972/26933 [24:59<02:46, 17.76it/s] 89%|████████▉ | 23976/26933 [24:59<02:46, 17.78it/s] 89%|████████▉ | 23980/26933 [25:00<02:46, 17.69it/s] 89%|████████▉ | 23984/26933 [25:00<02:46, 17.70it/s] 89%|████████▉ | 23988/26933 [25:00<02:46, 17.70it/s] 89%|████████▉ | 23992/26933 [25:00<02:46, 17.70it/s] 89%|████████▉ | 23996/26933 [25:01<02:46, 17.66it/s] 89%|████████▉ | 24000/26933 [25:01<02:46, 17.65it/s] 89%|████████▉ | 24004/26933 [25:01<02:45, 17.65it/s] 89%|████████▉ | 24008/26933 [25:01<02:45, 17.70it/s] 89%|████████▉ | 24012/26933 [25:02<02:44, 17.70it/s] 89%|████████▉ | 24016/26933 [25:02<02:44, 17.69it/s] 89%|████████▉ | 24020/26933 [25:02<02:44, 17.71it/s] 89%|████████▉ | 24024/26933 [25:02<02:44, 17.69it/s] 89%|████████▉ | 24028/26933 [25:02<02:43, 17.72it/s] 89%|████████▉ | 24032/26933 [25:03<02:44, 17.67it/s] 89%|████████▉ | 24036/26933 [25:03<02:43, 17.70it/s] 89%|████████▉ | 24040/26933 [25:03<02:43, 17.74it/s] 89%|████████▉ | 24044/26933 [25:03<02:42, 17.75it/s] 89%|████████▉ | 24048/26933 [25:04<02:42, 17.76it/s] 89%|████████▉ | 24052/26933 [25:04<02:42, 17.71it/s] 89%|████████▉ | 24056/26933 [25:04<02:42, 17.75it/s] 89%|████████▉ | 24060/26933 [25:04<02:41, 17.77it/s] 89%|████████▉ | 24064/26933 [25:04<02:41, 17.78it/s] 89%|████████▉ | 24068/26933 [25:05<02:41, 17.76it/s] 89%|████████▉ | 24072/26933 [25:05<02:40, 17.79it/s] 89%|████████▉ | 24076/26933 [25:05<02:40, 17.80it/s] 89%|████████▉ | 24080/26933 [25:05<02:40, 17.72it/s] 89%|████████▉ | 24084/26933 [25:06<02:40, 17.73it/s] 89%|████████▉ | 24088/26933 [25:06<02:40, 17.71it/s] 89%|████████▉ | 24092/26933 [25:06<02:40, 17.74it/s] 89%|████████▉ | 24096/26933 [25:06<02:40, 17.72it/s] 89%|████████▉ | 24100/26933 [25:06<02:39, 17.72it/s] 89%|████████▉ | 24104/26933 [25:07<02:40, 17.66it/s] 90%|████████▉ | 24108/26933 [25:07<02:39, 17.70it/s] 90%|████████▉ | 24112/26933 [25:07<02:39, 17.72it/s] 90%|████████▉ | 24116/26933 [25:07<02:38, 17.73it/s] 90%|████████▉ | 24120/26933 [25:08<02:38, 17.73it/s] 90%|████████▉ | 24124/26933 [25:08<02:38, 17.74it/s] 90%|████████▉ | 24128/26933 [25:08<02:38, 17.73it/s] 90%|████████▉ | 24132/26933 [25:08<02:38, 17.72it/s] 90%|████████▉ | 24136/26933 [25:09<02:41, 17.37it/s] 90%|████████▉ | 24140/26933 [25:09<02:40, 17.43it/s] 90%|████████▉ | 24144/26933 [25:09<02:38, 17.55it/s] 90%|████████▉ | 24148/26933 [25:09<02:37, 17.65it/s] 90%|████████▉ | 24152/26933 [25:09<02:37, 17.69it/s] 90%|████████▉ | 24156/26933 [25:10<02:37, 17.68it/s] 90%|████████▉ | 24160/26933 [25:10<02:36, 17.71it/s] 90%|████████▉ | 24164/26933 [25:10<02:36, 17.73it/s] 90%|████████▉ | 24168/26933 [25:10<02:35, 17.73it/s] 90%|████████▉ | 24172/26933 [25:11<02:35, 17.73it/s] 90%|████████▉ | 24176/26933 [25:11<02:35, 17.76it/s] 90%|████████▉ | 24180/26933 [25:11<02:34, 17.82it/s] 90%|████████▉ | 24184/26933 [25:11<02:34, 17.81it/s] 90%|████████▉ | 24188/26933 [25:11<02:34, 17.82it/s] 90%|████████▉ | 24192/26933 [25:12<02:34, 17.77it/s] 90%|████████▉ | 24196/26933 [25:12<02:33, 17.80it/s] 90%|████████▉ | 24200/26933 [25:12<02:33, 17.78it/s] 90%|████████▉ | 24204/26933 [25:12<02:33, 17.81it/s] 90%|████████▉ | 24208/26933 [25:13<02:33, 17.78it/s] 90%|████████▉ | 24212/26933 [25:13<02:33, 17.74it/s] 90%|████████▉ | 24216/26933 [25:13<02:33, 17.71it/s] 90%|████████▉ | 24220/26933 [25:13<02:33, 17.73it/s] 90%|████████▉ | 24224/26933 [25:13<02:32, 17.74it/s] 90%|████████▉ | 24228/26933 [25:14<02:32, 17.73it/s] 90%|████████▉ | 24232/26933 [25:14<02:31, 17.79it/s] 90%|████████▉ | 24236/26933 [25:14<02:31, 17.79it/s] 90%|█████████ | 24240/26933 [25:14<02:31, 17.83it/s] 90%|█████████ | 24244/26933 [25:15<02:30, 17.84it/s] 90%|█████████ | 24248/26933 [25:15<02:30, 17.84it/s] 90%|█████████ | 24252/26933 [25:15<02:30, 17.86it/s] 90%|█████████ | 24256/26933 [25:15<02:29, 17.87it/s] 90%|█████████ | 24260/26933 [25:15<02:29, 17.88it/s] 90%|█████████ | 24264/26933 [25:16<02:30, 17.78it/s] 90%|█████████ | 24268/26933 [25:16<02:29, 17.78it/s] 90%|█████████ | 24272/26933 [25:16<02:29, 17.77it/s] 90%|█████████ | 24276/26933 [25:16<02:29, 17.77it/s] 90%|█████████ | 24280/26933 [25:17<02:29, 17.70it/s] 90%|█████████ | 24284/26933 [25:17<02:29, 17.73it/s] 90%|█████████ | 24288/26933 [25:17<02:29, 17.73it/s] 90%|█████████ | 24292/26933 [25:17<02:28, 17.75it/s] 90%|█████████ | 24296/26933 [25:18<02:28, 17.79it/s] 90%|█████████ | 24300/26933 [25:18<02:28, 17.75it/s] 90%|█████████ | 24304/26933 [25:18<02:27, 17.78it/s] 90%|█████████ | 24308/26933 [25:18<02:27, 17.78it/s] 90%|█████████ | 24312/26933 [25:18<02:27, 17.76it/s] 90%|█████████ | 24316/26933 [25:19<02:28, 17.68it/s] 90%|█████████ | 24320/26933 [25:19<02:27, 17.68it/s] 90%|█████████ | 24324/26933 [25:19<02:27, 17.71it/s] 90%|█████████ | 24328/26933 [25:19<02:27, 17.72it/s] 90%|█████████ | 24332/26933 [25:20<02:27, 17.68it/s] 90%|█████████ | 24336/26933 [25:20<02:27, 17.65it/s] 90%|█████████ | 24340/26933 [25:20<02:26, 17.68it/s] 90%|█████████ | 24344/26933 [25:20<02:26, 17.68it/s] 90%|█████████ | 24348/26933 [25:20<02:26, 17.69it/s] 90%|█████████ | 24352/26933 [25:21<02:25, 17.69it/s] 90%|█████████ | 24356/26933 [25:21<02:25, 17.68it/s] 90%|█████████ | 24360/26933 [25:21<02:25, 17.74it/s] 90%|█████████ | 24364/26933 [25:21<02:24, 17.79it/s] 90%|█████████ | 24368/26933 [25:22<02:24, 17.76it/s] 90%|█████████ | 24372/26933 [25:22<02:23, 17.79it/s] 91%|█████████ | 24376/26933 [25:22<02:23, 17.78it/s] 91%|█████████ | 24380/26933 [25:22<02:23, 17.80it/s] 91%|█████████ | 24384/26933 [25:22<02:23, 17.80it/s] 91%|█████████ | 24388/26933 [25:23<02:22, 17.80it/s] 91%|█████████ | 24392/26933 [25:23<02:22, 17.85it/s] 91%|█████████ | 24396/26933 [25:23<02:21, 17.88it/s] 91%|█████████ | 24400/26933 [25:23<02:22, 17.83it/s] 91%|█████████ | 24404/26933 [25:24<02:22, 17.76it/s] 91%|█████████ | 24408/26933 [25:24<02:22, 17.75it/s] 91%|█████████ | 24412/26933 [25:24<02:22, 17.73it/s] 91%|█████████ | 24416/26933 [25:24<02:22, 17.72it/s] 91%|█████████ | 24420/26933 [25:25<02:21, 17.72it/s] 91%|█████████ | 24424/26933 [25:25<02:21, 17.68it/s] 91%|█████████ | 24428/26933 [25:25<02:21, 17.71it/s] 91%|█████████ | 24432/26933 [25:25<02:21, 17.66it/s] 91%|█████████ | 24436/26933 [25:25<02:21, 17.64it/s] 91%|█████████ | 24440/26933 [25:26<02:21, 17.62it/s] 91%|█████████ | 24444/26933 [25:26<02:20, 17.71it/s] 91%|█████████ | 24448/26933 [25:26<02:19, 17.78it/s] 91%|█████████ | 24452/26933 [25:26<02:19, 17.84it/s] 91%|█████████ | 24456/26933 [25:27<02:18, 17.87it/s] 91%|█████████ | 24460/26933 [25:27<02:19, 17.78it/s] 91%|█████████ | 24464/26933 [25:27<02:18, 17.78it/s] 91%|█████████ | 24468/26933 [25:27<02:19, 17.73it/s] 91%|█████████ | 24472/26933 [25:27<02:18, 17.72it/s] 91%|█████████ | 24476/26933 [25:28<02:18, 17.68it/s] 91%|█████████ | 24480/26933 [25:28<02:18, 17.72it/s] 91%|█████████ | 24484/26933 [25:28<02:17, 17.84it/s] 91%|█████████ | 24488/26933 [25:28<02:16, 17.91it/s] 91%|█████████ | 24492/26933 [25:29<02:15, 18.01it/s] 91%|█████████ | 24496/26933 [25:29<02:15, 18.04it/s] 91%|█████████ | 24500/26933 [25:29<02:14, 18.08it/s] 91%|█████████ | 24504/26933 [25:29<02:14, 18.08it/s] 91%|█████████ | 24508/26933 [25:29<02:13, 18.10it/s] 91%|█████████ | 24512/26933 [25:30<02:14, 18.06it/s] 91%|█████████ | 24516/26933 [25:30<02:13, 18.09it/s] 91%|█████████ | 24520/26933 [25:30<02:13, 18.11it/s] 91%|█████████ | 24524/26933 [25:30<02:13, 18.10it/s] 91%|█████████ | 24528/26933 [25:31<02:12, 18.11it/s] 91%|█████████ | 24532/26933 [25:31<02:12, 18.06it/s] 91%|█████████ | 24536/26933 [25:31<02:12, 18.10it/s] 91%|█████████ | 24540/26933 [25:31<02:12, 18.10it/s] 91%|█████████ | 24544/26933 [25:31<02:13, 17.95it/s] 91%|█████████ | 24548/26933 [25:32<02:13, 17.81it/s] 91%|█████████ | 24552/26933 [25:32<02:13, 17.77it/s] 91%|█████████ | 24556/26933 [25:32<02:14, 17.71it/s] 91%|█████████ | 24560/26933 [25:32<02:14, 17.71it/s] 91%|█████████ | 24564/26933 [25:33<02:13, 17.76it/s] 91%|█████████ | 24568/26933 [25:33<02:13, 17.77it/s] 91%|█████████ | 24572/26933 [25:33<02:12, 17.76it/s] 91%|█████████ | 24576/26933 [25:33<02:13, 17.70it/s] 91%|█████████▏| 24580/26933 [25:33<02:12, 17.71it/s] 91%|█████████▏| 24584/26933 [25:34<02:12, 17.75it/s] 91%|█████████▏| 24588/26933 [25:34<02:11, 17.83it/s] 91%|█████████▏| 24592/26933 [25:34<02:10, 17.88it/s] 91%|█████████▏| 24596/26933 [25:34<02:10, 17.88it/s] 91%|█████████▏| 24600/26933 [25:35<02:11, 17.76it/s] 91%|█████████▏| 24604/26933 [25:35<02:11, 17.71it/s] 91%|█████████▏| 24608/26933 [25:35<02:11, 17.72it/s] 91%|█████████▏| 24612/26933 [25:35<02:11, 17.69it/s] 91%|█████████▏| 24616/26933 [25:35<02:11, 17.67it/s] 91%|█████████▏| 24620/26933 [25:36<02:11, 17.63it/s] 91%|█████████▏| 24624/26933 [25:36<02:10, 17.68it/s] 91%|█████████▏| 24628/26933 [25:36<02:10, 17.67it/s] 91%|█████████▏| 24632/26933 [25:36<02:09, 17.73it/s] 91%|█████████▏| 24636/26933 [25:37<02:09, 17.69it/s] 91%|█████████▏| 24640/26933 [25:37<02:09, 17.71it/s] 92%|█████████▏| 24644/26933 [25:37<02:09, 17.74it/s] 92%|█████████▏| 24648/26933 [25:37<02:08, 17.76it/s] 92%|█████████▏| 24652/26933 [25:38<02:08, 17.72it/s] 92%|█████████▏| 24656/26933 [25:38<02:08, 17.69it/s] 92%|█████████▏| 24660/26933 [25:38<02:08, 17.68it/s] 92%|█████████▏| 24664/26933 [25:38<02:08, 17.72it/s] 92%|█████████▏| 24668/26933 [25:38<02:06, 17.86it/s] 92%|█████████▏| 24672/26933 [25:39<02:06, 17.92it/s] 92%|█████████▏| 24676/26933 [25:39<02:05, 18.00it/s] 92%|█████████▏| 24680/26933 [25:39<02:04, 18.06it/s] 92%|█████████▏| 24684/26933 [25:39<02:04, 18.10it/s] 92%|█████████▏| 24688/26933 [25:40<02:03, 18.13it/s] 92%|█████████▏| 24692/26933 [25:40<02:03, 18.09it/s] 92%|█████████▏| 24696/26933 [25:40<02:03, 18.14it/s] 92%|█████████▏| 24700/26933 [25:40<02:02, 18.17it/s] 92%|█████████▏| 24704/26933 [25:40<02:02, 18.19it/s] 92%|█████████▏| 24708/26933 [25:41<02:02, 18.15it/s] 92%|█████████▏| 24712/26933 [25:41<02:02, 18.20it/s] 92%|█████████▏| 24716/26933 [25:41<02:01, 18.19it/s] 92%|█████████▏| 24720/26933 [25:41<02:01, 18.23it/s] 92%|█████████▏| 24724/26933 [25:42<02:01, 18.24it/s] 92%|█████████▏| 24728/26933 [25:42<02:01, 18.21it/s] 92%|█████████▏| 24732/26933 [25:42<02:00, 18.21it/s] 92%|█████████▏| 24736/26933 [25:42<02:00, 18.20it/s] 92%|█████████▏| 24740/26933 [25:42<02:00, 18.19it/s] 92%|█████████▏| 24744/26933 [25:43<02:00, 18.12it/s] 92%|█████████▏| 24748/26933 [25:43<02:00, 18.16it/s] 92%|█████████▏| 24752/26933 [25:43<02:00, 18.16it/s] 92%|█████████▏| 24756/26933 [25:43<01:59, 18.15it/s] 92%|█████████▏| 24760/26933 [25:43<01:59, 18.15it/s] 92%|█████████▏| 24764/26933 [25:44<01:59, 18.14it/s] 92%|█████████▏| 24768/26933 [25:44<01:59, 18.17it/s] 92%|█████████▏| 24772/26933 [25:44<01:58, 18.17it/s] 92%|█████████▏| 24776/26933 [25:44<01:58, 18.19it/s] 92%|█████████▏| 24780/26933 [25:45<01:58, 18.16it/s] 92%|█████████▏| 24784/26933 [25:45<01:58, 18.17it/s] 92%|█████████▏| 24788/26933 [25:45<01:57, 18.19it/s] 92%|█████████▏| 24792/26933 [25:45<01:57, 18.22it/s] 92%|█████████▏| 24796/26933 [25:45<01:57, 18.24it/s] 92%|█████████▏| 24800/26933 [25:46<01:57, 18.19it/s] 92%|█████████▏| 24804/26933 [25:46<01:56, 18.22it/s] 92%|█████████▏| 24808/26933 [25:46<01:56, 18.21it/s] 92%|█████████▏| 24812/26933 [25:46<01:56, 18.21it/s] 92%|█████████▏| 24816/26933 [25:47<01:56, 18.22it/s] 92%|█████████▏| 24820/26933 [25:47<01:56, 18.16it/s] 92%|█████████▏| 24824/26933 [25:47<01:56, 18.04it/s] 92%|█████████▏| 24828/26933 [25:47<01:57, 17.95it/s] 92%|█████████▏| 24832/26933 [25:47<01:57, 17.91it/s] 92%|█████████▏| 24836/26933 [25:48<01:57, 17.83it/s] 92%|█████████▏| 24840/26933 [25:48<01:57, 17.85it/s] 92%|█████████▏| 24844/26933 [25:48<01:56, 17.87it/s] 92%|█████████▏| 24848/26933 [25:48<01:56, 17.89it/s] 92%|█████████▏| 24852/26933 [25:49<01:56, 17.85it/s] 92%|█████████▏| 24856/26933 [25:49<01:56, 17.85it/s] 92%|█████████▏| 24860/26933 [25:49<01:56, 17.79it/s] 92%|█████████▏| 24864/26933 [25:49<01:55, 17.88it/s] 92%|█████████▏| 24868/26933 [25:49<01:55, 17.95it/s] 92%|█████████▏| 24872/26933 [25:50<01:54, 17.93it/s] 92%|█████████▏| 24876/26933 [25:50<01:54, 17.96it/s] 92%|█████████▏| 24880/26933 [25:50<01:54, 17.98it/s] 92%|█████████▏| 24884/26933 [25:50<01:54, 17.96it/s] 92%|█████████▏| 24888/26933 [25:51<01:54, 17.87it/s] 92%|█████████▏| 24892/26933 [25:51<01:54, 17.87it/s] 92%|█████████▏| 24896/26933 [25:51<01:53, 17.93it/s] 92%|█████████▏| 24900/26933 [25:51<01:53, 17.98it/s] 92%|█████████▏| 24904/26933 [25:51<01:52, 17.98it/s] 92%|█████████▏| 24908/26933 [25:52<01:53, 17.91it/s] 92%|█████████▏| 24912/26933 [25:52<01:52, 17.91it/s] 93%|█████████▎| 24916/26933 [25:52<01:52, 17.90it/s] 93%|█████████▎| 24920/26933 [25:52<01:52, 17.86it/s] 93%|█████████▎| 24924/26933 [25:53<01:53, 17.76it/s] 93%|█████████▎| 24928/26933 [25:53<01:53, 17.73it/s] 93%|█████████▎| 24932/26933 [25:53<01:52, 17.75it/s] 93%|█████████▎| 24936/26933 [25:53<01:52, 17.78it/s] 93%|█████████▎| 24940/26933 [25:54<01:51, 17.83it/s] 93%|█████████▎| 24944/26933 [25:54<01:51, 17.84it/s] 93%|█████████▎| 24948/26933 [25:54<01:50, 17.94it/s] 93%|█████████▎| 24952/26933 [25:54<01:49, 18.01it/s] 93%|█████████▎| 24956/26933 [25:54<01:49, 18.09it/s] 93%|█████████▎| 24960/26933 [25:55<01:49, 18.02it/s] 93%|█████████▎| 24964/26933 [25:55<01:48, 18.07it/s] 93%|█████████▎| 24968/26933 [25:55<01:48, 18.10it/s] 93%|█████████▎| 24972/26933 [25:55<01:48, 18.14it/s] 93%|█████████▎| 24976/26933 [25:55<01:47, 18.15it/s] 93%|█████████▎| 24980/26933 [25:56<01:47, 18.08it/s] 93%|█████████▎| 24984/26933 [25:56<01:47, 18.12it/s] 93%|█████████▎| 24988/26933 [25:56<01:47, 18.17it/s] 93%|█████████▎| 24992/26933 [25:56<01:46, 18.19it/s] 93%|█████████▎| 24996/26933 [25:57<01:46, 18.12it/s] 93%|█████████▎| 25000/26933 [25:57<01:46, 18.18it/s] 93%|█████████▎| 25004/26933 [25:57<01:45, 18.20it/s] 93%|█████████▎| 25008/26933 [25:57<01:45, 18.23it/s] 93%|█████████▎| 25012/26933 [25:57<01:45, 18.25it/s] 93%|█████████▎| 25016/26933 [25:58<01:45, 18.16it/s] 93%|█████████▎| 25020/26933 [25:58<01:45, 18.15it/s] 93%|█████████▎| 25024/26933 [25:58<01:45, 18.17it/s] 93%|█████████▎| 25028/26933 [25:58<01:44, 18.18it/s] 93%|█████████▎| 25032/26933 [25:59<01:44, 18.19it/s] 93%|█████████▎| 25036/26933 [25:59<01:44, 18.13it/s] 93%|█████████▎| 25040/26933 [25:59<01:44, 18.13it/s] 93%|█████████▎| 25044/26933 [25:59<01:44, 18.15it/s] 93%|█████████▎| 25048/26933 [25:59<01:43, 18.16it/s] 93%|█████████▎| 25052/26933 [26:00<01:43, 18.10it/s] 93%|█████████▎| 25056/26933 [26:00<01:43, 18.15it/s] 93%|█████████▎| 25060/26933 [26:00<01:42, 18.19it/s] 93%|█████████▎| 25064/26933 [26:00<01:42, 18.17it/s] 93%|█████████▎| 25068/26933 [26:01<01:42, 18.20it/s] 93%|█████████▎| 25072/26933 [26:01<01:42, 18.15it/s] 93%|█████████▎| 25076/26933 [26:01<01:42, 18.16it/s] 93%|█████████▎| 25080/26933 [26:01<01:41, 18.18it/s] 93%|█████████▎| 25084/26933 [26:01<01:41, 18.20it/s] 93%|█████████▎| 25088/26933 [26:02<01:41, 18.14it/s] 93%|█████████▎| 25092/26933 [26:02<01:41, 18.15it/s] 93%|█████████▎| 25096/26933 [26:02<01:40, 18.19it/s] 93%|█████████▎| 25100/26933 [26:02<01:40, 18.23it/s] 93%|█████████▎| 25104/26933 [26:03<01:40, 18.22it/s] 93%|█████████▎| 25108/26933 [26:03<01:40, 18.18it/s] 93%|█████████▎| 25112/26933 [26:03<01:39, 18.21it/s] 93%|█████████▎| 25116/26933 [26:03<01:39, 18.23it/s] 93%|█████████▎| 25120/26933 [26:03<01:39, 18.23it/s] 93%|█████████▎| 25124/26933 [26:04<01:39, 18.20it/s] 93%|█████████▎| 25128/26933 [26:04<01:39, 18.18it/s] 93%|█████████▎| 25132/26933 [26:04<01:38, 18.21it/s] 93%|█████████▎| 25136/26933 [26:04<01:38, 18.26it/s] 93%|█████████▎| 25140/26933 [26:05<01:38, 18.28it/s] 93%|█████████▎| 25144/26933 [26:05<01:38, 18.22it/s] 93%|█████████▎| 25148/26933 [26:05<01:37, 18.24it/s] 93%|█████████▎| 25152/26933 [26:05<01:37, 18.22it/s] 93%|█████████▎| 25156/26933 [26:05<01:37, 18.19it/s] 93%|█████████▎| 25160/26933 [26:06<01:37, 18.11it/s] 93%|█████████▎| 25164/26933 [26:06<01:37, 18.11it/s] 93%|█████████▎| 25168/26933 [26:06<01:37, 18.11it/s] 93%|█████████▎| 25172/26933 [26:06<01:37, 18.11it/s] 93%|█████████▎| 25176/26933 [26:06<01:36, 18.12it/s] 93%|█████████▎| 25180/26933 [26:07<01:36, 18.12it/s] 94%|█████████▎| 25184/26933 [26:07<01:36, 18.15it/s] 94%|█████████▎| 25188/26933 [26:07<01:35, 18.20it/s] 94%|█████████▎| 25192/26933 [26:07<01:35, 18.20it/s] 94%|█████████▎| 25196/26933 [26:08<01:35, 18.16it/s] 94%|█████████▎| 25200/26933 [26:08<01:35, 18.20it/s] 94%|█████████▎| 25204/26933 [26:08<01:35, 18.19it/s] 94%|█████████▎| 25208/26933 [26:08<01:35, 18.10it/s] 94%|█████████▎| 25212/26933 [26:08<01:36, 17.93it/s] 94%|█████████▎| 25216/26933 [26:09<01:36, 17.74it/s] 94%|█████████▎| 25220/26933 [26:09<01:37, 17.60it/s] 94%|█████████▎| 25224/26933 [26:09<01:37, 17.58it/s] 94%|█████████▎| 25228/26933 [26:09<01:36, 17.67it/s] 94%|█████████▎| 25232/26933 [26:10<01:36, 17.67it/s] 94%|█████████▎| 25236/26933 [26:10<01:35, 17.74it/s] 94%|█████████▎| 25240/26933 [26:10<01:35, 17.80it/s] 94%|█████████▎| 25244/26933 [26:10<01:34, 17.84it/s] 94%|█████████▎| 25248/26933 [26:11<01:34, 17.88it/s] 94%|█████████▍| 25252/26933 [26:11<01:34, 17.87it/s] 94%|█████████▍| 25256/26933 [26:11<01:33, 17.92it/s] 94%|█████████▍| 25260/26933 [26:11<01:33, 17.89it/s] 94%|█████████▍| 25264/26933 [26:11<01:33, 17.90it/s] 94%|█████████▍| 25268/26933 [26:12<01:33, 17.83it/s] 94%|█████████▍| 25272/26933 [26:12<01:32, 17.88it/s] 94%|█████████▍| 25276/26933 [26:12<01:32, 17.89it/s] 94%|█████████▍| 25280/26933 [26:12<01:32, 17.92it/s] 94%|█████████▍| 25284/26933 [26:13<01:32, 17.90it/s] 94%|█████████▍| 25288/26933 [26:13<01:32, 17.87it/s] 94%|█████████▍| 25292/26933 [26:13<01:31, 17.91it/s] 94%|█████████▍| 25296/26933 [26:13<01:31, 17.96it/s] 94%|█████████▍| 25300/26933 [26:13<01:30, 17.97it/s] 94%|█████████▍| 25304/26933 [26:14<01:30, 17.91it/s] 94%|█████████▍| 25308/26933 [26:14<01:30, 17.92it/s] 94%|█████████▍| 25312/26933 [26:14<01:30, 17.94it/s] 94%|█████████▍| 25316/26933 [26:14<01:30, 17.95it/s] 94%|█████████▍| 25320/26933 [26:15<01:29, 17.95it/s] 94%|█████████▍| 25324/26933 [26:15<01:29, 17.93it/s] 94%|█████████▍| 25328/26933 [26:15<01:29, 17.97it/s] 94%|█████████▍| 25332/26933 [26:15<01:29, 17.96it/s] 94%|█████████▍| 25336/26933 [26:15<01:29, 17.94it/s] 94%|█████████▍| 25340/26933 [26:16<01:29, 17.89it/s] 94%|█████████▍| 25344/26933 [26:16<01:28, 17.92it/s] 94%|█████████▍| 25348/26933 [26:16<01:28, 17.93it/s] 94%|█████████▍| 25352/26933 [26:16<01:28, 17.95it/s] 94%|█████████▍| 25356/26933 [26:17<01:27, 17.96it/s] 94%|█████████▍| 25360/26933 [26:17<01:29, 17.50it/s] 94%|█████████▍| 25364/26933 [26:17<01:28, 17.68it/s] 94%|█████████▍| 25368/26933 [26:17<01:27, 17.86it/s] 94%|█████████▍| 25372/26933 [26:17<01:26, 18.00it/s] 94%|█████████▍| 25376/26933 [26:18<01:26, 18.03it/s] 94%|█████████▍| 25380/26933 [26:18<01:25, 18.10it/s] 94%|█████████▍| 25384/26933 [26:18<01:25, 18.14it/s] 94%|█████████▍| 25388/26933 [26:18<01:24, 18.20it/s] 94%|█████████▍| 25392/26933 [26:19<01:24, 18.22it/s] 94%|█████████▍| 25396/26933 [26:19<01:24, 18.18it/s] 94%|█████████▍| 25400/26933 [26:19<01:24, 18.19it/s] 94%|█████████▍| 25404/26933 [26:19<01:23, 18.21it/s] 94%|█████████▍| 25408/26933 [26:19<01:23, 18.24it/s] 94%|█████████▍| 25412/26933 [26:20<01:23, 18.22it/s] 94%|█████████▍| 25416/26933 [26:20<01:23, 18.25it/s] 94%|█████████▍| 25420/26933 [26:20<01:22, 18.29it/s] 94%|█████████▍| 25424/26933 [26:20<01:22, 18.27it/s] 94%|█████████▍| 25428/26933 [26:21<01:22, 18.17it/s] 94%|█████████▍| 25432/26933 [26:21<01:22, 18.17it/s] 94%|█████████▍| 25436/26933 [26:21<01:22, 18.18it/s] 94%|█████████▍| 25440/26933 [26:21<01:21, 18.22it/s] 94%|█████████▍| 25444/26933 [26:21<01:21, 18.24it/s] 94%|█████████▍| 25448/26933 [26:22<01:21, 18.24it/s] 95%|█████████▍| 25452/26933 [26:22<01:21, 18.28it/s] 95%|█████████▍| 25456/26933 [26:22<01:20, 18.27it/s] 95%|█████████▍| 25460/26933 [26:22<01:20, 18.27it/s] 95%|█████████▍| 25464/26933 [26:22<01:20, 18.32it/s] 95%|█████████▍| 25468/26933 [26:23<01:20, 18.26it/s] 95%|█████████▍| 25472/26933 [26:23<01:19, 18.33it/s] 95%|█████████▍| 25476/26933 [26:23<01:19, 18.38it/s] 95%|█████████▍| 25480/26933 [26:23<01:18, 18.42it/s] 95%|█████████▍| 25484/26933 [26:24<01:18, 18.39it/s] 95%|█████████▍| 25488/26933 [26:24<01:18, 18.33it/s] 95%|█████████▍| 25492/26933 [26:24<01:18, 18.35it/s] 95%|█████████▍| 25496/26933 [26:24<01:18, 18.38it/s] 95%|█████████▍| 25500/26933 [26:24<01:17, 18.39it/s] 95%|█████████▍| 25504/26933 [26:25<01:17, 18.34it/s] 95%|█████████▍| 25508/26933 [26:25<01:17, 18.31it/s] 95%|█████████▍| 25512/26933 [26:25<01:17, 18.34it/s] 95%|█████████▍| 25516/26933 [26:25<01:17, 18.33it/s] 95%|█████████▍| 25520/26933 [26:26<01:17, 18.32it/s] 95%|█████████▍| 25524/26933 [26:26<01:17, 18.28it/s] 95%|█████████▍| 25528/26933 [26:26<01:16, 18.34it/s] 95%|█████████▍| 25532/26933 [26:26<01:16, 18.35it/s] 95%|█████████▍| 25536/26933 [26:26<01:16, 18.34it/s] 95%|█████████▍| 25540/26933 [26:27<01:16, 18.27it/s] 95%|█████████▍| 25544/26933 [26:27<01:15, 18.30it/s] 95%|█████████▍| 25548/26933 [26:27<01:15, 18.32it/s] 95%|█████████▍| 25552/26933 [26:27<01:15, 18.35it/s] 95%|█████████▍| 25556/26933 [26:28<01:15, 18.36it/s] 95%|█████████▍| 25560/26933 [26:28<01:15, 18.31it/s] 95%|█████████▍| 25564/26933 [26:28<01:14, 18.34it/s] 95%|█████████▍| 25568/26933 [26:28<01:14, 18.38it/s] 95%|█████████▍| 25572/26933 [26:28<01:13, 18.40it/s] 95%|█████████▍| 25576/26933 [26:29<01:13, 18.35it/s] 95%|█████████▍| 25580/26933 [26:29<01:13, 18.36it/s] 95%|█████████▍| 25584/26933 [26:29<01:13, 18.37it/s] 95%|█████████▌| 25588/26933 [26:29<01:13, 18.35it/s] 95%|█████████▌| 25592/26933 [26:29<01:13, 18.36it/s] 95%|█████████▌| 25596/26933 [26:30<01:12, 18.32it/s] 95%|█████████▌| 25600/26933 [26:30<01:12, 18.36it/s] 95%|█████████▌| 25604/26933 [26:30<01:12, 18.38it/s] 95%|█████████▌| 25608/26933 [26:30<01:12, 18.35it/s] 95%|█████████▌| 25612/26933 [26:31<01:12, 18.31it/s] 95%|█████████▌| 25616/26933 [26:31<01:12, 18.28it/s] 95%|█████████▌| 25620/26933 [26:31<01:11, 18.34it/s] 95%|█████████▌| 25624/26933 [26:31<01:11, 18.34it/s] 95%|█████████▌| 25628/26933 [26:31<01:11, 18.36it/s] 95%|█████████▌| 25632/26933 [26:32<01:10, 18.35it/s] 95%|█████████▌| 25636/26933 [26:32<01:10, 18.36it/s] 95%|█████████▌| 25640/26933 [26:32<01:10, 18.38it/s] 95%|█████████▌| 25644/26933 [26:32<01:10, 18.41it/s] 95%|█████████▌| 25648/26933 [26:33<01:09, 18.39it/s] 95%|█████████▌| 25652/26933 [26:33<01:09, 18.33it/s] 95%|█████████▌| 25656/26933 [26:33<01:09, 18.29it/s] 95%|█████████▌| 25660/26933 [26:33<01:09, 18.31it/s] 95%|█████████▌| 25664/26933 [26:33<01:09, 18.31it/s] 95%|█████████▌| 25668/26933 [26:34<01:09, 18.28it/s] 95%|█████████▌| 25672/26933 [26:34<01:08, 18.31it/s] 95%|█████████▌| 25676/26933 [26:34<01:08, 18.36it/s] 95%|█████████▌| 25680/26933 [26:34<01:08, 18.34it/s] 95%|█████████▌| 25684/26933 [26:34<01:08, 18.25it/s] 95%|█████████▌| 25688/26933 [26:35<01:08, 18.19it/s] 95%|█████████▌| 25692/26933 [26:35<01:08, 18.18it/s] 95%|█████████▌| 25696/26933 [26:35<01:08, 18.18it/s] 95%|█████████▌| 25700/26933 [26:35<01:07, 18.19it/s] 95%|█████████▌| 25704/26933 [26:36<01:07, 18.11it/s] 95%|█████████▌| 25708/26933 [26:36<01:07, 18.14it/s] 95%|█████████▌| 25712/26933 [26:36<01:07, 18.15it/s] 95%|█████████▌| 25716/26933 [26:36<01:07, 18.14it/s] 95%|█████████▌| 25720/26933 [26:36<01:06, 18.11it/s] 96%|█████████▌| 25724/26933 [26:37<01:07, 18.02it/s] 96%|█████████▌| 25728/26933 [26:37<01:06, 18.05it/s] 96%|█████████▌| 25732/26933 [26:37<01:06, 18.08it/s] 96%|█████████▌| 25736/26933 [26:37<01:06, 18.07it/s] 96%|█████████▌| 25740/26933 [26:38<01:06, 18.03it/s] 96%|█████████▌| 25744/26933 [26:38<01:05, 18.03it/s] 96%|█████████▌| 25748/26933 [26:38<01:05, 18.04it/s] 96%|█████████▌| 25752/26933 [26:38<01:05, 18.04it/s] 96%|█████████▌| 25756/26933 [26:38<01:05, 18.04it/s] 96%|█████████▌| 25760/26933 [26:39<01:05, 18.02it/s] 96%|█████████▌| 25764/26933 [26:39<01:04, 18.06it/s] 96%|█████████▌| 25768/26933 [26:39<01:04, 18.10it/s] 96%|█████████▌| 25772/26933 [26:39<01:04, 18.02it/s] 96%|█████████▌| 25776/26933 [26:40<01:04, 18.05it/s] 96%|█████████▌| 25780/26933 [26:40<01:04, 17.76it/s] 96%|█████████▌| 25784/26933 [26:40<01:04, 17.86it/s] 96%|█████████▌| 25788/26933 [26:40<01:03, 17.92it/s] 96%|█████████▌| 25792/26933 [26:40<01:03, 17.99it/s] 96%|█████████▌| 25796/26933 [26:41<01:03, 17.99it/s] 96%|█████████▌| 25800/26933 [26:41<01:02, 18.04it/s] 96%|█████████▌| 25804/26933 [26:41<01:02, 18.05it/s] 96%|█████████▌| 25808/26933 [26:41<01:02, 18.05it/s] 96%|█████████▌| 25812/26933 [26:42<01:02, 18.02it/s] 96%|█████████▌| 25816/26933 [26:42<01:01, 18.03it/s] 96%|█████████▌| 25820/26933 [26:42<01:01, 18.04it/s] 96%|█████████▌| 25824/26933 [26:42<01:01, 18.02it/s] 96%|█████████▌| 25828/26933 [26:42<01:01, 18.06it/s] 96%|█████████▌| 25832/26933 [26:43<01:01, 18.02it/s] 96%|█████████▌| 25836/26933 [26:43<01:00, 18.06it/s] 96%|█████████▌| 25840/26933 [26:43<01:00, 18.08it/s] 96%|█████████▌| 25844/26933 [26:43<01:00, 18.08it/s] 96%|█████████▌| 25848/26933 [26:44<00:59, 18.10it/s] 96%|█████████▌| 25852/26933 [26:44<00:59, 18.08it/s] 96%|█████████▌| 25856/26933 [26:44<00:59, 18.11it/s] 96%|█████████▌| 25860/26933 [26:44<00:59, 18.11it/s] 96%|█████████▌| 25864/26933 [26:44<00:59, 18.09it/s] 96%|█████████▌| 25868/26933 [26:45<00:58, 18.06it/s] 96%|█████████▌| 25872/26933 [26:45<00:58, 18.09it/s] 96%|█████████▌| 25876/26933 [26:45<00:58, 18.16it/s] 96%|█████████▌| 25880/26933 [26:45<00:57, 18.17it/s] 96%|█████████▌| 25884/26933 [26:46<00:57, 18.15it/s] 96%|█████████▌| 25888/26933 [26:46<00:57, 18.10it/s] 96%|█████████▌| 25892/26933 [26:46<00:57, 18.10it/s] 96%|█████████▌| 25896/26933 [26:46<00:57, 18.11it/s] 96%|█████████▌| 25900/26933 [26:46<00:56, 18.12it/s] 96%|█████████▌| 25904/26933 [26:47<00:56, 18.10it/s] 96%|█████████▌| 25908/26933 [26:47<00:56, 18.06it/s] 96%|█████████▌| 25912/26933 [26:47<00:56, 18.09it/s] 96%|█████████▌| 25916/26933 [26:47<00:56, 18.12it/s] 96%|█████████▌| 25920/26933 [26:48<00:55, 18.12it/s] 96%|█████████▋| 25924/26933 [26:48<00:55, 18.08it/s] 96%|█████████▋| 25928/26933 [26:48<00:55, 18.08it/s] 96%|█████████▋| 25932/26933 [26:48<00:55, 18.05it/s] 96%|█████████▋| 25936/26933 [26:48<00:55, 18.06it/s] 96%|█████████▋| 25940/26933 [26:49<00:55, 18.03it/s] 96%|█████████▋| 25944/26933 [26:49<00:54, 18.04it/s] 96%|█████████▋| 25948/26933 [26:49<00:54, 18.05it/s] 96%|█████████▋| 25952/26933 [26:49<00:54, 18.06it/s] 96%|█████████▋| 25956/26933 [26:50<00:54, 18.06it/s] 96%|█████████▋| 25960/26933 [26:50<00:53, 18.02it/s] 96%|█████████▋| 25964/26933 [26:50<00:53, 18.03it/s] 96%|█████████▋| 25968/26933 [26:50<00:53, 18.05it/s] 96%|█████████▋| 25972/26933 [26:50<00:53, 18.05it/s] 96%|█████████▋| 25976/26933 [26:51<00:53, 18.03it/s] 96%|█████████▋| 25980/26933 [26:51<00:52, 18.06it/s] 96%|█████████▋| 25984/26933 [26:51<00:52, 18.08it/s] 96%|█████████▋| 25988/26933 [26:51<00:52, 18.09it/s] 97%|█████████▋| 25992/26933 [26:52<00:52, 18.07it/s] 97%|█████████▋| 25996/26933 [26:52<00:51, 18.04it/s] 97%|█████████▋| 26000/26933 [26:52<00:51, 18.07it/s] 97%|█████████▋| 26004/26933 [26:52<00:51, 18.11it/s] 97%|█████████▋| 26008/26933 [26:52<00:51, 18.09it/s] 97%|█████████▋| 26012/26933 [26:53<00:51, 18.03it/s] 97%|█████████▋| 26016/26933 [26:53<00:50, 18.03it/s] 97%|█████████▋| 26020/26933 [26:53<00:50, 18.08it/s] 97%|█████████▋| 26024/26933 [26:53<00:50, 18.09it/s] 97%|█████████▋| 26028/26933 [26:54<00:50, 18.07it/s] 97%|█████████▋| 26032/26933 [26:54<00:50, 18.02it/s] 97%|█████████▋| 26036/26933 [26:54<00:49, 18.03it/s] 97%|█████████▋| 26040/26933 [26:54<00:49, 18.06it/s] 97%|█████████▋| 26044/26933 [26:54<00:49, 18.08it/s] 97%|█████████▋| 26048/26933 [26:55<00:49, 18.04it/s] 97%|█████████▋| 26052/26933 [26:55<00:48, 18.07it/s] 97%|█████████▋| 26056/26933 [26:55<00:48, 18.11it/s] 97%|█████████▋| 26060/26933 [26:55<00:48, 18.13it/s] 97%|█████████▋| 26064/26933 [26:56<00:47, 18.13it/s] 97%|█████████▋| 26068/26933 [26:56<00:47, 18.04it/s] 97%|█████████▋| 26072/26933 [26:56<00:47, 18.08it/s] 97%|█████████▋| 26076/26933 [26:56<00:47, 18.10it/s] 97%|█████████▋| 26080/26933 [26:56<00:47, 18.13it/s] 97%|█████████▋| 26084/26933 [26:57<00:46, 18.08it/s] 97%|█████████▋| 26088/26933 [26:57<00:46, 18.06it/s] 97%|█████████▋| 26092/26933 [26:57<00:46, 18.09it/s] 97%|█████████▋| 26096/26933 [26:57<00:46, 17.85it/s] 97%|█████████▋| 26100/26933 [26:58<00:46, 17.92it/s] 97%|█████████▋| 26104/26933 [26:58<00:46, 17.95it/s] 97%|█████████▋| 26108/26933 [26:58<00:45, 17.97it/s] 97%|█████████▋| 26112/26933 [26:58<00:45, 18.00it/s] 97%|█████████▋| 26116/26933 [26:58<00:45, 18.03it/s] 97%|█████████▋| 26120/26933 [26:59<00:45, 18.05it/s] 97%|█████████▋| 26124/26933 [26:59<00:44, 18.08it/s] 97%|█████████▋| 26128/26933 [26:59<00:44, 18.09it/s] 97%|█████████▋| 26132/26933 [26:59<00:44, 18.10it/s] 97%|█████████▋| 26136/26933 [27:00<00:43, 18.14it/s] 97%|█████████▋| 26140/26933 [27:00<00:43, 18.10it/s] 97%|█████████▋| 26144/26933 [27:00<00:43, 18.15it/s] 97%|█████████▋| 26148/26933 [27:00<00:43, 18.16it/s] 97%|█████████▋| 26152/26933 [27:00<00:42, 18.17it/s] 97%|█████████▋| 26156/26933 [27:01<00:42, 18.10it/s] 97%|█████████▋| 26160/26933 [27:01<00:42, 18.11it/s] 97%|█████████▋| 26164/26933 [27:01<00:42, 18.12it/s] 97%|█████████▋| 26168/26933 [27:01<00:42, 18.13it/s] 97%|█████████▋| 26172/26933 [27:01<00:41, 18.14it/s] 97%|█████████▋| 26176/26933 [27:02<00:41, 18.10it/s] 97%|█████████▋| 26180/26933 [27:02<00:41, 18.11it/s] 97%|█████████▋| 26184/26933 [27:02<00:41, 18.13it/s] 97%|█████████▋| 26188/26933 [27:02<00:41, 18.13it/s] 97%|█████████▋| 26192/26933 [27:03<00:41, 17.98it/s] 97%|█████████▋| 26196/26933 [27:03<00:41, 17.94it/s] 97%|█████████▋| 26200/26933 [27:03<00:41, 17.86it/s] 97%|█████████▋| 26204/26933 [27:03<00:40, 17.91it/s] 97%|█████████▋| 26208/26933 [27:04<00:40, 17.94it/s] 97%|█████████▋| 26212/26933 [27:04<00:40, 17.93it/s] 97%|█████████▋| 26216/26933 [27:04<00:39, 17.97it/s] 97%|█████████▋| 26220/26933 [27:04<00:39, 18.00it/s] 97%|█████████▋| 26224/26933 [27:04<00:39, 18.01it/s] 97%|█████████▋| 26228/26933 [27:05<00:39, 17.71it/s] 97%|█████████▋| 26232/26933 [27:05<00:39, 17.81it/s] 97%|█████████▋| 26236/26933 [27:05<00:38, 17.88it/s] 97%|█████████▋| 26240/26933 [27:05<00:38, 17.94it/s] 97%|█████████▋| 26244/26933 [27:06<00:38, 17.93it/s] 97%|█████████▋| 26248/26933 [27:06<00:38, 17.63it/s] 97%|█████████▋| 26252/26933 [27:06<00:38, 17.76it/s] 97%|█████████▋| 26256/26933 [27:06<00:37, 17.82it/s] 98%|█████████▊| 26260/26933 [27:06<00:37, 17.85it/s] 98%|█████████▊| 26264/26933 [27:07<00:38, 17.60it/s] 98%|█████████▊| 26268/26933 [27:07<00:37, 17.74it/s] 98%|█████████▊| 26272/26933 [27:07<00:37, 17.83it/s] 98%|█████████▊| 26276/26933 [27:07<00:36, 17.92it/s] 98%|█████████▊| 26280/26933 [27:08<00:36, 17.94it/s] 98%|█████████▊| 26284/26933 [27:08<00:36, 17.65it/s] 98%|█████████▊| 26288/26933 [27:08<00:36, 17.76it/s] 98%|█████████▊| 26292/26933 [27:08<00:35, 17.85it/s] 98%|█████████▊| 26296/26933 [27:08<00:35, 17.92it/s] 98%|█████████▊| 26300/26933 [27:09<00:35, 17.65it/s] 98%|█████████▊| 26304/26933 [27:09<00:35, 17.77it/s] 98%|█████████▊| 26308/26933 [27:09<00:35, 17.84it/s] 98%|█████████▊| 26312/26933 [27:09<00:34, 17.93it/s] 98%|█████████▊| 26316/26933 [27:10<00:34, 17.98it/s] 98%|█████████▊| 26320/26933 [27:10<00:34, 17.68it/s] 98%|█████████▊| 26324/26933 [27:10<00:34, 17.80it/s] 98%|█████████▊| 26328/26933 [27:10<00:33, 17.87it/s] 98%|█████████▊| 26332/26933 [27:10<00:33, 17.92it/s] 98%|█████████▊| 26336/26933 [27:11<00:33, 17.64it/s] 98%|█████████▊| 26340/26933 [27:11<00:33, 17.78it/s] 98%|█████████▊| 26344/26933 [27:11<00:32, 17.85it/s] 98%|█████████▊| 26348/26933 [27:11<00:32, 17.86it/s] 98%|█████████▊| 26352/26933 [27:12<00:32, 17.91it/s] 98%|█████████▊| 26356/26933 [27:12<00:32, 17.61it/s] 98%|█████████▊| 26360/26933 [27:12<00:32, 17.51it/s] 98%|█████████▊| 26364/26933 [27:12<00:32, 17.51it/s] 98%|█████████▊| 26368/26933 [27:12<00:32, 17.58it/s] 98%|█████████▊| 26372/26933 [27:13<00:32, 17.37it/s] 98%|█████████▊| 26376/26933 [27:13<00:31, 17.52it/s] 98%|█████████▊| 26380/26933 [27:13<00:31, 17.59it/s] 98%|█████████▊| 26384/26933 [27:13<00:31, 17.69it/s] 98%|█████████▊| 26388/26933 [27:14<00:31, 17.45it/s] 98%|█████████▊| 26392/26933 [27:14<00:30, 17.52it/s] 98%|█████████▊| 26396/26933 [27:14<00:30, 17.61it/s] 98%|█████████▊| 26400/26933 [27:14<00:30, 17.71it/s] 98%|█████████▊| 26404/26933 [27:15<00:29, 17.79it/s] 98%|█████████▊| 26408/26933 [27:15<00:29, 17.52it/s] 98%|█████████▊| 26412/26933 [27:15<00:29, 17.65it/s] 98%|█████████▊| 26416/26933 [27:15<00:29, 17.77it/s] 98%|█████████▊| 26420/26933 [27:15<00:28, 17.85it/s] 98%|█████████▊| 26424/26933 [27:16<00:28, 17.66it/s] 98%|█████████▊| 26428/26933 [27:16<00:28, 17.79it/s] 98%|█████████▊| 26432/26933 [27:16<00:28, 17.88it/s] 98%|█████████▊| 26436/26933 [27:16<00:27, 17.94it/s] 98%|█████████▊| 26440/26933 [27:17<00:27, 17.94it/s] 98%|█████████▊| 26444/26933 [27:17<00:27, 17.61it/s] 98%|█████████▊| 26448/26933 [27:17<00:27, 17.70it/s] 98%|█████████▊| 26452/26933 [27:17<00:27, 17.73it/s] 98%|█████████▊| 26456/26933 [27:17<00:26, 17.79it/s] 98%|█████████▊| 26460/26933 [27:18<00:26, 17.57it/s] 98%|█████████▊| 26464/26933 [27:18<00:26, 17.69it/s] 98%|█████████▊| 26468/26933 [27:18<00:26, 17.77it/s] 98%|█████████▊| 26472/26933 [27:18<00:25, 17.82it/s] 98%|█████████▊| 26476/26933 [27:19<00:26, 17.57it/s] 98%|█████████▊| 26480/26933 [27:19<00:25, 17.63it/s] 98%|█████████▊| 26484/26933 [27:19<00:25, 17.66it/s] 98%|█████████▊| 26488/26933 [27:19<00:25, 17.71it/s] 98%|█████████▊| 26492/26933 [27:20<00:24, 17.74it/s] 98%|█████████▊| 26496/26933 [27:20<00:25, 17.46it/s] 98%|█████████▊| 26500/26933 [27:20<00:24, 17.59it/s] 98%|█████████▊| 26504/26933 [27:20<00:24, 17.68it/s] 98%|█████████▊| 26508/26933 [27:20<00:23, 17.74it/s] 98%|█████████▊| 26512/26933 [27:21<00:24, 17.48it/s] 98%|█████████▊| 26516/26933 [27:21<00:23, 17.62it/s] 98%|█████████▊| 26520/26933 [27:21<00:23, 17.70it/s] 98%|█████████▊| 26524/26933 [27:21<00:23, 17.76it/s] 98%|█████████▊| 26528/26933 [27:22<00:23, 17.46it/s] 99%|█████████▊| 26532/26933 [27:22<00:23, 17.30it/s] 99%|█████████▊| 26536/26933 [27:22<00:22, 17.49it/s] 99%|█████████▊| 26540/26933 [27:22<00:22, 17.58it/s] 99%|█████████▊| 26544/26933 [27:22<00:22, 17.66it/s] 99%|█████████▊| 26548/26933 [27:23<00:22, 17.45it/s] 99%|█████████▊| 26552/26933 [27:23<00:21, 17.56it/s] 99%|█████████▊| 26556/26933 [27:23<00:21, 17.64it/s] 99%|█████████▊| 26560/26933 [27:23<00:21, 17.69it/s] 99%|█████████▊| 26564/26933 [27:24<00:21, 17.46it/s] 99%|█████████▊| 26568/26933 [27:24<00:20, 17.58it/s] 99%|█████████▊| 26572/26933 [27:24<00:20, 17.65it/s] 99%|█████████▊| 26576/26933 [27:24<00:20, 17.71it/s] 99%|█████████▊| 26580/26933 [27:25<00:19, 17.76it/s] 99%|█████████▊| 26584/26933 [27:25<00:20, 17.20it/s] 99%|█████████▊| 26588/26933 [27:25<00:19, 17.45it/s] 99%|█████████▊| 26592/26933 [27:25<00:19, 17.63it/s] 99%|█████████▊| 26596/26933 [27:25<00:19, 17.73it/s] 99%|█████████▉| 26600/26933 [27:26<00:18, 17.55it/s] 99%|█████████▉| 26604/26933 [27:26<00:18, 17.66it/s] 99%|█████████▉| 26608/26933 [27:26<00:18, 17.75it/s] 99%|█████████▉| 26612/26933 [27:26<00:18, 17.78it/s] 99%|█████████▉| 26616/26933 [27:27<00:17, 17.81it/s] 99%|█████████▉| 26620/26933 [27:27<00:17, 17.52it/s] 99%|█████████▉| 26624/26933 [27:27<00:17, 17.62it/s] 99%|█████████▉| 26628/26933 [27:27<00:17, 17.65it/s] 99%|█████████▉| 26632/26933 [27:27<00:17, 17.70it/s] 99%|█████████▉| 26636/26933 [27:28<00:17, 17.43it/s] 99%|█████████▉| 26640/26933 [27:28<00:16, 17.53it/s] 99%|█████████▉| 26644/26933 [27:28<00:16, 17.59it/s] 99%|█████████▉| 26648/26933 [27:28<00:16, 17.63it/s] 99%|█████████▉| 26652/26933 [27:29<00:16, 17.35it/s] 99%|█████████▉| 26656/26933 [27:29<00:15, 17.44it/s] 99%|█████████▉| 26660/26933 [27:29<00:15, 17.51it/s] 99%|█████████▉| 26664/26933 [27:29<00:15, 17.57it/s] 99%|█████████▉| 26668/26933 [27:30<00:15, 17.63it/s] 99%|█████████▉| 26672/26933 [27:30<00:15, 17.39it/s] 99%|█████████▉| 26676/26933 [27:30<00:14, 17.51it/s] 99%|█████████▉| 26680/26933 [27:30<00:14, 17.56it/s] 99%|█████████▉| 26684/26933 [27:30<00:14, 17.64it/s] 99%|█████████▉| 26688/26933 [27:31<00:14, 17.40it/s] 99%|█████████▉| 26692/26933 [27:31<00:13, 17.50it/s] 99%|█████████▉| 26696/26933 [27:31<00:13, 17.63it/s] 99%|█████████▉| 26700/26933 [27:31<00:13, 17.73it/s] 99%|█████████▉| 26704/26933 [27:32<00:12, 17.83it/s] 99%|█████████▉| 26708/26933 [27:32<00:12, 17.59it/s] 99%|█████████▉| 26712/26933 [27:32<00:12, 17.53it/s] 99%|█████████▉| 26716/26933 [27:32<00:12, 17.60it/s] 99%|█████████▉| 26720/26933 [27:32<00:12, 17.72it/s] 99%|█████████▉| 26724/26933 [27:33<00:11, 17.53it/s] 99%|█████████▉| 26728/26933 [27:33<00:11, 17.64it/s] 99%|█████████▉| 26732/26933 [27:33<00:11, 17.75it/s] 99%|█████████▉| 26736/26933 [27:33<00:11, 17.77it/s] 99%|█████████▉| 26740/26933 [27:34<00:11, 17.49it/s] 99%|█████████▉| 26744/26933 [27:34<00:10, 17.58it/s] 99%|█████████▉| 26748/26933 [27:34<00:10, 17.63it/s] 99%|█████████▉| 26752/26933 [27:34<00:10, 17.72it/s] 99%|█████████▉| 26756/26933 [27:35<00:09, 17.80it/s] 99%|█████████▉| 26760/26933 [27:35<00:09, 17.52it/s] 99%|█████████▉| 26764/26933 [27:35<00:09, 17.61it/s] 99%|█████████▉| 26768/26933 [27:35<00:09, 17.61it/s] 99%|█████████▉| 26772/26933 [27:35<00:09, 17.68it/s] 99%|█████████▉| 26776/26933 [27:36<00:09, 17.43it/s] 99%|█████████▉| 26780/26933 [27:36<00:08, 17.52it/s] 99%|█████████▉| 26784/26933 [27:36<00:08, 17.59it/s] 99%|█████████▉| 26788/26933 [27:36<00:08, 17.64it/s] 99%|█████████▉| 26792/26933 [27:37<00:07, 17.68it/s] 99%|█████████▉| 26796/26933 [27:37<00:07, 17.45it/s]100%|█████████▉| 26800/26933 [27:37<00:07, 17.55it/s]100%|█████████▉| 26804/26933 [27:37<00:07, 17.61it/s]100%|█████████▉| 26808/26933 [27:37<00:07, 17.65it/s]100%|█████████▉| 26812/26933 [27:38<00:06, 17.44it/s]100%|█████████▉| 26816/26933 [27:38<00:06, 17.53it/s]100%|█████████▉| 26820/26933 [27:38<00:06, 17.26it/s]100%|█████████▉| 26824/26933 [27:38<00:06, 17.40it/s]100%|█████████▉| 26828/26933 [27:39<00:06, 17.27it/s]100%|█████████▉| 26832/26933 [27:39<00:05, 17.38it/s]100%|█████████▉| 26836/26933 [27:39<00:05, 17.46it/s]100%|█████████▉| 26840/26933 [27:39<00:05, 17.51it/s]100%|█████████▉| 26844/26933 [27:40<00:05, 17.57it/s]100%|█████████▉| 26848/26933 [27:40<00:04, 17.35it/s]100%|█████████▉| 26852/26933 [27:40<00:04, 17.50it/s]100%|█████████▉| 26856/26933 [27:40<00:04, 17.59it/s]100%|█████████▉| 26860/26933 [27:40<00:04, 17.66it/s]100%|█████████▉| 26864/26933 [27:41<00:03, 17.45it/s]100%|█████████▉| 26868/26933 [27:41<00:03, 17.63it/s]100%|█████████▉| 26872/26933 [27:41<00:03, 17.72it/s]100%|█████████▉| 26876/26933 [27:41<00:03, 17.80it/s]100%|█████████▉| 26880/26933 [27:42<00:03, 17.46it/s]100%|█████████▉| 26884/26933 [27:42<00:02, 17.60it/s]100%|█████████▉| 26888/26933 [27:42<00:02, 17.70it/s]100%|█████████▉| 26892/26933 [27:42<00:02, 17.76it/s]100%|█████████▉| 26896/26933 [27:42<00:02, 17.78it/s]100%|█████████▉| 26900/26933 [27:43<00:01, 17.44it/s]100%|█████████▉| 26904/26933 [27:43<00:01, 17.54it/s]100%|█████████▉| 26908/26933 [27:43<00:01, 17.62it/s]100%|█████████▉| 26912/26933 [27:43<00:01, 17.69it/s]100%|█████████▉| 26916/26933 [27:44<00:00, 17.42it/s]100%|█████████▉| 26920/26933 [27:44<00:00, 17.18it/s]100%|█████████▉| 26924/26933 [27:44<00:00, 17.37it/s]100%|█████████▉| 26928/26933 [27:44<00:00, 17.49it/s]100%|█████████▉| 26932/26933 [27:45<00:00, 17.61it/s]100%|██████████| 26933/26933 [27:45<00:00, 16.18it/s]
|    Task     |Version| Metric |Value |   |Stderr|
|-------------|------:|--------|-----:|---|-----:|
|arc_challenge|      0|acc     |0.3294|±  |0.0137|
|             |       |acc_norm|0.3584|±  |0.0140|
|arc_easy     |      0|acc     |0.6473|±  |0.0098|
|             |       |acc_norm|0.4882|±  |0.0103|
|boolq        |      1|acc     |0.6728|±  |0.0082|
|piqa         |      0|acc     |0.7410|±  |0.0102|
|             |       |acc_norm|0.7367|±  |0.0103|
|winogrande   |      0|acc     |0.6369|±  |0.0135|

W0908 03:32:13.057881 102843 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 03:32:13.083312 102843 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 03:32:13.197967 102843 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 03:32:13.198102 102843 utils.py:164] NumExpr defaulting to 16 threads.
I0908 03:32:13.359153 102843 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 03:32:13.600524 102843 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 03:32:35.760439 102843 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 03:33:01.634067 102843 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:54<00:54, 54.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:14<00:00, 34.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:14<00:00, 37.18s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0908 03:37:05.653364 103130 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 03:37:05.698962 103130 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 03:37:07.375051 103130 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 03:37:07.375201 103130 utils.py:164] NumExpr defaulting to 16 threads.
I0908 03:37:07.819804 103130 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 03:37:09.035756 103130 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 03:37:32.181300 103130 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.76it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.66it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.47it/s]
Some weights of the model checkpoint at meta-llama/Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.Qidxs', 'model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.Wscale', 'model.layers.0.mlp.down_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.Qidxs', 'model.layers.0.mlp.upgate_proj.SU', 'model.layers.0.mlp.upgate_proj.SV', 'model.layers.0.mlp.upgate_proj.Wscale', 'model.layers.0.mlp.upgate_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.fuse_scales', 'model.layers.0.self_attn.o_proj.Qidxs', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.Wscale', 'model.layers.0.self_attn.o_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.Qidxs', 'model.layers.0.self_attn.qkv_proj.SU', 'model.layers.0.self_attn.qkv_proj.SV', 'model.layers.0.self_attn.qkv_proj.Wscale', 'model.layers.0.self_attn.qkv_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.fuse_scales', 'model.layers.1.mlp.down_proj.Qidxs', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.Wscale', 'model.layers.1.mlp.down_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.Qidxs', 'model.layers.1.mlp.upgate_proj.SU', 'model.layers.1.mlp.upgate_proj.SV', 'model.layers.1.mlp.upgate_proj.Wscale', 'model.layers.1.mlp.upgate_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.fuse_scales', 'model.layers.1.self_attn.o_proj.Qidxs', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.Wscale', 'model.layers.1.self_attn.o_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.Qidxs', 'model.layers.1.self_attn.qkv_proj.SU', 'model.layers.1.self_attn.qkv_proj.SV', 'model.layers.1.self_attn.qkv_proj.Wscale', 'model.layers.1.self_attn.qkv_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.fuse_scales', 'model.layers.10.mlp.down_proj.Qidxs', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.Wscale', 'model.layers.10.mlp.down_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.Qidxs', 'model.layers.10.mlp.upgate_proj.SU', 'model.layers.10.mlp.upgate_proj.SV', 'model.layers.10.mlp.upgate_proj.Wscale', 'model.layers.10.mlp.upgate_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.fuse_scales', 'model.layers.10.self_attn.o_proj.Qidxs', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.Wscale', 'model.layers.10.self_attn.o_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.Qidxs', 'model.layers.10.self_attn.qkv_proj.SU', 'model.layers.10.self_attn.qkv_proj.SV', 'model.layers.10.self_attn.qkv_proj.Wscale', 'model.layers.10.self_attn.qkv_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.fuse_scales', 'model.layers.11.mlp.down_proj.Qidxs', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.Wscale', 'model.layers.11.mlp.down_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.Qidxs', 'model.layers.11.mlp.upgate_proj.SU', 'model.layers.11.mlp.upgate_proj.SV', 'model.layers.11.mlp.upgate_proj.Wscale', 'model.layers.11.mlp.upgate_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.fuse_scales', 'model.layers.11.self_attn.o_proj.Qidxs', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.Wscale', 'model.layers.11.self_attn.o_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.Qidxs', 'model.layers.11.self_attn.qkv_proj.SU', 'model.layers.11.self_attn.qkv_proj.SV', 'model.layers.11.self_attn.qkv_proj.Wscale', 'model.layers.11.self_attn.qkv_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.fuse_scales', 'model.layers.12.mlp.down_proj.Qidxs', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.Wscale', 'model.layers.12.mlp.down_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.Qidxs', 'model.layers.12.mlp.upgate_proj.SU', 'model.layers.12.mlp.upgate_proj.SV', 'model.layers.12.mlp.upgate_proj.Wscale', 'model.layers.12.mlp.upgate_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.fuse_scales', 'model.layers.12.self_attn.o_proj.Qidxs', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.Wscale', 'model.layers.12.self_attn.o_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.Qidxs', 'model.layers.12.self_attn.qkv_proj.SU', 'model.layers.12.self_attn.qkv_proj.SV', 'model.layers.12.self_attn.qkv_proj.Wscale', 'model.layers.12.self_attn.qkv_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.fuse_scales', 'model.layers.13.mlp.down_proj.Qidxs', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.Wscale', 'model.layers.13.mlp.down_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.Qidxs', 'model.layers.13.mlp.upgate_proj.SU', 'model.layers.13.mlp.upgate_proj.SV', 'model.layers.13.mlp.upgate_proj.Wscale', 'model.layers.13.mlp.upgate_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.fuse_scales', 'model.layers.13.self_attn.o_proj.Qidxs', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.Wscale', 'model.layers.13.self_attn.o_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.Qidxs', 'model.layers.13.self_attn.qkv_proj.SU', 'model.layers.13.self_attn.qkv_proj.SV', 'model.layers.13.self_attn.qkv_proj.Wscale', 'model.layers.13.self_attn.qkv_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.fuse_scales', 'model.layers.14.mlp.down_proj.Qidxs', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.Wscale', 'model.layers.14.mlp.down_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.Qidxs', 'model.layers.14.mlp.upgate_proj.SU', 'model.layers.14.mlp.upgate_proj.SV', 'model.layers.14.mlp.upgate_proj.Wscale', 'model.layers.14.mlp.upgate_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.fuse_scales', 'model.layers.14.self_attn.o_proj.Qidxs', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.Wscale', 'model.layers.14.self_attn.o_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.Qidxs', 'model.layers.14.self_attn.qkv_proj.SU', 'model.layers.14.self_attn.qkv_proj.SV', 'model.layers.14.self_attn.qkv_proj.Wscale', 'model.layers.14.self_attn.qkv_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.fuse_scales', 'model.layers.15.mlp.down_proj.Qidxs', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.Wscale', 'model.layers.15.mlp.down_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.Qidxs', 'model.layers.15.mlp.upgate_proj.SU', 'model.layers.15.mlp.upgate_proj.SV', 'model.layers.15.mlp.upgate_proj.Wscale', 'model.layers.15.mlp.upgate_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.fuse_scales', 'model.layers.15.self_attn.o_proj.Qidxs', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.Wscale', 'model.layers.15.self_attn.o_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.Qidxs', 'model.layers.15.self_attn.qkv_proj.SU', 'model.layers.15.self_attn.qkv_proj.SV', 'model.layers.15.self_attn.qkv_proj.Wscale', 'model.layers.15.self_attn.qkv_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.fuse_scales', 'model.layers.16.mlp.down_proj.Qidxs', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.Wscale', 'model.layers.16.mlp.down_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.Qidxs', 'model.layers.16.mlp.upgate_proj.SU', 'model.layers.16.mlp.upgate_proj.SV', 'model.layers.16.mlp.upgate_proj.Wscale', 'model.layers.16.mlp.upgate_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.fuse_scales', 'model.layers.16.self_attn.o_proj.Qidxs', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.Wscale', 'model.layers.16.self_attn.o_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.Qidxs', 'model.layers.16.self_attn.qkv_proj.SU', 'model.layers.16.self_attn.qkv_proj.SV', 'model.layers.16.self_attn.qkv_proj.Wscale', 'model.layers.16.self_attn.qkv_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.fuse_scales', 'model.layers.17.mlp.down_proj.Qidxs', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.Wscale', 'model.layers.17.mlp.down_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.Qidxs', 'model.layers.17.mlp.upgate_proj.SU', 'model.layers.17.mlp.upgate_proj.SV', 'model.layers.17.mlp.upgate_proj.Wscale', 'model.layers.17.mlp.upgate_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.fuse_scales', 'model.layers.17.self_attn.o_proj.Qidxs', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.Wscale', 'model.layers.17.self_attn.o_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.Qidxs', 'model.layers.17.self_attn.qkv_proj.SU', 'model.layers.17.self_attn.qkv_proj.SV', 'model.layers.17.self_attn.qkv_proj.Wscale', 'model.layers.17.self_attn.qkv_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.fuse_scales', 'model.layers.18.mlp.down_proj.Qidxs', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.Wscale', 'model.layers.18.mlp.down_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.Qidxs', 'model.layers.18.mlp.upgate_proj.SU', 'model.layers.18.mlp.upgate_proj.SV', 'model.layers.18.mlp.upgate_proj.Wscale', 'model.layers.18.mlp.upgate_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.fuse_scales', 'model.layers.18.self_attn.o_proj.Qidxs', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.Wscale', 'model.layers.18.self_attn.o_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.Qidxs', 'model.layers.18.self_attn.qkv_proj.SU', 'model.layers.18.self_attn.qkv_proj.SV', 'model.layers.18.self_attn.qkv_proj.Wscale', 'model.layers.18.self_attn.qkv_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.fuse_scales', 'model.layers.19.mlp.down_proj.Qidxs', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.Wscale', 'model.layers.19.mlp.down_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.Qidxs', 'model.layers.19.mlp.upgate_proj.SU', 'model.layers.19.mlp.upgate_proj.SV', 'model.layers.19.mlp.upgate_proj.Wscale', 'model.layers.19.mlp.upgate_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.fuse_scales', 'model.layers.19.self_attn.o_proj.Qidxs', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.Wscale', 'model.layers.19.self_attn.o_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.Qidxs', 'model.layers.19.self_attn.qkv_proj.SU', 'model.layers.19.self_attn.qkv_proj.SV', 'model.layers.19.self_attn.qkv_proj.Wscale', 'model.layers.19.self_attn.qkv_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.fuse_scales', 'model.layers.2.mlp.down_proj.Qidxs', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.Wscale', 'model.layers.2.mlp.down_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.Qidxs', 'model.layers.2.mlp.upgate_proj.SU', 'model.layers.2.mlp.upgate_proj.SV', 'model.layers.2.mlp.upgate_proj.Wscale', 'model.layers.2.mlp.upgate_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.fuse_scales', 'model.layers.2.self_attn.o_proj.Qidxs', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.Wscale', 'model.layers.2.self_attn.o_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.Qidxs', 'model.layers.2.self_attn.qkv_proj.SU', 'model.layers.2.self_attn.qkv_proj.SV', 'model.layers.2.self_attn.qkv_proj.Wscale', 'model.layers.2.self_attn.qkv_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.fuse_scales', 'model.layers.20.mlp.down_proj.Qidxs', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.Wscale', 'model.layers.20.mlp.down_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.Qidxs', 'model.layers.20.mlp.upgate_proj.SU', 'model.layers.20.mlp.upgate_proj.SV', 'model.layers.20.mlp.upgate_proj.Wscale', 'model.layers.20.mlp.upgate_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.fuse_scales', 'model.layers.20.self_attn.o_proj.Qidxs', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.Wscale', 'model.layers.20.self_attn.o_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.Qidxs', 'model.layers.20.self_attn.qkv_proj.SU', 'model.layers.20.self_attn.qkv_proj.SV', 'model.layers.20.self_attn.qkv_proj.Wscale', 'model.layers.20.self_attn.qkv_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.fuse_scales', 'model.layers.21.mlp.down_proj.Qidxs', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.Wscale', 'model.layers.21.mlp.down_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.Qidxs', 'model.layers.21.mlp.upgate_proj.SU', 'model.layers.21.mlp.upgate_proj.SV', 'model.layers.21.mlp.upgate_proj.Wscale', 'model.layers.21.mlp.upgate_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.fuse_scales', 'model.layers.21.self_attn.o_proj.Qidxs', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.Wscale', 'model.layers.21.self_attn.o_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.Qidxs', 'model.layers.21.self_attn.qkv_proj.SU', 'model.layers.21.self_attn.qkv_proj.SV', 'model.layers.21.self_attn.qkv_proj.Wscale', 'model.layers.21.self_attn.qkv_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.fuse_scales', 'model.layers.22.mlp.down_proj.Qidxs', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.Wscale', 'model.layers.22.mlp.down_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.Qidxs', 'model.layers.22.mlp.upgate_proj.SU', 'model.layers.22.mlp.upgate_proj.SV', 'model.layers.22.mlp.upgate_proj.Wscale', 'model.layers.22.mlp.upgate_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.fuse_scales', 'model.layers.22.self_attn.o_proj.Qidxs', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.Wscale', 'model.layers.22.self_attn.o_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.Qidxs', 'model.layers.22.self_attn.qkv_proj.SU', 'model.layers.22.self_attn.qkv_proj.SV', 'model.layers.22.self_attn.qkv_proj.Wscale', 'model.layers.22.self_attn.qkv_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.fuse_scales', 'model.layers.23.mlp.down_proj.Qidxs', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.Wscale', 'model.layers.23.mlp.down_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.Qidxs', 'model.layers.23.mlp.upgate_proj.SU', 'model.layers.23.mlp.upgate_proj.SV', 'model.layers.23.mlp.upgate_proj.Wscale', 'model.layers.23.mlp.upgate_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.fuse_scales', 'model.layers.23.self_attn.o_proj.Qidxs', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.Wscale', 'model.layers.23.self_attn.o_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.Qidxs', 'model.layers.23.self_attn.qkv_proj.SU', 'model.layers.23.self_attn.qkv_proj.SV', 'model.layers.23.self_attn.qkv_proj.Wscale', 'model.layers.23.self_attn.qkv_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.fuse_scales', 'model.layers.24.mlp.down_proj.Qidxs', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.Wscale', 'model.layers.24.mlp.down_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.Qidxs', 'model.layers.24.mlp.upgate_proj.SU', 'model.layers.24.mlp.upgate_proj.SV', 'model.layers.24.mlp.upgate_proj.Wscale', 'model.layers.24.mlp.upgate_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.fuse_scales', 'model.layers.24.self_attn.o_proj.Qidxs', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.Wscale', 'model.layers.24.self_attn.o_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.Qidxs', 'model.layers.24.self_attn.qkv_proj.SU', 'model.layers.24.self_attn.qkv_proj.SV', 'model.layers.24.self_attn.qkv_proj.Wscale', 'model.layers.24.self_attn.qkv_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.fuse_scales', 'model.layers.25.mlp.down_proj.Qidxs', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.Wscale', 'model.layers.25.mlp.down_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.Qidxs', 'model.layers.25.mlp.upgate_proj.SU', 'model.layers.25.mlp.upgate_proj.SV', 'model.layers.25.mlp.upgate_proj.Wscale', 'model.layers.25.mlp.upgate_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.fuse_scales', 'model.layers.25.self_attn.o_proj.Qidxs', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.Wscale', 'model.layers.25.self_attn.o_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.Qidxs', 'model.layers.25.self_attn.qkv_proj.SU', 'model.layers.25.self_attn.qkv_proj.SV', 'model.layers.25.self_attn.qkv_proj.Wscale', 'model.layers.25.self_attn.qkv_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.fuse_scales', 'model.layers.26.mlp.down_proj.Qidxs', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.Wscale', 'model.layers.26.mlp.down_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.Qidxs', 'model.layers.26.mlp.upgate_proj.SU', 'model.layers.26.mlp.upgate_proj.SV', 'model.layers.26.mlp.upgate_proj.Wscale', 'model.layers.26.mlp.upgate_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.fuse_scales', 'model.layers.26.self_attn.o_proj.Qidxs', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.Wscale', 'model.layers.26.self_attn.o_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.Qidxs', 'model.layers.26.self_attn.qkv_proj.SU', 'model.layers.26.self_attn.qkv_proj.SV', 'model.layers.26.self_attn.qkv_proj.Wscale', 'model.layers.26.self_attn.qkv_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.fuse_scales', 'model.layers.27.mlp.down_proj.Qidxs', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.Wscale', 'model.layers.27.mlp.down_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.Qidxs', 'model.layers.27.mlp.upgate_proj.SU', 'model.layers.27.mlp.upgate_proj.SV', 'model.layers.27.mlp.upgate_proj.Wscale', 'model.layers.27.mlp.upgate_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.fuse_scales', 'model.layers.27.self_attn.o_proj.Qidxs', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.Wscale', 'model.layers.27.self_attn.o_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.Qidxs', 'model.layers.27.self_attn.qkv_proj.SU', 'model.layers.27.self_attn.qkv_proj.SV', 'model.layers.27.self_attn.qkv_proj.Wscale', 'model.layers.27.self_attn.qkv_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.fuse_scales', 'model.layers.28.mlp.down_proj.Qidxs', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.Wscale', 'model.layers.28.mlp.down_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.Qidxs', 'model.layers.28.mlp.upgate_proj.SU', 'model.layers.28.mlp.upgate_proj.SV', 'model.layers.28.mlp.upgate_proj.Wscale', 'model.layers.28.mlp.upgate_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.fuse_scales', 'model.layers.28.self_attn.o_proj.Qidxs', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.Wscale', 'model.layers.28.self_attn.o_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.Qidxs', 'model.layers.28.self_attn.qkv_proj.SU', 'model.layers.28.self_attn.qkv_proj.SV', 'model.layers.28.self_attn.qkv_proj.Wscale', 'model.layers.28.self_attn.qkv_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.fuse_scales', 'model.layers.29.mlp.down_proj.Qidxs', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.Wscale', 'model.layers.29.mlp.down_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.Qidxs', 'model.layers.29.mlp.upgate_proj.SU', 'model.layers.29.mlp.upgate_proj.SV', 'model.layers.29.mlp.upgate_proj.Wscale', 'model.layers.29.mlp.upgate_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.fuse_scales', 'model.layers.29.self_attn.o_proj.Qidxs', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.Wscale', 'model.layers.29.self_attn.o_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.Qidxs', 'model.layers.29.self_attn.qkv_proj.SU', 'model.layers.29.self_attn.qkv_proj.SV', 'model.layers.29.self_attn.qkv_proj.Wscale', 'model.layers.29.self_attn.qkv_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.fuse_scales', 'model.layers.3.mlp.down_proj.Qidxs', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.Wscale', 'model.layers.3.mlp.down_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.Qidxs', 'model.layers.3.mlp.upgate_proj.SU', 'model.layers.3.mlp.upgate_proj.SV', 'model.layers.3.mlp.upgate_proj.Wscale', 'model.layers.3.mlp.upgate_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.fuse_scales', 'model.layers.3.self_attn.o_proj.Qidxs', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.Wscale', 'model.layers.3.self_attn.o_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.Qidxs', 'model.layers.3.self_attn.qkv_proj.SU', 'model.layers.3.self_attn.qkv_proj.SV', 'model.layers.3.self_attn.qkv_proj.Wscale', 'model.layers.3.self_attn.qkv_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.fuse_scales', 'model.layers.30.mlp.down_proj.Qidxs', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.Wscale', 'model.layers.30.mlp.down_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.Qidxs', 'model.layers.30.mlp.upgate_proj.SU', 'model.layers.30.mlp.upgate_proj.SV', 'model.layers.30.mlp.upgate_proj.Wscale', 'model.layers.30.mlp.upgate_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.fuse_scales', 'model.layers.30.self_attn.o_proj.Qidxs', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.Wscale', 'model.layers.30.self_attn.o_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.Qidxs', 'model.layers.30.self_attn.qkv_proj.SU', 'model.layers.30.self_attn.qkv_proj.SV', 'model.layers.30.self_attn.qkv_proj.Wscale', 'model.layers.30.self_attn.qkv_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.fuse_scales', 'model.layers.31.mlp.down_proj.Qidxs', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.Wscale', 'model.layers.31.mlp.down_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.Qidxs', 'model.layers.31.mlp.upgate_proj.SU', 'model.layers.31.mlp.upgate_proj.SV', 'model.layers.31.mlp.upgate_proj.Wscale', 'model.layers.31.mlp.upgate_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.fuse_scales', 'model.layers.31.self_attn.o_proj.Qidxs', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.Wscale', 'model.layers.31.self_attn.o_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.Qidxs', 'model.layers.31.self_attn.qkv_proj.SU', 'model.layers.31.self_attn.qkv_proj.SV', 'model.layers.31.self_attn.qkv_proj.Wscale', 'model.layers.31.self_attn.qkv_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.fuse_scales', 'model.layers.4.mlp.down_proj.Qidxs', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.Wscale', 'model.layers.4.mlp.down_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.Qidxs', 'model.layers.4.mlp.upgate_proj.SU', 'model.layers.4.mlp.upgate_proj.SV', 'model.layers.4.mlp.upgate_proj.Wscale', 'model.layers.4.mlp.upgate_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.fuse_scales', 'model.layers.4.self_attn.o_proj.Qidxs', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.Wscale', 'model.layers.4.self_attn.o_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.Qidxs', 'model.layers.4.self_attn.qkv_proj.SU', 'model.layers.4.self_attn.qkv_proj.SV', 'model.layers.4.self_attn.qkv_proj.Wscale', 'model.layers.4.self_attn.qkv_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.fuse_scales', 'model.layers.5.mlp.down_proj.Qidxs', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.Wscale', 'model.layers.5.mlp.down_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.Qidxs', 'model.layers.5.mlp.upgate_proj.SU', 'model.layers.5.mlp.upgate_proj.SV', 'model.layers.5.mlp.upgate_proj.Wscale', 'model.layers.5.mlp.upgate_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.fuse_scales', 'model.layers.5.self_attn.o_proj.Qidxs', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.Wscale', 'model.layers.5.self_attn.o_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.Qidxs', 'model.layers.5.self_attn.qkv_proj.SU', 'model.layers.5.self_attn.qkv_proj.SV', 'model.layers.5.self_attn.qkv_proj.Wscale', 'model.layers.5.self_attn.qkv_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.fuse_scales', 'model.layers.6.mlp.down_proj.Qidxs', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.Wscale', 'model.layers.6.mlp.down_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.Qidxs', 'model.layers.6.mlp.upgate_proj.SU', 'model.layers.6.mlp.upgate_proj.SV', 'model.layers.6.mlp.upgate_proj.Wscale', 'model.layers.6.mlp.upgate_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.fuse_scales', 'model.layers.6.self_attn.o_proj.Qidxs', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.Wscale', 'model.layers.6.self_attn.o_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.Qidxs', 'model.layers.6.self_attn.qkv_proj.SU', 'model.layers.6.self_attn.qkv_proj.SV', 'model.layers.6.self_attn.qkv_proj.Wscale', 'model.layers.6.self_attn.qkv_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.fuse_scales', 'model.layers.7.mlp.down_proj.Qidxs', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.Wscale', 'model.layers.7.mlp.down_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.Qidxs', 'model.layers.7.mlp.upgate_proj.SU', 'model.layers.7.mlp.upgate_proj.SV', 'model.layers.7.mlp.upgate_proj.Wscale', 'model.layers.7.mlp.upgate_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.fuse_scales', 'model.layers.7.self_attn.o_proj.Qidxs', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.Wscale', 'model.layers.7.self_attn.o_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.Qidxs', 'model.layers.7.self_attn.qkv_proj.SU', 'model.layers.7.self_attn.qkv_proj.SV', 'model.layers.7.self_attn.qkv_proj.Wscale', 'model.layers.7.self_attn.qkv_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.fuse_scales', 'model.layers.8.mlp.down_proj.Qidxs', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.Wscale', 'model.layers.8.mlp.down_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.Qidxs', 'model.layers.8.mlp.upgate_proj.SU', 'model.layers.8.mlp.upgate_proj.SV', 'model.layers.8.mlp.upgate_proj.Wscale', 'model.layers.8.mlp.upgate_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.fuse_scales', 'model.layers.8.self_attn.o_proj.Qidxs', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.Wscale', 'model.layers.8.self_attn.o_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.Qidxs', 'model.layers.8.self_attn.qkv_proj.SU', 'model.layers.8.self_attn.qkv_proj.SV', 'model.layers.8.self_attn.qkv_proj.Wscale', 'model.layers.8.self_attn.qkv_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.fuse_scales', 'model.layers.9.mlp.down_proj.Qidxs', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.Wscale', 'model.layers.9.mlp.down_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.Qidxs', 'model.layers.9.mlp.upgate_proj.SU', 'model.layers.9.mlp.upgate_proj.SV', 'model.layers.9.mlp.upgate_proj.Wscale', 'model.layers.9.mlp.upgate_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.fuse_scales', 'model.layers.9.self_attn.o_proj.Qidxs', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.Wscale', 'model.layers.9.self_attn.o_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.Qidxs', 'model.layers.9.self_attn.qkv_proj.SU', 'model.layers.9.self_attn.qkv_proj.SV', 'model.layers.9.self_attn.qkv_proj.Wscale', 'model.layers.9.self_attn.qkv_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.fuse_scales']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
I0908 03:37:33.769734 103130 hfize_llama.py:78] loaded layer 0 down
I0908 03:37:33.915443 103130 hfize_llama.py:78] loaded layer 1 down
I0908 03:37:34.059767 103130 hfize_llama.py:78] loaded layer 2 down
I0908 03:37:34.184677 103130 hfize_llama.py:78] loaded layer 3 down
I0908 03:37:34.331057 103130 hfize_llama.py:78] loaded layer 4 down
I0908 03:37:34.480770 103130 hfize_llama.py:78] loaded layer 5 down
I0908 03:37:34.593750 103130 hfize_llama.py:78] loaded layer 6 down
I0908 03:37:34.775274 103130 hfize_llama.py:78] loaded layer 7 down
I0908 03:37:34.922617 103130 hfize_llama.py:78] loaded layer 8 down
I0908 03:37:35.102321 103130 hfize_llama.py:78] loaded layer 9 down
I0908 03:37:35.266627 103130 hfize_llama.py:78] loaded layer 10 down
I0908 03:37:35.469777 103130 hfize_llama.py:78] loaded layer 11 down
I0908 03:37:35.664154 103130 hfize_llama.py:78] loaded layer 12 down
I0908 03:37:35.761406 103130 hfize_llama.py:78] loaded layer 13 down
I0908 03:37:35.935011 103130 hfize_llama.py:78] loaded layer 14 down
I0908 03:37:36.071337 103130 hfize_llama.py:78] loaded layer 15 down
I0908 03:37:36.242409 103130 hfize_llama.py:78] loaded layer 16 down
I0908 03:37:36.361576 103130 hfize_llama.py:78] loaded layer 17 down
I0908 03:37:36.506746 103130 hfize_llama.py:78] loaded layer 18 down
I0908 03:37:36.711863 103130 hfize_llama.py:78] loaded layer 19 down
I0908 03:37:36.911972 103130 hfize_llama.py:78] loaded layer 20 down
I0908 03:37:37.077975 103130 hfize_llama.py:78] loaded layer 21 down
I0908 03:37:37.574583 103130 hfize_llama.py:78] loaded layer 22 down
I0908 03:37:37.821532 103130 hfize_llama.py:78] loaded layer 23 down
I0908 03:37:38.008958 103130 hfize_llama.py:78] loaded layer 24 down
I0908 03:37:38.179542 103130 hfize_llama.py:78] loaded layer 25 down
I0908 03:37:38.385223 103130 hfize_llama.py:78] loaded layer 26 down
I0908 03:37:38.584967 103130 hfize_llama.py:78] loaded layer 27 down
I0908 03:37:38.726988 103130 hfize_llama.py:78] loaded layer 28 down
I0908 03:37:38.880696 103130 hfize_llama.py:78] loaded layer 29 down
I0908 03:37:39.065627 103130 hfize_llama.py:78] loaded layer 30 down
I0908 03:37:39.188854 103130 hfize_llama.py:78] loaded layer 31 down
I0908 03:37:39.189034 103130 hfize_llama.py:80] saving model...
I0908 03:37:47.500329 103130 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
I0908 03:37:48.219779 103130 hfize_llama.py:87] successfully loaded hfized model
I0908 03:37:48.219951 103130 hfize_llama.py:89] generating some text...
I0908 03:38:05.961135 103130 hfize_llama.py:100] <s> It is a truth universally acknowledged that when one is in the midst of a divorce, one must find a way to deal with all the emotions, and to try to stay calm and rational. The divorce process can be a difficult time for all parties involved, and it is important to find a way to deal with the emotions that come with
I0908 03:38:05.961357 103130 hfize_llama.py:101] elapsed: 17.741363286972046
W0908 03:38:10.628857 103278 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 03:38:10.633049 103278 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0908 03:38:10.719377 103278 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0908 03:38:32.145951 103278 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
W0908 03:38:35.109499 103278 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

  0%|          | 0/83 [00:00<?, ?it/s]I0908 03:39:10.603737 103278 graph_wrapper.py:36] Built CUDA graph of model.
avg_loss = 1.800432562828064:   0%|          | 0/83 [00:02<?, ?it/s]avg_loss = 1.800432562828064:   1%|          | 1/83 [00:02<03:10,  2.32s/it]avg_loss = 2.0238857865333557:   1%|          | 1/83 [00:02<03:10,  2.32s/it]avg_loss = 2.0238857865333557:   2%|▏         | 2/83 [00:02<01:34,  1.17s/it]avg_loss = 1.915138880411784:   2%|▏         | 2/83 [00:03<01:34,  1.17s/it] avg_loss = 1.915138880411784:   4%|▎         | 3/83 [00:03<01:04,  1.25it/s]avg_loss = 1.7988147735595703:   4%|▎         | 3/83 [00:03<01:04,  1.25it/s]avg_loss = 1.7988147735595703:   5%|▍         | 4/83 [00:03<00:49,  1.59it/s]avg_loss = 1.7978428840637206:   5%|▍         | 4/83 [00:03<00:49,  1.59it/s]avg_loss = 1.7978428840637206:   6%|▌         | 5/83 [00:03<00:41,  1.87it/s]avg_loss = 1.8197418649991353:   6%|▌         | 5/83 [00:04<00:41,  1.87it/s]avg_loss = 1.8197418649991353:   7%|▋         | 6/83 [00:04<00:36,  2.10it/s]avg_loss = 1.8248290164130074:   7%|▋         | 6/83 [00:04<00:36,  2.10it/s]avg_loss = 1.8248290164130074:   8%|▊         | 7/83 [00:04<00:33,  2.28it/s]avg_loss = 1.8567865937948227:   8%|▊         | 7/83 [00:04<00:33,  2.28it/s]avg_loss = 1.8567865937948227:  10%|▉         | 8/83 [00:04<00:31,  2.41it/s]avg_loss = 1.8857481082280476:  10%|▉         | 8/83 [00:05<00:31,  2.41it/s]avg_loss = 1.8857481082280476:  11%|█         | 9/83 [00:05<00:29,  2.50it/s]avg_loss = 1.9109119057655335:  11%|█         | 9/83 [00:05<00:29,  2.50it/s]avg_loss = 1.9109119057655335:  12%|█▏        | 10/83 [00:05<00:28,  2.57it/s]avg_loss = 1.8986366445367986:  12%|█▏        | 10/83 [00:05<00:28,  2.57it/s]avg_loss = 1.8986366445367986:  13%|█▎        | 11/83 [00:05<00:27,  2.62it/s]avg_loss = 1.8986334403355916:  13%|█▎        | 11/83 [00:06<00:27,  2.62it/s]avg_loss = 1.8986334403355916:  14%|█▍        | 12/83 [00:06<00:26,  2.66it/s]avg_loss = 1.9087336796980638:  14%|█▍        | 12/83 [00:06<00:26,  2.66it/s]avg_loss = 1.9087336796980638:  16%|█▌        | 13/83 [00:06<00:26,  2.68it/s]avg_loss = 1.9160202741622925:  16%|█▌        | 13/83 [00:07<00:26,  2.68it/s]avg_loss = 1.9160202741622925:  17%|█▋        | 14/83 [00:07<00:25,  2.70it/s]avg_loss = 1.926576026280721:  17%|█▋        | 14/83 [00:07<00:25,  2.70it/s] avg_loss = 1.926576026280721:  18%|█▊        | 15/83 [00:07<00:25,  2.71it/s]avg_loss = 1.944032222032547:  18%|█▊        | 15/83 [00:07<00:25,  2.71it/s]avg_loss = 1.944032222032547:  19%|█▉        | 16/83 [00:07<00:24,  2.72it/s]avg_loss = 1.9508044439203598:  19%|█▉        | 16/83 [00:08<00:24,  2.72it/s]avg_loss = 1.9508044439203598:  20%|██        | 17/83 [00:08<00:24,  2.73it/s]avg_loss = 1.9403730034828186:  20%|██        | 17/83 [00:08<00:24,  2.73it/s]avg_loss = 1.9403730034828186:  22%|██▏       | 18/83 [00:08<00:23,  2.73it/s]avg_loss = 1.9309813223387067:  22%|██▏       | 18/83 [00:08<00:23,  2.73it/s]avg_loss = 1.9309813223387067:  23%|██▎       | 19/83 [00:08<00:23,  2.74it/s]avg_loss = 1.932072788476944:  23%|██▎       | 19/83 [00:09<00:23,  2.74it/s] avg_loss = 1.932072788476944:  24%|██▍       | 20/83 [00:09<00:23,  2.74it/s]avg_loss = 1.9189965554646082:  24%|██▍       | 20/83 [00:09<00:23,  2.74it/s]avg_loss = 1.9189965554646082:  25%|██▌       | 21/83 [00:09<00:22,  2.74it/s]avg_loss = 1.8938751383261248:  25%|██▌       | 21/83 [00:09<00:22,  2.74it/s]avg_loss = 1.8938751383261248:  27%|██▋       | 22/83 [00:09<00:22,  2.74it/s]avg_loss = 1.869148850440979:  27%|██▋       | 22/83 [00:10<00:22,  2.74it/s] avg_loss = 1.869148850440979:  28%|██▊       | 23/83 [00:10<00:21,  2.74it/s]avg_loss = 1.8634629050890605:  28%|██▊       | 23/83 [00:10<00:21,  2.74it/s]avg_loss = 1.8634629050890605:  29%|██▉       | 24/83 [00:10<00:21,  2.74it/s]avg_loss = 1.8835592555999756:  29%|██▉       | 24/83 [00:11<00:21,  2.74it/s]avg_loss = 1.8835592555999756:  30%|███       | 25/83 [00:11<00:21,  2.74it/s]avg_loss = 1.8985089247043316:  30%|███       | 25/83 [00:11<00:21,  2.74it/s]avg_loss = 1.8985089247043316:  31%|███▏      | 26/83 [00:11<00:20,  2.74it/s]avg_loss = 1.9025666095592357:  31%|███▏      | 26/83 [00:11<00:20,  2.74it/s]avg_loss = 1.9025666095592357:  33%|███▎      | 27/83 [00:11<00:20,  2.74it/s]avg_loss = 1.9058762618473597:  33%|███▎      | 27/83 [00:12<00:20,  2.74it/s]avg_loss = 1.9058762618473597:  34%|███▎      | 28/83 [00:12<00:20,  2.74it/s]avg_loss = 1.9040488415750965:  34%|███▎      | 28/83 [00:12<00:20,  2.74it/s]avg_loss = 1.9040488415750965:  35%|███▍      | 29/83 [00:12<00:19,  2.74it/s]avg_loss = 1.8989725550015768:  35%|███▍      | 29/83 [00:12<00:19,  2.74it/s]avg_loss = 1.8989725550015768:  36%|███▌      | 30/83 [00:12<00:19,  2.74it/s]avg_loss = 1.8903024811898508:  36%|███▌      | 30/83 [00:13<00:19,  2.74it/s]avg_loss = 1.8903024811898508:  37%|███▋      | 31/83 [00:13<00:18,  2.74it/s]avg_loss = 1.8809406869113445:  37%|███▋      | 31/83 [00:13<00:18,  2.74it/s]avg_loss = 1.8809406869113445:  39%|███▊      | 32/83 [00:13<00:18,  2.74it/s]avg_loss = 1.8674885287429348:  39%|███▊      | 32/83 [00:13<00:18,  2.74it/s]avg_loss = 1.8674885287429348:  40%|███▉      | 33/83 [00:13<00:18,  2.74it/s]avg_loss = 1.863705203813665:  40%|███▉      | 33/83 [00:14<00:18,  2.74it/s] avg_loss = 1.863705203813665:  41%|████      | 34/83 [00:14<00:17,  2.74it/s]avg_loss = 1.8703846693038941:  41%|████      | 34/83 [00:14<00:17,  2.74it/s]avg_loss = 1.8703846693038941:  42%|████▏     | 35/83 [00:14<00:17,  2.74it/s]avg_loss = 1.8783587449126773:  42%|████▏     | 35/83 [00:15<00:17,  2.74it/s]avg_loss = 1.8783587449126773:  43%|████▎     | 36/83 [00:15<00:17,  2.74it/s]avg_loss = 1.8801879560625232:  43%|████▎     | 36/83 [00:15<00:17,  2.74it/s]avg_loss = 1.8801879560625232:  45%|████▍     | 37/83 [00:15<00:16,  2.74it/s]avg_loss = 1.8748017957336025:  45%|████▍     | 37/83 [00:15<00:16,  2.74it/s]avg_loss = 1.8748017957336025:  46%|████▌     | 38/83 [00:15<00:16,  2.74it/s]avg_loss = 1.8678705539458837:  46%|████▌     | 38/83 [00:16<00:16,  2.74it/s]avg_loss = 1.8678705539458837:  47%|████▋     | 39/83 [00:16<00:16,  2.74it/s]avg_loss = 1.8617507964372635:  47%|████▋     | 39/83 [00:16<00:16,  2.74it/s]avg_loss = 1.8617507964372635:  48%|████▊     | 40/83 [00:16<00:15,  2.74it/s]avg_loss = 1.855886683231447:  48%|████▊     | 40/83 [00:16<00:15,  2.74it/s] avg_loss = 1.855886683231447:  49%|████▉     | 41/83 [00:16<00:15,  2.74it/s]avg_loss = 1.8604618112246196:  49%|████▉     | 41/83 [00:17<00:15,  2.74it/s]avg_loss = 1.8604618112246196:  51%|█████     | 42/83 [00:17<00:14,  2.74it/s]avg_loss = 1.8613207756086838:  51%|█████     | 42/83 [00:17<00:14,  2.74it/s]avg_loss = 1.8613207756086838:  52%|█████▏    | 43/83 [00:17<00:14,  2.74it/s]avg_loss = 1.8602329330010847:  52%|█████▏    | 43/83 [00:17<00:14,  2.74it/s]avg_loss = 1.8602329330010847:  53%|█████▎    | 44/83 [00:17<00:14,  2.74it/s]avg_loss = 1.860935730404324:  53%|█████▎    | 44/83 [00:18<00:14,  2.74it/s] avg_loss = 1.860935730404324:  54%|█████▍    | 45/83 [00:18<00:13,  2.74it/s]avg_loss = 1.8620693839114646:  54%|█████▍    | 45/83 [00:18<00:13,  2.74it/s]avg_loss = 1.8620693839114646:  55%|█████▌    | 46/83 [00:18<00:13,  2.74it/s]avg_loss = 1.8634018365373002:  55%|█████▌    | 46/83 [00:19<00:13,  2.74it/s]avg_loss = 1.8634018365373002:  57%|█████▋    | 47/83 [00:19<00:13,  2.74it/s]avg_loss = 1.8597215289870899:  57%|█████▋    | 47/83 [00:19<00:13,  2.74it/s]avg_loss = 1.8597215289870899:  58%|█████▊    | 48/83 [00:19<00:12,  2.74it/s]avg_loss = 1.858747876420313:  58%|█████▊    | 48/83 [00:19<00:12,  2.74it/s] avg_loss = 1.858747876420313:  59%|█████▉    | 49/83 [00:19<00:12,  2.74it/s]avg_loss = 1.8542351150512695:  59%|█████▉    | 49/83 [00:20<00:12,  2.74it/s]avg_loss = 1.8542351150512695:  60%|██████    | 50/83 [00:20<00:12,  2.74it/s]avg_loss = 1.855518577145595:  60%|██████    | 50/83 [00:20<00:12,  2.74it/s] avg_loss = 1.855518577145595:  61%|██████▏   | 51/83 [00:20<00:11,  2.74it/s]avg_loss = 1.8592583697575789:  61%|██████▏   | 51/83 [00:20<00:11,  2.74it/s]avg_loss = 1.8592583697575789:  63%|██████▎   | 52/83 [00:20<00:11,  2.74it/s]avg_loss = 1.8706905594411887:  63%|██████▎   | 52/83 [00:21<00:11,  2.74it/s]avg_loss = 1.8706905594411887:  64%|██████▍   | 53/83 [00:21<00:10,  2.74it/s]avg_loss = 1.8774298230806987:  64%|██████▍   | 53/83 [00:21<00:10,  2.74it/s]avg_loss = 1.8774298230806987:  65%|██████▌   | 54/83 [00:21<00:10,  2.74it/s]avg_loss = 1.8860014070164073:  65%|██████▌   | 54/83 [00:22<00:10,  2.74it/s]avg_loss = 1.8860014070164073:  66%|██████▋   | 55/83 [00:22<00:10,  2.74it/s]avg_loss = 1.887857871396201:  66%|██████▋   | 55/83 [00:22<00:10,  2.74it/s] avg_loss = 1.887857871396201:  67%|██████▋   | 56/83 [00:22<00:09,  2.74it/s]avg_loss = 1.888617521838138:  67%|██████▋   | 56/83 [00:22<00:09,  2.74it/s]avg_loss = 1.888617521838138:  69%|██████▊   | 57/83 [00:22<00:09,  2.74it/s]avg_loss = 1.8869348620546276:  69%|██████▊   | 57/83 [00:23<00:09,  2.74it/s]avg_loss = 1.8869348620546276:  70%|██████▉   | 58/83 [00:23<00:09,  2.74it/s]avg_loss = 1.8884882724891274:  70%|██████▉   | 58/83 [00:23<00:09,  2.74it/s]avg_loss = 1.8884882724891274:  71%|███████   | 59/83 [00:23<00:08,  2.74it/s]avg_loss = 1.8900541583697001:  71%|███████   | 59/83 [00:23<00:08,  2.74it/s]avg_loss = 1.8900541583697001:  72%|███████▏  | 60/83 [00:23<00:08,  2.74it/s]avg_loss = 1.891869201034796:  72%|███████▏  | 60/83 [00:24<00:08,  2.74it/s] avg_loss = 1.891869201034796:  73%|███████▎  | 61/83 [00:24<00:08,  2.74it/s]avg_loss = 1.8918409443670703:  73%|███████▎  | 61/83 [00:24<00:08,  2.74it/s]avg_loss = 1.8918409443670703:  75%|███████▍  | 62/83 [00:24<00:07,  2.74it/s]avg_loss = 1.888686357982575:  75%|███████▍  | 62/83 [00:24<00:07,  2.74it/s] avg_loss = 1.888686357982575:  76%|███████▌  | 63/83 [00:24<00:07,  2.74it/s]avg_loss = 1.8860401902347803:  76%|███████▌  | 63/83 [00:25<00:07,  2.74it/s]avg_loss = 1.8860401902347803:  77%|███████▋  | 64/83 [00:25<00:06,  2.74it/s]avg_loss = 1.885210367349478:  77%|███████▋  | 64/83 [00:25<00:06,  2.74it/s] avg_loss = 1.885210367349478:  78%|███████▊  | 65/83 [00:25<00:06,  2.74it/s]avg_loss = 1.8872500983151523:  78%|███████▊  | 65/83 [00:26<00:06,  2.74it/s]avg_loss = 1.8872500983151523:  80%|███████▉  | 66/83 [00:26<00:06,  2.74it/s]avg_loss = 1.8897003522559779:  80%|███████▉  | 66/83 [00:26<00:06,  2.74it/s]avg_loss = 1.8897003522559779:  81%|████████  | 67/83 [00:26<00:05,  2.74it/s]avg_loss = 1.8868891105932348:  81%|████████  | 67/83 [00:26<00:05,  2.74it/s]avg_loss = 1.8868891105932348:  82%|████████▏ | 68/83 [00:26<00:05,  2.74it/s]avg_loss = 1.8881330369175344:  82%|████████▏ | 68/83 [00:27<00:05,  2.74it/s]avg_loss = 1.8881330369175344:  83%|████████▎ | 69/83 [00:27<00:05,  2.74it/s]avg_loss = 1.8854727625846863:  83%|████████▎ | 69/83 [00:27<00:05,  2.74it/s]avg_loss = 1.8854727625846863:  84%|████████▍ | 70/83 [00:27<00:04,  2.74it/s]avg_loss = 1.8838521443622214:  84%|████████▍ | 70/83 [00:27<00:04,  2.74it/s]avg_loss = 1.8838521443622214:  86%|████████▌ | 71/83 [00:27<00:04,  2.74it/s]avg_loss = 1.8835260321696599:  86%|████████▌ | 71/83 [00:28<00:04,  2.74it/s]avg_loss = 1.8835260321696599:  87%|████████▋ | 72/83 [00:28<00:04,  2.74it/s]avg_loss = 1.8813924087237006:  87%|████████▋ | 72/83 [00:28<00:04,  2.74it/s]avg_loss = 1.8813924087237006:  88%|████████▊ | 73/83 [00:28<00:03,  2.74it/s]avg_loss = 1.8786161799688597:  88%|████████▊ | 73/83 [00:28<00:03,  2.74it/s]avg_loss = 1.8786161799688597:  89%|████████▉ | 74/83 [00:28<00:03,  2.74it/s]avg_loss = 1.8779952065149943:  89%|████████▉ | 74/83 [00:29<00:03,  2.74it/s]avg_loss = 1.8779952065149943:  90%|█████████ | 75/83 [00:29<00:02,  2.74it/s]avg_loss = 1.8766931875755912:  90%|█████████ | 75/83 [00:29<00:02,  2.74it/s]avg_loss = 1.8766931875755912:  92%|█████████▏| 76/83 [00:29<00:02,  2.74it/s]avg_loss = 1.878434662695055:  92%|█████████▏| 76/83 [00:30<00:02,  2.74it/s] avg_loss = 1.878434662695055:  93%|█████████▎| 77/83 [00:30<00:02,  2.74it/s]avg_loss = 1.8776134390097399:  93%|█████████▎| 77/83 [00:30<00:02,  2.74it/s]avg_loss = 1.8776134390097399:  94%|█████████▍| 78/83 [00:30<00:01,  2.74it/s]avg_loss = 1.8702220735670645:  94%|█████████▍| 78/83 [00:30<00:01,  2.74it/s]avg_loss = 1.8702220735670645:  95%|█████████▌| 79/83 [00:30<00:01,  2.74it/s]avg_loss = 1.8720261842012405:  95%|█████████▌| 79/83 [00:31<00:01,  2.74it/s]avg_loss = 1.8720261842012405:  96%|█████████▋| 80/83 [00:31<00:01,  2.74it/s]avg_loss = 1.8751817868079668:  96%|█████████▋| 80/83 [00:31<00:01,  2.74it/s]avg_loss = 1.8751817868079668:  98%|█████████▊| 81/83 [00:31<00:00,  2.74it/s]avg_loss = 1.8761097570744956:  98%|█████████▊| 81/83 [00:31<00:00,  2.74it/s]avg_loss = 1.8761097570744956:  99%|█████████▉| 82/83 [00:31<00:00,  2.74it/s]avg_loss = 1.8782027342233314:  99%|█████████▉| 82/83 [00:32<00:00,  2.74it/s]avg_loss = 1.8782027342233314: 100%|██████████| 83/83 [00:32<00:00,  2.74it/s]avg_loss = 1.8782027342233314: 100%|██████████| 83/83 [00:32<00:00,  2.58it/s]
I0908 03:39:40.872791 103278 eval_ppl.py:63] wikitext2 perplexity: 6.541736602783203
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 4435 examples [00:00, 33034.17 examples/s]Generating train split: 13602 examples [00:00, 50104.83 examples/s]Generating train split: 22590 examples [00:00, 54829.50 examples/s]Generating train split: 32053 examples [00:00, 58030.15 examples/s]Generating train split: 41546 examples [00:00, 60339.41 examples/s]Generating train split: 50510 examples [00:00, 60378.10 examples/s]Generating train split: 59531 examples [00:01, 60531.96 examples/s]Generating train split: 68750 examples [00:01, 61150.26 examples/s]Generating train split: 77743 examples [00:01, 60964.98 examples/s]Generating train split: 86966 examples [00:01, 61153.76 examples/s]Generating train split: 95787 examples [00:01, 60697.40 examples/s]Generating train split: 104859 examples [00:01, 61007.03 examples/s]Generating train split: 113857 examples [00:01, 60740.81 examples/s]Generating train split: 122789 examples [00:02, 60457.06 examples/s]Generating train split: 132051 examples [00:02, 61110.42 examples/s]Generating train split: 141177 examples [00:02, 61237.46 examples/s]Generating train split: 150440 examples [00:02, 61908.35 examples/s]Generating train split: 159772 examples [00:02, 62652.27 examples/s]Generating train split: 168803 examples [00:02, 62152.83 examples/s]Generating train split: 177746 examples [00:02, 61701.12 examples/s]Generating train split: 186637 examples [00:03, 60620.58 examples/s]Generating train split: 195882 examples [00:03, 60811.94 examples/s]Generating train split: 204917 examples [00:03, 60811.92 examples/s]Generating train split: 214014 examples [00:03, 60532.98 examples/s]Generating train split: 223006 examples [00:03, 59972.57 examples/s]Generating train split: 232246 examples [00:03, 60267.60 examples/s]Generating train split: 241218 examples [00:04, 58346.51 examples/s]Generating train split: 250396 examples [00:04, 59563.20 examples/s]Generating train split: 259470 examples [00:04, 60346.78 examples/s]Generating train split: 268486 examples [00:04, 60673.07 examples/s]Generating train split: 277868 examples [00:04, 61251.32 examples/s]Generating train split: 286955 examples [00:04, 61221.10 examples/s]Generating train split: 295901 examples [00:04, 61059.78 examples/s]Generating train split: 304853 examples [00:05, 60843.62 examples/s]Generating train split: 314057 examples [00:05, 61318.24 examples/s]Generating train split: 323150 examples [00:05, 61433.24 examples/s]Generating train split: 332325 examples [00:05, 61642.86 examples/s]Generating train split: 341256 examples [00:05, 61217.32 examples/s]Generating train split: 350356 examples [00:05, 60908.51 examples/s]Generating train split: 356317 examples [00:05, 60400.24 examples/s]
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 9120 examples [00:00, 59471.75 examples/s]Generating validation split: 18229 examples [00:00, 59722.31 examples/s]Generating validation split: 27647 examples [00:00, 61486.31 examples/s]Generating validation split: 36991 examples [00:00, 62344.94 examples/s]Generating validation split: 45576 examples [00:00, 62455.38 examples/s]Generating validation split: 45576 examples [00:00, 61801.13 examples/s]
W0908 03:39:53.424194 103278 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

  0%|          | 0/256 [00:00<?, ?it/s]I0908 03:41:17.289999 103278 graph_wrapper.py:36] Built CUDA graph of model.
avg_loss = 1.91005539894104:   0%|          | 0/256 [00:01<?, ?it/s]avg_loss = 1.91005539894104:   0%|          | 1/256 [00:01<04:29,  1.06s/it]avg_loss = 1.9489168524742126:   0%|          | 1/256 [00:01<04:29,  1.06s/it]avg_loss = 1.9489168524742126:   1%|          | 2/256 [00:01<02:44,  1.54it/s]avg_loss = 2.144954721132914:   1%|          | 2/256 [00:01<02:44,  1.54it/s] avg_loss = 2.144954721132914:   1%|          | 3/256 [00:01<02:11,  1.93it/s]avg_loss = 2.1535896360874176:   1%|          | 3/256 [00:02<02:11,  1.93it/s]avg_loss = 2.1535896360874176:   2%|▏         | 4/256 [00:02<01:55,  2.19it/s]avg_loss = 2.0635371685028074:   2%|▏         | 4/256 [00:02<01:55,  2.19it/s]avg_loss = 2.0635371685028074:   2%|▏         | 5/256 [00:02<01:46,  2.36it/s]avg_loss = 2.0899124145507812:   2%|▏         | 5/256 [00:02<01:46,  2.36it/s]avg_loss = 2.0899124145507812:   2%|▏         | 6/256 [00:02<01:40,  2.48it/s]avg_loss = 2.0820041043417796:   2%|▏         | 6/256 [00:03<01:40,  2.48it/s]avg_loss = 2.0820041043417796:   3%|▎         | 7/256 [00:03<01:37,  2.56it/s]avg_loss = 2.1316202878952026:   3%|▎         | 7/256 [00:03<01:37,  2.56it/s]avg_loss = 2.1316202878952026:   3%|▎         | 8/256 [00:03<01:34,  2.62it/s]avg_loss = 2.128099944856432:   3%|▎         | 8/256 [00:03<01:34,  2.62it/s] avg_loss = 2.128099944856432:   4%|▎         | 9/256 [00:03<01:33,  2.65it/s]avg_loss = 2.038030505180359:   4%|▎         | 9/256 [00:04<01:33,  2.65it/s]avg_loss = 2.038030505180359:   4%|▍         | 10/256 [00:04<01:31,  2.68it/s]avg_loss = 2.0464235002344306:   4%|▍         | 10/256 [00:04<01:31,  2.68it/s]avg_loss = 2.0464235002344306:   4%|▍         | 11/256 [00:04<01:30,  2.70it/s]avg_loss = 2.035177489121755:   4%|▍         | 11/256 [00:05<01:30,  2.70it/s] avg_loss = 2.035177489121755:   5%|▍         | 12/256 [00:05<01:29,  2.71it/s]avg_loss = 2.176899249737079:   5%|▍         | 12/256 [00:05<01:29,  2.71it/s]avg_loss = 2.176899249737079:   5%|▌         | 13/256 [00:05<01:29,  2.72it/s]avg_loss = 2.0700820216110776:   5%|▌         | 13/256 [00:05<01:29,  2.72it/s]avg_loss = 2.0700820216110776:   5%|▌         | 14/256 [00:05<01:28,  2.73it/s]avg_loss = 2.0938963611920673:   5%|▌         | 14/256 [00:06<01:28,  2.73it/s]avg_loss = 2.0938963611920673:   6%|▌         | 15/256 [00:06<01:28,  2.73it/s]avg_loss = 2.100008998066187:   6%|▌         | 15/256 [00:06<01:28,  2.73it/s] avg_loss = 2.100008998066187:   6%|▋         | 16/256 [00:06<01:27,  2.74it/s]avg_loss = 2.094130112844355:   6%|▋         | 16/256 [00:06<01:27,  2.74it/s]avg_loss = 2.094130112844355:   7%|▋         | 17/256 [00:06<01:27,  2.74it/s]avg_loss = 2.095266663365894:   7%|▋         | 17/256 [00:07<01:27,  2.74it/s]avg_loss = 2.095266663365894:   7%|▋         | 18/256 [00:07<01:26,  2.74it/s]avg_loss = 2.089670717716217:   7%|▋         | 18/256 [00:07<01:26,  2.74it/s]avg_loss = 2.089670717716217:   7%|▋         | 19/256 [00:07<01:26,  2.74it/s]avg_loss = 2.0947638243436812:   7%|▋         | 19/256 [00:07<01:26,  2.74it/s]avg_loss = 2.0947638243436812:   8%|▊         | 20/256 [00:07<01:26,  2.74it/s]avg_loss = 2.036071811403547:   8%|▊         | 20/256 [00:08<01:26,  2.74it/s] avg_loss = 2.036071811403547:   8%|▊         | 21/256 [00:08<01:25,  2.74it/s]avg_loss = 2.054799784313549:   8%|▊         | 21/256 [00:08<01:25,  2.74it/s]avg_loss = 2.054799784313549:   9%|▊         | 22/256 [00:08<01:25,  2.74it/s]avg_loss = 2.059353082076363:   9%|▊         | 22/256 [00:09<01:25,  2.74it/s]avg_loss = 2.059353082076363:   9%|▉         | 23/256 [00:09<01:24,  2.74it/s]avg_loss = 2.063443193833033:   9%|▉         | 23/256 [00:09<01:24,  2.74it/s]avg_loss = 2.063443193833033:   9%|▉         | 24/256 [00:09<01:24,  2.74it/s]avg_loss = 2.0699942970275877:   9%|▉         | 24/256 [00:09<01:24,  2.74it/s]avg_loss = 2.0699942970275877:  10%|▉         | 25/256 [00:09<01:24,  2.74it/s]avg_loss = 2.0672672115839443:  10%|▉         | 25/256 [00:10<01:24,  2.74it/s]avg_loss = 2.0672672115839443:  10%|█         | 26/256 [00:10<01:23,  2.74it/s]avg_loss = 2.0650912170056945:  10%|█         | 26/256 [00:10<01:23,  2.74it/s]avg_loss = 2.0650912170056945:  11%|█         | 27/256 [00:10<01:23,  2.74it/s]avg_loss = 2.0446750947407315:  11%|█         | 27/256 [00:10<01:23,  2.74it/s]avg_loss = 2.0446750947407315:  11%|█         | 28/256 [00:10<01:23,  2.74it/s]avg_loss = 2.0846863943954994:  11%|█         | 28/256 [00:11<01:23,  2.74it/s]avg_loss = 2.0846863943954994:  11%|█▏        | 29/256 [00:11<01:22,  2.74it/s]avg_loss = 2.062406826019287:  11%|█▏        | 29/256 [00:11<01:22,  2.74it/s] avg_loss = 2.062406826019287:  12%|█▏        | 30/256 [00:11<01:22,  2.74it/s]avg_loss = 2.040829727726598:  12%|█▏        | 30/256 [00:11<01:22,  2.74it/s]avg_loss = 2.040829727726598:  12%|█▏        | 31/256 [00:11<01:21,  2.74it/s]avg_loss = 1.99272982776165:  12%|█▏        | 31/256 [00:12<01:21,  2.74it/s] avg_loss = 1.99272982776165:  12%|█▎        | 32/256 [00:12<01:21,  2.74it/s]avg_loss = 1.994185447692871:  12%|█▎        | 32/256 [00:12<01:21,  2.74it/s]avg_loss = 1.994185447692871:  13%|█▎        | 33/256 [00:12<01:21,  2.74it/s]avg_loss = 2.0033752777997185:  13%|█▎        | 33/256 [00:13<01:21,  2.74it/s]avg_loss = 2.0033752777997185:  13%|█▎        | 34/256 [00:13<01:20,  2.74it/s]avg_loss = 2.019048949650356:  13%|█▎        | 34/256 [00:13<01:20,  2.74it/s] avg_loss = 2.019048949650356:  14%|█▎        | 35/256 [00:13<01:20,  2.74it/s]avg_loss = 2.0335773825645447:  14%|█▎        | 35/256 [00:13<01:20,  2.74it/s]avg_loss = 2.0335773825645447:  14%|█▍        | 36/256 [00:13<01:20,  2.75it/s]avg_loss = 2.0228363017778137:  14%|█▍        | 36/256 [00:14<01:20,  2.75it/s]avg_loss = 2.0228363017778137:  14%|█▍        | 37/256 [00:14<01:19,  2.74it/s]avg_loss = 2.0223241887594523:  14%|█▍        | 37/256 [00:14<01:19,  2.74it/s]avg_loss = 2.0223241887594523:  15%|█▍        | 38/256 [00:14<01:19,  2.74it/s]avg_loss = 2.019445920601869:  15%|█▍        | 38/256 [00:14<01:19,  2.74it/s] avg_loss = 2.019445920601869:  15%|█▌        | 39/256 [00:14<01:19,  2.75it/s]avg_loss = 2.0446871399879454:  15%|█▌        | 39/256 [00:15<01:19,  2.75it/s]avg_loss = 2.0446871399879454:  16%|█▌        | 40/256 [00:15<01:18,  2.75it/s]avg_loss = 2.046920910114195:  16%|█▌        | 40/256 [00:15<01:18,  2.75it/s] avg_loss = 2.046920910114195:  16%|█▌        | 41/256 [00:15<01:18,  2.75it/s]avg_loss = 2.0823343765167963:  16%|█▌        | 41/256 [00:15<01:18,  2.75it/s]avg_loss = 2.0823343765167963:  16%|█▋        | 42/256 [00:15<01:17,  2.74it/s]avg_loss = 2.083904976068541:  16%|█▋        | 42/256 [00:16<01:17,  2.74it/s] avg_loss = 2.083904976068541:  17%|█▋        | 43/256 [00:16<01:17,  2.74it/s]avg_loss = 2.0872450958598745:  17%|█▋        | 43/256 [00:16<01:17,  2.74it/s]avg_loss = 2.0872450958598745:  17%|█▋        | 44/256 [00:16<01:17,  2.74it/s]avg_loss = 2.0835145658916896:  17%|█▋        | 44/256 [00:17<01:17,  2.74it/s]avg_loss = 2.0835145658916896:  18%|█▊        | 45/256 [00:17<01:16,  2.74it/s]avg_loss = 2.082992540753406:  18%|█▊        | 45/256 [00:17<01:16,  2.74it/s] avg_loss = 2.082992540753406:  18%|█▊        | 46/256 [00:17<01:16,  2.74it/s]avg_loss = 2.084991584432886:  18%|█▊        | 46/256 [00:17<01:16,  2.74it/s]avg_loss = 2.084991584432886:  18%|█▊        | 47/256 [00:17<01:16,  2.74it/s]avg_loss = 2.0767293522755303:  18%|█▊        | 47/256 [00:18<01:16,  2.74it/s]avg_loss = 2.0767293522755303:  19%|█▉        | 48/256 [00:18<01:15,  2.74it/s]avg_loss = 2.0810026538615323:  19%|█▉        | 48/256 [00:18<01:15,  2.74it/s]avg_loss = 2.0810026538615323:  19%|█▉        | 49/256 [00:18<01:15,  2.74it/s]avg_loss = 2.0804011631011963:  19%|█▉        | 49/256 [00:18<01:15,  2.74it/s]avg_loss = 2.0804011631011963:  20%|█▉        | 50/256 [00:18<01:15,  2.74it/s]avg_loss = 2.065057244955325:  20%|█▉        | 50/256 [00:19<01:15,  2.74it/s] avg_loss = 2.065057244955325:  20%|█▉        | 51/256 [00:19<01:14,  2.74it/s]avg_loss = 2.0450090720103336:  20%|█▉        | 51/256 [00:19<01:14,  2.74it/s]avg_loss = 2.0450090720103336:  20%|██        | 52/256 [00:19<01:14,  2.74it/s]avg_loss = 2.039933314863241:  20%|██        | 52/256 [00:19<01:14,  2.74it/s] avg_loss = 2.039933314863241:  21%|██        | 53/256 [00:19<01:13,  2.74it/s]avg_loss = 2.0371142427126565:  21%|██        | 53/256 [00:20<01:13,  2.74it/s]avg_loss = 2.0371142427126565:  21%|██        | 54/256 [00:20<01:13,  2.74it/s]avg_loss = 2.0474177772348576:  21%|██        | 54/256 [00:20<01:13,  2.74it/s]avg_loss = 2.0474177772348576:  21%|██▏       | 55/256 [00:20<01:13,  2.74it/s]avg_loss = 2.0346796895776476:  21%|██▏       | 55/256 [00:21<01:13,  2.74it/s]avg_loss = 2.0346796895776476:  22%|██▏       | 56/256 [00:21<01:12,  2.75it/s]avg_loss = 2.0169323139023363:  22%|██▏       | 56/256 [00:21<01:12,  2.75it/s]avg_loss = 2.0169323139023363:  22%|██▏       | 57/256 [00:21<01:12,  2.75it/s]avg_loss = 2.0188089465272836:  22%|██▏       | 57/256 [00:21<01:12,  2.75it/s]avg_loss = 2.0188089465272836:  23%|██▎       | 58/256 [00:21<01:12,  2.75it/s]avg_loss = 1.9998298944053003:  23%|██▎       | 58/256 [00:22<01:12,  2.75it/s]avg_loss = 1.9998298944053003:  23%|██▎       | 59/256 [00:22<01:11,  2.74it/s]avg_loss = 2.0073655684789022:  23%|██▎       | 59/256 [00:22<01:11,  2.74it/s]avg_loss = 2.0073655684789022:  23%|██▎       | 60/256 [00:22<01:11,  2.74it/s]avg_loss = 2.0062418003551294:  23%|██▎       | 60/256 [00:22<01:11,  2.74it/s]avg_loss = 2.0062418003551294:  24%|██▍       | 61/256 [00:22<01:11,  2.74it/s]avg_loss = 2.0178162570922606:  24%|██▍       | 61/256 [00:23<01:11,  2.74it/s]avg_loss = 2.0178162570922606:  24%|██▍       | 62/256 [00:23<01:10,  2.74it/s]avg_loss = 2.0235543345648144:  24%|██▍       | 62/256 [00:23<01:10,  2.74it/s]avg_loss = 2.0235543345648144:  25%|██▍       | 63/256 [00:23<01:10,  2.74it/s]avg_loss = 2.025636961683631:  25%|██▍       | 63/256 [00:24<01:10,  2.74it/s] avg_loss = 2.025636961683631:  25%|██▌       | 64/256 [00:24<01:09,  2.74it/s]avg_loss = 2.034564918738145:  25%|██▌       | 64/256 [00:24<01:09,  2.74it/s]avg_loss = 2.034564918738145:  25%|██▌       | 65/256 [00:24<01:09,  2.74it/s]avg_loss = 2.0343471747456174:  25%|██▌       | 65/256 [00:24<01:09,  2.74it/s]avg_loss = 2.0343471747456174:  26%|██▌       | 66/256 [00:24<01:09,  2.74it/s]avg_loss = 2.034554380089489:  26%|██▌       | 66/256 [00:25<01:09,  2.74it/s] avg_loss = 2.034554380089489:  26%|██▌       | 67/256 [00:25<01:08,  2.74it/s]avg_loss = 2.0363076381823597:  26%|██▌       | 67/256 [00:25<01:08,  2.74it/s]avg_loss = 2.0363076381823597:  27%|██▋       | 68/256 [00:25<01:08,  2.74it/s]avg_loss = 2.0386725284051206:  27%|██▋       | 68/256 [00:25<01:08,  2.74it/s]avg_loss = 2.0386725284051206:  27%|██▋       | 69/256 [00:25<01:08,  2.74it/s]avg_loss = 2.0359476804733276:  27%|██▋       | 69/256 [00:26<01:08,  2.74it/s]avg_loss = 2.0359476804733276:  27%|██▋       | 70/256 [00:26<01:07,  2.74it/s]avg_loss = 2.0440881991050612:  27%|██▋       | 70/256 [00:26<01:07,  2.74it/s]avg_loss = 2.0440881991050612:  28%|██▊       | 71/256 [00:26<01:07,  2.74it/s]avg_loss = 2.046871778037813:  28%|██▊       | 71/256 [00:26<01:07,  2.74it/s] avg_loss = 2.046871778037813:  28%|██▊       | 72/256 [00:26<01:07,  2.74it/s]avg_loss = 2.0455741719023823:  28%|██▊       | 72/256 [00:27<01:07,  2.74it/s]avg_loss = 2.0455741719023823:  29%|██▊       | 73/256 [00:27<01:06,  2.74it/s]avg_loss = 2.0476846018353023:  29%|██▊       | 73/256 [00:27<01:06,  2.74it/s]avg_loss = 2.0476846018353023:  29%|██▉       | 74/256 [00:27<01:06,  2.74it/s]avg_loss = 2.046176211039225:  29%|██▉       | 74/256 [00:28<01:06,  2.74it/s] avg_loss = 2.046176211039225:  29%|██▉       | 75/256 [00:28<01:05,  2.74it/s]avg_loss = 2.048183023929596:  29%|██▉       | 75/256 [00:28<01:05,  2.74it/s]avg_loss = 2.048183023929596:  30%|██▉       | 76/256 [00:28<01:05,  2.74it/s]avg_loss = 2.0590923327904243:  30%|██▉       | 76/256 [00:28<01:05,  2.74it/s]avg_loss = 2.0590923327904243:  30%|███       | 77/256 [00:28<01:05,  2.74it/s]avg_loss = 2.0673146033898377:  30%|███       | 77/256 [00:29<01:05,  2.74it/s]avg_loss = 2.0673146033898377:  30%|███       | 78/256 [00:29<01:04,  2.74it/s]avg_loss = 2.0764154693748376:  30%|███       | 78/256 [00:29<01:04,  2.74it/s]avg_loss = 2.0764154693748376:  31%|███       | 79/256 [00:29<01:04,  2.74it/s]avg_loss = 2.0808802396059036:  31%|███       | 79/256 [00:29<01:04,  2.74it/s]avg_loss = 2.0808802396059036:  31%|███▏      | 80/256 [00:29<01:04,  2.74it/s]avg_loss = 2.082318909374284:  31%|███▏      | 80/256 [00:30<01:04,  2.74it/s] avg_loss = 2.082318909374284:  32%|███▏      | 81/256 [00:30<01:03,  2.74it/s]avg_loss = 2.078883237955047:  32%|███▏      | 81/256 [00:30<01:03,  2.74it/s]avg_loss = 2.078883237955047:  32%|███▏      | 82/256 [00:30<01:03,  2.74it/s]avg_loss = 2.0787086113389717:  32%|███▏      | 82/256 [00:30<01:03,  2.74it/s]avg_loss = 2.0787086113389717:  32%|███▏      | 83/256 [00:30<01:03,  2.74it/s]avg_loss = 2.0812589412643794:  32%|███▏      | 83/256 [00:31<01:03,  2.74it/s]avg_loss = 2.0812589412643794:  33%|███▎      | 84/256 [00:31<01:02,  2.74it/s]avg_loss = 2.079464279904085:  33%|███▎      | 84/256 [00:31<01:02,  2.74it/s] avg_loss = 2.079464279904085:  33%|███▎      | 85/256 [00:31<01:02,  2.74it/s]avg_loss = 2.077165056106656:  33%|███▎      | 85/256 [00:32<01:02,  2.74it/s]avg_loss = 2.077165056106656:  34%|███▎      | 86/256 [00:32<01:01,  2.74it/s]avg_loss = 2.0803577201119783:  34%|███▎      | 86/256 [00:32<01:01,  2.74it/s]avg_loss = 2.0803577201119783:  34%|███▍      | 87/256 [00:32<01:01,  2.74it/s]avg_loss = 2.071758890693838:  34%|███▍      | 87/256 [00:32<01:01,  2.74it/s] avg_loss = 2.071758890693838:  34%|███▍      | 88/256 [00:32<01:01,  2.74it/s]avg_loss = 2.073072612955329:  34%|███▍      | 88/256 [00:33<01:01,  2.74it/s]avg_loss = 2.073072612955329:  35%|███▍      | 89/256 [00:33<01:00,  2.74it/s]avg_loss = 2.0737159066730078:  35%|███▍      | 89/256 [00:33<01:00,  2.74it/s]avg_loss = 2.0737159066730078:  35%|███▌      | 90/256 [00:33<01:00,  2.74it/s]avg_loss = 2.069603289876665:  35%|███▌      | 90/256 [00:33<01:00,  2.74it/s] avg_loss = 2.069603289876665:  36%|███▌      | 91/256 [00:33<01:00,  2.74it/s]avg_loss = 2.0603255383346393:  36%|███▌      | 91/256 [00:34<01:00,  2.74it/s]avg_loss = 2.0603255383346393:  36%|███▌      | 92/256 [00:34<00:59,  2.74it/s]avg_loss = 2.0612369929590533:  36%|███▌      | 92/256 [00:34<00:59,  2.74it/s]avg_loss = 2.0612369929590533:  36%|███▋      | 93/256 [00:34<00:59,  2.74it/s]avg_loss = 2.0570758441661265:  36%|███▋      | 93/256 [00:34<00:59,  2.74it/s]avg_loss = 2.0570758441661265:  37%|███▋      | 94/256 [00:34<00:59,  2.74it/s]avg_loss = 2.0517534657528524:  37%|███▋      | 94/256 [00:35<00:59,  2.74it/s]avg_loss = 2.0517534657528524:  37%|███▋      | 95/256 [00:35<00:58,  2.74it/s]avg_loss = 2.0515907605489097:  37%|███▋      | 95/256 [00:35<00:58,  2.74it/s]avg_loss = 2.0515907605489097:  38%|███▊      | 96/256 [00:35<00:58,  2.74it/s]avg_loss = 2.0495286233646355:  38%|███▊      | 96/256 [00:36<00:58,  2.74it/s]avg_loss = 2.0495286233646355:  38%|███▊      | 97/256 [00:36<00:57,  2.74it/s]avg_loss = 2.0543109616454767:  38%|███▊      | 97/256 [00:36<00:57,  2.74it/s]avg_loss = 2.0543109616454767:  38%|███▊      | 98/256 [00:36<00:57,  2.74it/s]avg_loss = 2.055220536511354:  38%|███▊      | 98/256 [00:36<00:57,  2.74it/s] avg_loss = 2.055220536511354:  39%|███▊      | 99/256 [00:36<00:57,  2.74it/s]avg_loss = 2.061831953525543:  39%|███▊      | 99/256 [00:37<00:57,  2.74it/s]avg_loss = 2.061831953525543:  39%|███▉      | 100/256 [00:37<00:56,  2.75it/s]avg_loss = 2.0625436424028756:  39%|███▉      | 100/256 [00:37<00:56,  2.75it/s]avg_loss = 2.0625436424028756:  39%|███▉      | 101/256 [00:37<00:56,  2.75it/s]avg_loss = 2.074209790603787:  39%|███▉      | 101/256 [00:37<00:56,  2.75it/s] avg_loss = 2.074209790603787:  40%|███▉      | 102/256 [00:37<00:56,  2.75it/s]avg_loss = 2.074724847830615:  40%|███▉      | 102/256 [00:38<00:56,  2.75it/s]avg_loss = 2.074724847830615:  40%|████      | 103/256 [00:38<00:55,  2.74it/s]avg_loss = 2.0782089691895704:  40%|████      | 103/256 [00:38<00:55,  2.74it/s]avg_loss = 2.0782089691895704:  41%|████      | 104/256 [00:38<00:55,  2.74it/s]avg_loss = 2.070156825156439:  41%|████      | 104/256 [00:38<00:55,  2.74it/s] avg_loss = 2.070156825156439:  41%|████      | 105/256 [00:38<00:55,  2.74it/s]avg_loss = 2.0735047527079313:  41%|████      | 105/256 [00:39<00:55,  2.74it/s]avg_loss = 2.0735047527079313:  41%|████▏     | 106/256 [00:39<00:54,  2.74it/s]avg_loss = 2.0753904380530956:  41%|████▏     | 106/256 [00:39<00:54,  2.74it/s]avg_loss = 2.0753904380530956:  42%|████▏     | 107/256 [00:39<00:54,  2.74it/s]avg_loss = 2.0743627614445157:  42%|████▏     | 107/256 [00:40<00:54,  2.74it/s]avg_loss = 2.0743627614445157:  42%|████▏     | 108/256 [00:40<00:53,  2.74it/s]avg_loss = 2.078973560158266:  42%|████▏     | 108/256 [00:40<00:53,  2.74it/s] avg_loss = 2.078973560158266:  43%|████▎     | 109/256 [00:40<00:53,  2.74it/s]avg_loss = 2.072564493526112:  43%|████▎     | 109/256 [00:40<00:53,  2.74it/s]avg_loss = 2.072564493526112:  43%|████▎     | 110/256 [00:40<00:53,  2.75it/s]avg_loss = 2.085786031173156:  43%|████▎     | 110/256 [00:41<00:53,  2.75it/s]avg_loss = 2.085786031173156:  43%|████▎     | 111/256 [00:41<00:52,  2.75it/s]avg_loss = 2.090721017548016:  43%|████▎     | 111/256 [00:41<00:52,  2.75it/s]avg_loss = 2.090721017548016:  44%|████▍     | 112/256 [00:41<00:52,  2.75it/s]avg_loss = 2.093619931060656:  44%|████▍     | 112/256 [00:41<00:52,  2.75it/s]avg_loss = 2.093619931060656:  44%|████▍     | 113/256 [00:41<00:52,  2.75it/s]avg_loss = 2.09655457421353:  44%|████▍     | 113/256 [00:42<00:52,  2.75it/s] avg_loss = 2.09655457421353:  45%|████▍     | 114/256 [00:42<00:51,  2.74it/s]avg_loss = 2.117518298522286:  45%|████▍     | 114/256 [00:42<00:51,  2.74it/s]avg_loss = 2.117518298522286:  45%|████▍     | 115/256 [00:42<00:51,  2.74it/s]avg_loss = 2.116071054647709:  45%|████▍     | 115/256 [00:42<00:51,  2.74it/s]avg_loss = 2.116071054647709:  45%|████▌     | 116/256 [00:42<00:51,  2.74it/s]avg_loss = 2.114907425692958:  45%|████▌     | 116/256 [00:43<00:51,  2.74it/s]avg_loss = 2.114907425692958:  46%|████▌     | 117/256 [00:43<00:50,  2.74it/s]avg_loss = 2.117259237725856:  46%|████▌     | 117/256 [00:43<00:50,  2.74it/s]avg_loss = 2.117259237725856:  46%|████▌     | 118/256 [00:43<00:50,  2.75it/s]avg_loss = 2.1267288131874147:  46%|████▌     | 118/256 [00:44<00:50,  2.75it/s]avg_loss = 2.1267288131874147:  46%|████▋     | 119/256 [00:44<00:49,  2.75it/s]avg_loss = 2.132269189755122:  46%|████▋     | 119/256 [00:44<00:49,  2.75it/s] avg_loss = 2.132269189755122:  47%|████▋     | 120/256 [00:44<00:49,  2.75it/s]avg_loss = 2.1341400067668315:  47%|████▋     | 120/256 [00:44<00:49,  2.75it/s]avg_loss = 2.1341400067668315:  47%|████▋     | 121/256 [00:44<00:49,  2.75it/s]avg_loss = 2.134246908250402:  47%|████▋     | 121/256 [00:45<00:49,  2.75it/s] avg_loss = 2.134246908250402:  48%|████▊     | 122/256 [00:45<00:48,  2.75it/s]avg_loss = 2.1328762401410235:  48%|████▊     | 122/256 [00:45<00:48,  2.75it/s]avg_loss = 2.1328762401410235:  48%|████▊     | 123/256 [00:45<00:48,  2.75it/s]avg_loss = 2.133107486271089:  48%|████▊     | 123/256 [00:45<00:48,  2.75it/s] avg_loss = 2.133107486271089:  48%|████▊     | 124/256 [00:45<00:48,  2.75it/s]avg_loss = 2.1312299242019654:  48%|████▊     | 124/256 [00:46<00:48,  2.75it/s]avg_loss = 2.1312299242019654:  49%|████▉     | 125/256 [00:46<00:47,  2.75it/s]avg_loss = 2.12846455119905:  49%|████▉     | 125/256 [00:46<00:47,  2.75it/s]  avg_loss = 2.12846455119905:  49%|████▉     | 126/256 [00:46<00:47,  2.75it/s]avg_loss = 2.1265339391438043:  49%|████▉     | 126/256 [00:46<00:47,  2.75it/s]avg_loss = 2.1265339391438043:  50%|████▉     | 127/256 [00:46<00:46,  2.75it/s]avg_loss = 2.1242976244539022:  50%|████▉     | 127/256 [00:47<00:46,  2.75it/s]avg_loss = 2.1242976244539022:  50%|█████     | 128/256 [00:47<00:46,  2.75it/s]avg_loss = 2.1223738248958144:  50%|█████     | 128/256 [00:47<00:46,  2.75it/s]avg_loss = 2.1223738248958144:  50%|█████     | 129/256 [00:47<00:46,  2.75it/s]avg_loss = 2.1151612685276913:  50%|█████     | 129/256 [00:48<00:46,  2.75it/s]avg_loss = 2.1151612685276913:  51%|█████     | 130/256 [00:48<00:45,  2.75it/s]avg_loss = 2.1160562675417833:  51%|█████     | 130/256 [00:48<00:45,  2.75it/s]avg_loss = 2.1160562675417833:  51%|█████     | 131/256 [00:48<00:45,  2.75it/s]avg_loss = 2.1145678930210345:  51%|█████     | 131/256 [00:48<00:45,  2.75it/s]avg_loss = 2.1145678930210345:  52%|█████▏    | 132/256 [00:48<00:45,  2.75it/s]avg_loss = 2.112706975829332:  52%|█████▏    | 132/256 [00:49<00:45,  2.75it/s] avg_loss = 2.112706975829332:  52%|█████▏    | 133/256 [00:49<00:44,  2.75it/s]avg_loss = 2.111870812835978:  52%|█████▏    | 133/256 [00:49<00:44,  2.75it/s]avg_loss = 2.111870812835978:  52%|█████▏    | 134/256 [00:49<00:44,  2.75it/s]avg_loss = 2.110202791955736:  52%|█████▏    | 134/256 [00:49<00:44,  2.75it/s]avg_loss = 2.110202791955736:  53%|█████▎    | 135/256 [00:49<00:44,  2.75it/s]avg_loss = 2.1085879346903633:  53%|█████▎    | 135/256 [00:50<00:44,  2.75it/s]avg_loss = 2.1085879346903633:  53%|█████▎    | 136/256 [00:50<00:43,  2.75it/s]avg_loss = 2.1105337142944336:  53%|█████▎    | 136/256 [00:50<00:43,  2.75it/s]avg_loss = 2.1105337142944336:  54%|█████▎    | 137/256 [00:50<00:43,  2.75it/s]avg_loss = 2.110669265622678:  54%|█████▎    | 137/256 [00:50<00:43,  2.75it/s] avg_loss = 2.110669265622678:  54%|█████▍    | 138/256 [00:50<00:42,  2.75it/s]avg_loss = 2.108205844172471:  54%|█████▍    | 138/256 [00:51<00:42,  2.75it/s]avg_loss = 2.108205844172471:  54%|█████▍    | 139/256 [00:51<00:42,  2.75it/s]avg_loss = 2.1105401865073614:  54%|█████▍    | 139/256 [00:51<00:42,  2.75it/s]avg_loss = 2.1105401865073614:  55%|█████▍    | 140/256 [00:51<00:42,  2.75it/s]avg_loss = 2.1133686709911266:  55%|█████▍    | 140/256 [00:52<00:42,  2.75it/s]avg_loss = 2.1133686709911266:  55%|█████▌    | 141/256 [00:52<00:41,  2.75it/s]avg_loss = 2.1260664505018316:  55%|█████▌    | 141/256 [00:52<00:41,  2.75it/s]avg_loss = 2.1260664505018316:  55%|█████▌    | 142/256 [00:52<00:41,  2.75it/s]avg_loss = 2.119885730576682:  55%|█████▌    | 142/256 [00:52<00:41,  2.75it/s] avg_loss = 2.119885730576682:  56%|█████▌    | 143/256 [00:52<00:41,  2.75it/s]avg_loss = 2.1209851114286318:  56%|█████▌    | 143/256 [00:53<00:41,  2.75it/s]avg_loss = 2.1209851114286318:  56%|█████▋    | 144/256 [00:53<00:40,  2.75it/s]avg_loss = 2.1230581291790664:  56%|█████▋    | 144/256 [00:53<00:40,  2.75it/s]avg_loss = 2.1230581291790664:  57%|█████▋    | 145/256 [00:53<00:40,  2.75it/s]avg_loss = 2.12510240322923:  57%|█████▋    | 145/256 [00:53<00:40,  2.75it/s]  avg_loss = 2.12510240322923:  57%|█████▋    | 146/256 [00:53<00:40,  2.75it/s]avg_loss = 2.120495005529754:  57%|█████▋    | 146/256 [00:54<00:40,  2.75it/s]avg_loss = 2.120495005529754:  57%|█████▋    | 147/256 [00:54<00:39,  2.75it/s]avg_loss = 2.1215959279923826:  57%|█████▋    | 147/256 [00:54<00:39,  2.75it/s]avg_loss = 2.1215959279923826:  58%|█████▊    | 148/256 [00:54<00:39,  2.75it/s]avg_loss = 2.1236374978251105:  58%|█████▊    | 148/256 [00:54<00:39,  2.75it/s]avg_loss = 2.1236374978251105:  58%|█████▊    | 149/256 [00:54<00:38,  2.75it/s]avg_loss = 2.123123203118642:  58%|█████▊    | 149/256 [00:55<00:38,  2.75it/s] avg_loss = 2.123123203118642:  59%|█████▊    | 150/256 [00:55<00:38,  2.75it/s]avg_loss = 2.127352576382113:  59%|█████▊    | 150/256 [00:55<00:38,  2.75it/s]avg_loss = 2.127352576382113:  59%|█████▉    | 151/256 [00:55<00:38,  2.75it/s]avg_loss = 2.142708568980819:  59%|█████▉    | 151/256 [00:56<00:38,  2.75it/s]avg_loss = 2.142708568980819:  59%|█████▉    | 152/256 [00:56<00:37,  2.75it/s]avg_loss = 2.144463027224821:  59%|█████▉    | 152/256 [00:56<00:37,  2.75it/s]avg_loss = 2.144463027224821:  60%|█████▉    | 153/256 [00:56<00:37,  2.75it/s]avg_loss = 2.1555466442913205:  60%|█████▉    | 153/256 [00:56<00:37,  2.75it/s]avg_loss = 2.1555466442913205:  60%|██████    | 154/256 [00:56<00:37,  2.75it/s]avg_loss = 2.1543173459268385:  60%|██████    | 154/256 [00:57<00:37,  2.75it/s]avg_loss = 2.1543173459268385:  61%|██████    | 155/256 [00:57<00:36,  2.75it/s]avg_loss = 2.153873678201284:  61%|██████    | 155/256 [00:57<00:36,  2.75it/s] avg_loss = 2.153873678201284:  61%|██████    | 156/256 [00:57<00:36,  2.75it/s]avg_loss = 2.156602078182682:  61%|██████    | 156/256 [00:57<00:36,  2.75it/s]avg_loss = 2.156602078182682:  61%|██████▏   | 157/256 [00:57<00:36,  2.75it/s]avg_loss = 2.1544753086717825:  61%|██████▏   | 157/256 [00:58<00:36,  2.75it/s]avg_loss = 2.1544753086717825:  62%|██████▏   | 158/256 [00:58<00:35,  2.75it/s]avg_loss = 2.1537682755188374:  62%|██████▏   | 158/256 [00:58<00:35,  2.75it/s]avg_loss = 2.1537682755188374:  62%|██████▏   | 159/256 [00:58<00:35,  2.75it/s]avg_loss = 2.1502188719809054:  62%|██████▏   | 159/256 [00:58<00:35,  2.75it/s]avg_loss = 2.1502188719809054:  62%|██████▎   | 160/256 [00:58<00:34,  2.75it/s]avg_loss = 2.1501877581851083:  62%|██████▎   | 160/256 [00:59<00:34,  2.75it/s]avg_loss = 2.1501877581851083:  63%|██████▎   | 161/256 [00:59<00:34,  2.75it/s]avg_loss = 2.1495169629285367:  63%|██████▎   | 161/256 [00:59<00:34,  2.75it/s]avg_loss = 2.1495169629285367:  63%|██████▎   | 162/256 [00:59<00:34,  2.75it/s]avg_loss = 2.1506081702519046:  63%|██████▎   | 162/256 [01:00<00:34,  2.75it/s]avg_loss = 2.1506081702519046:  64%|██████▎   | 163/256 [01:00<00:33,  2.75it/s]avg_loss = 2.154014880337366:  64%|██████▎   | 163/256 [01:00<00:33,  2.75it/s] avg_loss = 2.154014880337366:  64%|██████▍   | 164/256 [01:00<00:33,  2.75it/s]avg_loss = 2.1536816286318228:  64%|██████▍   | 164/256 [01:00<00:33,  2.75it/s]avg_loss = 2.1536816286318228:  64%|██████▍   | 165/256 [01:00<00:33,  2.75it/s]avg_loss = 2.1526302099227905:  64%|██████▍   | 165/256 [01:01<00:33,  2.75it/s]avg_loss = 2.1526302099227905:  65%|██████▍   | 166/256 [01:01<00:32,  2.75it/s]avg_loss = 2.1541672523864017:  65%|██████▍   | 166/256 [01:01<00:32,  2.75it/s]avg_loss = 2.1541672523864017:  65%|██████▌   | 167/256 [01:01<00:32,  2.75it/s]avg_loss = 2.1560496914954412:  65%|██████▌   | 167/256 [01:01<00:32,  2.75it/s]avg_loss = 2.1560496914954412:  66%|██████▌   | 168/256 [01:01<00:32,  2.75it/s]avg_loss = 2.155975532249586:  66%|██████▌   | 168/256 [01:02<00:32,  2.75it/s] avg_loss = 2.155975532249586:  66%|██████▌   | 169/256 [01:02<00:31,  2.75it/s]avg_loss = 2.1532273229430703:  66%|██████▌   | 169/256 [01:02<00:31,  2.75it/s]avg_loss = 2.1532273229430703:  66%|██████▋   | 170/256 [01:02<00:31,  2.75it/s]avg_loss = 2.153269249793382:  66%|██████▋   | 170/256 [01:02<00:31,  2.75it/s] avg_loss = 2.153269249793382:  67%|██████▋   | 171/256 [01:02<00:30,  2.75it/s]avg_loss = 2.147291831498922:  67%|██████▋   | 171/256 [01:03<00:30,  2.75it/s]avg_loss = 2.147291831498922:  67%|██████▋   | 172/256 [01:03<00:30,  2.75it/s]avg_loss = 2.1445677280426025:  67%|██████▋   | 172/256 [01:03<00:30,  2.75it/s]avg_loss = 2.1445677280426025:  68%|██████▊   | 173/256 [01:03<00:30,  2.75it/s]avg_loss = 2.1446875950385786:  68%|██████▊   | 173/256 [01:04<00:30,  2.75it/s]avg_loss = 2.1446875950385786:  68%|██████▊   | 174/256 [01:04<00:29,  2.75it/s]avg_loss = 2.141203466142927:  68%|██████▊   | 174/256 [01:04<00:29,  2.75it/s] avg_loss = 2.141203466142927:  68%|██████▊   | 175/256 [01:04<00:29,  2.75it/s]avg_loss = 2.1398302777246996:  68%|██████▊   | 175/256 [01:04<00:29,  2.75it/s]avg_loss = 2.1398302777246996:  69%|██████▉   | 176/256 [01:04<00:29,  2.75it/s]avg_loss = 2.137661997207814:  69%|██████▉   | 176/256 [01:05<00:29,  2.75it/s] avg_loss = 2.137661997207814:  69%|██████▉   | 177/256 [01:05<00:28,  2.75it/s]avg_loss = 2.1369837964518688:  69%|██████▉   | 177/256 [01:05<00:28,  2.75it/s]avg_loss = 2.1369837964518688:  70%|██████▉   | 178/256 [01:05<00:28,  2.75it/s]avg_loss = 2.137504239322087:  70%|██████▉   | 178/256 [01:05<00:28,  2.75it/s] avg_loss = 2.137504239322087:  70%|██████▉   | 179/256 [01:05<00:28,  2.75it/s]avg_loss = 2.139780588944753:  70%|██████▉   | 179/256 [01:06<00:28,  2.75it/s]avg_loss = 2.139780588944753:  70%|███████   | 180/256 [01:06<00:27,  2.75it/s]avg_loss = 2.136992972858703:  70%|███████   | 180/256 [01:06<00:27,  2.75it/s]avg_loss = 2.136992972858703:  71%|███████   | 181/256 [01:06<00:27,  2.75it/s]avg_loss = 2.138776035754235:  71%|███████   | 181/256 [01:06<00:27,  2.75it/s]avg_loss = 2.138776035754235:  71%|███████   | 182/256 [01:06<00:26,  2.75it/s]avg_loss = 2.139480771262789:  71%|███████   | 182/256 [01:07<00:26,  2.75it/s]avg_loss = 2.139480771262789:  71%|███████▏  | 183/256 [01:07<00:26,  2.75it/s]avg_loss = 2.1364303535741307:  71%|███████▏  | 183/256 [01:07<00:26,  2.75it/s]avg_loss = 2.1364303535741307:  72%|███████▏  | 184/256 [01:07<00:26,  2.75it/s]avg_loss = 2.1291881893132185:  72%|███████▏  | 184/256 [01:08<00:26,  2.75it/s]avg_loss = 2.1291881893132185:  72%|███████▏  | 185/256 [01:08<00:25,  2.75it/s]avg_loss = 2.1290931365182324:  72%|███████▏  | 185/256 [01:08<00:25,  2.75it/s]avg_loss = 2.1290931365182324:  73%|███████▎  | 186/256 [01:08<00:25,  2.75it/s]avg_loss = 2.1288885187337745:  73%|███████▎  | 186/256 [01:08<00:25,  2.75it/s]avg_loss = 2.1288885187337745:  73%|███████▎  | 187/256 [01:08<00:25,  2.75it/s]avg_loss = 2.129086820052025:  73%|███████▎  | 187/256 [01:09<00:25,  2.75it/s] avg_loss = 2.129086820052025:  73%|███████▎  | 188/256 [01:09<00:24,  2.75it/s]avg_loss = 2.131411159795428:  73%|███████▎  | 188/256 [01:09<00:24,  2.75it/s]avg_loss = 2.131411159795428:  74%|███████▍  | 189/256 [01:09<00:24,  2.75it/s]avg_loss = 2.1269112094452507:  74%|███████▍  | 189/256 [01:09<00:24,  2.75it/s]avg_loss = 2.1269112094452507:  74%|███████▍  | 190/256 [01:09<00:24,  2.75it/s]avg_loss = 2.1321752118190545:  74%|███████▍  | 190/256 [01:10<00:24,  2.75it/s]avg_loss = 2.1321752118190545:  75%|███████▍  | 191/256 [01:10<00:23,  2.75it/s]avg_loss = 2.1321716516589126:  75%|███████▍  | 191/256 [01:10<00:23,  2.75it/s]avg_loss = 2.1321716516589126:  75%|███████▌  | 192/256 [01:10<00:23,  2.75it/s]avg_loss = 2.133314584512167:  75%|███████▌  | 192/256 [01:10<00:23,  2.75it/s] avg_loss = 2.133314584512167:  75%|███████▌  | 193/256 [01:10<00:22,  2.75it/s]avg_loss = 2.1348183336331674:  75%|███████▌  | 193/256 [01:11<00:22,  2.75it/s]avg_loss = 2.1348183336331674:  76%|███████▌  | 194/256 [01:11<00:22,  2.75it/s]avg_loss = 2.1363787097808644:  76%|███████▌  | 194/256 [01:11<00:22,  2.75it/s]avg_loss = 2.1363787097808644:  76%|███████▌  | 195/256 [01:11<00:22,  2.75it/s]avg_loss = 2.136372270328658:  76%|███████▌  | 195/256 [01:12<00:22,  2.75it/s] avg_loss = 2.136372270328658:  77%|███████▋  | 196/256 [01:12<00:21,  2.75it/s]avg_loss = 2.1380286177402827:  77%|███████▋  | 196/256 [01:12<00:21,  2.75it/s]avg_loss = 2.1380286177402827:  77%|███████▋  | 197/256 [01:12<00:21,  2.75it/s]avg_loss = 2.1361081241959274:  77%|███████▋  | 197/256 [01:12<00:21,  2.75it/s]avg_loss = 2.1361081241959274:  77%|███████▋  | 198/256 [01:12<00:21,  2.75it/s]avg_loss = 2.134502924566892:  77%|███████▋  | 198/256 [01:13<00:21,  2.75it/s] avg_loss = 2.134502924566892:  78%|███████▊  | 199/256 [01:13<00:20,  2.75it/s]avg_loss = 2.1334581646323203:  78%|███████▊  | 199/256 [01:13<00:20,  2.75it/s]avg_loss = 2.1334581646323203:  78%|███████▊  | 200/256 [01:13<00:20,  2.75it/s]avg_loss = 2.1379937826104425:  78%|███████▊  | 200/256 [01:13<00:20,  2.75it/s]avg_loss = 2.1379937826104425:  79%|███████▊  | 201/256 [01:13<00:20,  2.75it/s]avg_loss = 2.1362925404959388:  79%|███████▊  | 201/256 [01:14<00:20,  2.75it/s]avg_loss = 2.1362925404959388:  79%|███████▉  | 202/256 [01:14<00:19,  2.75it/s]avg_loss = 2.1387961489226432:  79%|███████▉  | 202/256 [01:14<00:19,  2.75it/s]avg_loss = 2.1387961489226432:  79%|███████▉  | 203/256 [01:14<00:19,  2.75it/s]avg_loss = 2.1445006403852913:  79%|███████▉  | 203/256 [01:15<00:19,  2.75it/s]avg_loss = 2.1445006403852913:  80%|███████▉  | 204/256 [01:15<00:18,  2.75it/s]avg_loss = 2.14422080836645:  80%|███████▉  | 204/256 [01:15<00:18,  2.75it/s]  avg_loss = 2.14422080836645:  80%|████████  | 205/256 [01:15<00:18,  2.75it/s]avg_loss = 2.140480731008122:  80%|████████  | 205/256 [01:15<00:18,  2.75it/s]avg_loss = 2.140480731008122:  80%|████████  | 206/256 [01:15<00:18,  2.75it/s]avg_loss = 2.1398288774605536:  80%|████████  | 206/256 [01:16<00:18,  2.75it/s]avg_loss = 2.1398288774605536:  81%|████████  | 207/256 [01:16<00:17,  2.75it/s]avg_loss = 2.140613980877858:  81%|████████  | 207/256 [01:16<00:17,  2.75it/s] avg_loss = 2.140613980877858:  81%|████████▏ | 208/256 [01:16<00:17,  2.74it/s]avg_loss = 2.142014329798484:  81%|████████▏ | 208/256 [01:16<00:17,  2.74it/s]avg_loss = 2.142014329798484:  82%|████████▏ | 209/256 [01:16<00:17,  2.74it/s]avg_loss = 2.141723299877984:  82%|████████▏ | 209/256 [01:17<00:17,  2.74it/s]avg_loss = 2.141723299877984:  82%|████████▏ | 210/256 [01:17<00:16,  2.74it/s]avg_loss = 2.14178615243514:  82%|████████▏ | 210/256 [01:17<00:16,  2.74it/s] avg_loss = 2.14178615243514:  82%|████████▏ | 211/256 [01:17<00:16,  2.74it/s]avg_loss = 2.1420675029732146:  82%|████████▏ | 211/256 [01:17<00:16,  2.74it/s]avg_loss = 2.1420675029732146:  83%|████████▎ | 212/256 [01:17<00:16,  2.74it/s]avg_loss = 2.1379414133622614:  83%|████████▎ | 212/256 [01:18<00:16,  2.74it/s]avg_loss = 2.1379414133622614:  83%|████████▎ | 213/256 [01:18<00:15,  2.74it/s]avg_loss = 2.138624726333351:  83%|████████▎ | 213/256 [01:18<00:15,  2.74it/s] avg_loss = 2.138624726333351:  84%|████████▎ | 214/256 [01:18<00:15,  2.74it/s]avg_loss = 2.140665800904119:  84%|████████▎ | 214/256 [01:19<00:15,  2.74it/s]avg_loss = 2.140665800904119:  84%|████████▍ | 215/256 [01:19<00:14,  2.74it/s]avg_loss = 2.1396338224962905:  84%|████████▍ | 215/256 [01:19<00:14,  2.74it/s]avg_loss = 2.1396338224962905:  84%|████████▍ | 216/256 [01:19<00:14,  2.74it/s]avg_loss = 2.138056581196148:  84%|████████▍ | 216/256 [01:19<00:14,  2.74it/s] avg_loss = 2.138056581196148:  85%|████████▍ | 217/256 [01:19<00:14,  2.74it/s]avg_loss = 2.1386340317922996:  85%|████████▍ | 217/256 [01:20<00:14,  2.74it/s]avg_loss = 2.1386340317922996:  85%|████████▌ | 218/256 [01:20<00:13,  2.74it/s]avg_loss = 2.1400514484540514:  85%|████████▌ | 218/256 [01:20<00:13,  2.74it/s]avg_loss = 2.1400514484540514:  86%|████████▌ | 219/256 [01:20<00:13,  2.74it/s]avg_loss = 2.1404456515203822:  86%|████████▌ | 219/256 [01:20<00:13,  2.74it/s]avg_loss = 2.1404456515203822:  86%|████████▌ | 220/256 [01:20<00:13,  2.74it/s]avg_loss = 2.137352512702683:  86%|████████▌ | 220/256 [01:21<00:13,  2.74it/s] avg_loss = 2.137352512702683:  86%|████████▋ | 221/256 [01:21<00:12,  2.74it/s]avg_loss = 2.1372938303797095:  86%|████████▋ | 221/256 [01:21<00:12,  2.74it/s]avg_loss = 2.1372938303797095:  87%|████████▋ | 222/256 [01:21<00:12,  2.74it/s]avg_loss = 2.1386941396067494:  87%|████████▋ | 222/256 [01:21<00:12,  2.74it/s]avg_loss = 2.1386941396067494:  87%|████████▋ | 223/256 [01:21<00:12,  2.74it/s]avg_loss = 2.1397428126739606:  87%|████████▋ | 223/256 [01:22<00:12,  2.74it/s]avg_loss = 2.1397428126739606:  88%|████████▊ | 224/256 [01:22<00:11,  2.74it/s]avg_loss = 2.1375686642858716:  88%|████████▊ | 224/256 [01:22<00:11,  2.74it/s]avg_loss = 2.1375686642858716:  88%|████████▊ | 225/256 [01:22<00:11,  2.74it/s]avg_loss = 2.137557790078948:  88%|████████▊ | 225/256 [01:23<00:11,  2.74it/s] avg_loss = 2.137557790078948:  88%|████████▊ | 226/256 [01:23<00:10,  2.74it/s]avg_loss = 2.142587194096149:  88%|████████▊ | 226/256 [01:23<00:10,  2.74it/s]avg_loss = 2.142587194096149:  89%|████████▊ | 227/256 [01:23<00:10,  2.74it/s]avg_loss = 2.1438344526186324:  89%|████████▊ | 227/256 [01:23<00:10,  2.74it/s]avg_loss = 2.1438344526186324:  89%|████████▉ | 228/256 [01:23<00:10,  2.74it/s]avg_loss = 2.142801399053965:  89%|████████▉ | 228/256 [01:24<00:10,  2.74it/s] avg_loss = 2.142801399053965:  89%|████████▉ | 229/256 [01:24<00:09,  2.74it/s]avg_loss = 2.137232620819755:  89%|████████▉ | 229/256 [01:24<00:09,  2.74it/s]avg_loss = 2.137232620819755:  90%|████████▉ | 230/256 [01:24<00:09,  2.74it/s]avg_loss = 2.1369959482382903:  90%|████████▉ | 230/256 [01:24<00:09,  2.74it/s]avg_loss = 2.1369959482382903:  90%|█████████ | 231/256 [01:24<00:09,  2.74it/s]avg_loss = 2.1393334033160376:  90%|█████████ | 231/256 [01:25<00:09,  2.74it/s]avg_loss = 2.1393334033160376:  91%|█████████ | 232/256 [01:25<00:08,  2.74it/s]avg_loss = 2.13863014867889:  91%|█████████ | 232/256 [01:25<00:08,  2.74it/s]  avg_loss = 2.13863014867889:  91%|█████████ | 233/256 [01:25<00:08,  2.74it/s]avg_loss = 2.1383412139028564:  91%|█████████ | 233/256 [01:25<00:08,  2.74it/s]avg_loss = 2.1383412139028564:  91%|█████████▏| 234/256 [01:25<00:08,  2.74it/s]avg_loss = 2.139694057626927:  91%|█████████▏| 234/256 [01:26<00:08,  2.74it/s] avg_loss = 2.139694057626927:  92%|█████████▏| 235/256 [01:26<00:07,  2.74it/s]avg_loss = 2.140891222630517:  92%|█████████▏| 235/256 [01:26<00:07,  2.74it/s]avg_loss = 2.140891222630517:  92%|█████████▏| 236/256 [01:26<00:07,  2.74it/s]avg_loss = 2.1410509373065287:  92%|█████████▏| 236/256 [01:27<00:07,  2.74it/s]avg_loss = 2.1410509373065287:  93%|█████████▎| 237/256 [01:27<00:06,  2.74it/s]avg_loss = 2.142566040784371:  93%|█████████▎| 237/256 [01:27<00:06,  2.74it/s] avg_loss = 2.142566040784371:  93%|█████████▎| 238/256 [01:27<00:06,  2.74it/s]avg_loss = 2.1404287136748246:  93%|█████████▎| 238/256 [01:27<00:06,  2.74it/s]avg_loss = 2.1404287136748246:  93%|█████████▎| 239/256 [01:27<00:06,  2.74it/s]avg_loss = 2.1358374203244845:  93%|█████████▎| 239/256 [01:28<00:06,  2.74it/s]avg_loss = 2.1358374203244845:  94%|█████████▍| 240/256 [01:28<00:05,  2.74it/s]avg_loss = 2.136619388315193:  94%|█████████▍| 240/256 [01:28<00:05,  2.74it/s] avg_loss = 2.136619388315193:  94%|█████████▍| 241/256 [01:28<00:05,  2.74it/s]avg_loss = 2.1359797903328888:  94%|█████████▍| 241/256 [01:28<00:05,  2.74it/s]avg_loss = 2.1359797903328888:  95%|█████████▍| 242/256 [01:28<00:05,  2.74it/s]avg_loss = 2.135180228041033:  95%|█████████▍| 242/256 [01:29<00:05,  2.74it/s] avg_loss = 2.135180228041033:  95%|█████████▍| 243/256 [01:29<00:04,  2.74it/s]avg_loss = 2.1354999659491365:  95%|█████████▍| 243/256 [01:29<00:04,  2.74it/s]avg_loss = 2.1354999659491365:  95%|█████████▌| 244/256 [01:29<00:04,  2.74it/s]avg_loss = 2.1317064732921365:  95%|█████████▌| 244/256 [01:29<00:04,  2.74it/s]avg_loss = 2.1317064732921365:  96%|█████████▌| 245/256 [01:29<00:04,  2.74it/s]avg_loss = 2.130833117457909:  96%|█████████▌| 245/256 [01:30<00:04,  2.74it/s] avg_loss = 2.130833117457909:  96%|█████████▌| 246/256 [01:30<00:03,  2.74it/s]avg_loss = 2.129758911094202:  96%|█████████▌| 246/256 [01:30<00:03,  2.74it/s]avg_loss = 2.129758911094202:  96%|█████████▋| 247/256 [01:30<00:03,  2.74it/s]avg_loss = 2.1311126380197463:  96%|█████████▋| 247/256 [01:31<00:03,  2.74it/s]avg_loss = 2.1311126380197463:  97%|█████████▋| 248/256 [01:31<00:02,  2.74it/s]avg_loss = 2.130324076457196:  97%|█████████▋| 248/256 [01:31<00:02,  2.74it/s] avg_loss = 2.130324076457196:  97%|█████████▋| 249/256 [01:31<00:02,  2.74it/s]avg_loss = 2.1302844076156617:  97%|█████████▋| 249/256 [01:31<00:02,  2.74it/s]avg_loss = 2.1302844076156617:  98%|█████████▊| 250/256 [01:31<00:02,  2.74it/s]avg_loss = 2.137029685822141:  98%|█████████▊| 250/256 [01:32<00:02,  2.74it/s] avg_loss = 2.137029685822141:  98%|█████████▊| 251/256 [01:32<00:01,  2.74it/s]avg_loss = 2.1351495001997267:  98%|█████████▊| 251/256 [01:32<00:01,  2.74it/s]avg_loss = 2.1351495001997267:  98%|█████████▊| 252/256 [01:32<00:01,  2.74it/s]avg_loss = 2.1332846177896494:  98%|█████████▊| 252/256 [01:32<00:01,  2.74it/s]avg_loss = 2.1332846177896494:  99%|█████████▉| 253/256 [01:32<00:01,  2.74it/s]avg_loss = 2.13464431875334:  99%|█████████▉| 253/256 [01:33<00:01,  2.74it/s]  avg_loss = 2.13464431875334:  99%|█████████▉| 254/256 [01:33<00:00,  2.74it/s]avg_loss = 2.1316928924298755:  99%|█████████▉| 254/256 [01:33<00:00,  2.74it/s]avg_loss = 2.1316928924298755: 100%|█████████▉| 255/256 [01:33<00:00,  2.74it/s]avg_loss = 2.1318077840842307: 100%|█████████▉| 255/256 [01:33<00:00,  2.74it/s]avg_loss = 2.1318077840842307: 100%|██████████| 256/256 [01:33<00:00,  2.74it/s]avg_loss = 2.1318077840842307: 100%|██████████| 256/256 [01:33<00:00,  2.72it/s]
I0908 03:42:50.558743 103278 eval_ppl.py:63] c4 perplexity: 8.430092811584473
W0908 03:42:55.035376 103554 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 03:42:55.098425 103554 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0908 03:42:55.111333 103554 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0908 03:43:16.485188 103554 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
W0908 03:43:17.313695 103554 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 03:43:17.482479 103554 eval_zeroshot.py:33] loaded model!
Running loglikelihood requests
  0%|          | 0/26933 [00:00<?, ?it/s]  0%|          | 4/26933 [00:02<5:03:05,  1.48it/s]  0%|          | 8/26933 [00:03<2:48:41,  2.66it/s]  0%|          | 12/26933 [00:03<2:03:38,  3.63it/s]  0%|          | 16/26933 [00:04<1:41:20,  4.43it/s]  0%|          | 20/26933 [00:05<1:27:24,  5.13it/s]  0%|          | 24/26933 [00:05<1:18:24,  5.72it/s]  0%|          | 28/26933 [00:06<1:12:03,  6.22it/s]  0%|          | 32/26933 [00:06<1:07:42,  6.62it/s]  0%|          | 36/26933 [00:07<1:04:20,  6.97it/s]  0%|          | 40/26933 [00:07<1:01:33,  7.28it/s]  0%|          | 44/26933 [00:08<59:33,  7.52it/s]    0%|          | 48/26933 [00:08<58:00,  7.72it/s]  0%|          | 52/26933 [00:09<57:22,  7.81it/s]  0%|          | 56/26933 [00:09<55:53,  8.01it/s]  0%|          | 60/26933 [00:10<54:48,  8.17it/s]  0%|          | 64/26933 [00:10<53:54,  8.31it/s]  0%|          | 68/26933 [00:11<53:32,  8.36it/s]  0%|          | 72/26933 [00:11<52:46,  8.48it/s]  0%|          | 76/26933 [00:12<52:17,  8.56it/s]  0%|          | 80/26933 [00:12<51:50,  8.63it/s]  0%|          | 84/26933 [00:12<51:22,  8.71it/s]  0%|          | 88/26933 [00:13<50:26,  8.87it/s]  0%|          | 92/26933 [00:13<48:07,  9.30it/s]  0%|          | 96/26933 [00:14<46:24,  9.64it/s]  0%|          | 100/26933 [00:14<45:05,  9.92it/s]  0%|          | 104/26933 [00:14<44:16, 10.10it/s]  0%|          | 108/26933 [00:15<43:26, 10.29it/s]  0%|          | 112/26933 [00:15<42:52, 10.43it/s]  0%|          | 116/26933 [00:15<42:40, 10.47it/s]  0%|          | 120/26933 [00:16<42:09, 10.60it/s]  0%|          | 124/26933 [00:16<41:51, 10.67it/s]  0%|          | 128/26933 [00:17<41:29, 10.77it/s]  0%|          | 132/26933 [00:17<41:19, 10.81it/s]  1%|          | 136/26933 [00:17<41:13, 10.84it/s]  1%|          | 140/26933 [00:18<40:56, 10.91it/s]  1%|          | 144/26933 [00:18<40:43, 10.96it/s]  1%|          | 148/26933 [00:18<40:49, 10.93it/s]  1%|          | 152/26933 [00:19<40:42, 10.96it/s]  1%|          | 156/26933 [00:19<40:39, 10.97it/s]  1%|          | 160/26933 [00:19<40:34, 11.00it/s]  1%|          | 164/26933 [00:20<40:27, 11.03it/s]  1%|          | 168/26933 [00:20<40:17, 11.07it/s]  1%|          | 172/26933 [00:21<40:07, 11.12it/s]  1%|          | 176/26933 [00:21<40:01, 11.14it/s]  1%|          | 180/26933 [00:21<39:54, 11.17it/s]  1%|          | 184/26933 [00:22<39:59, 11.15it/s]  1%|          | 188/26933 [00:22<39:50, 11.19it/s]  1%|          | 192/26933 [00:22<39:56, 11.16it/s]  1%|          | 196/26933 [00:23<39:42, 11.22it/s]  1%|          | 200/26933 [00:23<39:37, 11.24it/s]  1%|          | 204/26933 [00:23<39:41, 11.22it/s]  1%|          | 208/26933 [00:24<39:32, 11.27it/s]  1%|          | 212/26933 [00:24<39:21, 11.32it/s]  1%|          | 216/26933 [00:24<39:28, 11.28it/s]  1%|          | 220/26933 [00:25<39:23, 11.30it/s]  1%|          | 224/26933 [00:25<39:15, 11.34it/s]  1%|          | 228/26933 [00:26<39:19, 11.32it/s]  1%|          | 232/26933 [00:26<39:10, 11.36it/s]  1%|          | 236/26933 [00:26<39:04, 11.38it/s]  1%|          | 240/26933 [00:27<38:59, 11.41it/s]  1%|          | 244/26933 [00:27<38:56, 11.42it/s]  1%|          | 248/26933 [00:27<39:08, 11.36it/s]  1%|          | 252/26933 [00:28<39:00, 11.40it/s]  1%|          | 256/26933 [00:28<38:54, 11.43it/s]  1%|          | 260/26933 [00:28<39:00, 11.40it/s]  1%|          | 264/26933 [00:29<38:50, 11.44it/s]  1%|          | 268/26933 [00:29<38:47, 11.46it/s]  1%|          | 272/26933 [00:29<38:44, 11.47it/s]  1%|          | 276/26933 [00:30<38:38, 11.50it/s]  1%|          | 280/26933 [00:30<38:30, 11.54it/s]  1%|          | 284/26933 [00:30<38:35, 11.51it/s]  1%|          | 288/26933 [00:31<38:29, 11.54it/s]  1%|          | 292/26933 [00:31<38:27, 11.55it/s]  1%|          | 296/26933 [00:31<38:28, 11.54it/s]  1%|          | 300/26933 [00:32<38:20, 11.58it/s]  1%|          | 304/26933 [00:32<38:17, 11.59it/s]  1%|          | 308/26933 [00:32<38:18, 11.58it/s]  1%|          | 312/26933 [00:33<38:12, 11.61it/s]  1%|          | 316/26933 [00:33<38:06, 11.64it/s]  1%|          | 320/26933 [00:34<38:10, 11.62it/s]  1%|          | 324/26933 [00:34<38:02, 11.66it/s]  1%|          | 328/26933 [00:34<37:54, 11.70it/s]  1%|          | 332/26933 [00:35<37:56, 11.69it/s]  1%|          | 336/26933 [00:35<37:49, 11.72it/s]  1%|▏         | 340/26933 [00:35<37:57, 11.67it/s]  1%|▏         | 344/26933 [00:36<37:52, 11.70it/s]  1%|▏         | 348/26933 [00:36<37:46, 11.73it/s]  1%|▏         | 352/26933 [00:36<37:42, 11.75it/s]  1%|▏         | 356/26933 [00:37<37:34, 11.79it/s]  1%|▏         | 360/26933 [00:37<37:25, 11.84it/s]  1%|▏         | 364/26933 [00:37<37:25, 11.83it/s]  1%|▏         | 368/26933 [00:38<37:21, 11.85it/s]  1%|▏         | 372/26933 [00:38<37:16, 11.88it/s]  1%|▏         | 376/26933 [00:38<37:14, 11.89it/s]  1%|▏         | 380/26933 [00:39<37:13, 11.89it/s]  1%|▏         | 384/26933 [00:39<37:14, 11.88it/s]  1%|▏         | 388/26933 [00:39<37:09, 11.90it/s]  1%|▏         | 392/26933 [00:40<37:08, 11.91it/s]  1%|▏         | 396/26933 [00:40<37:07, 11.92it/s]  1%|▏         | 400/26933 [00:40<37:17, 11.86it/s]  2%|▏         | 404/26933 [00:41<37:13, 11.88it/s]  2%|▏         | 408/26933 [00:41<37:13, 11.88it/s]  2%|▏         | 412/26933 [00:41<37:14, 11.87it/s]  2%|▏         | 416/26933 [00:42<37:08, 11.90it/s]  2%|▏         | 420/26933 [00:42<37:03, 11.92it/s]  2%|▏         | 424/26933 [00:42<37:06, 11.91it/s]  2%|▏         | 428/26933 [00:43<36:58, 11.95it/s]  2%|▏         | 432/26933 [00:43<36:54, 11.97it/s]  2%|▏         | 436/26933 [00:43<38:31, 11.46it/s]  2%|▏         | 440/26933 [00:44<39:30, 11.18it/s]  2%|▏         | 444/26933 [00:44<40:09, 10.99it/s]  2%|▏         | 448/26933 [00:44<40:43, 10.84it/s]  2%|▏         | 452/26933 [00:45<41:07, 10.73it/s]  2%|▏         | 456/26933 [00:45<41:14, 10.70it/s]  2%|▏         | 460/26933 [00:46<41:18, 10.68it/s]  2%|▏         | 464/26933 [00:46<41:22, 10.66it/s]  2%|▏         | 468/26933 [00:46<41:32, 10.62it/s]  2%|▏         | 472/26933 [00:47<41:27, 10.64it/s]  2%|▏         | 476/26933 [00:47<41:23, 10.65it/s]  2%|▏         | 480/26933 [00:47<41:26, 10.64it/s]  2%|▏         | 484/26933 [00:48<41:26, 10.64it/s]  2%|▏         | 488/26933 [00:48<41:22, 10.65it/s]  2%|▏         | 492/26933 [00:49<41:14, 10.69it/s]  2%|▏         | 496/26933 [00:49<41:11, 10.70it/s]  2%|▏         | 500/26933 [00:49<41:16, 10.67it/s]  2%|▏         | 504/26933 [00:50<41:10, 10.70it/s]  2%|▏         | 508/26933 [00:50<41:11, 10.69it/s]  2%|▏         | 512/26933 [00:50<41:25, 10.63it/s]  2%|▏         | 516/26933 [00:51<41:32, 10.60it/s]  2%|▏         | 520/26933 [00:51<41:25, 10.63it/s]  2%|▏         | 524/26933 [00:52<41:18, 10.66it/s]  2%|▏         | 528/26933 [00:52<41:14, 10.67it/s]  2%|▏         | 532/26933 [00:52<41:13, 10.68it/s]  2%|▏         | 536/26933 [00:53<41:01, 10.72it/s]  2%|▏         | 540/26933 [00:53<40:54, 10.75it/s]  2%|▏         | 544/26933 [00:53<40:54, 10.75it/s]  2%|▏         | 548/26933 [00:54<40:42, 10.80it/s]  2%|▏         | 552/26933 [00:54<40:46, 10.78it/s]  2%|▏         | 556/26933 [00:55<40:41, 10.80it/s]  2%|▏         | 560/26933 [00:55<40:41, 10.80it/s]  2%|▏         | 564/26933 [00:55<40:48, 10.77it/s]  2%|▏         | 568/26933 [00:56<40:44, 10.79it/s]  2%|▏         | 572/26933 [00:56<40:41, 10.80it/s]  2%|▏         | 576/26933 [00:56<40:45, 10.78it/s]  2%|▏         | 580/26933 [00:57<40:40, 10.80it/s]  2%|▏         | 584/26933 [00:57<40:39, 10.80it/s]  2%|▏         | 588/26933 [00:58<40:39, 10.80it/s]  2%|▏         | 592/26933 [00:58<40:41, 10.79it/s]  2%|▏         | 596/26933 [00:58<40:39, 10.80it/s]  2%|▏         | 600/26933 [00:59<40:35, 10.81it/s]  2%|▏         | 604/26933 [00:59<40:30, 10.83it/s]  2%|▏         | 608/26933 [00:59<40:30, 10.83it/s]  2%|▏         | 612/26933 [01:00<40:25, 10.85it/s]  2%|▏         | 616/26933 [01:00<40:17, 10.88it/s]  2%|▏         | 620/26933 [01:01<40:23, 10.86it/s]  2%|▏         | 624/26933 [01:01<40:25, 10.85it/s]  2%|▏         | 628/26933 [01:01<40:17, 10.88it/s]  2%|▏         | 632/26933 [01:02<38:59, 11.24it/s]  2%|▏         | 636/26933 [01:02<37:53, 11.57it/s]  2%|▏         | 640/26933 [01:02<37:10, 11.79it/s]  2%|▏         | 644/26933 [01:03<36:37, 11.97it/s]  2%|▏         | 648/26933 [01:03<36:15, 12.08it/s]  2%|▏         | 652/26933 [01:03<35:57, 12.18it/s]  2%|▏         | 656/26933 [01:04<35:52, 12.21it/s]  2%|▏         | 660/26933 [01:04<35:39, 12.28it/s]  2%|▏         | 664/26933 [01:04<35:31, 12.32it/s]  2%|▏         | 668/26933 [01:04<35:29, 12.33it/s]  2%|▏         | 672/26933 [01:05<35:23, 12.36it/s]  3%|▎         | 676/26933 [01:05<35:20, 12.38it/s]  3%|▎         | 680/26933 [01:05<35:22, 12.37it/s]  3%|▎         | 684/26933 [01:06<35:28, 12.34it/s]  3%|▎         | 688/26933 [01:06<35:18, 12.39it/s]  3%|▎         | 692/26933 [01:06<35:15, 12.40it/s]  3%|▎         | 696/26933 [01:07<35:11, 12.42it/s]  3%|▎         | 700/26933 [01:07<35:06, 12.45it/s]  3%|▎         | 704/26933 [01:07<35:09, 12.43it/s]  3%|▎         | 708/26933 [01:08<35:07, 12.44it/s]  3%|▎         | 712/26933 [01:08<35:07, 12.44it/s]  3%|▎         | 716/26933 [01:08<35:11, 12.42it/s]  3%|▎         | 720/26933 [01:09<35:54, 12.16it/s]  3%|▎         | 724/26933 [01:09<36:53, 11.84it/s]  3%|▎         | 728/26933 [01:09<37:39, 11.60it/s]  3%|▎         | 732/26933 [01:10<36:57, 11.81it/s]  3%|▎         | 736/26933 [01:10<36:16, 12.04it/s]  3%|▎         | 740/26933 [01:10<35:47, 12.20it/s]  3%|▎         | 744/26933 [01:11<35:24, 12.33it/s]  3%|▎         | 748/26933 [01:11<35:16, 12.37it/s]  3%|▎         | 752/26933 [01:11<35:07, 12.42it/s]  3%|▎         | 756/26933 [01:12<34:56, 12.49it/s]  3%|▎         | 760/26933 [01:12<34:54, 12.50it/s]  3%|▎         | 764/26933 [01:12<34:51, 12.51it/s]  3%|▎         | 768/26933 [01:13<34:48, 12.53it/s]  3%|▎         | 772/26933 [01:13<36:13, 12.04it/s]  3%|▎         | 776/26933 [01:13<37:04, 11.76it/s]  3%|▎         | 780/26933 [01:14<37:44, 11.55it/s]  3%|▎         | 784/26933 [01:14<38:01, 11.46it/s]  3%|▎         | 788/26933 [01:14<38:13, 11.40it/s]  3%|▎         | 792/26933 [01:15<38:21, 11.36it/s]  3%|▎         | 796/26933 [01:15<38:24, 11.34it/s]  3%|▎         | 800/26933 [01:15<38:39, 11.27it/s]  3%|▎         | 804/26933 [01:16<38:39, 11.26it/s]  3%|▎         | 808/26933 [01:16<38:47, 11.23it/s]  3%|▎         | 812/26933 [01:17<38:49, 11.21it/s]  3%|▎         | 816/26933 [01:17<38:43, 11.24it/s]  3%|▎         | 820/26933 [01:17<38:33, 11.29it/s]  3%|▎         | 824/26933 [01:18<38:34, 11.28it/s]  3%|▎         | 828/26933 [01:18<38:34, 11.28it/s]  3%|▎         | 832/26933 [01:18<38:39, 11.25it/s]  3%|▎         | 836/26933 [01:19<38:35, 11.27it/s]  3%|▎         | 840/26933 [01:19<38:30, 11.29it/s]  3%|▎         | 844/26933 [01:19<38:31, 11.29it/s]  3%|▎         | 848/26933 [01:20<37:22, 11.63it/s]  3%|▎         | 852/26933 [01:20<36:25, 11.93it/s]  3%|▎         | 856/26933 [01:20<35:48, 12.14it/s]  3%|▎         | 860/26933 [01:21<35:19, 12.30it/s]  3%|▎         | 864/26933 [01:21<34:59, 12.41it/s]  3%|▎         | 868/26933 [01:21<35:54, 12.10it/s]  3%|▎         | 872/26933 [01:22<36:33, 11.88it/s]  3%|▎         | 876/26933 [01:22<37:04, 11.71it/s]  3%|▎         | 880/26933 [01:22<37:24, 11.61it/s]  3%|▎         | 884/26933 [01:23<37:35, 11.55it/s]  3%|▎         | 888/26933 [01:23<37:51, 11.47it/s]  3%|▎         | 892/26933 [01:23<37:54, 11.45it/s]  3%|▎         | 896/26933 [01:24<37:54, 11.45it/s]  3%|▎         | 900/26933 [01:24<37:57, 11.43it/s]  3%|▎         | 904/26933 [01:24<37:59, 11.42it/s]  3%|▎         | 908/26933 [01:25<37:55, 11.44it/s]  3%|▎         | 912/26933 [01:25<37:59, 11.42it/s]  3%|▎         | 916/26933 [01:26<37:59, 11.41it/s]  3%|▎         | 920/26933 [01:26<37:59, 11.41it/s]  3%|▎         | 924/26933 [01:26<37:59, 11.41it/s]  3%|▎         | 928/26933 [01:27<38:04, 11.38it/s]  3%|▎         | 932/26933 [01:27<38:03, 11.39it/s]  3%|▎         | 936/26933 [01:27<37:57, 11.42it/s]  3%|▎         | 940/26933 [01:28<37:53, 11.43it/s]  4%|▎         | 944/26933 [01:28<37:52, 11.44it/s]  4%|▎         | 948/26933 [01:28<37:47, 11.46it/s]  4%|▎         | 952/26933 [01:29<37:46, 11.46it/s]  4%|▎         | 956/26933 [01:29<37:39, 11.50it/s]  4%|▎         | 960/26933 [01:29<37:39, 11.49it/s]  4%|▎         | 964/26933 [01:30<37:39, 11.49it/s]  4%|▎         | 968/26933 [01:30<37:31, 11.53it/s]  4%|▎         | 972/26933 [01:30<37:28, 11.55it/s]  4%|▎         | 976/26933 [01:31<37:23, 11.57it/s]  4%|▎         | 980/26933 [01:31<37:28, 11.54it/s]  4%|▎         | 984/26933 [01:31<37:34, 11.51it/s]  4%|▎         | 988/26933 [01:32<37:26, 11.55it/s]  4%|▎         | 992/26933 [01:32<37:14, 11.61it/s]  4%|▎         | 996/26933 [01:32<37:16, 11.60it/s]  4%|▎         | 1000/26933 [01:33<37:13, 11.61it/s]  4%|▎         | 1004/26933 [01:33<37:09, 11.63it/s]  4%|▎         | 1008/26933 [01:33<37:11, 11.62it/s]  4%|▍         | 1012/26933 [01:34<37:06, 11.64it/s]  4%|▍         | 1016/26933 [01:34<37:01, 11.67it/s]  4%|▍         | 1020/26933 [01:35<37:02, 11.66it/s]  4%|▍         | 1024/26933 [01:35<36:58, 11.68it/s]  4%|▍         | 1028/26933 [01:35<37:03, 11.65it/s]  4%|▍         | 1032/26933 [01:36<37:15, 11.59it/s]  4%|▍         | 1036/26933 [01:36<37:13, 11.59it/s]  4%|▍         | 1040/26933 [01:36<37:18, 11.57it/s]  4%|▍         | 1044/26933 [01:37<37:22, 11.54it/s]  4%|▍         | 1048/26933 [01:37<37:20, 11.56it/s]  4%|▍         | 1052/26933 [01:37<37:24, 11.53it/s]  4%|▍         | 1056/26933 [01:38<37:20, 11.55it/s]  4%|▍         | 1060/26933 [01:38<37:19, 11.55it/s]  4%|▍         | 1064/26933 [01:38<37:28, 11.50it/s]  4%|▍         | 1068/26933 [01:39<37:20, 11.55it/s]  4%|▍         | 1072/26933 [01:39<37:16, 11.57it/s]  4%|▍         | 1076/26933 [01:39<37:12, 11.58it/s]  4%|▍         | 1080/26933 [01:40<37:06, 11.61it/s]  4%|▍         | 1084/26933 [01:40<37:05, 11.61it/s]  4%|▍         | 1088/26933 [01:40<37:10, 11.59it/s]  4%|▍         | 1092/26933 [01:41<37:21, 11.53it/s]  4%|▍         | 1096/26933 [01:41<36:45, 11.72it/s]  4%|▍         | 1100/26933 [01:41<35:49, 12.02it/s]  4%|▍         | 1104/26933 [01:42<35:04, 12.28it/s]  4%|▍         | 1108/26933 [01:42<34:38, 12.42it/s]  4%|▍         | 1112/26933 [01:42<34:25, 12.50it/s]  4%|▍         | 1116/26933 [01:43<34:15, 12.56it/s]  4%|▍         | 1120/26933 [01:43<34:11, 12.58it/s]  4%|▍         | 1124/26933 [01:43<34:06, 12.61it/s]  4%|▍         | 1128/26933 [01:44<33:57, 12.67it/s]  4%|▍         | 1132/26933 [01:44<33:54, 12.68it/s]  4%|▍         | 1136/26933 [01:44<33:56, 12.67it/s]  4%|▍         | 1140/26933 [01:45<33:52, 12.69it/s]  4%|▍         | 1144/26933 [01:45<33:41, 12.75it/s]  4%|▍         | 1148/26933 [01:45<33:35, 12.79it/s]  4%|▍         | 1152/26933 [01:45<33:37, 12.78it/s]  4%|▍         | 1156/26933 [01:46<33:38, 12.77it/s]  4%|▍         | 1160/26933 [01:46<33:37, 12.78it/s]  4%|▍         | 1164/26933 [01:46<33:35, 12.79it/s]  4%|▍         | 1168/26933 [01:47<33:31, 12.81it/s]  4%|▍         | 1172/26933 [01:47<33:31, 12.80it/s]  4%|▍         | 1176/26933 [01:47<33:24, 12.85it/s]  4%|▍         | 1180/26933 [01:48<33:17, 12.90it/s]  4%|▍         | 1184/26933 [01:48<33:22, 12.86it/s]  4%|▍         | 1188/26933 [01:48<33:20, 12.87it/s]  4%|▍         | 1192/26933 [01:49<33:12, 12.92it/s]  4%|▍         | 1196/26933 [01:49<33:08, 12.94it/s]  4%|▍         | 1200/26933 [01:49<33:06, 12.95it/s]  4%|▍         | 1204/26933 [01:49<33:05, 12.96it/s]  4%|▍         | 1208/26933 [01:50<33:01, 12.98it/s]  5%|▍         | 1212/26933 [01:50<32:58, 13.00it/s]  5%|▍         | 1216/26933 [01:50<32:58, 13.00it/s]  5%|▍         | 1220/26933 [01:51<32:54, 13.03it/s]  5%|▍         | 1224/26933 [01:51<32:47, 13.06it/s]  5%|▍         | 1228/26933 [01:51<32:45, 13.08it/s]  5%|▍         | 1232/26933 [01:52<32:43, 13.09it/s]  5%|▍         | 1236/26933 [01:52<32:42, 13.09it/s]  5%|▍         | 1240/26933 [01:52<33:10, 12.91it/s]  5%|▍         | 1244/26933 [01:53<33:01, 12.96it/s]  5%|▍         | 1248/26933 [01:53<32:53, 13.02it/s]  5%|▍         | 1252/26933 [01:53<32:46, 13.06it/s]  5%|▍         | 1256/26933 [01:53<32:46, 13.06it/s]  5%|▍         | 1260/26933 [01:54<32:42, 13.08it/s]  5%|▍         | 1264/26933 [01:54<32:40, 13.09it/s]  5%|▍         | 1268/26933 [01:54<32:44, 13.06it/s]  5%|▍         | 1272/26933 [01:55<32:41, 13.08it/s]  5%|▍         | 1276/26933 [01:55<32:37, 13.11it/s]  5%|▍         | 1280/26933 [01:55<32:35, 13.12it/s]  5%|▍         | 1284/26933 [01:56<32:34, 13.12it/s]  5%|▍         | 1288/26933 [01:56<32:30, 13.15it/s]  5%|▍         | 1292/26933 [01:56<32:30, 13.15it/s]  5%|▍         | 1296/26933 [01:57<32:32, 13.13it/s]  5%|▍         | 1300/26933 [01:57<32:34, 13.11it/s]  5%|▍         | 1304/26933 [01:57<32:33, 13.12it/s]  5%|▍         | 1308/26933 [01:57<32:33, 13.12it/s]  5%|▍         | 1312/26933 [01:58<32:28, 13.15it/s]  5%|▍         | 1316/26933 [01:58<32:27, 13.16it/s]  5%|▍         | 1320/26933 [01:58<32:27, 13.15it/s]  5%|▍         | 1324/26933 [01:59<32:26, 13.16it/s]  5%|▍         | 1328/26933 [01:59<32:26, 13.15it/s]  5%|▍         | 1332/26933 [01:59<32:24, 13.16it/s]  5%|▍         | 1336/26933 [02:00<32:25, 13.16it/s]  5%|▍         | 1340/26933 [02:00<32:24, 13.17it/s]  5%|▍         | 1344/26933 [02:00<32:30, 13.12it/s]  5%|▌         | 1348/26933 [02:00<32:29, 13.12it/s]  5%|▌         | 1352/26933 [02:01<32:27, 13.13it/s]  5%|▌         | 1356/26933 [02:01<32:24, 13.16it/s]  5%|▌         | 1360/26933 [02:01<32:25, 13.14it/s]  5%|▌         | 1364/26933 [02:02<32:25, 13.14it/s]  5%|▌         | 1368/26933 [02:02<32:22, 13.16it/s]  5%|▌         | 1372/26933 [02:02<32:23, 13.15it/s]  5%|▌         | 1376/26933 [02:03<32:22, 13.16it/s]  5%|▌         | 1380/26933 [02:03<32:20, 13.17it/s]  5%|▌         | 1384/26933 [02:03<32:26, 13.13it/s]  5%|▌         | 1388/26933 [02:04<33:22, 12.76it/s]  5%|▌         | 1392/26933 [02:04<34:06, 12.48it/s]  5%|▌         | 1396/26933 [02:04<34:36, 12.30it/s]  5%|▌         | 1400/26933 [02:05<34:55, 12.18it/s]  5%|▌         | 1404/26933 [02:05<35:10, 12.10it/s]  5%|▌         | 1408/26933 [02:05<35:21, 12.03it/s]  5%|▌         | 1412/26933 [02:06<35:24, 12.01it/s]  5%|▌         | 1416/26933 [02:06<35:28, 11.99it/s]  5%|▌         | 1420/26933 [02:06<35:30, 11.98it/s]  5%|▌         | 1424/26933 [02:07<35:36, 11.94it/s]  5%|▌         | 1428/26933 [02:07<35:33, 11.95it/s]  5%|▌         | 1432/26933 [02:07<34:40, 12.26it/s]  5%|▌         | 1436/26933 [02:08<33:55, 12.53it/s]  5%|▌         | 1440/26933 [02:08<33:15, 12.78it/s]  5%|▌         | 1444/26933 [02:08<32:51, 12.93it/s]  5%|▌         | 1448/26933 [02:08<32:33, 13.05it/s]  5%|▌         | 1452/26933 [02:09<32:20, 13.13it/s]  5%|▌         | 1456/26933 [02:09<32:09, 13.20it/s]  5%|▌         | 1460/26933 [02:09<32:03, 13.24it/s]  5%|▌         | 1464/26933 [02:10<31:56, 13.29it/s]  5%|▌         | 1468/26933 [02:10<31:49, 13.34it/s]  5%|▌         | 1472/26933 [02:10<32:45, 12.95it/s]  5%|▌         | 1476/26933 [02:11<33:24, 12.70it/s]  5%|▌         | 1480/26933 [02:11<33:52, 12.52it/s]  6%|▌         | 1484/26933 [02:11<34:09, 12.41it/s]  6%|▌         | 1488/26933 [02:12<34:22, 12.34it/s]  6%|▌         | 1492/26933 [02:12<34:32, 12.27it/s]  6%|▌         | 1496/26933 [02:12<34:41, 12.22it/s]  6%|▌         | 1500/26933 [02:13<34:42, 12.21it/s]  6%|▌         | 1504/26933 [02:13<34:44, 12.20it/s]  6%|▌         | 1508/26933 [02:13<34:48, 12.17it/s]  6%|▌         | 1512/26933 [02:14<34:48, 12.17it/s]  6%|▌         | 1516/26933 [02:14<34:54, 12.14it/s]  6%|▌         | 1520/26933 [02:14<34:56, 12.12it/s]  6%|▌         | 1524/26933 [02:15<34:58, 12.11it/s]  6%|▌         | 1528/26933 [02:15<34:56, 12.12it/s]  6%|▌         | 1532/26933 [02:15<34:56, 12.11it/s]  6%|▌         | 1536/26933 [02:16<35:05, 12.06it/s]  6%|▌         | 1540/26933 [02:16<35:01, 12.08it/s]  6%|▌         | 1544/26933 [02:16<34:58, 12.10it/s]  6%|▌         | 1548/26933 [02:17<34:56, 12.11it/s]  6%|▌         | 1552/26933 [02:17<34:57, 12.10it/s]  6%|▌         | 1556/26933 [02:17<35:01, 12.07it/s]  6%|▌         | 1560/26933 [02:18<35:06, 12.05it/s]  6%|▌         | 1564/26933 [02:18<35:00, 12.08it/s]  6%|▌         | 1568/26933 [02:18<34:56, 12.10it/s]  6%|▌         | 1572/26933 [02:19<35:01, 12.07it/s]  6%|▌         | 1576/26933 [02:19<35:04, 12.05it/s]  6%|▌         | 1580/26933 [02:19<34:58, 12.08it/s]  6%|▌         | 1584/26933 [02:20<34:54, 12.11it/s]  6%|▌         | 1588/26933 [02:20<34:53, 12.10it/s]  6%|▌         | 1592/26933 [02:20<34:50, 12.12it/s]  6%|▌         | 1596/26933 [02:20<34:49, 12.13it/s]  6%|▌         | 1600/26933 [02:21<34:45, 12.15it/s]  6%|▌         | 1604/26933 [02:21<34:46, 12.14it/s]  6%|▌         | 1608/26933 [02:21<34:47, 12.13it/s]  6%|▌         | 1612/26933 [02:22<34:47, 12.13it/s]  6%|▌         | 1616/26933 [02:22<34:48, 12.12it/s]  6%|▌         | 1620/26933 [02:22<34:49, 12.12it/s]  6%|▌         | 1624/26933 [02:23<34:40, 12.16it/s]  6%|▌         | 1628/26933 [02:23<33:52, 12.45it/s]  6%|▌         | 1632/26933 [02:23<33:07, 12.73it/s]  6%|▌         | 1636/26933 [02:24<32:33, 12.95it/s]  6%|▌         | 1640/26933 [02:24<32:10, 13.11it/s]  6%|▌         | 1644/26933 [02:24<31:54, 13.21it/s]  6%|▌         | 1648/26933 [02:25<31:38, 13.32it/s]  6%|▌         | 1652/26933 [02:25<31:27, 13.39it/s]  6%|▌         | 1656/26933 [02:25<31:24, 13.42it/s]  6%|▌         | 1660/26933 [02:25<31:21, 13.43it/s]  6%|▌         | 1664/26933 [02:26<31:15, 13.48it/s]  6%|▌         | 1668/26933 [02:26<31:11, 13.50it/s]  6%|▌         | 1672/26933 [02:26<31:15, 13.47it/s]  6%|▌         | 1676/26933 [02:27<31:13, 13.48it/s]  6%|▌         | 1680/26933 [02:27<31:12, 13.49it/s]  6%|▋         | 1684/26933 [02:27<31:14, 13.47it/s]  6%|▋         | 1688/26933 [02:28<31:16, 13.45it/s]  6%|▋         | 1692/26933 [02:28<31:13, 13.47it/s]  6%|▋         | 1696/26933 [02:28<31:12, 13.48it/s]  6%|▋         | 1700/26933 [02:28<31:14, 13.46it/s]  6%|▋         | 1704/26933 [02:29<31:07, 13.51it/s]  6%|▋         | 1708/26933 [02:29<31:08, 13.50it/s]  6%|▋         | 1712/26933 [02:29<31:05, 13.52it/s]  6%|▋         | 1716/26933 [02:30<31:04, 13.53it/s]  6%|▋         | 1720/26933 [02:30<30:55, 13.59it/s]  6%|▋         | 1724/26933 [02:30<30:54, 13.59it/s]  6%|▋         | 1728/26933 [02:30<30:54, 13.59it/s]  6%|▋         | 1732/26933 [02:31<30:55, 13.58it/s]  6%|▋         | 1736/26933 [02:31<30:55, 13.58it/s]  6%|▋         | 1740/26933 [02:31<31:05, 13.51it/s]  6%|▋         | 1744/26933 [02:32<31:01, 13.53it/s]  6%|▋         | 1748/26933 [02:32<30:59, 13.54it/s]  7%|▋         | 1752/26933 [02:32<31:07, 13.48it/s]  7%|▋         | 1756/26933 [02:33<31:01, 13.53it/s]  7%|▋         | 1760/26933 [02:33<31:01, 13.52it/s]  7%|▋         | 1764/26933 [02:33<30:57, 13.55it/s]  7%|▋         | 1768/26933 [02:33<30:55, 13.56it/s]  7%|▋         | 1772/26933 [02:34<30:51, 13.59it/s]  7%|▋         | 1776/26933 [02:34<30:50, 13.59it/s]  7%|▋         | 1780/26933 [02:34<30:51, 13.58it/s]  7%|▋         | 1784/26933 [02:35<30:50, 13.59it/s]  7%|▋         | 1788/26933 [02:35<30:46, 13.62it/s]  7%|▋         | 1792/26933 [02:35<30:44, 13.63it/s]  7%|▋         | 1796/26933 [02:36<30:43, 13.63it/s]  7%|▋         | 1800/26933 [02:36<30:42, 13.64it/s]  7%|▋         | 1804/26933 [02:36<30:40, 13.65it/s]  7%|▋         | 1808/26933 [02:36<30:42, 13.64it/s]  7%|▋         | 1812/26933 [02:37<30:39, 13.66it/s]  7%|▋         | 1816/26933 [02:37<30:39, 13.66it/s]  7%|▋         | 1820/26933 [02:37<31:42, 13.20it/s]  7%|▋         | 1824/26933 [02:38<32:19, 12.95it/s]  7%|▋         | 1828/26933 [02:38<32:51, 12.74it/s]  7%|▋         | 1832/26933 [02:38<33:07, 12.63it/s]  7%|▋         | 1836/26933 [02:39<33:17, 12.56it/s]  7%|▋         | 1840/26933 [02:39<33:25, 12.51it/s]  7%|▋         | 1844/26933 [02:39<33:27, 12.49it/s]  7%|▋         | 1848/26933 [02:40<33:27, 12.49it/s]  7%|▋         | 1852/26933 [02:40<33:29, 12.48it/s]  7%|▋         | 1856/26933 [02:40<33:27, 12.49it/s]  7%|▋         | 1860/26933 [02:40<32:45, 12.75it/s]  7%|▋         | 1864/26933 [02:41<32:09, 13.00it/s]  7%|▋         | 1868/26933 [02:41<31:42, 13.18it/s]  7%|▋         | 1872/26933 [02:41<31:20, 13.32it/s]  7%|▋         | 1876/26933 [02:42<31:02, 13.45it/s]  7%|▋         | 1880/26933 [02:42<30:50, 13.54it/s]  7%|▋         | 1884/26933 [02:42<30:49, 13.54it/s]  7%|▋         | 1888/26933 [02:43<30:42, 13.59it/s]  7%|▋         | 1892/26933 [02:43<31:25, 13.28it/s]  7%|▋         | 1896/26933 [02:43<32:04, 13.01it/s]  7%|▋         | 1900/26933 [02:44<32:30, 12.83it/s]  7%|▋         | 1904/26933 [02:44<32:47, 12.72it/s]  7%|▋         | 1908/26933 [02:44<32:56, 12.66it/s]  7%|▋         | 1912/26933 [02:44<33:03, 12.61it/s]  7%|▋         | 1916/26933 [02:45<33:07, 12.59it/s]  7%|▋         | 1920/26933 [02:45<33:11, 12.56it/s]  7%|▋         | 1924/26933 [02:45<33:21, 12.50it/s]  7%|▋         | 1928/26933 [02:46<33:22, 12.49it/s]  7%|▋         | 1932/26933 [02:46<33:22, 12.49it/s]  7%|▋         | 1936/26933 [02:46<33:24, 12.47it/s]  7%|▋         | 1940/26933 [02:47<33:24, 12.47it/s]  7%|▋         | 1944/26933 [02:47<33:22, 12.48it/s]  7%|▋         | 1948/26933 [02:47<33:26, 12.45it/s]  7%|▋         | 1952/26933 [02:48<33:22, 12.47it/s]  7%|▋         | 1956/26933 [02:48<33:18, 12.50it/s]  7%|▋         | 1960/26933 [02:48<33:20, 12.48it/s]  7%|▋         | 1964/26933 [02:49<33:20, 12.48it/s]  7%|▋         | 1968/26933 [02:49<33:17, 12.50it/s]  7%|▋         | 1972/26933 [02:49<33:14, 12.52it/s]  7%|▋         | 1976/26933 [02:50<33:06, 12.56it/s]  7%|▋         | 1980/26933 [02:50<33:09, 12.54it/s]  7%|▋         | 1984/26933 [02:50<33:09, 12.54it/s]  7%|▋         | 1988/26933 [02:51<33:08, 12.54it/s]  7%|▋         | 1992/26933 [02:51<33:05, 12.56it/s]  7%|▋         | 1996/26933 [02:51<33:03, 12.57it/s]  7%|▋         | 2000/26933 [02:52<33:14, 12.50it/s]  7%|▋         | 2004/26933 [02:52<33:14, 12.50it/s]  7%|▋         | 2008/26933 [02:52<33:13, 12.50it/s]  7%|▋         | 2012/26933 [02:52<33:12, 12.51it/s]  7%|▋         | 2016/26933 [02:53<33:06, 12.54it/s]  8%|▊         | 2020/26933 [02:53<33:02, 12.57it/s]  8%|▊         | 2024/26933 [02:53<33:03, 12.56it/s]  8%|▊         | 2028/26933 [02:54<33:05, 12.54it/s]  8%|▊         | 2032/26933 [02:54<33:06, 12.53it/s]  8%|▊         | 2036/26933 [02:54<33:05, 12.54it/s]  8%|▊         | 2040/26933 [02:55<33:02, 12.56it/s]  8%|▊         | 2044/26933 [02:55<33:03, 12.55it/s]  8%|▊         | 2048/26933 [02:55<33:01, 12.56it/s]  8%|▊         | 2052/26933 [02:56<33:01, 12.55it/s]  8%|▊         | 2056/26933 [02:56<32:55, 12.59it/s]  8%|▊         | 2060/26933 [02:56<32:55, 12.59it/s]  8%|▊         | 2064/26933 [02:57<32:56, 12.58it/s]  8%|▊         | 2068/26933 [02:57<33:00, 12.56it/s]  8%|▊         | 2072/26933 [02:57<33:00, 12.55it/s]  8%|▊         | 2076/26933 [02:58<33:01, 12.55it/s]  8%|▊         | 2080/26933 [02:58<33:04, 12.53it/s]  8%|▊         | 2084/26933 [02:58<33:10, 12.48it/s]  8%|▊         | 2088/26933 [02:59<33:12, 12.47it/s]  8%|▊         | 2092/26933 [02:59<33:14, 12.45it/s]  8%|▊         | 2096/26933 [02:59<33:13, 12.46it/s]  8%|▊         | 2100/26933 [02:59<33:16, 12.44it/s]  8%|▊         | 2104/26933 [03:00<33:13, 12.45it/s]  8%|▊         | 2108/26933 [03:00<33:14, 12.45it/s]  8%|▊         | 2112/26933 [03:00<33:15, 12.44it/s]  8%|▊         | 2116/26933 [03:01<33:10, 12.47it/s]  8%|▊         | 2120/26933 [03:01<33:08, 12.48it/s]  8%|▊         | 2124/26933 [03:01<33:06, 12.49it/s]  8%|▊         | 2128/26933 [03:02<33:00, 12.53it/s]  8%|▊         | 2132/26933 [03:02<32:57, 12.54it/s]  8%|▊         | 2136/26933 [03:02<32:56, 12.55it/s]  8%|▊         | 2140/26933 [03:03<32:47, 12.60it/s]  8%|▊         | 2144/26933 [03:03<32:45, 12.61it/s]  8%|▊         | 2148/26933 [03:03<32:47, 12.59it/s]  8%|▊         | 2152/26933 [03:04<32:49, 12.58it/s]  8%|▊         | 2156/26933 [03:04<32:54, 12.55it/s]  8%|▊         | 2160/26933 [03:04<33:03, 12.49it/s]  8%|▊         | 2164/26933 [03:05<33:01, 12.50it/s]  8%|▊         | 2168/26933 [03:05<33:02, 12.49it/s]  8%|▊         | 2172/26933 [03:05<33:25, 12.35it/s]  8%|▊         | 2176/26933 [03:06<33:21, 12.37it/s]  8%|▊         | 2180/26933 [03:06<33:15, 12.40it/s]  8%|▊         | 2184/26933 [03:06<33:12, 12.42it/s]  8%|▊         | 2188/26933 [03:07<33:13, 12.41it/s]  8%|▊         | 2192/26933 [03:07<33:13, 12.41it/s]  8%|▊         | 2196/26933 [03:07<33:18, 12.37it/s]  8%|▊         | 2200/26933 [03:08<33:14, 12.40it/s]  8%|▊         | 2204/26933 [03:08<33:09, 12.43it/s]  8%|▊         | 2208/26933 [03:08<33:04, 12.46it/s]  8%|▊         | 2212/26933 [03:08<33:04, 12.46it/s]  8%|▊         | 2216/26933 [03:09<33:04, 12.46it/s]  8%|▊         | 2220/26933 [03:09<32:58, 12.49it/s]  8%|▊         | 2224/26933 [03:09<32:58, 12.49it/s]  8%|▊         | 2228/26933 [03:10<32:55, 12.50it/s]  8%|▊         | 2232/26933 [03:10<33:00, 12.47it/s]  8%|▊         | 2236/26933 [03:10<33:10, 12.41it/s]  8%|▊         | 2240/26933 [03:11<33:01, 12.46it/s]  8%|▊         | 2244/26933 [03:11<32:56, 12.49it/s]  8%|▊         | 2248/26933 [03:11<32:56, 12.49it/s]  8%|▊         | 2252/26933 [03:12<32:56, 12.49it/s]  8%|▊         | 2256/26933 [03:12<32:53, 12.50it/s]  8%|▊         | 2260/26933 [03:12<33:11, 12.39it/s]  8%|▊         | 2264/26933 [03:13<33:02, 12.44it/s]  8%|▊         | 2268/26933 [03:13<32:58, 12.47it/s]  8%|▊         | 2272/26933 [03:13<32:57, 12.47it/s]  8%|▊         | 2276/26933 [03:14<32:55, 12.48it/s]  8%|▊         | 2280/26933 [03:14<32:54, 12.48it/s]  8%|▊         | 2284/26933 [03:14<33:01, 12.44it/s]  8%|▊         | 2288/26933 [03:15<32:56, 12.47it/s]  9%|▊         | 2292/26933 [03:15<32:49, 12.51it/s]  9%|▊         | 2296/26933 [03:15<32:43, 12.55it/s]  9%|▊         | 2300/26933 [03:16<32:45, 12.53it/s]  9%|▊         | 2304/26933 [03:16<32:43, 12.54it/s]  9%|▊         | 2308/26933 [03:16<32:40, 12.56it/s]  9%|▊         | 2312/26933 [03:16<32:42, 12.55it/s]  9%|▊         | 2316/26933 [03:17<32:39, 12.56it/s]  9%|▊         | 2320/26933 [03:17<32:59, 12.43it/s]  9%|▊         | 2324/26933 [03:17<32:55, 12.46it/s]  9%|▊         | 2328/26933 [03:18<33:01, 12.42it/s]  9%|▊         | 2332/26933 [03:18<33:34, 12.21it/s]  9%|▊         | 2336/26933 [03:18<33:21, 12.29it/s]  9%|▊         | 2340/26933 [03:19<33:06, 12.38it/s]  9%|▊         | 2344/26933 [03:19<32:58, 12.43it/s]  9%|▊         | 2348/26933 [03:19<32:57, 12.43it/s]  9%|▊         | 2352/26933 [03:20<32:48, 12.48it/s]  9%|▊         | 2356/26933 [03:20<32:42, 12.52it/s]  9%|▉         | 2360/26933 [03:20<32:42, 12.52it/s]  9%|▉         | 2364/26933 [03:21<32:42, 12.52it/s]  9%|▉         | 2368/26933 [03:21<32:41, 12.52it/s]  9%|▉         | 2372/26933 [03:21<32:41, 12.52it/s]  9%|▉         | 2376/26933 [03:22<32:38, 12.54it/s]  9%|▉         | 2380/26933 [03:22<32:43, 12.51it/s]  9%|▉         | 2384/26933 [03:22<32:46, 12.48it/s]  9%|▉         | 2388/26933 [03:23<32:49, 12.46it/s]  9%|▉         | 2392/26933 [03:23<32:51, 12.45it/s]  9%|▉         | 2396/26933 [03:23<32:47, 12.47it/s]  9%|▉         | 2400/26933 [03:24<32:43, 12.50it/s]  9%|▉         | 2404/26933 [03:24<32:43, 12.49it/s]  9%|▉         | 2408/26933 [03:24<32:39, 12.52it/s]  9%|▉         | 2412/26933 [03:24<32:36, 12.54it/s]  9%|▉         | 2416/26933 [03:25<32:33, 12.55it/s]  9%|▉         | 2420/26933 [03:25<32:31, 12.56it/s]  9%|▉         | 2424/26933 [03:25<32:35, 12.54it/s]  9%|▉         | 2428/26933 [03:26<32:34, 12.54it/s]  9%|▉         | 2432/26933 [03:26<32:33, 12.54it/s]  9%|▉         | 2436/26933 [03:26<32:34, 12.53it/s]  9%|▉         | 2440/26933 [03:27<32:31, 12.55it/s]  9%|▉         | 2444/26933 [03:27<32:28, 12.57it/s]  9%|▉         | 2448/26933 [03:27<32:28, 12.56it/s]  9%|▉         | 2452/26933 [03:28<32:21, 12.61it/s]  9%|▉         | 2456/26933 [03:28<32:20, 12.61it/s]  9%|▉         | 2460/26933 [03:28<32:16, 12.64it/s]  9%|▉         | 2464/26933 [03:29<32:10, 12.68it/s]  9%|▉         | 2468/26933 [03:29<32:07, 12.69it/s]  9%|▉         | 2472/26933 [03:29<32:02, 12.72it/s]  9%|▉         | 2476/26933 [03:30<32:00, 12.73it/s]  9%|▉         | 2480/26933 [03:30<32:02, 12.72it/s]  9%|▉         | 2484/26933 [03:30<32:02, 12.72it/s]  9%|▉         | 2488/26933 [03:31<32:04, 12.70it/s]  9%|▉         | 2492/26933 [03:31<32:04, 12.70it/s]  9%|▉         | 2496/26933 [03:31<32:06, 12.68it/s]  9%|▉         | 2500/26933 [03:31<32:07, 12.68it/s]  9%|▉         | 2504/26933 [03:32<32:05, 12.69it/s]  9%|▉         | 2508/26933 [03:32<31:55, 12.75it/s]  9%|▉         | 2512/26933 [03:32<31:53, 12.77it/s]  9%|▉         | 2516/26933 [03:33<31:47, 12.80it/s]  9%|▉         | 2520/26933 [03:33<31:46, 12.80it/s]  9%|▉         | 2524/26933 [03:33<31:46, 12.80it/s]  9%|▉         | 2528/26933 [03:34<31:43, 12.82it/s]  9%|▉         | 2532/26933 [03:34<31:43, 12.82it/s]  9%|▉         | 2536/26933 [03:34<31:45, 12.80it/s]  9%|▉         | 2540/26933 [03:35<31:43, 12.81it/s]  9%|▉         | 2544/26933 [03:35<31:40, 12.83it/s]  9%|▉         | 2548/26933 [03:35<31:41, 12.82it/s]  9%|▉         | 2552/26933 [03:36<31:49, 12.77it/s]  9%|▉         | 2556/26933 [03:36<31:48, 12.77it/s] 10%|▉         | 2560/26933 [03:36<31:48, 12.77it/s] 10%|▉         | 2564/26933 [03:36<31:44, 12.79it/s] 10%|▉         | 2568/26933 [03:37<31:43, 12.80it/s] 10%|▉         | 2572/26933 [03:37<31:41, 12.81it/s] 10%|▉         | 2576/26933 [03:37<31:43, 12.80it/s] 10%|▉         | 2580/26933 [03:38<31:39, 12.82it/s] 10%|▉         | 2584/26933 [03:38<31:39, 12.82it/s] 10%|▉         | 2588/26933 [03:38<31:38, 12.83it/s] 10%|▉         | 2592/26933 [03:39<31:34, 12.85it/s] 10%|▉         | 2596/26933 [03:39<31:35, 12.84it/s] 10%|▉         | 2600/26933 [03:39<31:32, 12.85it/s] 10%|▉         | 2604/26933 [03:40<31:31, 12.86it/s] 10%|▉         | 2608/26933 [03:40<31:32, 12.85it/s] 10%|▉         | 2612/26933 [03:40<31:33, 12.85it/s] 10%|▉         | 2616/26933 [03:41<31:33, 12.84it/s] 10%|▉         | 2620/26933 [03:41<31:29, 12.87it/s] 10%|▉         | 2624/26933 [03:41<31:32, 12.84it/s] 10%|▉         | 2628/26933 [03:41<31:28, 12.87it/s] 10%|▉         | 2632/26933 [03:42<31:24, 12.89it/s] 10%|▉         | 2636/26933 [03:42<31:24, 12.89it/s] 10%|▉         | 2640/26933 [03:42<31:24, 12.89it/s] 10%|▉         | 2644/26933 [03:43<31:20, 12.91it/s] 10%|▉         | 2648/26933 [03:43<31:20, 12.91it/s] 10%|▉         | 2652/26933 [03:43<31:23, 12.89it/s] 10%|▉         | 2656/26933 [03:44<31:24, 12.88it/s] 10%|▉         | 2660/26933 [03:44<31:23, 12.89it/s] 10%|▉         | 2664/26933 [03:44<31:22, 12.89it/s] 10%|▉         | 2668/26933 [03:45<31:20, 12.90it/s] 10%|▉         | 2672/26933 [03:45<31:18, 12.91it/s] 10%|▉         | 2676/26933 [03:45<31:17, 12.92it/s] 10%|▉         | 2680/26933 [03:45<31:21, 12.89it/s] 10%|▉         | 2684/26933 [03:46<31:26, 12.85it/s] 10%|▉         | 2688/26933 [03:46<31:28, 12.84it/s] 10%|▉         | 2692/26933 [03:46<30:57, 13.05it/s] 10%|█         | 2696/26933 [03:47<30:29, 13.24it/s] 10%|█         | 2700/26933 [03:47<30:15, 13.35it/s] 10%|█         | 2704/26933 [03:47<29:51, 13.53it/s] 10%|█         | 2708/26933 [03:48<29:30, 13.68it/s] 10%|█         | 2712/26933 [03:48<29:17, 13.79it/s] 10%|█         | 2716/26933 [03:48<29:05, 13.88it/s] 10%|█         | 2720/26933 [03:48<29:01, 13.90it/s] 10%|█         | 2724/26933 [03:49<29:06, 13.86it/s] 10%|█         | 2728/26933 [03:49<29:10, 13.83it/s] 10%|█         | 2732/26933 [03:49<30:00, 13.44it/s] 10%|█         | 2736/26933 [03:50<30:26, 13.24it/s] 10%|█         | 2740/26933 [03:50<30:45, 13.11it/s] 10%|█         | 2744/26933 [03:50<31:00, 13.00it/s] 10%|█         | 2748/26933 [03:51<31:09, 12.94it/s] 10%|█         | 2752/26933 [03:51<31:14, 12.90it/s] 10%|█         | 2756/26933 [03:51<31:13, 12.91it/s] 10%|█         | 2760/26933 [03:51<31:18, 12.87it/s] 10%|█         | 2764/26933 [03:52<31:20, 12.85it/s] 10%|█         | 2768/26933 [03:52<31:22, 12.83it/s] 10%|█         | 2772/26933 [03:52<31:24, 12.82it/s] 10%|█         | 2776/26933 [03:53<31:20, 12.85it/s] 10%|█         | 2780/26933 [03:53<31:16, 12.87it/s] 10%|█         | 2784/26933 [03:53<31:18, 12.85it/s] 10%|█         | 2788/26933 [03:54<31:13, 12.88it/s] 10%|█         | 2792/26933 [03:54<31:17, 12.86it/s] 10%|█         | 2796/26933 [03:54<30:42, 13.10it/s] 10%|█         | 2800/26933 [03:55<30:08, 13.34it/s] 10%|█         | 2804/26933 [03:55<29:40, 13.55it/s] 10%|█         | 2808/26933 [03:55<29:18, 13.72it/s] 10%|█         | 2812/26933 [03:55<29:05, 13.82it/s] 10%|█         | 2816/26933 [03:56<28:54, 13.90it/s] 10%|█         | 2820/26933 [03:56<29:34, 13.59it/s] 10%|█         | 2824/26933 [03:56<30:09, 13.32it/s] 11%|█         | 2828/26933 [03:57<30:31, 13.16it/s] 11%|█         | 2832/26933 [03:57<30:47, 13.04it/s] 11%|█         | 2836/26933 [03:57<30:57, 12.97it/s] 11%|█         | 2840/26933 [03:58<31:01, 12.94it/s] 11%|█         | 2844/26933 [03:58<31:03, 12.93it/s] 11%|█         | 2848/26933 [03:58<31:01, 12.94it/s] 11%|█         | 2852/26933 [03:58<31:08, 12.89it/s] 11%|█         | 2856/26933 [03:59<31:06, 12.90it/s] 11%|█         | 2860/26933 [03:59<31:02, 12.93it/s] 11%|█         | 2864/26933 [03:59<30:59, 12.94it/s] 11%|█         | 2868/26933 [04:00<30:56, 12.96it/s] 11%|█         | 2872/26933 [04:00<30:53, 12.98it/s] 11%|█         | 2876/26933 [04:00<30:53, 12.98it/s] 11%|█         | 2880/26933 [04:01<30:49, 13.00it/s] 11%|█         | 2884/26933 [04:01<30:50, 13.00it/s] 11%|█         | 2888/26933 [04:01<30:51, 12.98it/s] 11%|█         | 2892/26933 [04:02<30:52, 12.98it/s] 11%|█         | 2896/26933 [04:02<30:56, 12.95it/s] 11%|█         | 2900/26933 [04:02<30:58, 12.93it/s] 11%|█         | 2904/26933 [04:03<31:00, 12.92it/s] 11%|█         | 2908/26933 [04:03<30:55, 12.94it/s] 11%|█         | 2912/26933 [04:03<30:54, 12.95it/s] 11%|█         | 2916/26933 [04:03<30:55, 12.94it/s] 11%|█         | 2920/26933 [04:04<30:51, 12.97it/s] 11%|█         | 2924/26933 [04:04<30:47, 13.00it/s] 11%|█         | 2928/26933 [04:04<30:46, 13.00it/s] 11%|█         | 2932/26933 [04:05<30:44, 13.01it/s] 11%|█         | 2936/26933 [04:05<30:44, 13.01it/s] 11%|█         | 2940/26933 [04:05<30:45, 13.00it/s] 11%|█         | 2944/26933 [04:06<30:44, 13.00it/s] 11%|█         | 2948/26933 [04:06<30:47, 12.98it/s] 11%|█         | 2952/26933 [04:06<30:45, 13.00it/s] 11%|█         | 2956/26933 [04:07<30:47, 12.98it/s] 11%|█         | 2960/26933 [04:07<30:48, 12.97it/s] 11%|█         | 2964/26933 [04:07<30:46, 12.98it/s] 11%|█         | 2968/26933 [04:07<30:48, 12.97it/s] 11%|█         | 2972/26933 [04:08<30:04, 13.28it/s] 11%|█         | 2976/26933 [04:08<29:30, 13.53it/s] 11%|█         | 2980/26933 [04:08<29:08, 13.70it/s] 11%|█         | 2984/26933 [04:09<28:48, 13.85it/s] 11%|█         | 2988/26933 [04:09<29:05, 13.72it/s] 11%|█         | 2992/26933 [04:09<29:36, 13.47it/s] 11%|█         | 2996/26933 [04:09<29:53, 13.35it/s] 11%|█         | 3000/26933 [04:10<30:03, 13.27it/s] 11%|█         | 3004/26933 [04:10<30:08, 13.23it/s] 11%|█         | 3008/26933 [04:10<30:20, 13.14it/s] 11%|█         | 3012/26933 [04:11<30:23, 13.12it/s] 11%|█         | 3016/26933 [04:11<30:23, 13.12it/s] 11%|█         | 3020/26933 [04:11<30:24, 13.11it/s] 11%|█         | 3024/26933 [04:12<30:22, 13.12it/s] 11%|█         | 3028/26933 [04:12<30:20, 13.13it/s] 11%|█▏        | 3032/26933 [04:12<30:23, 13.11it/s] 11%|█▏        | 3036/26933 [04:13<30:22, 13.11it/s] 11%|█▏        | 3040/26933 [04:13<30:24, 13.09it/s] 11%|█▏        | 3044/26933 [04:13<30:24, 13.09it/s] 11%|█▏        | 3048/26933 [04:13<30:30, 13.05it/s] 11%|█▏        | 3052/26933 [04:14<30:27, 13.07it/s] 11%|█▏        | 3056/26933 [04:14<30:26, 13.07it/s] 11%|█▏        | 3060/26933 [04:14<30:26, 13.07it/s] 11%|█▏        | 3064/26933 [04:15<30:23, 13.09it/s] 11%|█▏        | 3068/26933 [04:15<30:20, 13.11it/s] 11%|█▏        | 3072/26933 [04:15<30:16, 13.14it/s] 11%|█▏        | 3076/26933 [04:16<30:10, 13.18it/s] 11%|█▏        | 3080/26933 [04:16<30:08, 13.19it/s] 11%|█▏        | 3084/26933 [04:16<30:07, 13.20it/s] 11%|█▏        | 3088/26933 [04:16<30:07, 13.20it/s] 11%|█▏        | 3092/26933 [04:17<30:08, 13.18it/s] 11%|█▏        | 3096/26933 [04:17<30:14, 13.13it/s] 12%|█▏        | 3100/26933 [04:17<30:22, 13.08it/s] 12%|█▏        | 3104/26933 [04:18<29:47, 13.33it/s] 12%|█▏        | 3108/26933 [04:18<29:08, 13.63it/s] 12%|█▏        | 3112/26933 [04:18<28:47, 13.79it/s] 12%|█▏        | 3116/26933 [04:19<28:26, 13.96it/s] 12%|█▏        | 3120/26933 [04:19<28:13, 14.06it/s] 12%|█▏        | 3124/26933 [04:19<28:02, 14.15it/s] 12%|█▏        | 3128/26933 [04:19<28:01, 14.16it/s] 12%|█▏        | 3132/26933 [04:20<27:59, 14.17it/s] 12%|█▏        | 3136/26933 [04:20<27:55, 14.20it/s] 12%|█▏        | 3140/26933 [04:20<27:50, 14.24it/s] 12%|█▏        | 3144/26933 [04:20<27:54, 14.21it/s] 12%|█▏        | 3148/26933 [04:21<27:53, 14.21it/s] 12%|█▏        | 3152/26933 [04:21<27:51, 14.23it/s] 12%|█▏        | 3156/26933 [04:21<27:53, 14.20it/s] 12%|█▏        | 3160/26933 [04:22<27:47, 14.26it/s] 12%|█▏        | 3164/26933 [04:22<27:46, 14.26it/s] 12%|█▏        | 3168/26933 [04:22<27:43, 14.28it/s] 12%|█▏        | 3172/26933 [04:22<27:42, 14.29it/s] 12%|█▏        | 3176/26933 [04:23<27:38, 14.32it/s] 12%|█▏        | 3180/26933 [04:23<28:13, 14.02it/s] 12%|█▏        | 3184/26933 [04:23<28:50, 13.72it/s] 12%|█▏        | 3188/26933 [04:24<29:14, 13.53it/s] 12%|█▏        | 3192/26933 [04:24<29:28, 13.42it/s] 12%|█▏        | 3196/26933 [04:24<29:38, 13.35it/s] 12%|█▏        | 3200/26933 [04:25<29:44, 13.30it/s] 12%|█▏        | 3204/26933 [04:25<29:51, 13.25it/s] 12%|█▏        | 3208/26933 [04:25<29:54, 13.22it/s] 12%|█▏        | 3212/26933 [04:25<30:00, 13.17it/s] 12%|█▏        | 3216/26933 [04:26<29:59, 13.18it/s] 12%|█▏        | 3220/26933 [04:26<29:57, 13.19it/s] 12%|█▏        | 3224/26933 [04:26<29:55, 13.20it/s] 12%|█▏        | 3228/26933 [04:27<29:50, 13.24it/s] 12%|█▏        | 3232/26933 [04:27<29:57, 13.18it/s] 12%|█▏        | 3236/26933 [04:27<29:54, 13.20it/s] 12%|█▏        | 3240/26933 [04:28<29:52, 13.22it/s] 12%|█▏        | 3244/26933 [04:28<29:51, 13.22it/s] 12%|█▏        | 3248/26933 [04:28<29:51, 13.22it/s] 12%|█▏        | 3252/26933 [04:29<30:02, 13.14it/s] 12%|█▏        | 3256/26933 [04:29<30:01, 13.14it/s] 12%|█▏        | 3260/26933 [04:29<30:01, 13.14it/s] 12%|█▏        | 3264/26933 [04:29<30:00, 13.15it/s] 12%|█▏        | 3268/26933 [04:30<29:57, 13.17it/s] 12%|█▏        | 3272/26933 [04:30<29:55, 13.18it/s] 12%|█▏        | 3276/26933 [04:30<30:01, 13.13it/s] 12%|█▏        | 3280/26933 [04:31<29:57, 13.16it/s] 12%|█▏        | 3284/26933 [04:31<29:58, 13.15it/s] 12%|█▏        | 3288/26933 [04:31<29:52, 13.19it/s] 12%|█▏        | 3292/26933 [04:32<29:48, 13.22it/s] 12%|█▏        | 3296/26933 [04:32<29:46, 13.23it/s] 12%|█▏        | 3300/26933 [04:32<29:47, 13.22it/s] 12%|█▏        | 3304/26933 [04:32<29:47, 13.22it/s] 12%|█▏        | 3308/26933 [04:33<29:49, 13.20it/s] 12%|█▏        | 3312/26933 [04:33<29:44, 13.23it/s] 12%|█▏        | 3316/26933 [04:33<29:47, 13.21it/s] 12%|█▏        | 3320/26933 [04:34<29:46, 13.22it/s] 12%|█▏        | 3324/26933 [04:34<29:46, 13.22it/s] 12%|█▏        | 3328/26933 [04:34<29:47, 13.21it/s] 12%|█▏        | 3332/26933 [04:35<29:54, 13.15it/s] 12%|█▏        | 3336/26933 [04:35<29:49, 13.19it/s] 12%|█▏        | 3340/26933 [04:35<29:48, 13.19it/s] 12%|█▏        | 3344/26933 [04:35<29:51, 13.17it/s] 12%|█▏        | 3348/26933 [04:36<29:50, 13.17it/s] 12%|█▏        | 3352/26933 [04:36<29:53, 13.15it/s] 12%|█▏        | 3356/26933 [04:36<30:03, 13.07it/s] 12%|█▏        | 3360/26933 [04:37<29:56, 13.12it/s] 12%|█▏        | 3364/26933 [04:37<29:50, 13.16it/s] 13%|█▎        | 3368/26933 [04:37<29:57, 13.11it/s] 13%|█▎        | 3372/26933 [04:38<29:51, 13.15it/s] 13%|█▎        | 3376/26933 [04:38<29:45, 13.19it/s] 13%|█▎        | 3380/26933 [04:38<29:41, 13.22it/s] 13%|█▎        | 3384/26933 [04:39<29:36, 13.26it/s] 13%|█▎        | 3388/26933 [04:39<29:36, 13.25it/s] 13%|█▎        | 3392/26933 [04:39<29:37, 13.25it/s] 13%|█▎        | 3396/26933 [04:39<29:41, 13.21it/s] 13%|█▎        | 3400/26933 [04:40<29:43, 13.19it/s] 13%|█▎        | 3404/26933 [04:40<29:45, 13.18it/s] 13%|█▎        | 3408/26933 [04:40<29:45, 13.18it/s] 13%|█▎        | 3412/26933 [04:41<29:40, 13.21it/s] 13%|█▎        | 3416/26933 [04:41<29:41, 13.20it/s] 13%|█▎        | 3420/26933 [04:41<29:38, 13.22it/s] 13%|█▎        | 3424/26933 [04:42<29:37, 13.23it/s] 13%|█▎        | 3428/26933 [04:42<29:36, 13.23it/s] 13%|█▎        | 3432/26933 [04:42<29:36, 13.23it/s] 13%|█▎        | 3436/26933 [04:42<29:35, 13.24it/s] 13%|█▎        | 3440/26933 [04:43<29:31, 13.26it/s] 13%|█▎        | 3444/26933 [04:43<29:29, 13.27it/s] 13%|█▎        | 3448/26933 [04:43<29:26, 13.30it/s] 13%|█▎        | 3452/26933 [04:44<29:23, 13.32it/s] 13%|█▎        | 3456/26933 [04:44<29:20, 13.34it/s] 13%|█▎        | 3460/26933 [04:44<29:15, 13.37it/s] 13%|█▎        | 3464/26933 [04:45<29:12, 13.39it/s] 13%|█▎        | 3468/26933 [04:45<29:14, 13.37it/s] 13%|█▎        | 3472/26933 [04:45<29:18, 13.34it/s] 13%|█▎        | 3476/26933 [04:45<29:22, 13.31it/s] 13%|█▎        | 3480/26933 [04:46<29:22, 13.31it/s] 13%|█▎        | 3484/26933 [04:46<29:28, 13.26it/s] 13%|█▎        | 3488/26933 [04:46<29:28, 13.26it/s] 13%|█▎        | 3492/26933 [04:47<29:24, 13.29it/s] 13%|█▎        | 3496/26933 [04:47<29:23, 13.29it/s] 13%|█▎        | 3500/26933 [04:47<29:19, 13.32it/s] 13%|█▎        | 3504/26933 [04:48<29:17, 13.33it/s] 13%|█▎        | 3508/26933 [04:48<29:18, 13.32it/s] 13%|█▎        | 3512/26933 [04:48<29:17, 13.33it/s] 13%|█▎        | 3516/26933 [04:48<29:17, 13.33it/s] 13%|█▎        | 3520/26933 [04:49<29:13, 13.35it/s] 13%|█▎        | 3524/26933 [04:49<28:37, 13.63it/s] 13%|█▎        | 3528/26933 [04:49<28:08, 13.86it/s] 13%|█▎        | 3532/26933 [04:50<27:45, 14.05it/s] 13%|█▎        | 3536/26933 [04:50<27:31, 14.17it/s] 13%|█▎        | 3540/26933 [04:50<27:21, 14.26it/s] 13%|█▎        | 3544/26933 [04:50<27:17, 14.29it/s] 13%|█▎        | 3548/26933 [04:51<27:12, 14.33it/s] 13%|█▎        | 3552/26933 [04:51<27:03, 14.40it/s] 13%|█▎        | 3556/26933 [04:51<27:03, 14.40it/s] 13%|█▎        | 3560/26933 [04:52<27:00, 14.43it/s] 13%|█▎        | 3564/26933 [04:52<26:59, 14.43it/s] 13%|█▎        | 3568/26933 [04:52<26:54, 14.47it/s] 13%|█▎        | 3572/26933 [04:52<26:56, 14.45it/s] 13%|█▎        | 3576/26933 [04:53<26:55, 14.46it/s] 13%|█▎        | 3580/26933 [04:53<27:40, 14.06it/s] 13%|█▎        | 3584/26933 [04:53<27:32, 14.13it/s] 13%|█▎        | 3588/26933 [04:53<27:18, 14.25it/s] 13%|█▎        | 3592/26933 [04:54<27:10, 14.31it/s] 13%|█▎        | 3596/26933 [04:54<27:01, 14.39it/s] 13%|█▎        | 3600/26933 [04:54<26:58, 14.41it/s] 13%|█▎        | 3604/26933 [04:55<26:54, 14.45it/s] 13%|█▎        | 3608/26933 [04:55<26:50, 14.48it/s] 13%|█▎        | 3612/26933 [04:55<26:49, 14.49it/s] 13%|█▎        | 3616/26933 [04:55<26:53, 14.45it/s] 13%|█▎        | 3620/26933 [04:56<26:53, 14.45it/s] 13%|█▎        | 3624/26933 [04:56<26:59, 14.39it/s] 13%|█▎        | 3628/26933 [04:56<27:18, 14.22it/s] 13%|█▎        | 3632/26933 [04:57<27:21, 14.19it/s] 14%|█▎        | 3636/26933 [04:57<27:10, 14.29it/s] 14%|█▎        | 3640/26933 [04:57<27:00, 14.37it/s] 14%|█▎        | 3644/26933 [04:57<26:53, 14.43it/s] 14%|█▎        | 3648/26933 [04:58<26:46, 14.50it/s] 14%|█▎        | 3652/26933 [04:58<26:40, 14.55it/s] 14%|█▎        | 3656/26933 [04:58<26:35, 14.59it/s] 14%|█▎        | 3660/26933 [04:58<26:40, 14.54it/s] 14%|█▎        | 3664/26933 [04:59<26:37, 14.56it/s] 14%|█▎        | 3668/26933 [04:59<26:38, 14.55it/s] 14%|█▎        | 3672/26933 [04:59<26:38, 14.55it/s] 14%|█▎        | 3676/26933 [05:00<26:37, 14.56it/s] 14%|█▎        | 3680/26933 [05:00<26:33, 14.60it/s] 14%|█▎        | 3684/26933 [05:00<26:31, 14.61it/s] 14%|█▎        | 3688/26933 [05:00<26:34, 14.58it/s] 14%|█▎        | 3692/26933 [05:01<26:31, 14.61it/s] 14%|█▎        | 3696/26933 [05:01<26:30, 14.61it/s] 14%|█▎        | 3700/26933 [05:01<26:37, 14.54it/s] 14%|█▍        | 3704/26933 [05:01<26:31, 14.60it/s] 14%|█▍        | 3708/26933 [05:02<26:29, 14.61it/s] 14%|█▍        | 3712/26933 [05:02<26:29, 14.61it/s] 14%|█▍        | 3716/26933 [05:02<26:31, 14.58it/s] 14%|█▍        | 3720/26933 [05:03<26:38, 14.53it/s] 14%|█▍        | 3724/26933 [05:03<26:40, 14.50it/s] 14%|█▍        | 3728/26933 [05:03<26:36, 14.53it/s] 14%|█▍        | 3732/26933 [05:03<26:33, 14.56it/s] 14%|█▍        | 3736/26933 [05:04<26:31, 14.58it/s] 14%|█▍        | 3740/26933 [05:04<26:26, 14.62it/s] 14%|█▍        | 3744/26933 [05:04<26:26, 14.62it/s] 14%|█▍        | 3748/26933 [05:05<26:26, 14.62it/s] 14%|█▍        | 3752/26933 [05:05<26:28, 14.59it/s] 14%|█▍        | 3756/26933 [05:05<26:28, 14.59it/s] 14%|█▍        | 3760/26933 [05:05<26:26, 14.61it/s] 14%|█▍        | 3764/26933 [05:06<26:21, 14.65it/s] 14%|█▍        | 3768/26933 [05:06<26:18, 14.68it/s] 14%|█▍        | 3772/26933 [05:06<26:15, 14.70it/s] 14%|█▍        | 3776/26933 [05:06<26:20, 14.65it/s] 14%|█▍        | 3780/26933 [05:07<26:15, 14.70it/s] 14%|█▍        | 3784/26933 [05:07<26:16, 14.69it/s] 14%|█▍        | 3788/26933 [05:07<26:17, 14.68it/s] 14%|█▍        | 3792/26933 [05:08<26:16, 14.68it/s] 14%|█▍        | 3796/26933 [05:08<26:14, 14.69it/s] 14%|█▍        | 3800/26933 [05:08<26:16, 14.68it/s] 14%|█▍        | 3804/26933 [05:08<26:22, 14.61it/s] 14%|█▍        | 3808/26933 [05:09<26:23, 14.60it/s] 14%|█▍        | 3812/26933 [05:09<26:23, 14.60it/s] 14%|█▍        | 3816/26933 [05:09<26:24, 14.59it/s] 14%|█▍        | 3820/26933 [05:09<26:26, 14.57it/s] 14%|█▍        | 3824/26933 [05:10<26:28, 14.54it/s] 14%|█▍        | 3828/26933 [05:10<26:27, 14.56it/s] 14%|█▍        | 3832/26933 [05:10<26:28, 14.54it/s] 14%|█▍        | 3836/26933 [05:11<26:21, 14.60it/s] 14%|█▍        | 3840/26933 [05:11<26:18, 14.63it/s] 14%|█▍        | 3844/26933 [05:11<26:16, 14.65it/s] 14%|█▍        | 3848/26933 [05:11<26:19, 14.61it/s] 14%|█▍        | 3852/26933 [05:12<26:18, 14.62it/s] 14%|█▍        | 3856/26933 [05:12<26:19, 14.61it/s] 14%|█▍        | 3860/26933 [05:12<26:18, 14.61it/s] 14%|█▍        | 3864/26933 [05:12<26:17, 14.62it/s] 14%|█▍        | 3868/26933 [05:13<26:14, 14.65it/s] 14%|█▍        | 3872/26933 [05:13<26:12, 14.67it/s] 14%|█▍        | 3876/26933 [05:13<26:14, 14.64it/s] 14%|█▍        | 3880/26933 [05:14<26:12, 14.66it/s] 14%|█▍        | 3884/26933 [05:14<26:13, 14.65it/s] 14%|█▍        | 3888/26933 [05:14<26:12, 14.65it/s] 14%|█▍        | 3892/26933 [05:14<26:18, 14.60it/s] 14%|█▍        | 3896/26933 [05:15<26:17, 14.61it/s] 14%|█▍        | 3900/26933 [05:15<26:14, 14.62it/s] 14%|█▍        | 3904/26933 [05:15<26:53, 14.27it/s] 15%|█▍        | 3908/26933 [05:15<27:22, 14.02it/s] 15%|█▍        | 3912/26933 [05:16<27:40, 13.87it/s] 15%|█▍        | 3916/26933 [05:16<27:50, 13.78it/s] 15%|█▍        | 3920/26933 [05:16<27:59, 13.71it/s] 15%|█▍        | 3924/26933 [05:17<27:58, 13.71it/s] 15%|█▍        | 3928/26933 [05:17<28:06, 13.64it/s] 15%|█▍        | 3932/26933 [05:17<28:10, 13.60it/s] 15%|█▍        | 3936/26933 [05:18<28:12, 13.59it/s] 15%|█▍        | 3940/26933 [05:18<27:45, 13.80it/s] 15%|█▍        | 3944/26933 [05:18<27:16, 14.04it/s] 15%|█▍        | 3948/26933 [05:18<26:56, 14.22it/s] 15%|█▍        | 3952/26933 [05:19<26:40, 14.36it/s] 15%|█▍        | 3956/26933 [05:19<26:28, 14.46it/s] 15%|█▍        | 3960/26933 [05:19<26:21, 14.52it/s] 15%|█▍        | 3964/26933 [05:19<26:19, 14.55it/s] 15%|█▍        | 3968/26933 [05:20<26:14, 14.59it/s] 15%|█▍        | 3972/26933 [05:20<26:09, 14.63it/s] 15%|█▍        | 3976/26933 [05:20<26:08, 14.63it/s] 15%|█▍        | 3980/26933 [05:21<26:01, 14.70it/s] 15%|█▍        | 3984/26933 [05:21<25:58, 14.72it/s] 15%|█▍        | 3988/26933 [05:21<25:56, 14.74it/s] 15%|█▍        | 3992/26933 [05:21<26:02, 14.69it/s] 15%|█▍        | 3996/26933 [05:22<26:03, 14.67it/s] 15%|█▍        | 4000/26933 [05:22<26:02, 14.68it/s] 15%|█▍        | 4004/26933 [05:22<25:57, 14.72it/s] 15%|█▍        | 4008/26933 [05:22<26:02, 14.68it/s] 15%|█▍        | 4012/26933 [05:23<26:04, 14.65it/s] 15%|█▍        | 4016/26933 [05:23<26:06, 14.63it/s] 15%|█▍        | 4020/26933 [05:23<26:07, 14.62it/s] 15%|█▍        | 4024/26933 [05:24<25:56, 14.72it/s] 15%|█▍        | 4028/26933 [05:24<25:54, 14.74it/s] 15%|█▍        | 4032/26933 [05:24<25:45, 14.82it/s] 15%|█▍        | 4036/26933 [05:24<25:45, 14.82it/s] 15%|█▌        | 4040/26933 [05:25<25:37, 14.89it/s] 15%|█▌        | 4044/26933 [05:25<25:35, 14.90it/s] 15%|█▌        | 4048/26933 [05:25<25:34, 14.91it/s] 15%|█▌        | 4052/26933 [05:25<25:44, 14.81it/s] 15%|█▌        | 4056/26933 [05:26<25:47, 14.79it/s] 15%|█▌        | 4060/26933 [05:26<25:47, 14.78it/s] 15%|█▌        | 4064/26933 [05:26<25:59, 14.67it/s] 15%|█▌        | 4068/26933 [05:27<25:52, 14.73it/s] 15%|█▌        | 4072/26933 [05:27<25:52, 14.73it/s] 15%|█▌        | 4076/26933 [05:27<25:55, 14.69it/s] 15%|█▌        | 4080/26933 [05:27<25:56, 14.68it/s] 15%|█▌        | 4084/26933 [05:28<25:57, 14.67it/s] 15%|█▌        | 4088/26933 [05:28<25:52, 14.71it/s] 15%|█▌        | 4092/26933 [05:28<25:53, 14.70it/s] 15%|█▌        | 4096/26933 [05:28<25:55, 14.68it/s] 15%|█▌        | 4100/26933 [05:29<25:51, 14.72it/s] 15%|█▌        | 4104/26933 [05:29<25:53, 14.69it/s] 15%|█▌        | 4108/26933 [05:29<25:59, 14.64it/s] 15%|█▌        | 4112/26933 [05:30<25:56, 14.66it/s] 15%|█▌        | 4116/26933 [05:30<26:01, 14.61it/s] 15%|█▌        | 4120/26933 [05:30<25:58, 14.64it/s] 15%|█▌        | 4124/26933 [05:30<25:56, 14.65it/s] 15%|█▌        | 4128/26933 [05:31<25:52, 14.69it/s] 15%|█▌        | 4132/26933 [05:31<25:50, 14.71it/s] 15%|█▌        | 4136/26933 [05:31<25:44, 14.76it/s] 15%|█▌        | 4140/26933 [05:31<25:44, 14.75it/s] 15%|█▌        | 4144/26933 [05:32<25:40, 14.79it/s] 15%|█▌        | 4148/26933 [05:32<25:42, 14.78it/s] 15%|█▌        | 4152/26933 [05:32<25:36, 14.83it/s] 15%|█▌        | 4156/26933 [05:32<25:36, 14.82it/s] 15%|█▌        | 4160/26933 [05:33<25:34, 14.84it/s] 15%|█▌        | 4164/26933 [05:33<25:34, 14.84it/s] 15%|█▌        | 4168/26933 [05:33<25:39, 14.79it/s] 15%|█▌        | 4172/26933 [05:34<25:42, 14.76it/s] 16%|█▌        | 4176/26933 [05:34<25:42, 14.75it/s] 16%|█▌        | 4180/26933 [05:34<26:23, 14.37it/s] 16%|█▌        | 4184/26933 [05:34<26:50, 14.13it/s] 16%|█▌        | 4188/26933 [05:35<27:04, 14.00it/s] 16%|█▌        | 4192/26933 [05:35<27:17, 13.89it/s] 16%|█▌        | 4196/26933 [05:35<27:28, 13.79it/s] 16%|█▌        | 4200/26933 [05:36<27:31, 13.77it/s] 16%|█▌        | 4204/26933 [05:36<27:10, 13.94it/s] 16%|█▌        | 4208/26933 [05:36<26:44, 14.16it/s] 16%|█▌        | 4212/26933 [05:36<26:27, 14.31it/s] 16%|█▌        | 4216/26933 [05:37<26:14, 14.43it/s] 16%|█▌        | 4220/26933 [05:37<26:02, 14.53it/s] 16%|█▌        | 4224/26933 [05:37<26:01, 14.54it/s] 16%|█▌        | 4228/26933 [05:38<25:54, 14.61it/s] 16%|█▌        | 4232/26933 [05:38<25:48, 14.66it/s] 16%|█▌        | 4236/26933 [05:38<25:44, 14.69it/s] 16%|█▌        | 4240/26933 [05:38<25:48, 14.65it/s] 16%|█▌        | 4244/26933 [05:39<25:42, 14.71it/s] 16%|█▌        | 4248/26933 [05:39<25:37, 14.75it/s] 16%|█▌        | 4252/26933 [05:39<25:35, 14.77it/s] 16%|█▌        | 4256/26933 [05:39<25:45, 14.68it/s] 16%|█▌        | 4260/26933 [05:40<25:45, 14.67it/s] 16%|█▌        | 4264/26933 [05:40<25:43, 14.69it/s] 16%|█▌        | 4268/26933 [05:40<25:47, 14.65it/s] 16%|█▌        | 4272/26933 [05:41<25:40, 14.71it/s] 16%|█▌        | 4276/26933 [05:41<25:39, 14.72it/s] 16%|█▌        | 4280/26933 [05:41<25:39, 14.71it/s] 16%|█▌        | 4284/26933 [05:41<25:58, 14.54it/s] 16%|█▌        | 4288/26933 [05:42<25:49, 14.62it/s] 16%|█▌        | 4292/26933 [05:42<25:38, 14.71it/s] 16%|█▌        | 4296/26933 [05:42<25:31, 14.78it/s] 16%|█▌        | 4300/26933 [05:42<25:32, 14.77it/s] 16%|█▌        | 4304/26933 [05:43<25:29, 14.79it/s] 16%|█▌        | 4308/26933 [05:43<25:30, 14.78it/s] 16%|█▌        | 4312/26933 [05:43<25:26, 14.82it/s] 16%|█▌        | 4316/26933 [05:43<25:26, 14.82it/s] 16%|█▌        | 4320/26933 [05:44<25:23, 14.85it/s] 16%|█▌        | 4324/26933 [05:44<25:27, 14.81it/s] 16%|█▌        | 4328/26933 [05:44<25:30, 14.77it/s] 16%|█▌        | 4332/26933 [05:45<25:30, 14.77it/s] 16%|█▌        | 4336/26933 [05:45<25:26, 14.80it/s] 16%|█▌        | 4340/26933 [05:45<25:25, 14.81it/s] 16%|█▌        | 4344/26933 [05:45<25:28, 14.78it/s] 16%|█▌        | 4348/26933 [05:46<25:23, 14.82it/s] 16%|█▌        | 4352/26933 [05:46<25:23, 14.83it/s] 16%|█▌        | 4356/26933 [05:46<25:23, 14.82it/s] 16%|█▌        | 4360/26933 [05:46<25:25, 14.80it/s] 16%|█▌        | 4364/26933 [05:47<25:23, 14.82it/s] 16%|█▌        | 4368/26933 [05:47<25:30, 14.75it/s] 16%|█▌        | 4372/26933 [05:47<25:27, 14.77it/s] 16%|█▌        | 4376/26933 [05:48<25:22, 14.82it/s] 16%|█▋        | 4380/26933 [05:48<25:19, 14.85it/s] 16%|█▋        | 4384/26933 [05:48<25:13, 14.89it/s] 16%|█▋        | 4388/26933 [05:48<25:15, 14.88it/s] 16%|█▋        | 4392/26933 [05:49<25:11, 14.92it/s] 16%|█▋        | 4396/26933 [05:49<25:09, 14.93it/s] 16%|█▋        | 4400/26933 [05:49<25:06, 14.95it/s] 16%|█▋        | 4404/26933 [05:49<25:12, 14.90it/s] 16%|█▋        | 4408/26933 [05:50<25:14, 14.87it/s] 16%|█▋        | 4412/26933 [05:50<25:10, 14.91it/s] 16%|█▋        | 4416/26933 [05:50<25:11, 14.90it/s] 16%|█▋        | 4420/26933 [05:51<25:10, 14.91it/s] 16%|█▋        | 4424/26933 [05:51<25:08, 14.92it/s] 16%|█▋        | 4428/26933 [05:51<25:08, 14.92it/s] 16%|█▋        | 4432/26933 [05:51<25:13, 14.86it/s] 16%|█▋        | 4436/26933 [05:52<25:11, 14.88it/s] 16%|█▋        | 4440/26933 [05:52<25:09, 14.90it/s] 17%|█▋        | 4444/26933 [05:52<25:06, 14.93it/s] 17%|█▋        | 4448/26933 [05:52<25:09, 14.90it/s] 17%|█▋        | 4452/26933 [05:53<25:09, 14.90it/s] 17%|█▋        | 4456/26933 [05:53<25:02, 14.96it/s] 17%|█▋        | 4460/26933 [05:53<25:01, 14.96it/s] 17%|█▋        | 4464/26933 [05:53<25:05, 14.92it/s] 17%|█▋        | 4468/26933 [05:54<25:03, 14.94it/s] 17%|█▋        | 4472/26933 [05:54<25:43, 14.55it/s] 17%|█▋        | 4476/26933 [05:54<26:05, 14.34it/s] 17%|█▋        | 4480/26933 [05:55<26:23, 14.18it/s] 17%|█▋        | 4484/26933 [05:55<26:30, 14.11it/s] 17%|█▋        | 4488/26933 [05:55<26:36, 14.06it/s] 17%|█▋        | 4492/26933 [05:55<26:43, 13.99it/s] 17%|█▋        | 4496/26933 [05:56<26:44, 13.98it/s] 17%|█▋        | 4500/26933 [05:56<26:46, 13.96it/s] 17%|█▋        | 4504/26933 [05:56<26:55, 13.89it/s] 17%|█▋        | 4508/26933 [05:57<26:54, 13.89it/s] 17%|█▋        | 4512/26933 [05:57<26:54, 13.88it/s] 17%|█▋        | 4516/26933 [05:57<26:51, 13.91it/s] 17%|█▋        | 4520/26933 [05:57<27:08, 13.77it/s] 17%|█▋        | 4524/26933 [05:58<27:10, 13.75it/s] 17%|█▋        | 4528/26933 [05:58<27:07, 13.77it/s] 17%|█▋        | 4532/26933 [05:58<27:03, 13.80it/s] 17%|█▋        | 4536/26933 [05:59<26:57, 13.85it/s] 17%|█▋        | 4540/26933 [05:59<26:55, 13.86it/s] 17%|█▋        | 4544/26933 [05:59<26:58, 13.83it/s] 17%|█▋        | 4548/26933 [05:59<26:50, 13.90it/s] 17%|█▋        | 4552/26933 [06:00<26:48, 13.92it/s] 17%|█▋        | 4556/26933 [06:00<26:50, 13.90it/s] 17%|█▋        | 4560/26933 [06:00<26:52, 13.88it/s] 17%|█▋        | 4564/26933 [06:01<26:49, 13.90it/s] 17%|█▋        | 4568/26933 [06:01<26:24, 14.12it/s] 17%|█▋        | 4572/26933 [06:01<25:57, 14.36it/s] 17%|█▋        | 4576/26933 [06:01<25:42, 14.50it/s] 17%|█▋        | 4580/26933 [06:02<25:28, 14.63it/s] 17%|█▋        | 4584/26933 [06:02<25:18, 14.72it/s] 17%|█▋        | 4588/26933 [06:02<25:17, 14.72it/s] 17%|█▋        | 4592/26933 [06:03<25:09, 14.80it/s] 17%|█▋        | 4596/26933 [06:03<25:05, 14.84it/s] 17%|█▋        | 4600/26933 [06:03<25:01, 14.87it/s] 17%|█▋        | 4604/26933 [06:03<25:04, 14.85it/s] 17%|█▋        | 4608/26933 [06:04<24:59, 14.89it/s] 17%|█▋        | 4612/26933 [06:04<25:00, 14.88it/s] 17%|█▋        | 4616/26933 [06:04<24:55, 14.92it/s] 17%|█▋        | 4620/26933 [06:04<24:58, 14.89it/s] 17%|█▋        | 4624/26933 [06:05<24:56, 14.91it/s] 17%|█▋        | 4628/26933 [06:05<24:56, 14.90it/s] 17%|█▋        | 4632/26933 [06:05<24:56, 14.91it/s] 17%|█▋        | 4636/26933 [06:05<24:57, 14.89it/s] 17%|█▋        | 4640/26933 [06:06<24:53, 14.93it/s] 17%|█▋        | 4644/26933 [06:06<24:54, 14.92it/s] 17%|█▋        | 4648/26933 [06:06<25:04, 14.82it/s] 17%|█▋        | 4652/26933 [06:07<24:58, 14.87it/s] 17%|█▋        | 4656/26933 [06:07<24:58, 14.87it/s] 17%|█▋        | 4660/26933 [06:07<24:59, 14.86it/s] 17%|█▋        | 4664/26933 [06:07<24:55, 14.89it/s] 17%|█▋        | 4668/26933 [06:08<24:54, 14.90it/s] 17%|█▋        | 4672/26933 [06:08<24:56, 14.88it/s] 17%|█▋        | 4676/26933 [06:08<24:59, 14.85it/s] 17%|█▋        | 4680/26933 [06:08<25:05, 14.78it/s] 17%|█▋        | 4684/26933 [06:09<25:06, 14.77it/s] 17%|█▋        | 4688/26933 [06:09<25:07, 14.75it/s] 17%|█▋        | 4692/26933 [06:09<25:06, 14.76it/s] 17%|█▋        | 4696/26933 [06:10<25:02, 14.80it/s] 17%|█▋        | 4700/26933 [06:10<24:57, 14.85it/s] 17%|█▋        | 4704/26933 [06:10<24:55, 14.86it/s] 17%|█▋        | 4708/26933 [06:10<24:54, 14.87it/s] 17%|█▋        | 4712/26933 [06:11<24:50, 14.90it/s] 18%|█▊        | 4716/26933 [06:11<24:49, 14.92it/s] 18%|█▊        | 4720/26933 [06:11<24:46, 14.95it/s] 18%|█▊        | 4724/26933 [06:11<24:47, 14.93it/s] 18%|█▊        | 4728/26933 [06:12<24:44, 14.95it/s] 18%|█▊        | 4732/26933 [06:12<25:24, 14.57it/s] 18%|█▊        | 4736/26933 [06:12<25:49, 14.33it/s] 18%|█▊        | 4740/26933 [06:13<26:04, 14.18it/s] 18%|█▊        | 4744/26933 [06:13<26:16, 14.07it/s] 18%|█▊        | 4748/26933 [06:13<26:25, 13.99it/s] 18%|█▊        | 4752/26933 [06:13<26:29, 13.95it/s] 18%|█▊        | 4756/26933 [06:14<26:37, 13.88it/s] 18%|█▊        | 4760/26933 [06:14<26:39, 13.86it/s] 18%|█▊        | 4764/26933 [06:14<26:38, 13.86it/s] 18%|█▊        | 4768/26933 [06:15<26:39, 13.86it/s] 18%|█▊        | 4772/26933 [06:15<26:35, 13.89it/s] 18%|█▊        | 4776/26933 [06:15<26:31, 13.92it/s] 18%|█▊        | 4780/26933 [06:15<26:33, 13.90it/s] 18%|█▊        | 4784/26933 [06:16<26:31, 13.92it/s] 18%|█▊        | 4788/26933 [06:16<26:30, 13.93it/s] 18%|█▊        | 4792/26933 [06:16<26:31, 13.91it/s] 18%|█▊        | 4796/26933 [06:17<26:33, 13.90it/s] 18%|█▊        | 4800/26933 [06:17<26:32, 13.90it/s] 18%|█▊        | 4804/26933 [06:17<26:34, 13.88it/s] 18%|█▊        | 4808/26933 [06:17<26:34, 13.87it/s] 18%|█▊        | 4812/26933 [06:18<26:30, 13.91it/s] 18%|█▊        | 4816/26933 [06:18<26:28, 13.92it/s] 18%|█▊        | 4820/26933 [06:18<26:32, 13.89it/s] 18%|█▊        | 4824/26933 [06:19<26:32, 13.89it/s] 18%|█▊        | 4828/26933 [06:19<26:28, 13.92it/s] 18%|█▊        | 4832/26933 [06:19<26:27, 13.92it/s] 18%|█▊        | 4836/26933 [06:19<26:30, 13.89it/s] 18%|█▊        | 4840/26933 [06:20<26:28, 13.90it/s] 18%|█▊        | 4844/26933 [06:20<26:24, 13.94it/s] 18%|█▊        | 4848/26933 [06:20<26:23, 13.94it/s] 18%|█▊        | 4852/26933 [06:21<26:26, 13.92it/s] 18%|█▊        | 4856/26933 [06:21<26:20, 13.97it/s] 18%|█▊        | 4860/26933 [06:21<26:16, 14.00it/s] 18%|█▊        | 4864/26933 [06:21<26:16, 14.00it/s] 18%|█▊        | 4868/26933 [06:22<26:19, 13.97it/s] 18%|█▊        | 4872/26933 [06:22<26:18, 13.97it/s] 18%|█▊        | 4876/26933 [06:22<26:26, 13.91it/s] 18%|█▊        | 4880/26933 [06:23<26:20, 13.96it/s] 18%|█▊        | 4884/26933 [06:23<26:18, 13.97it/s] 18%|█▊        | 4888/26933 [06:23<26:18, 13.96it/s] 18%|█▊        | 4892/26933 [06:23<26:22, 13.93it/s] 18%|█▊        | 4896/26933 [06:24<26:17, 13.97it/s] 18%|█▊        | 4900/26933 [06:24<26:20, 13.94it/s] 18%|█▊        | 4904/26933 [06:24<26:21, 13.93it/s] 18%|█▊        | 4908/26933 [06:25<26:17, 13.96it/s] 18%|█▊        | 4912/26933 [06:25<26:15, 13.97it/s] 18%|█▊        | 4916/26933 [06:25<26:07, 14.05it/s] 18%|█▊        | 4920/26933 [06:25<26:08, 14.03it/s] 18%|█▊        | 4924/26933 [06:26<26:01, 14.10it/s] 18%|█▊        | 4928/26933 [06:26<25:56, 14.14it/s] 18%|█▊        | 4932/26933 [06:26<25:58, 14.12it/s] 18%|█▊        | 4936/26933 [06:27<26:03, 14.07it/s] 18%|█▊        | 4940/26933 [06:27<26:02, 14.08it/s] 18%|█▊        | 4944/26933 [06:27<26:26, 13.86it/s] 18%|█▊        | 4948/26933 [06:27<26:26, 13.86it/s] 18%|█▊        | 4952/26933 [06:28<26:21, 13.90it/s] 18%|█▊        | 4956/26933 [06:28<26:17, 13.93it/s] 18%|█▊        | 4960/26933 [06:28<26:17, 13.93it/s] 18%|█▊        | 4964/26933 [06:29<26:12, 13.97it/s] 18%|█▊        | 4968/26933 [06:29<26:13, 13.96it/s] 18%|█▊        | 4972/26933 [06:29<26:16, 13.93it/s] 18%|█▊        | 4976/26933 [06:29<26:16, 13.93it/s] 18%|█▊        | 4980/26933 [06:30<26:17, 13.92it/s] 19%|█▊        | 4984/26933 [06:30<26:10, 13.98it/s] 19%|█▊        | 4988/26933 [06:30<26:15, 13.93it/s] 19%|█▊        | 4992/26933 [06:31<26:12, 13.95it/s] 19%|█▊        | 4996/26933 [06:31<26:06, 14.01it/s] 19%|█▊        | 5000/26933 [06:31<26:01, 14.05it/s] 19%|█▊        | 5004/26933 [06:31<26:05, 14.01it/s] 19%|█▊        | 5008/26933 [06:32<26:01, 14.04it/s] 19%|█▊        | 5012/26933 [06:32<25:59, 14.06it/s] 19%|█▊        | 5016/26933 [06:32<26:07, 13.98it/s] 19%|█▊        | 5020/26933 [06:33<26:05, 14.00it/s] 19%|█▊        | 5024/26933 [06:33<26:01, 14.03it/s] 19%|█▊        | 5028/26933 [06:33<25:58, 14.05it/s] 19%|█▊        | 5032/26933 [06:33<26:04, 14.00it/s] 19%|█▊        | 5036/26933 [06:34<26:02, 14.01it/s] 19%|█▊        | 5040/26933 [06:34<26:00, 14.03it/s] 19%|█▊        | 5044/26933 [06:34<26:09, 13.94it/s] 19%|█▊        | 5048/26933 [06:35<26:09, 13.94it/s] 19%|█▉        | 5052/26933 [06:35<26:04, 13.99it/s] 19%|█▉        | 5056/26933 [06:35<25:57, 14.04it/s] 19%|█▉        | 5060/26933 [06:35<26:06, 13.96it/s] 19%|█▉        | 5064/26933 [06:36<26:03, 13.99it/s] 19%|█▉        | 5068/26933 [06:36<26:10, 13.92it/s] 19%|█▉        | 5072/26933 [06:36<26:12, 13.90it/s] 19%|█▉        | 5076/26933 [06:37<26:02, 13.99it/s] 19%|█▉        | 5080/26933 [06:37<25:57, 14.03it/s] 19%|█▉        | 5084/26933 [06:37<25:59, 14.01it/s] 19%|█▉        | 5088/26933 [06:37<26:07, 13.94it/s] 19%|█▉        | 5092/26933 [06:38<26:01, 13.98it/s] 19%|█▉        | 5096/26933 [06:38<25:58, 14.02it/s] 19%|█▉        | 5100/26933 [06:38<26:01, 13.98it/s] 19%|█▉        | 5104/26933 [06:39<25:55, 14.04it/s] 19%|█▉        | 5108/26933 [06:39<25:51, 14.07it/s] 19%|█▉        | 5112/26933 [06:39<25:47, 14.10it/s] 19%|█▉        | 5116/26933 [06:39<26:03, 13.96it/s] 19%|█▉        | 5120/26933 [06:40<25:56, 14.02it/s] 19%|█▉        | 5124/26933 [06:40<25:49, 14.08it/s] 19%|█▉        | 5128/26933 [06:40<25:50, 14.06it/s] 19%|█▉        | 5132/26933 [06:41<25:44, 14.11it/s] 19%|█▉        | 5136/26933 [06:41<25:38, 14.17it/s] 19%|█▉        | 5140/26933 [06:41<25:34, 14.20it/s] 19%|█▉        | 5144/26933 [06:41<25:43, 14.12it/s] 19%|█▉        | 5148/26933 [06:42<25:42, 14.13it/s] 19%|█▉        | 5152/26933 [06:42<25:38, 14.16it/s] 19%|█▉        | 5156/26933 [06:42<25:48, 14.06it/s] 19%|█▉        | 5160/26933 [06:43<25:11, 14.41it/s] 19%|█▉        | 5164/26933 [06:43<24:39, 14.71it/s] 19%|█▉        | 5168/26933 [06:43<24:17, 14.93it/s] 19%|█▉        | 5172/26933 [06:43<24:41, 14.69it/s] 19%|█▉        | 5176/26933 [06:44<24:56, 14.54it/s] 19%|█▉        | 5180/26933 [06:44<25:08, 14.42it/s] 19%|█▉        | 5184/26933 [06:44<25:16, 14.34it/s] 19%|█▉        | 5188/26933 [06:44<25:20, 14.31it/s] 19%|█▉        | 5192/26933 [06:45<25:21, 14.29it/s] 19%|█▉        | 5196/26933 [06:45<25:22, 14.28it/s] 19%|█▉        | 5200/26933 [06:45<25:32, 14.18it/s] 19%|█▉        | 5204/26933 [06:46<25:29, 14.21it/s] 19%|█▉        | 5208/26933 [06:46<25:27, 14.23it/s] 19%|█▉        | 5212/26933 [06:46<25:29, 14.20it/s] 19%|█▉        | 5216/26933 [06:46<25:34, 14.15it/s] 19%|█▉        | 5220/26933 [06:47<25:37, 14.12it/s] 19%|█▉        | 5224/26933 [06:47<25:31, 14.17it/s] 19%|█▉        | 5228/26933 [06:47<25:40, 14.09it/s] 19%|█▉        | 5232/26933 [06:48<25:36, 14.12it/s] 19%|█▉        | 5236/26933 [06:48<25:33, 14.15it/s] 19%|█▉        | 5240/26933 [06:48<25:32, 14.16it/s] 19%|█▉        | 5244/26933 [06:48<25:35, 14.13it/s] 19%|█▉        | 5248/26933 [06:49<25:29, 14.18it/s] 20%|█▉        | 5252/26933 [06:49<25:28, 14.18it/s] 20%|█▉        | 5256/26933 [06:49<25:31, 14.15it/s] 20%|█▉        | 5260/26933 [06:50<25:27, 14.19it/s] 20%|█▉        | 5264/26933 [06:50<25:24, 14.21it/s] 20%|█▉        | 5268/26933 [06:50<25:24, 14.21it/s] 20%|█▉        | 5272/26933 [06:50<25:29, 14.16it/s] 20%|█▉        | 5276/26933 [06:51<25:28, 14.17it/s] 20%|█▉        | 5280/26933 [06:51<25:32, 14.12it/s] 20%|█▉        | 5284/26933 [06:51<25:34, 14.11it/s] 20%|█▉        | 5288/26933 [06:52<25:25, 14.19it/s] 20%|█▉        | 5292/26933 [06:52<25:22, 14.22it/s] 20%|█▉        | 5296/26933 [06:52<25:17, 14.25it/s] 20%|█▉        | 5300/26933 [06:52<25:17, 14.25it/s] 20%|█▉        | 5304/26933 [06:53<25:13, 14.29it/s] 20%|█▉        | 5308/26933 [06:53<25:20, 14.22it/s] 20%|█▉        | 5312/26933 [06:53<25:17, 14.24it/s] 20%|█▉        | 5316/26933 [06:54<25:24, 14.18it/s] 20%|█▉        | 5320/26933 [06:54<25:22, 14.20it/s] 20%|█▉        | 5324/26933 [06:54<25:29, 14.12it/s] 20%|█▉        | 5328/26933 [06:54<25:38, 14.05it/s] 20%|█▉        | 5332/26933 [06:55<25:33, 14.09it/s] 20%|█▉        | 5336/26933 [06:55<25:31, 14.10it/s] 20%|█▉        | 5340/26933 [06:55<25:29, 14.12it/s] 20%|█▉        | 5344/26933 [06:56<25:27, 14.13it/s] 20%|█▉        | 5348/26933 [06:56<25:24, 14.16it/s] 20%|█▉        | 5352/26933 [06:56<25:24, 14.15it/s] 20%|█▉        | 5356/26933 [06:56<25:32, 14.08it/s] 20%|█▉        | 5360/26933 [06:57<25:26, 14.13it/s] 20%|█▉        | 5364/26933 [06:57<25:24, 14.15it/s] 20%|█▉        | 5368/26933 [06:57<25:17, 14.21it/s] 20%|█▉        | 5372/26933 [06:57<25:21, 14.17it/s] 20%|█▉        | 5376/26933 [06:58<25:20, 14.18it/s] 20%|█▉        | 5380/26933 [06:58<25:14, 14.23it/s] 20%|█▉        | 5384/26933 [06:58<25:13, 14.24it/s] 20%|██        | 5388/26933 [06:59<25:06, 14.31it/s] 20%|██        | 5392/26933 [06:59<25:04, 14.32it/s] 20%|██        | 5396/26933 [06:59<25:00, 14.35it/s] 20%|██        | 5400/26933 [06:59<25:06, 14.29it/s] 20%|██        | 5404/26933 [07:00<25:24, 14.12it/s] 20%|██        | 5408/26933 [07:00<25:31, 14.06it/s] 20%|██        | 5412/26933 [07:00<25:28, 14.08it/s] 20%|██        | 5416/26933 [07:01<25:19, 14.16it/s] 20%|██        | 5420/26933 [07:01<25:12, 14.22it/s] 20%|██        | 5424/26933 [07:01<25:10, 14.24it/s] 20%|██        | 5428/26933 [07:01<25:10, 14.24it/s] 20%|██        | 5432/26933 [07:02<25:06, 14.27it/s] 20%|██        | 5436/26933 [07:02<25:01, 14.31it/s] 20%|██        | 5440/26933 [07:02<25:02, 14.31it/s] 20%|██        | 5444/26933 [07:03<24:54, 14.38it/s] 20%|██        | 5448/26933 [07:03<24:51, 14.41it/s] 20%|██        | 5452/26933 [07:03<25:03, 14.29it/s] 20%|██        | 5456/26933 [07:03<25:03, 14.28it/s] 20%|██        | 5460/26933 [07:04<25:01, 14.30it/s] 20%|██        | 5464/26933 [07:04<24:58, 14.33it/s] 20%|██        | 5468/26933 [07:04<24:55, 14.36it/s] 20%|██        | 5472/26933 [07:04<24:53, 14.37it/s] 20%|██        | 5476/26933 [07:05<24:54, 14.36it/s] 20%|██        | 5480/26933 [07:05<24:51, 14.38it/s] 20%|██        | 5484/26933 [07:05<24:59, 14.31it/s] 20%|██        | 5488/26933 [07:06<24:55, 14.34it/s] 20%|██        | 5492/26933 [07:06<24:56, 14.32it/s] 20%|██        | 5496/26933 [07:06<25:08, 14.21it/s] 20%|██        | 5500/26933 [07:06<25:06, 14.23it/s] 20%|██        | 5504/26933 [07:07<25:04, 14.24it/s] 20%|██        | 5508/26933 [07:07<24:59, 14.29it/s] 20%|██        | 5512/26933 [07:07<25:00, 14.28it/s] 20%|██        | 5516/26933 [07:08<24:55, 14.32it/s] 20%|██        | 5520/26933 [07:08<24:56, 14.31it/s] 21%|██        | 5524/26933 [07:08<24:50, 14.36it/s] 21%|██        | 5528/26933 [07:08<24:54, 14.32it/s] 21%|██        | 5532/26933 [07:09<24:51, 14.35it/s] 21%|██        | 5536/26933 [07:09<24:51, 14.35it/s] 21%|██        | 5540/26933 [07:09<24:52, 14.33it/s] 21%|██        | 5544/26933 [07:10<24:55, 14.30it/s] 21%|██        | 5548/26933 [07:10<25:02, 14.23it/s] 21%|██        | 5552/26933 [07:10<24:55, 14.30it/s] 21%|██        | 5556/26933 [07:10<24:58, 14.26it/s] 21%|██        | 5560/26933 [07:11<24:54, 14.30it/s] 21%|██        | 5564/26933 [07:11<24:50, 14.33it/s] 21%|██        | 5568/26933 [07:11<24:50, 14.34it/s] 21%|██        | 5572/26933 [07:11<24:51, 14.32it/s] 21%|██        | 5576/26933 [07:12<24:51, 14.32it/s] 21%|██        | 5580/26933 [07:12<24:51, 14.32it/s] 21%|██        | 5584/26933 [07:12<24:49, 14.34it/s] 21%|██        | 5588/26933 [07:13<24:43, 14.38it/s] 21%|██        | 5592/26933 [07:13<24:38, 14.43it/s] 21%|██        | 5596/26933 [07:13<24:40, 14.41it/s] 21%|██        | 5600/26933 [07:13<24:42, 14.39it/s] 21%|██        | 5604/26933 [07:14<24:38, 14.42it/s] 21%|██        | 5608/26933 [07:14<24:40, 14.41it/s] 21%|██        | 5612/26933 [07:14<24:44, 14.36it/s] 21%|██        | 5616/26933 [07:15<24:41, 14.39it/s] 21%|██        | 5620/26933 [07:15<24:39, 14.41it/s] 21%|██        | 5624/26933 [07:15<24:38, 14.41it/s] 21%|██        | 5628/26933 [07:15<24:42, 14.37it/s] 21%|██        | 5632/26933 [07:16<24:42, 14.37it/s] 21%|██        | 5636/26933 [07:16<24:40, 14.39it/s] 21%|██        | 5640/26933 [07:16<24:34, 14.44it/s] 21%|██        | 5644/26933 [07:16<24:37, 14.41it/s] 21%|██        | 5648/26933 [07:17<24:32, 14.46it/s] 21%|██        | 5652/26933 [07:17<24:32, 14.46it/s] 21%|██        | 5656/26933 [07:17<24:32, 14.45it/s] 21%|██        | 5660/26933 [07:18<24:31, 14.46it/s] 21%|██        | 5664/26933 [07:18<24:31, 14.46it/s] 21%|██        | 5668/26933 [07:18<24:32, 14.44it/s] 21%|██        | 5672/26933 [07:18<24:35, 14.41it/s] 21%|██        | 5676/26933 [07:19<24:34, 14.42it/s] 21%|██        | 5680/26933 [07:19<24:34, 14.41it/s] 21%|██        | 5684/26933 [07:19<24:34, 14.41it/s] 21%|██        | 5688/26933 [07:20<24:29, 14.46it/s] 21%|██        | 5692/26933 [07:20<24:27, 14.47it/s] 21%|██        | 5696/26933 [07:20<24:24, 14.50it/s] 21%|██        | 5700/26933 [07:20<24:32, 14.42it/s] 21%|██        | 5704/26933 [07:21<24:31, 14.43it/s] 21%|██        | 5708/26933 [07:21<24:39, 14.35it/s] 21%|██        | 5712/26933 [07:21<24:41, 14.33it/s] 21%|██        | 5716/26933 [07:21<24:19, 14.54it/s] 21%|██        | 5720/26933 [07:22<23:53, 14.80it/s] 21%|██▏       | 5724/26933 [07:22<23:34, 14.99it/s] 21%|██▏       | 5728/26933 [07:22<23:23, 15.10it/s] 21%|██▏       | 5732/26933 [07:22<23:12, 15.22it/s] 21%|██▏       | 5736/26933 [07:23<23:38, 14.94it/s] 21%|██▏       | 5740/26933 [07:23<23:56, 14.75it/s] 21%|██▏       | 5744/26933 [07:23<24:23, 14.47it/s] 21%|██▏       | 5748/26933 [07:24<24:30, 14.41it/s] 21%|██▏       | 5752/26933 [07:24<24:34, 14.37it/s] 21%|██▏       | 5756/26933 [07:24<24:54, 14.17it/s] 21%|██▏       | 5760/26933 [07:24<24:59, 14.12it/s] 21%|██▏       | 5764/26933 [07:25<25:14, 13.98it/s] 21%|██▏       | 5768/26933 [07:25<24:52, 14.18it/s] 21%|██▏       | 5772/26933 [07:25<24:42, 14.27it/s] 21%|██▏       | 5776/26933 [07:26<24:29, 14.40it/s] 21%|██▏       | 5780/26933 [07:26<24:21, 14.47it/s] 21%|██▏       | 5784/26933 [07:26<24:17, 14.51it/s] 21%|██▏       | 5788/26933 [07:26<24:20, 14.48it/s] 22%|██▏       | 5792/26933 [07:27<24:17, 14.50it/s] 22%|██▏       | 5796/26933 [07:27<24:19, 14.49it/s] 22%|██▏       | 5800/26933 [07:27<24:16, 14.51it/s] 22%|██▏       | 5804/26933 [07:28<24:19, 14.48it/s] 22%|██▏       | 5808/26933 [07:28<24:15, 14.51it/s] 22%|██▏       | 5812/26933 [07:28<24:15, 14.51it/s] 22%|██▏       | 5816/26933 [07:28<24:17, 14.49it/s] 22%|██▏       | 5820/26933 [07:29<24:14, 14.52it/s] 22%|██▏       | 5824/26933 [07:29<24:13, 14.52it/s] 22%|██▏       | 5828/26933 [07:29<24:17, 14.48it/s] 22%|██▏       | 5832/26933 [07:29<24:21, 14.43it/s] 22%|██▏       | 5836/26933 [07:30<24:17, 14.47it/s] 22%|██▏       | 5840/26933 [07:30<24:30, 14.34it/s] 22%|██▏       | 5844/26933 [07:30<24:28, 14.36it/s] 22%|██▏       | 5848/26933 [07:31<24:19, 14.45it/s] 22%|██▏       | 5852/26933 [07:31<24:15, 14.49it/s] 22%|██▏       | 5856/26933 [07:31<24:15, 14.48it/s] 22%|██▏       | 5860/26933 [07:31<24:15, 14.48it/s] 22%|██▏       | 5864/26933 [07:32<24:12, 14.51it/s] 22%|██▏       | 5868/26933 [07:32<24:11, 14.52it/s] 22%|██▏       | 5872/26933 [07:32<24:05, 14.57it/s] 22%|██▏       | 5876/26933 [07:32<23:59, 14.62it/s] 22%|██▏       | 5880/26933 [07:33<23:58, 14.63it/s] 22%|██▏       | 5884/26933 [07:33<23:58, 14.63it/s] 22%|██▏       | 5888/26933 [07:33<24:01, 14.60it/s] 22%|██▏       | 5892/26933 [07:34<23:57, 14.64it/s] 22%|██▏       | 5896/26933 [07:34<24:02, 14.59it/s] 22%|██▏       | 5900/26933 [07:34<24:00, 14.60it/s] 22%|██▏       | 5904/26933 [07:34<24:02, 14.57it/s] 22%|██▏       | 5908/26933 [07:35<24:00, 14.59it/s] 22%|██▏       | 5912/26933 [07:35<24:00, 14.59it/s] 22%|██▏       | 5916/26933 [07:35<24:00, 14.59it/s] 22%|██▏       | 5920/26933 [07:36<24:00, 14.58it/s] 22%|██▏       | 5924/26933 [07:36<23:58, 14.60it/s] 22%|██▏       | 5928/26933 [07:36<24:07, 14.51it/s] 22%|██▏       | 5932/26933 [07:36<24:06, 14.52it/s] 22%|██▏       | 5936/26933 [07:37<24:02, 14.56it/s] 22%|██▏       | 5940/26933 [07:37<23:59, 14.59it/s] 22%|██▏       | 5944/26933 [07:37<23:56, 14.61it/s] 22%|██▏       | 5948/26933 [07:37<23:59, 14.58it/s] 22%|██▏       | 5952/26933 [07:38<23:59, 14.57it/s] 22%|██▏       | 5956/26933 [07:38<23:56, 14.61it/s] 22%|██▏       | 5960/26933 [07:38<23:56, 14.60it/s] 22%|██▏       | 5964/26933 [07:39<23:53, 14.63it/s] 22%|██▏       | 5968/26933 [07:39<23:50, 14.65it/s] 22%|██▏       | 5972/26933 [07:39<23:51, 14.64it/s] 22%|██▏       | 5976/26933 [07:39<23:53, 14.62it/s] 22%|██▏       | 5980/26933 [07:40<23:51, 14.64it/s] 22%|██▏       | 5984/26933 [07:40<23:49, 14.65it/s] 22%|██▏       | 5988/26933 [07:40<23:47, 14.67it/s] 22%|██▏       | 5992/26933 [07:40<23:50, 14.64it/s] 22%|██▏       | 5996/26933 [07:41<23:49, 14.64it/s] 22%|██▏       | 6000/26933 [07:41<23:46, 14.67it/s] 22%|██▏       | 6004/26933 [07:41<23:47, 14.66it/s] 22%|██▏       | 6008/26933 [07:42<23:41, 14.72it/s] 22%|██▏       | 6012/26933 [07:42<23:39, 14.74it/s] 22%|██▏       | 6016/26933 [07:42<23:40, 14.73it/s] 22%|██▏       | 6020/26933 [07:42<23:44, 14.68it/s] 22%|██▏       | 6024/26933 [07:43<23:41, 14.71it/s] 22%|██▏       | 6028/26933 [07:43<23:42, 14.69it/s] 22%|██▏       | 6032/26933 [07:43<23:46, 14.65it/s] 22%|██▏       | 6036/26933 [07:43<23:59, 14.52it/s] 22%|██▏       | 6040/26933 [07:44<23:56, 14.54it/s] 22%|██▏       | 6044/26933 [07:44<23:56, 14.54it/s] 22%|██▏       | 6048/26933 [07:44<23:57, 14.53it/s] 22%|██▏       | 6052/26933 [07:45<23:54, 14.56it/s] 22%|██▏       | 6056/26933 [07:45<23:49, 14.60it/s] 23%|██▎       | 6060/26933 [07:45<23:43, 14.66it/s] 23%|██▎       | 6064/26933 [07:45<23:45, 14.64it/s] 23%|██▎       | 6068/26933 [07:46<23:41, 14.67it/s] 23%|██▎       | 6072/26933 [07:46<23:38, 14.71it/s] 23%|██▎       | 6076/26933 [07:46<23:36, 14.73it/s] 23%|██▎       | 6080/26933 [07:46<23:35, 14.73it/s] 23%|██▎       | 6084/26933 [07:47<23:34, 14.74it/s] 23%|██▎       | 6088/26933 [07:47<23:34, 14.74it/s] 23%|██▎       | 6092/26933 [07:47<23:36, 14.72it/s] 23%|██▎       | 6096/26933 [07:48<23:32, 14.75it/s] 23%|██▎       | 6100/26933 [07:48<23:33, 14.74it/s] 23%|██▎       | 6104/26933 [07:48<23:32, 14.75it/s] 23%|██▎       | 6108/26933 [07:48<23:38, 14.68it/s] 23%|██▎       | 6112/26933 [07:49<23:37, 14.69it/s] 23%|██▎       | 6116/26933 [07:49<23:35, 14.71it/s] 23%|██▎       | 6120/26933 [07:49<23:33, 14.72it/s] 23%|██▎       | 6124/26933 [07:49<23:36, 14.69it/s] 23%|██▎       | 6128/26933 [07:50<23:33, 14.72it/s] 23%|██▎       | 6132/26933 [07:50<23:31, 14.73it/s] 23%|██▎       | 6136/26933 [07:50<23:29, 14.76it/s] 23%|██▎       | 6140/26933 [07:51<23:28, 14.76it/s] 23%|██▎       | 6144/26933 [07:51<23:29, 14.75it/s] 23%|██▎       | 6148/26933 [07:51<23:25, 14.78it/s] 23%|██▎       | 6152/26933 [07:51<23:24, 14.79it/s] 23%|██▎       | 6156/26933 [07:52<23:26, 14.77it/s] 23%|██▎       | 6160/26933 [07:52<23:25, 14.78it/s] 23%|██▎       | 6164/26933 [07:52<23:26, 14.77it/s] 23%|██▎       | 6168/26933 [07:52<23:27, 14.76it/s] 23%|██▎       | 6172/26933 [07:53<23:26, 14.76it/s] 23%|██▎       | 6176/26933 [07:53<23:24, 14.78it/s] 23%|██▎       | 6180/26933 [07:53<23:23, 14.79it/s] 23%|██▎       | 6184/26933 [07:53<23:21, 14.80it/s] 23%|██▎       | 6188/26933 [07:54<23:20, 14.81it/s] 23%|██▎       | 6192/26933 [07:54<23:20, 14.81it/s] 23%|██▎       | 6196/26933 [07:54<23:23, 14.78it/s] 23%|██▎       | 6200/26933 [07:55<23:21, 14.79it/s] 23%|██▎       | 6204/26933 [07:55<23:19, 14.82it/s] 23%|██▎       | 6208/26933 [07:55<23:23, 14.76it/s] 23%|██▎       | 6212/26933 [07:55<23:24, 14.76it/s] 23%|██▎       | 6216/26933 [07:56<23:04, 14.96it/s] 23%|██▎       | 6220/26933 [07:56<23:10, 14.89it/s] 23%|██▎       | 6224/26933 [07:56<23:14, 14.85it/s] 23%|██▎       | 6228/26933 [07:56<23:19, 14.79it/s] 23%|██▎       | 6232/26933 [07:57<23:19, 14.79it/s] 23%|██▎       | 6236/26933 [07:57<23:14, 14.84it/s] 23%|██▎       | 6240/26933 [07:57<23:14, 14.84it/s] 23%|██▎       | 6244/26933 [07:58<23:10, 14.88it/s] 23%|██▎       | 6248/26933 [07:58<23:09, 14.89it/s] 23%|██▎       | 6252/26933 [07:58<23:08, 14.89it/s] 23%|██▎       | 6256/26933 [07:58<23:14, 14.83it/s] 23%|██▎       | 6260/26933 [07:59<23:10, 14.87it/s] 23%|██▎       | 6264/26933 [07:59<23:09, 14.88it/s] 23%|██▎       | 6268/26933 [07:59<23:14, 14.82it/s] 23%|██▎       | 6272/26933 [07:59<23:15, 14.80it/s] 23%|██▎       | 6276/26933 [08:00<23:12, 14.83it/s] 23%|██▎       | 6280/26933 [08:00<23:11, 14.84it/s] 23%|██▎       | 6284/26933 [08:00<23:13, 14.82it/s] 23%|██▎       | 6288/26933 [08:00<23:13, 14.82it/s] 23%|██▎       | 6292/26933 [08:01<23:12, 14.83it/s] 23%|██▎       | 6296/26933 [08:01<23:09, 14.86it/s] 23%|██▎       | 6300/26933 [08:01<23:12, 14.81it/s] 23%|██▎       | 6304/26933 [08:02<23:09, 14.85it/s] 23%|██▎       | 6308/26933 [08:02<23:04, 14.89it/s] 23%|██▎       | 6312/26933 [08:02<23:05, 14.89it/s] 23%|██▎       | 6316/26933 [08:02<23:09, 14.84it/s] 23%|██▎       | 6320/26933 [08:03<23:06, 14.87it/s] 23%|██▎       | 6324/26933 [08:03<23:04, 14.88it/s] 23%|██▎       | 6328/26933 [08:03<23:04, 14.88it/s] 24%|██▎       | 6332/26933 [08:03<23:08, 14.84it/s] 24%|██▎       | 6336/26933 [08:04<23:02, 14.90it/s] 24%|██▎       | 6340/26933 [08:04<23:00, 14.92it/s] 24%|██▎       | 6344/26933 [08:04<23:03, 14.88it/s] 24%|██▎       | 6348/26933 [08:05<22:59, 14.92it/s] 24%|██▎       | 6352/26933 [08:05<23:00, 14.91it/s] 24%|██▎       | 6356/26933 [08:05<22:58, 14.93it/s] 24%|██▎       | 6360/26933 [08:05<22:58, 14.92it/s] 24%|██▎       | 6364/26933 [08:06<22:59, 14.91it/s] 24%|██▎       | 6368/26933 [08:06<22:57, 14.92it/s] 24%|██▎       | 6372/26933 [08:06<22:55, 14.94it/s] 24%|██▎       | 6376/26933 [08:06<22:58, 14.91it/s] 24%|██▎       | 6380/26933 [08:07<23:03, 14.86it/s] 24%|██▎       | 6384/26933 [08:07<23:08, 14.80it/s] 24%|██▎       | 6388/26933 [08:07<23:06, 14.81it/s] 24%|██▎       | 6392/26933 [08:07<23:07, 14.80it/s] 24%|██▎       | 6396/26933 [08:08<23:05, 14.82it/s] 24%|██▍       | 6400/26933 [08:08<23:03, 14.84it/s] 24%|██▍       | 6404/26933 [08:08<23:04, 14.83it/s] 24%|██▍       | 6408/26933 [08:09<23:07, 14.79it/s] 24%|██▍       | 6412/26933 [08:09<23:11, 14.75it/s] 24%|██▍       | 6416/26933 [08:09<23:07, 14.79it/s] 24%|██▍       | 6420/26933 [08:09<23:03, 14.83it/s] 24%|██▍       | 6424/26933 [08:10<22:59, 14.86it/s] 24%|██▍       | 6428/26933 [08:10<22:54, 14.91it/s] 24%|██▍       | 6432/26933 [08:10<22:55, 14.91it/s] 24%|██▍       | 6436/26933 [08:10<23:00, 14.85it/s] 24%|██▍       | 6440/26933 [08:11<23:06, 14.78it/s] 24%|██▍       | 6444/26933 [08:11<23:02, 14.82it/s] 24%|██▍       | 6448/26933 [08:11<23:00, 14.84it/s] 24%|██▍       | 6452/26933 [08:12<22:54, 14.90it/s] 24%|██▍       | 6456/26933 [08:12<22:57, 14.86it/s] 24%|██▍       | 6460/26933 [08:12<22:53, 14.90it/s] 24%|██▍       | 6464/26933 [08:12<22:52, 14.91it/s] 24%|██▍       | 6468/26933 [08:13<22:50, 14.93it/s] 24%|██▍       | 6472/26933 [08:13<22:51, 14.92it/s] 24%|██▍       | 6476/26933 [08:13<22:50, 14.93it/s] 24%|██▍       | 6480/26933 [08:13<22:56, 14.86it/s] 24%|██▍       | 6484/26933 [08:14<22:57, 14.84it/s] 24%|██▍       | 6488/26933 [08:14<23:01, 14.80it/s] 24%|██▍       | 6492/26933 [08:14<23:01, 14.79it/s] 24%|██▍       | 6496/26933 [08:14<23:01, 14.80it/s] 24%|██▍       | 6500/26933 [08:15<23:00, 14.80it/s] 24%|██▍       | 6504/26933 [08:15<22:56, 14.84it/s] 24%|██▍       | 6508/26933 [08:15<22:57, 14.83it/s] 24%|██▍       | 6512/26933 [08:16<22:56, 14.83it/s] 24%|██▍       | 6516/26933 [08:16<22:55, 14.84it/s] 24%|██▍       | 6520/26933 [08:16<22:51, 14.88it/s] 24%|██▍       | 6524/26933 [08:16<22:54, 14.85it/s] 24%|██▍       | 6528/26933 [08:17<22:49, 14.89it/s] 24%|██▍       | 6532/26933 [08:17<22:46, 14.93it/s] 24%|██▍       | 6536/26933 [08:17<22:42, 14.97it/s] 24%|██▍       | 6540/26933 [08:17<22:41, 14.98it/s] 24%|██▍       | 6544/26933 [08:18<22:41, 14.98it/s] 24%|██▍       | 6548/26933 [08:18<22:40, 14.98it/s] 24%|██▍       | 6552/26933 [08:18<22:41, 14.97it/s] 24%|██▍       | 6556/26933 [08:19<22:42, 14.95it/s] 24%|██▍       | 6560/26933 [08:19<22:41, 14.96it/s] 24%|██▍       | 6564/26933 [08:19<22:41, 14.97it/s] 24%|██▍       | 6568/26933 [08:19<22:43, 14.93it/s] 24%|██▍       | 6572/26933 [08:20<22:42, 14.95it/s] 24%|██▍       | 6576/26933 [08:20<22:42, 14.95it/s] 24%|██▍       | 6580/26933 [08:20<22:40, 14.96it/s] 24%|██▍       | 6584/26933 [08:20<22:40, 14.95it/s] 24%|██▍       | 6588/26933 [08:21<22:38, 14.97it/s] 24%|██▍       | 6592/26933 [08:21<22:39, 14.96it/s] 24%|██▍       | 6596/26933 [08:21<22:35, 15.01it/s] 25%|██▍       | 6600/26933 [08:21<22:34, 15.01it/s] 25%|██▍       | 6604/26933 [08:22<22:30, 15.05it/s] 25%|██▍       | 6608/26933 [08:22<22:28, 15.07it/s] 25%|██▍       | 6612/26933 [08:22<22:31, 15.03it/s] 25%|██▍       | 6616/26933 [08:23<22:30, 15.04it/s] 25%|██▍       | 6620/26933 [08:23<22:28, 15.06it/s] 25%|██▍       | 6624/26933 [08:23<22:29, 15.05it/s] 25%|██▍       | 6628/26933 [08:23<22:31, 15.03it/s] 25%|██▍       | 6632/26933 [08:24<22:39, 14.93it/s] 25%|██▍       | 6636/26933 [08:24<22:41, 14.91it/s] 25%|██▍       | 6640/26933 [08:24<22:37, 14.95it/s] 25%|██▍       | 6644/26933 [08:24<22:36, 14.95it/s] 25%|██▍       | 6648/26933 [08:25<22:37, 14.94it/s] 25%|██▍       | 6652/26933 [08:25<22:39, 14.91it/s] 25%|██▍       | 6656/26933 [08:25<22:39, 14.92it/s] 25%|██▍       | 6660/26933 [08:25<22:41, 14.89it/s] 25%|██▍       | 6664/26933 [08:26<22:38, 14.92it/s] 25%|██▍       | 6668/26933 [08:26<22:41, 14.89it/s] 25%|██▍       | 6672/26933 [08:26<22:43, 14.86it/s] 25%|██▍       | 6676/26933 [08:27<22:38, 14.91it/s] 25%|██▍       | 6680/26933 [08:27<22:33, 14.96it/s] 25%|██▍       | 6684/26933 [08:27<22:31, 14.98it/s] 25%|██▍       | 6688/26933 [08:27<22:31, 14.98it/s] 25%|██▍       | 6692/26933 [08:28<22:27, 15.02it/s] 25%|██▍       | 6696/26933 [08:28<22:25, 15.04it/s] 25%|██▍       | 6700/26933 [08:28<22:26, 15.03it/s] 25%|██▍       | 6704/26933 [08:28<22:31, 14.96it/s] 25%|██▍       | 6708/26933 [08:29<22:33, 14.94it/s] 25%|██▍       | 6712/26933 [08:29<22:35, 14.92it/s] 25%|██▍       | 6716/26933 [08:29<22:32, 14.95it/s] 25%|██▍       | 6720/26933 [08:29<22:32, 14.95it/s] 25%|██▍       | 6724/26933 [08:30<22:27, 15.00it/s] 25%|██▍       | 6728/26933 [08:30<22:25, 15.01it/s] 25%|██▍       | 6732/26933 [08:30<22:26, 15.01it/s] 25%|██▌       | 6736/26933 [08:31<22:24, 15.03it/s] 25%|██▌       | 6740/26933 [08:31<22:30, 14.95it/s] 25%|██▌       | 6744/26933 [08:31<22:24, 15.02it/s] 25%|██▌       | 6748/26933 [08:31<22:22, 15.04it/s] 25%|██▌       | 6752/26933 [08:32<22:20, 15.05it/s] 25%|██▌       | 6756/26933 [08:32<22:21, 15.04it/s] 25%|██▌       | 6760/26933 [08:32<22:27, 14.98it/s] 25%|██▌       | 6764/26933 [08:32<22:33, 14.90it/s] 25%|██▌       | 6768/26933 [08:33<22:36, 14.86it/s] 25%|██▌       | 6772/26933 [08:33<22:40, 14.82it/s] 25%|██▌       | 6776/26933 [08:33<22:35, 14.87it/s] 25%|██▌       | 6780/26933 [08:33<22:29, 14.93it/s] 25%|██▌       | 6784/26933 [08:34<22:29, 14.93it/s] 25%|██▌       | 6788/26933 [08:34<22:29, 14.93it/s] 25%|██▌       | 6792/26933 [08:34<22:32, 14.89it/s] 25%|██▌       | 6796/26933 [08:35<22:34, 14.87it/s] 25%|██▌       | 6800/26933 [08:35<22:43, 14.76it/s] 25%|██▌       | 6804/26933 [08:35<22:39, 14.81it/s] 25%|██▌       | 6808/26933 [08:35<22:35, 14.85it/s] 25%|██▌       | 6812/26933 [08:36<22:37, 14.82it/s] 25%|██▌       | 6816/26933 [08:36<22:36, 14.83it/s] 25%|██▌       | 6820/26933 [08:36<22:34, 14.85it/s] 25%|██▌       | 6824/26933 [08:36<22:28, 14.91it/s] 25%|██▌       | 6828/26933 [08:37<22:25, 14.94it/s] 25%|██▌       | 6832/26933 [08:37<22:21, 14.98it/s] 25%|██▌       | 6836/26933 [08:37<22:20, 15.00it/s] 25%|██▌       | 6840/26933 [08:38<22:19, 15.00it/s] 25%|██▌       | 6844/26933 [08:38<22:16, 15.03it/s] 25%|██▌       | 6848/26933 [08:38<22:17, 15.01it/s] 25%|██▌       | 6852/26933 [08:38<22:21, 14.97it/s] 25%|██▌       | 6856/26933 [08:39<22:20, 14.98it/s] 25%|██▌       | 6860/26933 [08:39<22:20, 14.98it/s] 25%|██▌       | 6864/26933 [08:39<22:22, 14.95it/s] 26%|██▌       | 6868/26933 [08:39<22:24, 14.92it/s] 26%|██▌       | 6872/26933 [08:40<22:15, 15.02it/s] 26%|██▌       | 6876/26933 [08:40<22:07, 15.11it/s] 26%|██▌       | 6880/26933 [08:40<22:04, 15.14it/s] 26%|██▌       | 6884/26933 [08:40<22:02, 15.15it/s] 26%|██▌       | 6888/26933 [08:41<21:56, 15.22it/s] 26%|██▌       | 6892/26933 [08:41<21:58, 15.19it/s] 26%|██▌       | 6896/26933 [08:41<22:00, 15.17it/s] 26%|██▌       | 6900/26933 [08:41<22:00, 15.17it/s] 26%|██▌       | 6904/26933 [08:42<22:01, 15.15it/s] 26%|██▌       | 6908/26933 [08:42<22:01, 15.15it/s] 26%|██▌       | 6912/26933 [08:42<22:05, 15.10it/s] 26%|██▌       | 6916/26933 [08:43<22:02, 15.13it/s] 26%|██▌       | 6920/26933 [08:43<22:10, 15.05it/s] 26%|██▌       | 6924/26933 [08:43<22:17, 14.95it/s] 26%|██▌       | 6928/26933 [08:43<22:27, 14.84it/s] 26%|██▌       | 6932/26933 [08:44<22:19, 14.93it/s] 26%|██▌       | 6936/26933 [08:44<22:02, 15.12it/s] 26%|██▌       | 6940/26933 [08:44<21:41, 15.36it/s] 26%|██▌       | 6944/26933 [08:44<21:24, 15.56it/s] 26%|██▌       | 6948/26933 [08:45<21:11, 15.72it/s] 26%|██▌       | 6952/26933 [08:45<21:02, 15.82it/s] 26%|██▌       | 6956/26933 [08:45<21:02, 15.82it/s] 26%|██▌       | 6960/26933 [08:45<21:01, 15.84it/s] 26%|██▌       | 6964/26933 [08:46<20:57, 15.89it/s] 26%|██▌       | 6968/26933 [08:46<20:54, 15.92it/s] 26%|██▌       | 6972/26933 [08:46<20:50, 15.97it/s] 26%|██▌       | 6976/26933 [08:46<20:53, 15.92it/s] 26%|██▌       | 6980/26933 [08:47<20:51, 15.94it/s] 26%|██▌       | 6984/26933 [08:47<20:51, 15.93it/s] 26%|██▌       | 6988/26933 [08:47<20:49, 15.96it/s] 26%|██▌       | 6992/26933 [08:47<20:51, 15.94it/s] 26%|██▌       | 6996/26933 [08:48<20:49, 15.96it/s] 26%|██▌       | 7000/26933 [08:48<20:46, 15.99it/s] 26%|██▌       | 7004/26933 [08:48<20:44, 16.02it/s] 26%|██▌       | 7008/26933 [08:48<20:43, 16.02it/s] 26%|██▌       | 7012/26933 [08:49<20:47, 15.97it/s] 26%|██▌       | 7016/26933 [08:49<20:44, 16.00it/s] 26%|██▌       | 7020/26933 [08:49<20:40, 16.05it/s] 26%|██▌       | 7024/26933 [08:49<20:40, 16.05it/s] 26%|██▌       | 7028/26933 [08:50<20:39, 16.06it/s] 26%|██▌       | 7032/26933 [08:50<20:39, 16.05it/s] 26%|██▌       | 7036/26933 [08:50<20:37, 16.08it/s] 26%|██▌       | 7040/26933 [08:50<20:38, 16.06it/s] 26%|██▌       | 7044/26933 [08:51<20:37, 16.07it/s] 26%|██▌       | 7048/26933 [08:51<20:37, 16.07it/s] 26%|██▌       | 7052/26933 [08:51<20:38, 16.06it/s] 26%|██▌       | 7056/26933 [08:51<20:39, 16.04it/s] 26%|██▌       | 7060/26933 [08:52<20:37, 16.05it/s] 26%|██▌       | 7064/26933 [08:52<20:36, 16.06it/s] 26%|██▌       | 7068/26933 [08:52<20:37, 16.06it/s] 26%|██▋       | 7072/26933 [08:52<20:38, 16.03it/s] 26%|██▋       | 7076/26933 [08:53<20:36, 16.06it/s] 26%|██▋       | 7080/26933 [08:53<20:38, 16.03it/s] 26%|██▋       | 7084/26933 [08:53<20:41, 15.99it/s] 26%|██▋       | 7088/26933 [08:53<20:36, 16.05it/s] 26%|██▋       | 7092/26933 [08:54<20:53, 15.83it/s] 26%|██▋       | 7096/26933 [08:54<21:08, 15.64it/s] 26%|██▋       | 7100/26933 [08:54<21:26, 15.42it/s] 26%|██▋       | 7104/26933 [08:54<21:34, 15.31it/s] 26%|██▋       | 7108/26933 [08:55<21:33, 15.32it/s] 26%|██▋       | 7112/26933 [08:55<21:35, 15.30it/s] 26%|██▋       | 7116/26933 [08:55<21:19, 15.49it/s] 26%|██▋       | 7120/26933 [08:55<21:05, 15.65it/s] 26%|██▋       | 7124/26933 [08:56<20:51, 15.83it/s] 26%|██▋       | 7128/26933 [08:56<21:00, 15.71it/s] 26%|██▋       | 7132/26933 [08:56<21:11, 15.57it/s] 26%|██▋       | 7136/26933 [08:56<21:20, 15.46it/s] 27%|██▋       | 7140/26933 [08:57<21:21, 15.45it/s] 27%|██▋       | 7144/26933 [08:57<21:22, 15.43it/s] 27%|██▋       | 7148/26933 [08:57<21:28, 15.36it/s] 27%|██▋       | 7152/26933 [08:58<21:34, 15.28it/s] 27%|██▋       | 7156/26933 [08:58<21:39, 15.22it/s] 27%|██▋       | 7160/26933 [08:58<21:39, 15.21it/s] 27%|██▋       | 7164/26933 [08:58<21:42, 15.18it/s] 27%|██▋       | 7168/26933 [08:59<21:43, 15.16it/s] 27%|██▋       | 7172/26933 [08:59<21:42, 15.17it/s] 27%|██▋       | 7176/26933 [08:59<21:51, 15.06it/s] 27%|██▋       | 7180/26933 [08:59<21:51, 15.06it/s] 27%|██▋       | 7184/26933 [09:00<21:48, 15.10it/s] 27%|██▋       | 7188/26933 [09:00<21:59, 14.97it/s] 27%|██▋       | 7192/26933 [09:00<21:57, 14.98it/s] 27%|██▋       | 7196/26933 [09:00<21:35, 15.23it/s] 27%|██▋       | 7200/26933 [09:01<21:10, 15.54it/s] 27%|██▋       | 7204/26933 [09:01<20:49, 15.79it/s] 27%|██▋       | 7208/26933 [09:01<20:38, 15.93it/s] 27%|██▋       | 7212/26933 [09:01<20:31, 16.01it/s] 27%|██▋       | 7216/26933 [09:02<20:24, 16.10it/s] 27%|██▋       | 7220/26933 [09:02<20:17, 16.20it/s] 27%|██▋       | 7224/26933 [09:02<20:13, 16.24it/s] 27%|██▋       | 7228/26933 [09:02<20:14, 16.23it/s] 27%|██▋       | 7232/26933 [09:03<20:12, 16.25it/s] 27%|██▋       | 7236/26933 [09:03<20:10, 16.27it/s] 27%|██▋       | 7240/26933 [09:03<20:10, 16.27it/s] 27%|██▋       | 7244/26933 [09:03<20:10, 16.27it/s] 27%|██▋       | 7248/26933 [09:04<20:08, 16.29it/s] 27%|██▋       | 7252/26933 [09:04<20:03, 16.36it/s] 27%|██▋       | 7256/26933 [09:04<20:02, 16.36it/s] 27%|██▋       | 7260/26933 [09:04<20:07, 16.30it/s] 27%|██▋       | 7264/26933 [09:05<20:07, 16.29it/s] 27%|██▋       | 7268/26933 [09:05<20:05, 16.32it/s] 27%|██▋       | 7272/26933 [09:05<20:05, 16.31it/s] 27%|██▋       | 7276/26933 [09:05<20:04, 16.31it/s] 27%|██▋       | 7280/26933 [09:06<20:08, 16.26it/s] 27%|██▋       | 7284/26933 [09:06<20:32, 15.95it/s] 27%|██▋       | 7288/26933 [09:06<20:43, 15.79it/s] 27%|██▋       | 7292/26933 [09:06<20:56, 15.63it/s] 27%|██▋       | 7296/26933 [09:07<21:05, 15.51it/s] 27%|██▋       | 7300/26933 [09:07<21:10, 15.45it/s] 27%|██▋       | 7304/26933 [09:07<21:13, 15.41it/s] 27%|██▋       | 7308/26933 [09:07<21:18, 15.35it/s] 27%|██▋       | 7312/26933 [09:08<21:19, 15.34it/s] 27%|██▋       | 7316/26933 [09:08<21:21, 15.31it/s] 27%|██▋       | 7320/26933 [09:08<21:27, 15.24it/s] 27%|██▋       | 7324/26933 [09:08<21:30, 15.19it/s] 27%|██▋       | 7328/26933 [09:09<21:35, 15.14it/s] 27%|██▋       | 7332/26933 [09:09<21:33, 15.16it/s] 27%|██▋       | 7336/26933 [09:09<21:47, 14.99it/s] 27%|██▋       | 7340/26933 [09:10<21:55, 14.89it/s] 27%|██▋       | 7344/26933 [09:10<22:01, 14.82it/s] 27%|██▋       | 7348/26933 [09:10<22:01, 14.82it/s] 27%|██▋       | 7352/26933 [09:10<21:52, 14.92it/s] 27%|██▋       | 7356/26933 [09:11<21:41, 15.04it/s] 27%|██▋       | 7360/26933 [09:11<21:29, 15.18it/s] 27%|██▋       | 7364/26933 [09:11<21:06, 15.45it/s] 27%|██▋       | 7368/26933 [09:11<21:14, 15.35it/s] 27%|██▋       | 7372/26933 [09:12<21:17, 15.32it/s] 27%|██▋       | 7376/26933 [09:12<21:21, 15.26it/s] 27%|██▋       | 7380/26933 [09:12<21:21, 15.26it/s] 27%|██▋       | 7384/26933 [09:12<21:28, 15.17it/s] 27%|██▋       | 7388/26933 [09:13<21:29, 15.16it/s] 27%|██▋       | 7392/26933 [09:13<21:24, 15.21it/s] 27%|██▋       | 7396/26933 [09:13<21:21, 15.24it/s] 27%|██▋       | 7400/26933 [09:13<21:21, 15.24it/s] 27%|██▋       | 7404/26933 [09:14<21:18, 15.27it/s] 28%|██▊       | 7408/26933 [09:14<21:16, 15.29it/s] 28%|██▊       | 7412/26933 [09:14<21:18, 15.27it/s] 28%|██▊       | 7416/26933 [09:15<21:14, 15.31it/s] 28%|██▊       | 7420/26933 [09:15<21:15, 15.30it/s] 28%|██▊       | 7424/26933 [09:15<21:09, 15.37it/s] 28%|██▊       | 7428/26933 [09:15<21:10, 15.35it/s] 28%|██▊       | 7432/26933 [09:16<21:07, 15.38it/s] 28%|██▊       | 7436/26933 [09:16<21:14, 15.30it/s] 28%|██▊       | 7440/26933 [09:16<21:17, 15.26it/s] 28%|██▊       | 7444/26933 [09:16<21:19, 15.24it/s] 28%|██▊       | 7448/26933 [09:17<21:21, 15.21it/s] 28%|██▊       | 7452/26933 [09:17<21:22, 15.18it/s] 28%|██▊       | 7456/26933 [09:17<21:21, 15.20it/s] 28%|██▊       | 7460/26933 [09:17<21:25, 15.15it/s] 28%|██▊       | 7464/26933 [09:18<21:24, 15.15it/s] 28%|██▊       | 7468/26933 [09:18<21:28, 15.10it/s] 28%|██▊       | 7472/26933 [09:18<21:25, 15.14it/s] 28%|██▊       | 7476/26933 [09:18<21:38, 14.99it/s] 28%|██▊       | 7480/26933 [09:19<21:39, 14.97it/s] 28%|██▊       | 7484/26933 [09:19<21:35, 15.02it/s] 28%|██▊       | 7488/26933 [09:19<21:30, 15.07it/s] 28%|██▊       | 7492/26933 [09:20<21:28, 15.09it/s] 28%|██▊       | 7496/26933 [09:20<21:21, 15.17it/s] 28%|██▊       | 7500/26933 [09:20<21:17, 15.21it/s] 28%|██▊       | 7504/26933 [09:20<21:13, 15.26it/s] 28%|██▊       | 7508/26933 [09:21<21:09, 15.31it/s] 28%|██▊       | 7512/26933 [09:21<21:10, 15.29it/s] 28%|██▊       | 7516/26933 [09:21<21:12, 15.26it/s] 28%|██▊       | 7520/26933 [09:21<21:11, 15.26it/s] 28%|██▊       | 7524/26933 [09:22<21:09, 15.29it/s] 28%|██▊       | 7528/26933 [09:22<21:08, 15.30it/s] 28%|██▊       | 7532/26933 [09:22<21:11, 15.26it/s] 28%|██▊       | 7536/26933 [09:22<21:12, 15.24it/s] 28%|██▊       | 7540/26933 [09:23<21:16, 15.19it/s] 28%|██▊       | 7544/26933 [09:23<21:19, 15.16it/s] 28%|██▊       | 7548/26933 [09:23<21:21, 15.13it/s] 28%|██▊       | 7552/26933 [09:23<21:20, 15.13it/s] 28%|██▊       | 7556/26933 [09:24<21:31, 15.01it/s] 28%|██▊       | 7560/26933 [09:24<21:30, 15.01it/s] 28%|██▊       | 7564/26933 [09:24<21:28, 15.03it/s] 28%|██▊       | 7568/26933 [09:25<21:25, 15.06it/s] 28%|██▊       | 7572/26933 [09:25<21:26, 15.05it/s] 28%|██▊       | 7576/26933 [09:25<21:25, 15.06it/s] 28%|██▊       | 7580/26933 [09:25<21:23, 15.08it/s] 28%|██▊       | 7584/26933 [09:26<21:22, 15.09it/s] 28%|██▊       | 7588/26933 [09:26<21:23, 15.07it/s] 28%|██▊       | 7592/26933 [09:26<21:19, 15.11it/s] 28%|██▊       | 7596/26933 [09:26<21:19, 15.11it/s] 28%|██▊       | 7600/26933 [09:27<21:15, 15.16it/s] 28%|██▊       | 7604/26933 [09:27<21:12, 15.19it/s] 28%|██▊       | 7608/26933 [09:27<21:12, 15.19it/s] 28%|██▊       | 7612/26933 [09:27<21:14, 15.16it/s] 28%|██▊       | 7616/26933 [09:28<21:10, 15.21it/s] 28%|██▊       | 7620/26933 [09:28<21:10, 15.20it/s] 28%|██▊       | 7624/26933 [09:28<21:14, 15.15it/s] 28%|██▊       | 7628/26933 [09:29<21:10, 15.20it/s] 28%|██▊       | 7632/26933 [09:29<21:02, 15.28it/s] 28%|██▊       | 7636/26933 [09:29<20:56, 15.36it/s] 28%|██▊       | 7640/26933 [09:29<20:55, 15.37it/s] 28%|██▊       | 7644/26933 [09:30<20:51, 15.41it/s] 28%|██▊       | 7648/26933 [09:30<20:57, 15.33it/s] 28%|██▊       | 7652/26933 [09:30<20:54, 15.37it/s] 28%|██▊       | 7656/26933 [09:30<20:55, 15.35it/s] 28%|██▊       | 7660/26933 [09:31<20:48, 15.43it/s] 28%|██▊       | 7664/26933 [09:31<20:48, 15.43it/s] 28%|██▊       | 7668/26933 [09:31<20:46, 15.45it/s] 28%|██▊       | 7672/26933 [09:31<20:49, 15.42it/s] 29%|██▊       | 7676/26933 [09:32<20:49, 15.42it/s] 29%|██▊       | 7680/26933 [09:32<20:49, 15.40it/s] 29%|██▊       | 7684/26933 [09:32<20:47, 15.43it/s] 29%|██▊       | 7688/26933 [09:32<20:48, 15.41it/s] 29%|██▊       | 7692/26933 [09:33<20:49, 15.40it/s] 29%|██▊       | 7696/26933 [09:33<20:46, 15.43it/s] 29%|██▊       | 7700/26933 [09:33<20:44, 15.46it/s] 29%|██▊       | 7704/26933 [09:33<20:45, 15.43it/s] 29%|██▊       | 7708/26933 [09:34<20:53, 15.33it/s] 29%|██▊       | 7712/26933 [09:34<21:05, 15.18it/s] 29%|██▊       | 7716/26933 [09:34<21:01, 15.24it/s] 29%|██▊       | 7720/26933 [09:34<21:00, 15.24it/s] 29%|██▊       | 7724/26933 [09:35<20:55, 15.30it/s] 29%|██▊       | 7728/26933 [09:35<21:00, 15.24it/s] 29%|██▊       | 7732/26933 [09:35<20:58, 15.25it/s] 29%|██▊       | 7736/26933 [09:36<20:54, 15.30it/s] 29%|██▊       | 7740/26933 [09:36<20:49, 15.36it/s] 29%|██▉       | 7744/26933 [09:36<20:48, 15.37it/s] 29%|██▉       | 7748/26933 [09:36<20:49, 15.36it/s] 29%|██▉       | 7752/26933 [09:37<20:47, 15.37it/s] 29%|██▉       | 7756/26933 [09:37<20:48, 15.36it/s] 29%|██▉       | 7760/26933 [09:37<20:50, 15.34it/s] 29%|██▉       | 7764/26933 [09:37<20:51, 15.31it/s] 29%|██▉       | 7768/26933 [09:38<20:49, 15.34it/s] 29%|██▉       | 7772/26933 [09:38<20:47, 15.36it/s] 29%|██▉       | 7776/26933 [09:38<20:46, 15.37it/s] 29%|██▉       | 7780/26933 [09:38<20:45, 15.38it/s] 29%|██▉       | 7784/26933 [09:39<20:42, 15.41it/s] 29%|██▉       | 7788/26933 [09:39<20:42, 15.41it/s] 29%|██▉       | 7792/26933 [09:39<20:43, 15.40it/s] 29%|██▉       | 7796/26933 [09:39<20:45, 15.36it/s] 29%|██▉       | 7800/26933 [09:40<20:41, 15.41it/s] 29%|██▉       | 7804/26933 [09:40<20:40, 15.41it/s] 29%|██▉       | 7808/26933 [09:40<20:41, 15.41it/s] 29%|██▉       | 7812/26933 [09:40<20:44, 15.37it/s] 29%|██▉       | 7816/26933 [09:41<20:40, 15.41it/s] 29%|██▉       | 7820/26933 [09:41<20:34, 15.48it/s] 29%|██▉       | 7824/26933 [09:41<20:33, 15.49it/s] 29%|██▉       | 7828/26933 [09:42<20:34, 15.47it/s] 29%|██▉       | 7832/26933 [09:42<20:30, 15.53it/s] 29%|██▉       | 7836/26933 [09:42<20:27, 15.56it/s] 29%|██▉       | 7840/26933 [09:42<20:30, 15.52it/s] 29%|██▉       | 7844/26933 [09:43<20:28, 15.54it/s] 29%|██▉       | 7848/26933 [09:43<20:27, 15.55it/s] 29%|██▉       | 7852/26933 [09:43<20:25, 15.57it/s] 29%|██▉       | 7856/26933 [09:43<20:26, 15.55it/s] 29%|██▉       | 7860/26933 [09:44<20:26, 15.55it/s] 29%|██▉       | 7864/26933 [09:44<20:25, 15.57it/s] 29%|██▉       | 7868/26933 [09:44<20:26, 15.54it/s] 29%|██▉       | 7872/26933 [09:44<20:27, 15.53it/s] 29%|██▉       | 7876/26933 [09:45<20:27, 15.53it/s] 29%|██▉       | 7880/26933 [09:45<20:25, 15.55it/s] 29%|██▉       | 7884/26933 [09:45<20:24, 15.56it/s] 29%|██▉       | 7888/26933 [09:45<20:25, 15.55it/s] 29%|██▉       | 7892/26933 [09:46<20:22, 15.58it/s] 29%|██▉       | 7896/26933 [09:46<20:22, 15.57it/s] 29%|██▉       | 7900/26933 [09:46<20:27, 15.50it/s] 29%|██▉       | 7904/26933 [09:46<20:31, 15.45it/s] 29%|██▉       | 7908/26933 [09:47<20:29, 15.47it/s] 29%|██▉       | 7912/26933 [09:47<20:30, 15.46it/s] 29%|██▉       | 7916/26933 [09:47<20:28, 15.49it/s] 29%|██▉       | 7920/26933 [09:47<20:26, 15.50it/s] 29%|██▉       | 7924/26933 [09:48<20:21, 15.57it/s] 29%|██▉       | 7928/26933 [09:48<20:19, 15.59it/s] 29%|██▉       | 7932/26933 [09:48<20:23, 15.53it/s] 29%|██▉       | 7936/26933 [09:48<20:25, 15.51it/s] 29%|██▉       | 7940/26933 [09:49<20:26, 15.48it/s] 29%|██▉       | 7944/26933 [09:49<20:20, 15.55it/s] 30%|██▉       | 7948/26933 [09:49<20:16, 15.61it/s] 30%|██▉       | 7952/26933 [09:49<20:23, 15.51it/s] 30%|██▉       | 7956/26933 [09:50<20:24, 15.50it/s] 30%|██▉       | 7960/26933 [09:50<20:23, 15.51it/s] 30%|██▉       | 7964/26933 [09:50<20:21, 15.53it/s] 30%|██▉       | 7968/26933 [09:51<20:20, 15.53it/s] 30%|██▉       | 7972/26933 [09:51<20:20, 15.54it/s] 30%|██▉       | 7976/26933 [09:51<20:17, 15.57it/s] 30%|██▉       | 7980/26933 [09:51<20:23, 15.49it/s] 30%|██▉       | 7984/26933 [09:52<20:21, 15.52it/s] 30%|██▉       | 7988/26933 [09:52<20:20, 15.52it/s] 30%|██▉       | 7992/26933 [09:52<20:20, 15.52it/s] 30%|██▉       | 7996/26933 [09:52<20:19, 15.52it/s] 30%|██▉       | 8000/26933 [09:53<20:15, 15.57it/s] 30%|██▉       | 8004/26933 [09:53<20:13, 15.60it/s] 30%|██▉       | 8008/26933 [09:53<20:12, 15.61it/s] 30%|██▉       | 8012/26933 [09:53<20:13, 15.59it/s] 30%|██▉       | 8016/26933 [09:54<20:10, 15.63it/s] 30%|██▉       | 8020/26933 [09:54<20:11, 15.61it/s] 30%|██▉       | 8024/26933 [09:54<20:08, 15.64it/s] 30%|██▉       | 8028/26933 [09:54<19:57, 15.79it/s] 30%|██▉       | 8032/26933 [09:55<19:54, 15.83it/s] 30%|██▉       | 8036/26933 [09:55<19:51, 15.86it/s] 30%|██▉       | 8040/26933 [09:55<19:18, 16.31it/s] 30%|██▉       | 8044/26933 [09:55<19:31, 16.12it/s] 30%|██▉       | 8048/26933 [09:56<19:37, 16.04it/s] 30%|██▉       | 8052/26933 [09:56<19:09, 16.43it/s] 30%|██▉       | 8056/26933 [09:56<19:23, 16.22it/s] 30%|██▉       | 8060/26933 [09:56<19:35, 16.05it/s] 30%|██▉       | 8064/26933 [09:57<19:10, 16.40it/s] 30%|██▉       | 8068/26933 [09:57<19:23, 16.21it/s] 30%|██▉       | 8072/26933 [09:57<19:32, 16.08it/s] 30%|██▉       | 8076/26933 [09:57<19:09, 16.41it/s] 30%|███       | 8080/26933 [09:58<19:20, 16.25it/s] 30%|███       | 8084/26933 [09:58<19:27, 16.15it/s] 30%|███       | 8088/26933 [09:58<19:03, 16.48it/s] 30%|███       | 8092/26933 [09:58<19:20, 16.24it/s] 30%|███       | 8096/26933 [09:59<19:30, 16.09it/s] 30%|███       | 8100/26933 [09:59<19:08, 16.40it/s] 30%|███       | 8104/26933 [09:59<19:39, 15.97it/s] 30%|███       | 8108/26933 [09:59<19:10, 16.37it/s] 30%|███       | 8112/26933 [10:00<18:46, 16.70it/s] 30%|███       | 8116/26933 [10:00<19:03, 16.46it/s] 30%|███       | 8120/26933 [10:00<18:43, 16.75it/s] 30%|███       | 8124/26933 [10:00<18:30, 16.93it/s] 30%|███       | 8128/26933 [10:00<18:19, 17.11it/s] 30%|███       | 8132/26933 [10:01<18:09, 17.26it/s] 30%|███       | 8136/26933 [10:01<18:03, 17.36it/s] 30%|███       | 8140/26933 [10:01<17:58, 17.42it/s] 30%|███       | 8144/26933 [10:01<18:00, 17.38it/s] 30%|███       | 8148/26933 [10:02<17:57, 17.44it/s] 30%|███       | 8152/26933 [10:02<17:57, 17.43it/s] 30%|███       | 8156/26933 [10:02<17:54, 17.48it/s] 30%|███       | 8160/26933 [10:02<17:54, 17.47it/s] 30%|███       | 8164/26933 [10:03<17:57, 17.43it/s] 30%|███       | 8168/26933 [10:03<17:56, 17.43it/s] 30%|███       | 8172/26933 [10:03<17:57, 17.42it/s] 30%|███       | 8176/26933 [10:03<17:55, 17.44it/s] 30%|███       | 8180/26933 [10:03<17:57, 17.40it/s] 30%|███       | 8184/26933 [10:04<17:57, 17.40it/s] 30%|███       | 8188/26933 [10:04<17:58, 17.39it/s] 30%|███       | 8192/26933 [10:04<17:56, 17.41it/s] 30%|███       | 8196/26933 [10:04<17:58, 17.37it/s] 30%|███       | 8200/26933 [10:05<17:56, 17.39it/s] 30%|███       | 8204/26933 [10:05<18:03, 17.29it/s] 30%|███       | 8208/26933 [10:05<18:02, 17.30it/s] 30%|███       | 8212/26933 [10:05<18:02, 17.29it/s] 31%|███       | 8216/26933 [10:06<18:00, 17.32it/s] 31%|███       | 8220/26933 [10:06<17:57, 17.36it/s] 31%|███       | 8224/26933 [10:06<17:54, 17.41it/s] 31%|███       | 8228/26933 [10:06<17:51, 17.46it/s] 31%|███       | 8232/26933 [10:06<17:50, 17.47it/s] 31%|███       | 8236/26933 [10:07<17:48, 17.49it/s] 31%|███       | 8240/26933 [10:07<17:48, 17.50it/s] 31%|███       | 8244/26933 [10:07<17:47, 17.50it/s] 31%|███       | 8248/26933 [10:07<17:49, 17.48it/s] 31%|███       | 8252/26933 [10:08<17:46, 17.52it/s] 31%|███       | 8256/26933 [10:08<17:44, 17.54it/s] 31%|███       | 8260/26933 [10:08<17:49, 17.47it/s] 31%|███       | 8264/26933 [10:08<17:53, 17.40it/s] 31%|███       | 8268/26933 [10:08<17:48, 17.46it/s] 31%|███       | 8272/26933 [10:09<17:48, 17.47it/s] 31%|███       | 8276/26933 [10:09<17:46, 17.49it/s] 31%|███       | 8280/26933 [10:09<17:45, 17.51it/s] 31%|███       | 8284/26933 [10:09<17:46, 17.49it/s] 31%|███       | 8288/26933 [10:10<17:43, 17.53it/s] 31%|███       | 8292/26933 [10:10<17:42, 17.54it/s] 31%|███       | 8296/26933 [10:10<17:41, 17.56it/s] 31%|███       | 8300/26933 [10:10<17:41, 17.56it/s] 31%|███       | 8304/26933 [10:11<17:39, 17.57it/s] 31%|███       | 8308/26933 [10:11<17:39, 17.58it/s] 31%|███       | 8312/26933 [10:11<17:36, 17.62it/s] 31%|███       | 8316/26933 [10:11<17:37, 17.60it/s] 31%|███       | 8320/26933 [10:11<17:39, 17.56it/s] 31%|███       | 8324/26933 [10:12<17:38, 17.57it/s] 31%|███       | 8328/26933 [10:12<17:42, 17.52it/s] 31%|███       | 8332/26933 [10:12<17:41, 17.52it/s] 31%|███       | 8336/26933 [10:12<17:46, 17.44it/s] 31%|███       | 8340/26933 [10:13<17:54, 17.30it/s] 31%|███       | 8344/26933 [10:13<17:56, 17.28it/s] 31%|███       | 8348/26933 [10:13<18:15, 16.97it/s] 31%|███       | 8352/26933 [10:13<18:32, 16.70it/s] 31%|███       | 8356/26933 [10:14<18:17, 16.93it/s] 31%|███       | 8360/26933 [10:14<18:06, 17.09it/s] 31%|███       | 8364/26933 [10:14<17:54, 17.29it/s] 31%|███       | 8368/26933 [10:14<17:48, 17.38it/s] 31%|███       | 8372/26933 [10:14<17:38, 17.53it/s] 31%|███       | 8376/26933 [10:15<17:31, 17.65it/s] 31%|███       | 8380/26933 [10:15<17:27, 17.71it/s] 31%|███       | 8384/26933 [10:15<17:23, 17.77it/s] 31%|███       | 8388/26933 [10:15<17:23, 17.77it/s] 31%|███       | 8392/26933 [10:16<17:21, 17.81it/s] 31%|███       | 8396/26933 [10:16<17:18, 17.85it/s] 31%|███       | 8400/26933 [10:16<17:18, 17.85it/s] 31%|███       | 8404/26933 [10:16<17:20, 17.81it/s] 31%|███       | 8408/26933 [10:16<17:17, 17.85it/s] 31%|███       | 8412/26933 [10:17<17:16, 17.86it/s] 31%|███       | 8416/26933 [10:17<17:18, 17.82it/s] 31%|███▏      | 8420/26933 [10:17<17:16, 17.87it/s] 31%|███▏      | 8424/26933 [10:17<17:17, 17.84it/s] 31%|███▏      | 8428/26933 [10:18<17:15, 17.87it/s] 31%|███▏      | 8432/26933 [10:18<17:14, 17.88it/s] 31%|███▏      | 8436/26933 [10:18<17:13, 17.89it/s] 31%|███▏      | 8440/26933 [10:18<17:16, 17.84it/s] 31%|███▏      | 8444/26933 [10:18<17:17, 17.82it/s] 31%|███▏      | 8448/26933 [10:19<17:17, 17.82it/s] 31%|███▏      | 8452/26933 [10:19<17:16, 17.82it/s] 31%|███▏      | 8456/26933 [10:19<17:13, 17.87it/s] 31%|███▏      | 8460/26933 [10:19<17:14, 17.87it/s] 31%|███▏      | 8464/26933 [10:20<17:12, 17.89it/s] 31%|███▏      | 8468/26933 [10:20<17:14, 17.84it/s] 31%|███▏      | 8472/26933 [10:20<17:21, 17.72it/s] 31%|███▏      | 8476/26933 [10:20<17:36, 17.46it/s] 31%|███▏      | 8480/26933 [10:21<17:36, 17.46it/s] 32%|███▏      | 8484/26933 [10:21<17:39, 17.41it/s] 32%|███▏      | 8488/26933 [10:21<17:41, 17.38it/s] 32%|███▏      | 8492/26933 [10:21<17:42, 17.36it/s] 32%|███▏      | 8496/26933 [10:21<17:39, 17.41it/s] 32%|███▏      | 8500/26933 [10:22<17:36, 17.44it/s] 32%|███▏      | 8504/26933 [10:22<17:37, 17.43it/s] 32%|███▏      | 8508/26933 [10:22<17:34, 17.47it/s] 32%|███▏      | 8512/26933 [10:22<17:35, 17.45it/s] 32%|███▏      | 8516/26933 [10:23<17:32, 17.50it/s] 32%|███▏      | 8520/26933 [10:23<17:30, 17.53it/s] 32%|███▏      | 8524/26933 [10:23<17:31, 17.51it/s] 32%|███▏      | 8528/26933 [10:23<17:33, 17.48it/s] 32%|███▏      | 8532/26933 [10:24<17:31, 17.49it/s] 32%|███▏      | 8536/26933 [10:24<17:31, 17.50it/s] 32%|███▏      | 8540/26933 [10:24<17:29, 17.52it/s] 32%|███▏      | 8544/26933 [10:24<17:28, 17.54it/s] 32%|███▏      | 8548/26933 [10:24<17:31, 17.48it/s] 32%|███▏      | 8552/26933 [10:25<17:30, 17.50it/s] 32%|███▏      | 8556/26933 [10:25<17:28, 17.52it/s] 32%|███▏      | 8560/26933 [10:25<17:27, 17.55it/s] 32%|███▏      | 8564/26933 [10:25<17:30, 17.48it/s] 32%|███▏      | 8568/26933 [10:26<17:27, 17.53it/s] 32%|███▏      | 8572/26933 [10:26<17:37, 17.37it/s] 32%|███▏      | 8576/26933 [10:26<17:31, 17.45it/s] 32%|███▏      | 8580/26933 [10:26<17:30, 17.48it/s] 32%|███▏      | 8584/26933 [10:26<17:27, 17.52it/s] 32%|███▏      | 8588/26933 [10:27<17:25, 17.55it/s] 32%|███▏      | 8592/26933 [10:27<17:24, 17.56it/s] 32%|███▏      | 8596/26933 [10:27<17:24, 17.56it/s] 32%|███▏      | 8600/26933 [10:27<17:25, 17.53it/s] 32%|███▏      | 8604/26933 [10:28<17:24, 17.55it/s] 32%|███▏      | 8608/26933 [10:28<17:23, 17.56it/s] 32%|███▏      | 8612/26933 [10:28<17:27, 17.50it/s] 32%|███▏      | 8616/26933 [10:28<17:28, 17.46it/s] 32%|███▏      | 8620/26933 [10:29<17:28, 17.47it/s] 32%|███▏      | 8624/26933 [10:29<17:26, 17.49it/s] 32%|███▏      | 8628/26933 [10:29<17:26, 17.49it/s] 32%|███▏      | 8632/26933 [10:29<17:31, 17.41it/s] 32%|███▏      | 8636/26933 [10:29<17:29, 17.44it/s] 32%|███▏      | 8640/26933 [10:30<17:28, 17.45it/s] 32%|███▏      | 8644/26933 [10:30<17:28, 17.45it/s] 32%|███▏      | 8648/26933 [10:30<17:30, 17.40it/s] 32%|███▏      | 8652/26933 [10:30<17:40, 17.23it/s] 32%|███▏      | 8656/26933 [10:31<17:33, 17.34it/s] 32%|███▏      | 8660/26933 [10:31<17:31, 17.38it/s] 32%|███▏      | 8664/26933 [10:31<17:30, 17.39it/s] 32%|███▏      | 8668/26933 [10:31<17:29, 17.41it/s] 32%|███▏      | 8672/26933 [10:32<17:27, 17.44it/s] 32%|███▏      | 8676/26933 [10:32<17:28, 17.41it/s] 32%|███▏      | 8680/26933 [10:32<17:24, 17.47it/s] 32%|███▏      | 8684/26933 [10:32<17:23, 17.49it/s] 32%|███▏      | 8688/26933 [10:32<17:28, 17.41it/s] 32%|███▏      | 8692/26933 [10:33<17:25, 17.44it/s] 32%|███▏      | 8696/26933 [10:33<17:25, 17.45it/s] 32%|███▏      | 8700/26933 [10:33<17:33, 17.31it/s] 32%|███▏      | 8704/26933 [10:33<17:31, 17.34it/s] 32%|███▏      | 8708/26933 [10:34<17:27, 17.40it/s] 32%|███▏      | 8712/26933 [10:34<17:29, 17.37it/s] 32%|███▏      | 8716/26933 [10:34<17:26, 17.41it/s] 32%|███▏      | 8720/26933 [10:34<17:25, 17.42it/s] 32%|███▏      | 8724/26933 [10:35<17:28, 17.37it/s] 32%|███▏      | 8728/26933 [10:35<17:26, 17.40it/s] 32%|███▏      | 8732/26933 [10:35<17:26, 17.39it/s] 32%|███▏      | 8736/26933 [10:35<17:25, 17.40it/s] 32%|███▏      | 8740/26933 [10:35<17:28, 17.35it/s] 32%|███▏      | 8744/26933 [10:36<17:26, 17.38it/s] 32%|███▏      | 8748/26933 [10:36<17:23, 17.42it/s] 32%|███▏      | 8752/26933 [10:36<17:19, 17.49it/s] 33%|███▎      | 8756/26933 [10:36<17:18, 17.50it/s] 33%|███▎      | 8760/26933 [10:37<17:18, 17.50it/s] 33%|███▎      | 8764/26933 [10:37<17:16, 17.54it/s] 33%|███▎      | 8768/26933 [10:37<17:16, 17.53it/s] 33%|███▎      | 8772/26933 [10:37<17:19, 17.47it/s] 33%|███▎      | 8776/26933 [10:37<17:18, 17.48it/s] 33%|███▎      | 8780/26933 [10:38<17:17, 17.49it/s] 33%|███▎      | 8784/26933 [10:38<17:20, 17.44it/s] 33%|███▎      | 8788/26933 [10:38<17:21, 17.42it/s] 33%|███▎      | 8792/26933 [10:38<17:20, 17.43it/s] 33%|███▎      | 8796/26933 [10:39<17:20, 17.44it/s] 33%|███▎      | 8800/26933 [10:39<17:16, 17.49it/s] 33%|███▎      | 8804/26933 [10:39<17:14, 17.53it/s] 33%|███▎      | 8808/26933 [10:39<17:14, 17.52it/s] 33%|███▎      | 8812/26933 [10:40<17:12, 17.55it/s] 33%|███▎      | 8816/26933 [10:40<17:12, 17.54it/s] 33%|███▎      | 8820/26933 [10:40<17:12, 17.55it/s] 33%|███▎      | 8824/26933 [10:40<17:11, 17.55it/s] 33%|███▎      | 8828/26933 [10:40<17:11, 17.55it/s] 33%|███▎      | 8832/26933 [10:41<17:10, 17.56it/s] 33%|███▎      | 8836/26933 [10:41<17:10, 17.56it/s] 33%|███▎      | 8840/26933 [10:41<17:09, 17.58it/s] 33%|███▎      | 8844/26933 [10:41<17:10, 17.55it/s] 33%|███▎      | 8848/26933 [10:42<17:16, 17.45it/s] 33%|███▎      | 8852/26933 [10:42<17:14, 17.48it/s] 33%|███▎      | 8856/26933 [10:42<17:13, 17.49it/s] 33%|███▎      | 8860/26933 [10:42<17:15, 17.46it/s] 33%|███▎      | 8864/26933 [10:43<17:17, 17.41it/s] 33%|███▎      | 8868/26933 [10:43<17:14, 17.46it/s] 33%|███▎      | 8872/26933 [10:43<17:11, 17.50it/s] 33%|███▎      | 8876/26933 [10:43<17:11, 17.51it/s] 33%|███▎      | 8880/26933 [10:43<17:13, 17.47it/s] 33%|███▎      | 8884/26933 [10:44<17:11, 17.50it/s] 33%|███▎      | 8888/26933 [10:44<17:09, 17.53it/s] 33%|███▎      | 8892/26933 [10:44<17:10, 17.51it/s] 33%|███▎      | 8896/26933 [10:44<17:10, 17.50it/s] 33%|███▎      | 8900/26933 [10:45<17:07, 17.54it/s] 33%|███▎      | 8904/26933 [10:45<17:06, 17.56it/s] 33%|███▎      | 8908/26933 [10:45<17:07, 17.55it/s] 33%|███▎      | 8912/26933 [10:45<17:05, 17.57it/s] 33%|███▎      | 8916/26933 [10:45<17:07, 17.54it/s] 33%|███▎      | 8920/26933 [10:46<17:05, 17.57it/s] 33%|███▎      | 8924/26933 [10:46<17:06, 17.54it/s] 33%|███▎      | 8928/26933 [10:46<17:07, 17.53it/s] 33%|███▎      | 8932/26933 [10:46<17:10, 17.47it/s] 33%|███▎      | 8936/26933 [10:47<17:08, 17.49it/s] 33%|███▎      | 8940/26933 [10:47<17:07, 17.52it/s] 33%|███▎      | 8944/26933 [10:47<17:06, 17.52it/s] 33%|███▎      | 8948/26933 [10:47<17:06, 17.52it/s] 33%|███▎      | 8952/26933 [10:48<17:09, 17.46it/s] 33%|███▎      | 8956/26933 [10:48<17:09, 17.47it/s] 33%|███▎      | 8960/26933 [10:48<17:14, 17.37it/s] 33%|███▎      | 8964/26933 [10:48<17:32, 17.07it/s] 33%|███▎      | 8968/26933 [10:48<17:32, 17.08it/s] 33%|███▎      | 8972/26933 [10:49<17:24, 17.20it/s] 33%|███▎      | 8976/26933 [10:49<17:22, 17.23it/s] 33%|███▎      | 8980/26933 [10:49<17:20, 17.25it/s] 33%|███▎      | 8984/26933 [10:49<17:26, 17.15it/s] 33%|███▎      | 8988/26933 [10:50<17:24, 17.19it/s] 33%|███▎      | 8992/26933 [10:50<17:36, 16.98it/s] 33%|███▎      | 8996/26933 [10:50<17:29, 17.09it/s] 33%|███▎      | 9000/26933 [10:50<17:30, 17.07it/s] 33%|███▎      | 9004/26933 [10:51<17:42, 16.88it/s] 33%|███▎      | 9008/26933 [10:51<17:48, 16.78it/s] 33%|███▎      | 9012/26933 [10:51<17:57, 16.63it/s] 33%|███▎      | 9016/26933 [10:51<17:43, 16.84it/s] 33%|███▎      | 9020/26933 [10:52<17:32, 17.02it/s] 34%|███▎      | 9024/26933 [10:52<17:24, 17.15it/s] 34%|███▎      | 9028/26933 [10:52<17:20, 17.22it/s] 34%|███▎      | 9032/26933 [10:52<17:17, 17.25it/s] 34%|███▎      | 9036/26933 [10:52<17:10, 17.36it/s] 34%|███▎      | 9040/26933 [10:53<17:09, 17.38it/s] 34%|███▎      | 9044/26933 [10:53<17:06, 17.43it/s] 34%|███▎      | 9048/26933 [10:53<17:18, 17.22it/s] 34%|███▎      | 9052/26933 [10:53<17:25, 17.10it/s] 34%|███▎      | 9056/26933 [10:54<17:20, 17.18it/s] 34%|███▎      | 9060/26933 [10:54<17:17, 17.23it/s] 34%|███▎      | 9064/26933 [10:54<17:15, 17.26it/s] 34%|███▎      | 9068/26933 [10:54<17:20, 17.18it/s] 34%|███▎      | 9072/26933 [10:55<17:16, 17.23it/s] 34%|███▎      | 9076/26933 [10:55<17:17, 17.21it/s] 34%|███▎      | 9080/26933 [10:55<17:14, 17.26it/s] 34%|███▎      | 9084/26933 [10:55<17:25, 17.07it/s] 34%|███▎      | 9088/26933 [10:55<17:27, 17.04it/s] 34%|███▍      | 9092/26933 [10:56<17:28, 17.01it/s] 34%|███▍      | 9096/26933 [10:56<17:26, 17.04it/s] 34%|███▍      | 9100/26933 [10:56<17:19, 17.16it/s] 34%|███▍      | 9104/26933 [10:56<17:15, 17.21it/s] 34%|███▍      | 9108/26933 [10:57<17:10, 17.30it/s] 34%|███▍      | 9112/26933 [10:57<17:06, 17.35it/s] 34%|███▍      | 9116/26933 [10:57<17:03, 17.41it/s] 34%|███▍      | 9120/26933 [10:57<17:13, 17.23it/s] 34%|███▍      | 9124/26933 [10:58<17:14, 17.22it/s] 34%|███▍      | 9128/26933 [10:58<17:37, 16.83it/s] 34%|███▍      | 9132/26933 [10:58<17:54, 16.56it/s] 34%|███▍      | 9136/26933 [10:58<17:39, 16.79it/s] 34%|███▍      | 9140/26933 [10:59<17:30, 16.93it/s] 34%|███▍      | 9144/26933 [10:59<17:21, 17.09it/s] 34%|███▍      | 9148/26933 [10:59<17:11, 17.24it/s] 34%|███▍      | 9152/26933 [10:59<17:08, 17.28it/s] 34%|███▍      | 9156/26933 [10:59<17:04, 17.35it/s] 34%|███▍      | 9160/26933 [11:00<17:02, 17.38it/s] 34%|███▍      | 9164/26933 [11:00<17:00, 17.41it/s] 34%|███▍      | 9168/26933 [11:00<17:00, 17.41it/s] 34%|███▍      | 9172/26933 [11:00<17:02, 17.36it/s] 34%|███▍      | 9176/26933 [11:01<17:02, 17.37it/s] 34%|███▍      | 9180/26933 [11:01<17:02, 17.37it/s] 34%|███▍      | 9184/26933 [11:01<17:02, 17.36it/s] 34%|███▍      | 9188/26933 [11:01<16:59, 17.41it/s] 34%|███▍      | 9192/26933 [11:02<16:53, 17.51it/s] 34%|███▍      | 9196/26933 [11:02<16:47, 17.60it/s] 34%|███▍      | 9200/26933 [11:02<16:43, 17.66it/s] 34%|███▍      | 9204/26933 [11:02<16:45, 17.63it/s] 34%|███▍      | 9208/26933 [11:02<16:46, 17.61it/s] 34%|███▍      | 9212/26933 [11:03<16:42, 17.68it/s] 34%|███▍      | 9216/26933 [11:03<16:36, 17.78it/s] 34%|███▍      | 9220/26933 [11:03<16:34, 17.81it/s] 34%|███▍      | 9224/26933 [11:03<16:45, 17.61it/s] 34%|███▍      | 9228/26933 [11:04<16:44, 17.62it/s] 34%|███▍      | 9232/26933 [11:04<16:47, 17.57it/s] 34%|███▍      | 9236/26933 [11:04<16:48, 17.55it/s] 34%|███▍      | 9240/26933 [11:04<16:52, 17.47it/s] 34%|███▍      | 9244/26933 [11:04<16:53, 17.45it/s] 34%|███▍      | 9248/26933 [11:05<16:51, 17.48it/s] 34%|███▍      | 9252/26933 [11:05<16:56, 17.40it/s] 34%|███▍      | 9256/26933 [11:05<16:53, 17.45it/s] 34%|███▍      | 9260/26933 [11:05<16:54, 17.42it/s] 34%|███▍      | 9264/26933 [11:06<16:52, 17.46it/s] 34%|███▍      | 9268/26933 [11:06<16:54, 17.41it/s] 34%|███▍      | 9272/26933 [11:06<16:52, 17.44it/s] 34%|███▍      | 9276/26933 [11:06<16:56, 17.36it/s] 34%|███▍      | 9280/26933 [11:07<16:57, 17.34it/s] 34%|███▍      | 9284/26933 [11:07<16:53, 17.41it/s] 34%|███▍      | 9288/26933 [11:07<16:54, 17.39it/s] 35%|███▍      | 9292/26933 [11:07<16:54, 17.38it/s] 35%|███▍      | 9296/26933 [11:07<16:52, 17.41it/s] 35%|███▍      | 9300/26933 [11:08<16:54, 17.38it/s] 35%|███▍      | 9304/26933 [11:08<16:52, 17.41it/s] 35%|███▍      | 9308/26933 [11:08<16:51, 17.43it/s] 35%|███▍      | 9312/26933 [11:08<16:54, 17.37it/s] 35%|███▍      | 9316/26933 [11:09<16:53, 17.38it/s] 35%|███▍      | 9320/26933 [11:09<16:53, 17.38it/s] 35%|███▍      | 9324/26933 [11:09<16:49, 17.44it/s] 35%|███▍      | 9328/26933 [11:09<16:49, 17.43it/s] 35%|███▍      | 9332/26933 [11:10<16:46, 17.49it/s] 35%|███▍      | 9336/26933 [11:10<16:44, 17.51it/s] 35%|███▍      | 9340/26933 [11:10<16:42, 17.54it/s] 35%|███▍      | 9344/26933 [11:10<16:44, 17.52it/s] 35%|███▍      | 9348/26933 [11:10<16:47, 17.45it/s] 35%|███▍      | 9352/26933 [11:11<16:49, 17.42it/s] 35%|███▍      | 9356/26933 [11:11<16:48, 17.43it/s] 35%|███▍      | 9360/26933 [11:11<16:46, 17.46it/s] 35%|███▍      | 9364/26933 [11:11<16:49, 17.40it/s] 35%|███▍      | 9368/26933 [11:12<16:49, 17.41it/s] 35%|███▍      | 9372/26933 [11:12<16:50, 17.38it/s] 35%|███▍      | 9376/26933 [11:12<16:47, 17.42it/s] 35%|███▍      | 9380/26933 [11:12<16:49, 17.39it/s] 35%|███▍      | 9384/26933 [11:12<16:45, 17.46it/s] 35%|███▍      | 9388/26933 [11:13<16:45, 17.45it/s] 35%|███▍      | 9392/26933 [11:13<16:50, 17.36it/s] 35%|███▍      | 9396/26933 [11:13<16:46, 17.42it/s] 35%|███▍      | 9400/26933 [11:13<16:46, 17.42it/s] 35%|███▍      | 9404/26933 [11:14<16:44, 17.45it/s] 35%|███▍      | 9408/26933 [11:14<16:41, 17.50it/s] 35%|███▍      | 9412/26933 [11:14<16:39, 17.53it/s] 35%|███▍      | 9416/26933 [11:14<16:44, 17.44it/s] 35%|███▍      | 9420/26933 [11:15<16:40, 17.51it/s] 35%|███▍      | 9424/26933 [11:15<16:38, 17.53it/s] 35%|███▌      | 9428/26933 [11:15<16:39, 17.51it/s] 35%|███▌      | 9432/26933 [11:15<16:41, 17.48it/s] 35%|███▌      | 9436/26933 [11:15<16:39, 17.51it/s] 35%|███▌      | 9440/26933 [11:16<16:36, 17.56it/s] 35%|███▌      | 9444/26933 [11:16<16:35, 17.58it/s] 35%|███▌      | 9448/26933 [11:16<16:34, 17.59it/s] 35%|███▌      | 9452/26933 [11:16<16:33, 17.59it/s] 35%|███▌      | 9456/26933 [11:17<16:32, 17.60it/s] 35%|███▌      | 9460/26933 [11:17<16:31, 17.63it/s] 35%|███▌      | 9464/26933 [11:17<16:30, 17.63it/s] 35%|███▌      | 9468/26933 [11:17<16:34, 17.56it/s] 35%|███▌      | 9472/26933 [11:18<16:32, 17.59it/s] 35%|███▌      | 9476/26933 [11:18<16:29, 17.64it/s] 35%|███▌      | 9480/26933 [11:18<16:29, 17.64it/s] 35%|███▌      | 9484/26933 [11:18<16:30, 17.62it/s] 35%|███▌      | 9488/26933 [11:18<16:32, 17.57it/s] 35%|███▌      | 9492/26933 [11:19<16:36, 17.51it/s] 35%|███▌      | 9496/26933 [11:19<16:38, 17.47it/s] 35%|███▌      | 9500/26933 [11:19<16:39, 17.44it/s] 35%|███▌      | 9504/26933 [11:19<16:42, 17.38it/s] 35%|███▌      | 9508/26933 [11:20<16:42, 17.39it/s] 35%|███▌      | 9512/26933 [11:20<16:41, 17.39it/s] 35%|███▌      | 9516/26933 [11:20<16:41, 17.39it/s] 35%|███▌      | 9520/26933 [11:20<16:45, 17.32it/s] 35%|███▌      | 9524/26933 [11:20<16:43, 17.34it/s] 35%|███▌      | 9528/26933 [11:21<16:43, 17.35it/s] 35%|███▌      | 9532/26933 [11:21<16:42, 17.36it/s] 35%|███▌      | 9536/26933 [11:21<16:41, 17.37it/s] 35%|███▌      | 9540/26933 [11:21<16:48, 17.25it/s] 35%|███▌      | 9544/26933 [11:22<16:45, 17.29it/s] 35%|███▌      | 9548/26933 [11:22<16:47, 17.25it/s] 35%|███▌      | 9552/26933 [11:22<16:49, 17.22it/s] 35%|███▌      | 9556/26933 [11:22<16:49, 17.22it/s] 35%|███▌      | 9560/26933 [11:23<16:45, 17.28it/s] 36%|███▌      | 9564/26933 [11:23<16:43, 17.32it/s] 36%|███▌      | 9568/26933 [11:23<16:38, 17.38it/s] 36%|███▌      | 9572/26933 [11:23<16:39, 17.38it/s] 36%|███▌      | 9576/26933 [11:24<16:41, 17.33it/s] 36%|███▌      | 9580/26933 [11:24<16:54, 17.11it/s] 36%|███▌      | 9584/26933 [11:24<16:56, 17.07it/s] 36%|███▌      | 9588/26933 [11:24<17:02, 16.97it/s] 36%|███▌      | 9592/26933 [11:24<17:11, 16.81it/s] 36%|███▌      | 9596/26933 [11:25<17:11, 16.80it/s] 36%|███▌      | 9600/26933 [11:25<17:16, 16.73it/s] 36%|███▌      | 9604/26933 [11:25<17:12, 16.79it/s] 36%|███▌      | 9608/26933 [11:25<17:17, 16.69it/s] 36%|███▌      | 9612/26933 [11:26<17:16, 16.71it/s] 36%|███▌      | 9616/26933 [11:26<17:12, 16.77it/s] 36%|███▌      | 9620/26933 [11:26<17:11, 16.79it/s] 36%|███▌      | 9624/26933 [11:26<17:16, 16.70it/s] 36%|███▌      | 9628/26933 [11:27<17:15, 16.72it/s] 36%|███▌      | 9632/26933 [11:27<17:13, 16.74it/s] 36%|███▌      | 9636/26933 [11:27<17:10, 16.78it/s] 36%|███▌      | 9640/26933 [11:27<17:01, 16.93it/s] 36%|███▌      | 9644/26933 [11:28<16:49, 17.12it/s] 36%|███▌      | 9648/26933 [11:28<16:43, 17.23it/s] 36%|███▌      | 9652/26933 [11:28<16:37, 17.33it/s] 36%|███▌      | 9656/26933 [11:28<16:35, 17.35it/s] 36%|███▌      | 9660/26933 [11:28<16:32, 17.40it/s] 36%|███▌      | 9664/26933 [11:29<16:30, 17.44it/s] 36%|███▌      | 9668/26933 [11:29<16:32, 17.40it/s] 36%|███▌      | 9672/26933 [11:29<16:28, 17.47it/s] 36%|███▌      | 9676/26933 [11:29<16:29, 17.44it/s] 36%|███▌      | 9680/26933 [11:30<16:27, 17.47it/s] 36%|███▌      | 9684/26933 [11:30<16:26, 17.49it/s] 36%|███▌      | 9688/26933 [11:30<16:24, 17.51it/s] 36%|███▌      | 9692/26933 [11:30<16:26, 17.48it/s] 36%|███▌      | 9696/26933 [11:31<16:22, 17.54it/s] 36%|███▌      | 9700/26933 [11:31<16:22, 17.54it/s] 36%|███▌      | 9704/26933 [11:31<16:20, 17.57it/s] 36%|███▌      | 9708/26933 [11:31<16:20, 17.57it/s] 36%|███▌      | 9712/26933 [11:31<16:22, 17.53it/s] 36%|███▌      | 9716/26933 [11:32<16:23, 17.50it/s] 36%|███▌      | 9720/26933 [11:32<16:22, 17.51it/s] 36%|███▌      | 9724/26933 [11:32<16:22, 17.52it/s] 36%|███▌      | 9728/26933 [11:32<16:24, 17.48it/s] 36%|███▌      | 9732/26933 [11:33<16:22, 17.51it/s] 36%|███▌      | 9736/26933 [11:33<16:23, 17.48it/s] 36%|███▌      | 9740/26933 [11:33<16:23, 17.49it/s] 36%|███▌      | 9744/26933 [11:33<16:26, 17.42it/s] 36%|███▌      | 9748/26933 [11:33<16:24, 17.45it/s] 36%|███▌      | 9752/26933 [11:34<16:26, 17.41it/s] 36%|███▌      | 9756/26933 [11:34<16:33, 17.28it/s] 36%|███▌      | 9760/26933 [11:34<16:29, 17.36it/s] 36%|███▋      | 9764/26933 [11:34<16:30, 17.33it/s] 36%|███▋      | 9768/26933 [11:35<16:29, 17.35it/s] 36%|███▋      | 9772/26933 [11:35<16:27, 17.39it/s] 36%|███▋      | 9776/26933 [11:35<16:23, 17.44it/s] 36%|███▋      | 9780/26933 [11:35<16:24, 17.42it/s] 36%|███▋      | 9784/26933 [11:36<16:22, 17.46it/s] 36%|███▋      | 9788/26933 [11:36<16:19, 17.50it/s] 36%|███▋      | 9792/26933 [11:36<16:18, 17.52it/s] 36%|███▋      | 9796/26933 [11:36<16:20, 17.48it/s] 36%|███▋      | 9800/26933 [11:36<16:16, 17.54it/s] 36%|███▋      | 9804/26933 [11:37<16:17, 17.53it/s] 36%|███▋      | 9808/26933 [11:37<16:15, 17.55it/s] 36%|███▋      | 9812/26933 [11:37<16:16, 17.54it/s] 36%|███▋      | 9816/26933 [11:37<16:18, 17.49it/s] 36%|███▋      | 9820/26933 [11:38<16:17, 17.51it/s] 36%|███▋      | 9824/26933 [11:38<16:17, 17.50it/s] 36%|███▋      | 9828/26933 [11:38<16:18, 17.49it/s] 37%|███▋      | 9832/26933 [11:38<16:21, 17.43it/s] 37%|███▋      | 9836/26933 [11:39<16:17, 17.49it/s] 37%|███▋      | 9840/26933 [11:39<16:15, 17.52it/s] 37%|███▋      | 9844/26933 [11:39<16:12, 17.57it/s] 37%|███▋      | 9848/26933 [11:39<16:12, 17.56it/s] 37%|███▋      | 9852/26933 [11:39<16:15, 17.50it/s] 37%|███▋      | 9856/26933 [11:40<16:14, 17.53it/s] 37%|███▋      | 9860/26933 [11:40<16:14, 17.53it/s] 37%|███▋      | 9864/26933 [11:40<16:13, 17.53it/s] 37%|███▋      | 9868/26933 [11:40<16:15, 17.50it/s] 37%|███▋      | 9872/26933 [11:41<16:11, 17.55it/s] 37%|███▋      | 9876/26933 [11:41<16:10, 17.57it/s] 37%|███▋      | 9880/26933 [11:41<16:08, 17.60it/s] 37%|███▋      | 9884/26933 [11:41<16:12, 17.54it/s] 37%|███▋      | 9888/26933 [11:41<16:09, 17.59it/s] 37%|███▋      | 9892/26933 [11:42<16:07, 17.61it/s] 37%|███▋      | 9896/26933 [11:42<16:08, 17.60it/s] 37%|███▋      | 9900/26933 [11:42<16:06, 17.63it/s] 37%|███▋      | 9904/26933 [11:42<16:10, 17.55it/s] 37%|███▋      | 9908/26933 [11:43<16:10, 17.55it/s] 37%|███▋      | 9912/26933 [11:43<16:14, 17.46it/s] 37%|███▋      | 9916/26933 [11:43<16:13, 17.47it/s] 37%|███▋      | 9920/26933 [11:43<16:18, 17.39it/s] 37%|███▋      | 9924/26933 [11:44<16:19, 17.36it/s] 37%|███▋      | 9928/26933 [11:44<16:14, 17.46it/s] 37%|███▋      | 9932/26933 [11:44<16:14, 17.45it/s] 37%|███▋      | 9936/26933 [11:44<16:12, 17.48it/s] 37%|███▋      | 9940/26933 [11:44<16:10, 17.52it/s] 37%|███▋      | 9944/26933 [11:45<16:06, 17.58it/s] 37%|███▋      | 9948/26933 [11:45<16:06, 17.58it/s] 37%|███▋      | 9952/26933 [11:45<16:04, 17.61it/s] 37%|███▋      | 9956/26933 [11:45<16:04, 17.60it/s] 37%|███▋      | 9960/26933 [11:46<16:04, 17.60it/s] 37%|███▋      | 9964/26933 [11:46<16:05, 17.58it/s] 37%|███▋      | 9968/26933 [11:46<16:05, 17.57it/s] 37%|███▋      | 9972/26933 [11:46<16:08, 17.51it/s] 37%|███▋      | 9976/26933 [11:47<16:05, 17.57it/s] 37%|███▋      | 9980/26933 [11:47<16:03, 17.59it/s] 37%|███▋      | 9984/26933 [11:47<16:02, 17.61it/s] 37%|███▋      | 9988/26933 [11:47<16:02, 17.60it/s] 37%|███▋      | 9992/26933 [11:47<16:05, 17.54it/s] 37%|███▋      | 9996/26933 [11:48<16:04, 17.56it/s] 37%|███▋      | 10000/26933 [11:48<16:04, 17.56it/s] 37%|███▋      | 10004/26933 [11:48<16:03, 17.58it/s] 37%|███▋      | 10008/26933 [11:48<16:04, 17.54it/s] 37%|███▋      | 10012/26933 [11:49<16:05, 17.52it/s] 37%|███▋      | 10016/26933 [11:49<16:03, 17.56it/s] 37%|███▋      | 10020/26933 [11:49<16:03, 17.56it/s] 37%|███▋      | 10024/26933 [11:49<16:05, 17.51it/s] 37%|███▋      | 10028/26933 [11:49<16:05, 17.50it/s] 37%|███▋      | 10032/26933 [11:50<16:03, 17.54it/s] 37%|███▋      | 10036/26933 [11:50<16:01, 17.57it/s] 37%|███▋      | 10040/26933 [11:50<15:59, 17.60it/s] 37%|███▋      | 10044/26933 [11:50<16:01, 17.56it/s] 37%|███▋      | 10048/26933 [11:51<16:00, 17.58it/s] 37%|███▋      | 10052/26933 [11:51<16:00, 17.58it/s] 37%|███▋      | 10056/26933 [11:51<16:00, 17.57it/s] 37%|███▋      | 10060/26933 [11:51<16:01, 17.55it/s] 37%|███▋      | 10064/26933 [11:52<16:00, 17.57it/s] 37%|███▋      | 10068/26933 [11:52<15:58, 17.59it/s] 37%|███▋      | 10072/26933 [11:52<15:56, 17.62it/s] 37%|███▋      | 10076/26933 [11:52<15:56, 17.63it/s] 37%|███▋      | 10080/26933 [11:52<15:58, 17.58it/s] 37%|███▋      | 10084/26933 [11:53<15:58, 17.57it/s] 37%|███▋      | 10088/26933 [11:53<15:58, 17.57it/s] 37%|███▋      | 10092/26933 [11:53<15:59, 17.55it/s] 37%|███▋      | 10096/26933 [11:53<16:01, 17.51it/s] 38%|███▊      | 10100/26933 [11:54<16:00, 17.53it/s] 38%|███▊      | 10104/26933 [11:54<16:03, 17.47it/s] 38%|███▊      | 10108/26933 [11:54<16:03, 17.47it/s] 38%|███▊      | 10112/26933 [11:54<16:04, 17.44it/s] 38%|███▊      | 10116/26933 [11:54<16:02, 17.48it/s] 38%|███▊      | 10120/26933 [11:55<16:01, 17.49it/s] 38%|███▊      | 10124/26933 [11:55<15:59, 17.51it/s] 38%|███▊      | 10128/26933 [11:55<16:10, 17.32it/s] 38%|███▊      | 10132/26933 [11:55<16:33, 16.91it/s] 38%|███▊      | 10136/26933 [11:56<16:21, 17.11it/s] 38%|███▊      | 10140/26933 [11:56<16:13, 17.25it/s] 38%|███▊      | 10144/26933 [11:56<16:06, 17.37it/s] 38%|███▊      | 10148/26933 [11:56<16:07, 17.35it/s] 38%|███▊      | 10152/26933 [11:57<16:03, 17.42it/s] 38%|███▊      | 10156/26933 [11:57<16:02, 17.43it/s] 38%|███▊      | 10160/26933 [11:57<16:01, 17.44it/s] 38%|███▊      | 10164/26933 [11:57<16:02, 17.42it/s] 38%|███▊      | 10168/26933 [11:57<16:00, 17.46it/s] 38%|███▊      | 10172/26933 [11:58<16:01, 17.43it/s] 38%|███▊      | 10176/26933 [11:58<15:59, 17.46it/s] 38%|███▊      | 10180/26933 [11:58<15:59, 17.46it/s] 38%|███▊      | 10184/26933 [11:58<16:04, 17.37it/s] 38%|███▊      | 10188/26933 [11:59<16:02, 17.40it/s] 38%|███▊      | 10192/26933 [11:59<15:58, 17.46it/s] 38%|███▊      | 10196/26933 [11:59<15:55, 17.52it/s] 38%|███▊      | 10200/26933 [11:59<15:58, 17.46it/s] 38%|███▊      | 10204/26933 [12:00<15:56, 17.50it/s] 38%|███▊      | 10208/26933 [12:00<15:55, 17.50it/s] 38%|███▊      | 10212/26933 [12:00<15:54, 17.52it/s] 38%|███▊      | 10216/26933 [12:00<15:58, 17.44it/s] 38%|███▊      | 10220/26933 [12:00<15:58, 17.44it/s] 38%|███▊      | 10224/26933 [12:01<15:57, 17.46it/s] 38%|███▊      | 10228/26933 [12:01<15:57, 17.45it/s] 38%|███▊      | 10232/26933 [12:01<15:55, 17.47it/s] 38%|███▊      | 10236/26933 [12:01<15:58, 17.42it/s] 38%|███▊      | 10240/26933 [12:02<15:55, 17.46it/s] 38%|███▊      | 10244/26933 [12:02<15:56, 17.45it/s] 38%|███▊      | 10248/26933 [12:02<15:55, 17.47it/s] 38%|███▊      | 10252/26933 [12:02<15:55, 17.45it/s] 38%|███▊      | 10256/26933 [12:03<15:53, 17.48it/s] 38%|███▊      | 10260/26933 [12:03<15:53, 17.49it/s] 38%|███▊      | 10264/26933 [12:03<15:51, 17.52it/s] 38%|███▊      | 10268/26933 [12:03<15:51, 17.52it/s] 38%|███▊      | 10272/26933 [12:03<15:54, 17.46it/s] 38%|███▊      | 10276/26933 [12:04<15:53, 17.47it/s] 38%|███▊      | 10280/26933 [12:04<15:53, 17.47it/s] 38%|███▊      | 10284/26933 [12:04<15:52, 17.49it/s] 38%|███▊      | 10288/26933 [12:04<15:57, 17.38it/s] 38%|███▊      | 10292/26933 [12:05<15:56, 17.39it/s] 38%|███▊      | 10296/26933 [12:05<15:53, 17.45it/s] 38%|███▊      | 10300/26933 [12:05<15:50, 17.51it/s] 38%|███▊      | 10304/26933 [12:05<15:56, 17.39it/s] 38%|███▊      | 10308/26933 [12:06<15:55, 17.41it/s] 38%|███▊      | 10312/26933 [12:06<15:53, 17.43it/s] 38%|███▊      | 10316/26933 [12:06<15:51, 17.46it/s] 38%|███▊      | 10320/26933 [12:06<15:49, 17.50it/s] 38%|███▊      | 10324/26933 [12:06<15:54, 17.39it/s] 38%|███▊      | 10328/26933 [12:07<15:54, 17.40it/s] 38%|███▊      | 10332/26933 [12:07<15:57, 17.34it/s] 38%|███▊      | 10336/26933 [12:07<15:53, 17.40it/s] 38%|███▊      | 10340/26933 [12:07<15:57, 17.34it/s] 38%|███▊      | 10344/26933 [12:08<15:53, 17.41it/s] 38%|███▊      | 10348/26933 [12:08<15:50, 17.45it/s] 38%|███▊      | 10352/26933 [12:08<15:48, 17.48it/s] 38%|███▊      | 10356/26933 [12:08<15:54, 17.37it/s] 38%|███▊      | 10360/26933 [12:08<15:50, 17.44it/s] 38%|███▊      | 10364/26933 [12:09<15:50, 17.43it/s] 38%|███▊      | 10368/26933 [12:09<15:50, 17.43it/s] 39%|███▊      | 10372/26933 [12:09<15:46, 17.50it/s] 39%|███▊      | 10376/26933 [12:09<15:52, 17.38it/s] 39%|███▊      | 10380/26933 [12:10<15:50, 17.41it/s] 39%|███▊      | 10384/26933 [12:10<15:49, 17.43it/s] 39%|███▊      | 10388/26933 [12:10<15:47, 17.47it/s] 39%|███▊      | 10392/26933 [12:10<15:52, 17.36it/s] 39%|███▊      | 10396/26933 [12:11<15:48, 17.43it/s] 39%|███▊      | 10400/26933 [12:11<15:45, 17.48it/s] 39%|███▊      | 10404/26933 [12:11<15:42, 17.54it/s] 39%|███▊      | 10408/26933 [12:11<15:46, 17.46it/s] 39%|███▊      | 10412/26933 [12:11<15:43, 17.50it/s] 39%|███▊      | 10416/26933 [12:12<15:44, 17.49it/s] 39%|███▊      | 10420/26933 [12:12<15:43, 17.51it/s] 39%|███▊      | 10424/26933 [12:12<15:42, 17.52it/s] 39%|███▊      | 10428/26933 [12:12<15:49, 17.39it/s] 39%|███▊      | 10432/26933 [12:13<15:46, 17.43it/s] 39%|███▊      | 10436/26933 [12:13<15:43, 17.48it/s] 39%|███▉      | 10440/26933 [12:13<15:40, 17.54it/s] 39%|███▉      | 10444/26933 [12:13<15:45, 17.44it/s] 39%|███▉      | 10448/26933 [12:14<15:41, 17.51it/s] 39%|███▉      | 10452/26933 [12:14<15:40, 17.53it/s] 39%|███▉      | 10456/26933 [12:14<15:38, 17.55it/s] 39%|███▉      | 10460/26933 [12:14<15:37, 17.57it/s] 39%|███▉      | 10464/26933 [12:14<15:37, 17.56it/s] 39%|███▉      | 10468/26933 [12:15<15:39, 17.53it/s] 39%|███▉      | 10472/26933 [12:15<15:40, 17.50it/s] 39%|███▉      | 10476/26933 [12:15<15:41, 17.48it/s] 39%|███▉      | 10480/26933 [12:15<15:45, 17.40it/s] 39%|███▉      | 10484/26933 [12:16<15:43, 17.42it/s] 39%|███▉      | 10488/26933 [12:16<15:42, 17.45it/s] 39%|███▉      | 10492/26933 [12:16<15:40, 17.48it/s] 39%|███▉      | 10496/26933 [12:16<15:45, 17.38it/s] 39%|███▉      | 10500/26933 [12:17<15:44, 17.41it/s] 39%|███▉      | 10504/26933 [12:17<15:41, 17.45it/s] 39%|███▉      | 10508/26933 [12:17<15:41, 17.45it/s] 39%|███▉      | 10512/26933 [12:17<15:40, 17.46it/s] 39%|███▉      | 10516/26933 [12:17<15:44, 17.38it/s] 39%|███▉      | 10520/26933 [12:18<15:41, 17.44it/s] 39%|███▉      | 10524/26933 [12:18<15:39, 17.47it/s] 39%|███▉      | 10528/26933 [12:18<15:37, 17.50it/s] 39%|███▉      | 10532/26933 [12:18<15:42, 17.40it/s] 39%|███▉      | 10536/26933 [12:19<15:39, 17.46it/s] 39%|███▉      | 10540/26933 [12:19<15:38, 17.47it/s] 39%|███▉      | 10544/26933 [12:19<15:37, 17.49it/s] 39%|███▉      | 10548/26933 [12:19<15:41, 17.41it/s] 39%|███▉      | 10552/26933 [12:19<15:38, 17.46it/s] 39%|███▉      | 10556/26933 [12:20<15:37, 17.48it/s] 39%|███▉      | 10560/26933 [12:20<15:35, 17.50it/s] 39%|███▉      | 10564/26933 [12:20<15:45, 17.31it/s] 39%|███▉      | 10568/26933 [12:20<15:47, 17.26it/s] 39%|███▉      | 10572/26933 [12:21<15:43, 17.34it/s] 39%|███▉      | 10576/26933 [12:21<15:40, 17.38it/s] 39%|███▉      | 10580/26933 [12:21<15:43, 17.34it/s] 39%|███▉      | 10584/26933 [12:21<15:47, 17.26it/s] 39%|███▉      | 10588/26933 [12:22<15:44, 17.30it/s] 39%|███▉      | 10592/26933 [12:22<15:42, 17.33it/s] 39%|███▉      | 10596/26933 [12:22<15:40, 17.37it/s] 39%|███▉      | 10600/26933 [12:22<15:43, 17.30it/s] 39%|███▉      | 10604/26933 [12:22<15:41, 17.34it/s] 39%|███▉      | 10608/26933 [12:23<15:37, 17.41it/s] 39%|███▉      | 10612/26933 [12:23<15:34, 17.46it/s] 39%|███▉      | 10616/26933 [12:23<15:33, 17.48it/s] 39%|███▉      | 10620/26933 [12:23<15:38, 17.38it/s] 39%|███▉      | 10624/26933 [12:24<15:37, 17.40it/s] 39%|███▉      | 10628/26933 [12:24<15:34, 17.44it/s] 39%|███▉      | 10632/26933 [12:24<15:34, 17.45it/s] 39%|███▉      | 10636/26933 [12:24<15:40, 17.33it/s] 40%|███▉      | 10640/26933 [12:25<15:36, 17.39it/s] 40%|███▉      | 10644/26933 [12:25<15:34, 17.44it/s] 40%|███▉      | 10648/26933 [12:25<15:31, 17.48it/s] 40%|███▉      | 10652/26933 [12:25<15:36, 17.39it/s] 40%|███▉      | 10656/26933 [12:25<15:34, 17.42it/s] 40%|███▉      | 10660/26933 [12:26<15:31, 17.46it/s] 40%|███▉      | 10664/26933 [12:26<15:30, 17.49it/s] 40%|███▉      | 10668/26933 [12:26<15:28, 17.52it/s] 40%|███▉      | 10672/26933 [12:26<15:32, 17.43it/s] 40%|███▉      | 10676/26933 [12:27<15:31, 17.46it/s] 40%|███▉      | 10680/26933 [12:27<15:31, 17.45it/s] 40%|███▉      | 10684/26933 [12:27<15:27, 17.52it/s] 40%|███▉      | 10688/26933 [12:27<15:30, 17.45it/s] 40%|███▉      | 10692/26933 [12:28<15:30, 17.46it/s] 40%|███▉      | 10696/26933 [12:28<15:28, 17.50it/s] 40%|███▉      | 10700/26933 [12:28<15:27, 17.50it/s] 40%|███▉      | 10704/26933 [12:28<15:31, 17.42it/s] 40%|███▉      | 10708/26933 [12:28<15:28, 17.48it/s] 40%|███▉      | 10712/26933 [12:29<15:26, 17.51it/s] 40%|███▉      | 10716/26933 [12:29<15:25, 17.53it/s] 40%|███▉      | 10720/26933 [12:29<15:24, 17.53it/s] 40%|███▉      | 10724/26933 [12:29<15:28, 17.45it/s] 40%|███▉      | 10728/26933 [12:30<15:28, 17.46it/s] 40%|███▉      | 10732/26933 [12:30<15:27, 17.47it/s] 40%|███▉      | 10736/26933 [12:30<15:26, 17.49it/s] 40%|███▉      | 10740/26933 [12:30<15:31, 17.39it/s] 40%|███▉      | 10744/26933 [12:31<15:29, 17.41it/s] 40%|███▉      | 10748/26933 [12:31<15:28, 17.42it/s] 40%|███▉      | 10752/26933 [12:31<15:27, 17.44it/s] 40%|███▉      | 10756/26933 [12:31<15:26, 17.46it/s] 40%|███▉      | 10760/26933 [12:31<15:31, 17.37it/s] 40%|███▉      | 10764/26933 [12:32<15:27, 17.43it/s] 40%|███▉      | 10768/26933 [12:32<15:26, 17.45it/s] 40%|███▉      | 10772/26933 [12:32<15:24, 17.47it/s] 40%|████      | 10776/26933 [12:32<15:28, 17.40it/s] 40%|████      | 10780/26933 [12:33<15:25, 17.45it/s] 40%|████      | 10784/26933 [12:33<15:25, 17.45it/s] 40%|████      | 10788/26933 [12:33<15:32, 17.32it/s] 40%|████      | 10792/26933 [12:33<15:31, 17.32it/s] 40%|████      | 10796/26933 [12:33<15:29, 17.37it/s] 40%|████      | 10800/26933 [12:34<15:25, 17.43it/s] 40%|████      | 10804/26933 [12:34<15:23, 17.46it/s] 40%|████      | 10808/26933 [12:34<15:22, 17.48it/s] 40%|████      | 10812/26933 [12:34<15:26, 17.40it/s] 40%|████      | 10816/26933 [12:35<15:23, 17.45it/s] 40%|████      | 10820/26933 [12:35<15:21, 17.48it/s] 40%|████      | 10824/26933 [12:35<15:22, 17.46it/s] 40%|████      | 10828/26933 [12:35<15:26, 17.38it/s] 40%|████      | 10832/26933 [12:36<15:24, 17.41it/s] 40%|████      | 10836/26933 [12:36<15:23, 17.42it/s] 40%|████      | 10840/26933 [12:36<15:22, 17.44it/s] 40%|████      | 10844/26933 [12:36<15:25, 17.39it/s] 40%|████      | 10848/26933 [12:36<15:21, 17.45it/s] 40%|████      | 10852/26933 [12:37<15:19, 17.49it/s] 40%|████      | 10856/26933 [12:37<15:18, 17.51it/s] 40%|████      | 10860/26933 [12:37<15:16, 17.53it/s] 40%|████      | 10864/26933 [12:37<15:20, 17.47it/s] 40%|████      | 10868/26933 [12:38<15:17, 17.51it/s] 40%|████      | 10872/26933 [12:38<15:18, 17.49it/s] 40%|████      | 10876/26933 [12:38<15:16, 17.51it/s] 40%|████      | 10880/26933 [12:38<15:18, 17.47it/s] 40%|████      | 10884/26933 [12:39<15:19, 17.44it/s] 40%|████      | 10888/26933 [12:39<15:20, 17.43it/s] 40%|████      | 10892/26933 [12:39<15:20, 17.43it/s] 40%|████      | 10896/26933 [12:39<15:22, 17.39it/s] 40%|████      | 10900/26933 [12:39<15:19, 17.43it/s] 40%|████      | 10904/26933 [12:40<15:17, 17.46it/s] 41%|████      | 10908/26933 [12:40<15:16, 17.48it/s] 41%|████      | 10912/26933 [12:40<15:14, 17.53it/s] 41%|████      | 10916/26933 [12:40<15:16, 17.47it/s] 41%|████      | 10920/26933 [12:41<15:16, 17.47it/s] 41%|████      | 10924/26933 [12:41<15:13, 17.52it/s] 41%|████      | 10928/26933 [12:41<15:13, 17.52it/s] 41%|████      | 10932/26933 [12:41<15:16, 17.46it/s] 41%|████      | 10936/26933 [12:42<15:15, 17.48it/s] 41%|████      | 10940/26933 [12:42<15:14, 17.50it/s] 41%|████      | 10944/26933 [12:42<15:14, 17.49it/s] 41%|████      | 10948/26933 [12:42<15:12, 17.52it/s] 41%|████      | 10952/26933 [12:42<15:15, 17.45it/s] 41%|████      | 10956/26933 [12:43<15:15, 17.46it/s] 41%|████      | 10960/26933 [12:43<15:15, 17.45it/s] 41%|████      | 10964/26933 [12:43<15:15, 17.45it/s] 41%|████      | 10968/26933 [12:43<15:17, 17.40it/s] 41%|████      | 10972/26933 [12:44<15:16, 17.42it/s] 41%|████      | 10976/26933 [12:44<15:14, 17.45it/s] 41%|████      | 10980/26933 [12:44<15:11, 17.50it/s] 41%|████      | 10984/26933 [12:44<15:13, 17.46it/s] 41%|████      | 10988/26933 [12:44<15:12, 17.47it/s] 41%|████      | 10992/26933 [12:45<15:11, 17.49it/s] 41%|████      | 10996/26933 [12:45<15:09, 17.53it/s] 41%|████      | 11000/26933 [12:45<15:07, 17.56it/s] 41%|████      | 11004/26933 [12:45<15:12, 17.45it/s] 41%|████      | 11008/26933 [12:46<15:11, 17.48it/s] 41%|████      | 11012/26933 [12:46<15:09, 17.51it/s] 41%|████      | 11016/26933 [12:46<15:09, 17.50it/s] 41%|████      | 11020/26933 [12:46<15:06, 17.56it/s] 41%|████      | 11024/26933 [12:47<15:00, 17.66it/s] 41%|████      | 11028/26933 [12:47<14:55, 17.76it/s] 41%|████      | 11032/26933 [12:47<14:55, 17.75it/s] 41%|████      | 11036/26933 [12:47<14:53, 17.79it/s] 41%|████      | 11040/26933 [12:47<14:53, 17.79it/s] 41%|████      | 11044/26933 [12:48<14:50, 17.84it/s] 41%|████      | 11048/26933 [12:48<14:49, 17.86it/s] 41%|████      | 11052/26933 [12:48<14:48, 17.87it/s] 41%|████      | 11056/26933 [12:48<14:49, 17.85it/s] 41%|████      | 11060/26933 [12:49<14:48, 17.87it/s] 41%|████      | 11064/26933 [12:49<14:44, 17.93it/s] 41%|████      | 11068/26933 [12:49<14:43, 17.95it/s] 41%|████      | 11072/26933 [12:49<14:45, 17.91it/s] 41%|████      | 11076/26933 [12:49<14:43, 17.95it/s] 41%|████      | 11080/26933 [12:50<14:42, 17.97it/s] 41%|████      | 11084/26933 [12:50<14:41, 17.98it/s] 41%|████      | 11088/26933 [12:50<14:42, 17.96it/s] 41%|████      | 11092/26933 [12:50<14:43, 17.93it/s] 41%|████      | 11096/26933 [12:51<14:46, 17.87it/s] 41%|████      | 11100/26933 [12:51<14:48, 17.82it/s] 41%|████      | 11104/26933 [12:51<14:49, 17.80it/s] 41%|████      | 11108/26933 [12:51<14:53, 17.70it/s] 41%|████▏     | 11112/26933 [12:51<14:53, 17.71it/s] 41%|████▏     | 11116/26933 [12:52<14:55, 17.67it/s] 41%|████▏     | 11120/26933 [12:52<14:53, 17.69it/s] 41%|████▏     | 11124/26933 [12:52<14:54, 17.67it/s] 41%|████▏     | 11128/26933 [12:52<14:58, 17.58it/s] 41%|████▏     | 11132/26933 [12:53<14:58, 17.59it/s] 41%|████▏     | 11136/26933 [12:53<14:58, 17.59it/s] 41%|████▏     | 11140/26933 [12:53<14:57, 17.59it/s] 41%|████▏     | 11144/26933 [12:53<14:58, 17.57it/s] 41%|████▏     | 11148/26933 [12:54<14:54, 17.64it/s] 41%|████▏     | 11152/26933 [12:54<14:52, 17.68it/s] 41%|████▏     | 11156/26933 [12:54<14:53, 17.66it/s] 41%|████▏     | 11160/26933 [12:54<14:52, 17.68it/s] 41%|████▏     | 11164/26933 [12:54<14:55, 17.62it/s] 41%|████▏     | 11168/26933 [12:55<14:53, 17.65it/s] 41%|████▏     | 11172/26933 [12:55<14:53, 17.64it/s] 41%|████▏     | 11176/26933 [12:55<14:52, 17.65it/s] 42%|████▏     | 11180/26933 [12:55<14:53, 17.63it/s] 42%|████▏     | 11184/26933 [12:56<14:51, 17.66it/s] 42%|████▏     | 11188/26933 [12:56<14:50, 17.68it/s] 42%|████▏     | 11192/26933 [12:56<14:51, 17.66it/s] 42%|████▏     | 11196/26933 [12:56<14:54, 17.59it/s] 42%|████▏     | 11200/26933 [12:56<14:53, 17.61it/s] 42%|████▏     | 11204/26933 [12:57<14:53, 17.60it/s] 42%|████▏     | 11208/26933 [12:57<14:53, 17.60it/s] 42%|████▏     | 11212/26933 [12:57<14:51, 17.64it/s] 42%|████▏     | 11216/26933 [12:57<14:53, 17.59it/s] 42%|████▏     | 11220/26933 [12:58<14:52, 17.60it/s] 42%|████▏     | 11224/26933 [12:58<14:52, 17.61it/s] 42%|████▏     | 11228/26933 [12:58<14:50, 17.63it/s] 42%|████▏     | 11232/26933 [12:58<14:52, 17.59it/s] 42%|████▏     | 11236/26933 [12:59<14:51, 17.61it/s] 42%|████▏     | 11240/26933 [12:59<14:49, 17.63it/s] 42%|████▏     | 11244/26933 [12:59<14:47, 17.67it/s] 42%|████▏     | 11248/26933 [12:59<14:50, 17.62it/s] 42%|████▏     | 11252/26933 [12:59<14:48, 17.65it/s] 42%|████▏     | 11256/26933 [13:00<14:46, 17.68it/s] 42%|████▏     | 11260/26933 [13:00<14:44, 17.71it/s] 42%|████▏     | 11264/26933 [13:00<14:45, 17.70it/s] 42%|████▏     | 11268/26933 [13:00<14:48, 17.63it/s] 42%|████▏     | 11272/26933 [13:01<14:48, 17.62it/s] 42%|████▏     | 11276/26933 [13:01<14:47, 17.65it/s] 42%|████▏     | 11280/26933 [13:01<14:44, 17.69it/s] 42%|████▏     | 11284/26933 [13:01<14:48, 17.61it/s] 42%|████▏     | 11288/26933 [13:01<14:49, 17.60it/s] 42%|████▏     | 11292/26933 [13:02<14:53, 17.50it/s] 42%|████▏     | 11296/26933 [13:02<14:56, 17.44it/s] 42%|████▏     | 11300/26933 [13:02<14:55, 17.47it/s] 42%|████▏     | 11304/26933 [13:02<14:56, 17.44it/s] 42%|████▏     | 11308/26933 [13:03<14:51, 17.52it/s] 42%|████▏     | 11312/26933 [13:03<14:52, 17.51it/s] 42%|████▏     | 11316/26933 [13:03<14:50, 17.54it/s] 42%|████▏     | 11320/26933 [13:03<14:50, 17.53it/s] 42%|████▏     | 11324/26933 [13:04<14:47, 17.58it/s] 42%|████▏     | 11328/26933 [13:04<14:48, 17.56it/s] 42%|████▏     | 11332/26933 [13:04<14:47, 17.59it/s] 42%|████▏     | 11336/26933 [13:04<14:45, 17.62it/s] 42%|████▏     | 11340/26933 [13:04<14:47, 17.57it/s] 42%|████▏     | 11344/26933 [13:05<14:47, 17.57it/s] 42%|████▏     | 11348/26933 [13:05<14:46, 17.57it/s] 42%|████▏     | 11352/26933 [13:05<14:48, 17.54it/s] 42%|████▏     | 11356/26933 [13:05<14:50, 17.50it/s] 42%|████▏     | 11360/26933 [13:06<14:49, 17.51it/s] 42%|████▏     | 11364/26933 [13:06<14:49, 17.50it/s] 42%|████▏     | 11368/26933 [13:06<14:54, 17.40it/s] 42%|████▏     | 11372/26933 [13:06<15:14, 17.01it/s] 42%|████▏     | 11376/26933 [13:07<15:11, 17.07it/s] 42%|████▏     | 11380/26933 [13:07<15:23, 16.84it/s] 42%|████▏     | 11384/26933 [13:07<15:20, 16.89it/s] 42%|████▏     | 11388/26933 [13:07<15:12, 17.04it/s] 42%|████▏     | 11392/26933 [13:07<15:25, 16.80it/s] 42%|████▏     | 11396/26933 [13:08<15:16, 16.96it/s] 42%|████▏     | 11400/26933 [13:08<15:09, 17.08it/s] 42%|████▏     | 11404/26933 [13:08<15:02, 17.21it/s] 42%|████▏     | 11408/26933 [13:08<15:00, 17.25it/s] 42%|████▏     | 11412/26933 [13:09<14:57, 17.29it/s] 42%|████▏     | 11416/26933 [13:09<14:53, 17.37it/s] 42%|████▏     | 11420/26933 [13:09<14:50, 17.43it/s] 42%|████▏     | 11424/26933 [13:09<14:49, 17.44it/s] 42%|████▏     | 11428/26933 [13:10<14:47, 17.48it/s] 42%|████▏     | 11432/26933 [13:10<14:45, 17.50it/s] 42%|████▏     | 11436/26933 [13:10<14:46, 17.49it/s] 42%|████▏     | 11440/26933 [13:10<14:45, 17.49it/s] 42%|████▏     | 11444/26933 [13:10<14:46, 17.47it/s] 43%|████▎     | 11448/26933 [13:11<14:44, 17.50it/s] 43%|████▎     | 11452/26933 [13:11<14:44, 17.50it/s] 43%|████▎     | 11456/26933 [13:11<14:42, 17.54it/s] 43%|████▎     | 11460/26933 [13:11<14:44, 17.49it/s] 43%|████▎     | 11464/26933 [13:12<14:41, 17.54it/s] 43%|████▎     | 11468/26933 [13:12<14:41, 17.54it/s] 43%|████▎     | 11472/26933 [13:12<14:40, 17.56it/s] 43%|████▎     | 11476/26933 [13:12<14:43, 17.49it/s] 43%|████▎     | 11480/26933 [13:12<14:42, 17.52it/s] 43%|████▎     | 11484/26933 [13:13<14:41, 17.52it/s] 43%|████▎     | 11488/26933 [13:13<14:41, 17.52it/s] 43%|████▎     | 11492/26933 [13:13<14:40, 17.54it/s] 43%|████▎     | 11496/26933 [13:13<14:42, 17.49it/s] 43%|████▎     | 11500/26933 [13:14<14:42, 17.48it/s] 43%|████▎     | 11504/26933 [13:14<14:39, 17.53it/s] 43%|████▎     | 11508/26933 [13:14<14:38, 17.57it/s] 43%|████▎     | 11512/26933 [13:14<14:39, 17.53it/s] 43%|████▎     | 11516/26933 [13:15<14:37, 17.56it/s] 43%|████▎     | 11520/26933 [13:15<14:39, 17.53it/s] 43%|████▎     | 11524/26933 [13:15<14:38, 17.55it/s] 43%|████▎     | 11528/26933 [13:15<14:38, 17.53it/s] 43%|████▎     | 11532/26933 [13:15<14:39, 17.51it/s] 43%|████▎     | 11536/26933 [13:16<14:39, 17.51it/s] 43%|████▎     | 11540/26933 [13:16<14:50, 17.28it/s] 43%|████▎     | 11544/26933 [13:16<14:46, 17.36it/s] 43%|████▎     | 11548/26933 [13:16<14:53, 17.22it/s] 43%|████▎     | 11552/26933 [13:17<14:50, 17.27it/s] 43%|████▎     | 11556/26933 [13:17<14:46, 17.36it/s] 43%|████▎     | 11560/26933 [13:17<14:42, 17.42it/s] 43%|████▎     | 11564/26933 [13:17<14:41, 17.43it/s] 43%|████▎     | 11568/26933 [13:18<14:39, 17.47it/s] 43%|████▎     | 11572/26933 [13:18<14:37, 17.51it/s] 43%|████▎     | 11576/26933 [13:18<14:36, 17.52it/s] 43%|████▎     | 11580/26933 [13:18<14:36, 17.53it/s] 43%|████▎     | 11584/26933 [13:18<14:37, 17.48it/s] 43%|████▎     | 11588/26933 [13:19<14:35, 17.52it/s] 43%|████▎     | 11592/26933 [13:19<14:34, 17.54it/s] 43%|████▎     | 11596/26933 [13:19<14:32, 17.58it/s] 43%|████▎     | 11600/26933 [13:19<14:31, 17.59it/s] 43%|████▎     | 11604/26933 [13:20<14:30, 17.62it/s] 43%|████▎     | 11608/26933 [13:20<14:29, 17.62it/s] 43%|████▎     | 11612/26933 [13:20<14:27, 17.65it/s] 43%|████▎     | 11616/26933 [13:20<14:29, 17.61it/s] 43%|████▎     | 11620/26933 [13:20<14:28, 17.62it/s] 43%|████▎     | 11624/26933 [13:21<14:28, 17.62it/s] 43%|████▎     | 11628/26933 [13:21<14:28, 17.63it/s] 43%|████▎     | 11632/26933 [13:21<14:27, 17.64it/s] 43%|████▎     | 11636/26933 [13:21<14:28, 17.62it/s] 43%|████▎     | 11640/26933 [13:22<14:27, 17.62it/s] 43%|████▎     | 11644/26933 [13:22<14:27, 17.63it/s] 43%|████▎     | 11648/26933 [13:22<14:27, 17.62it/s] 43%|████▎     | 11652/26933 [13:22<14:29, 17.58it/s] 43%|████▎     | 11656/26933 [13:23<14:28, 17.59it/s] 43%|████▎     | 11660/26933 [13:23<14:26, 17.63it/s] 43%|████▎     | 11664/26933 [13:23<14:25, 17.64it/s] 43%|████▎     | 11668/26933 [13:23<14:24, 17.66it/s] 43%|████▎     | 11672/26933 [13:23<14:25, 17.63it/s] 43%|████▎     | 11676/26933 [13:24<14:24, 17.65it/s] 43%|████▎     | 11680/26933 [13:24<14:28, 17.56it/s] 43%|████▎     | 11684/26933 [13:24<14:29, 17.53it/s] 43%|████▎     | 11688/26933 [13:24<14:34, 17.44it/s] 43%|████▎     | 11692/26933 [13:25<14:33, 17.45it/s] 43%|████▎     | 11696/26933 [13:25<14:32, 17.47it/s] 43%|████▎     | 11700/26933 [13:25<14:35, 17.40it/s] 43%|████▎     | 11704/26933 [13:25<14:37, 17.36it/s] 43%|████▎     | 11708/26933 [13:26<14:33, 17.43it/s] 43%|████▎     | 11712/26933 [13:26<14:31, 17.47it/s] 44%|████▎     | 11716/26933 [13:26<14:29, 17.50it/s] 44%|████▎     | 11720/26933 [13:26<14:30, 17.47it/s] 44%|████▎     | 11724/26933 [13:26<14:31, 17.45it/s] 44%|████▎     | 11728/26933 [13:27<14:30, 17.48it/s] 44%|████▎     | 11732/26933 [13:27<14:30, 17.46it/s] 44%|████▎     | 11736/26933 [13:27<14:31, 17.44it/s] 44%|████▎     | 11740/26933 [13:27<14:39, 17.27it/s] 44%|████▎     | 11744/26933 [13:28<14:32, 17.40it/s] 44%|████▎     | 11748/26933 [13:28<14:29, 17.46it/s] 44%|████▎     | 11752/26933 [13:28<14:27, 17.50it/s] 44%|████▎     | 11756/26933 [13:28<14:27, 17.49it/s] 44%|████▎     | 11760/26933 [13:28<14:25, 17.53it/s] 44%|████▎     | 11764/26933 [13:29<14:23, 17.56it/s] 44%|████▎     | 11768/26933 [13:29<14:23, 17.56it/s] 44%|████▎     | 11772/26933 [13:29<14:22, 17.58it/s] 44%|████▎     | 11776/26933 [13:29<14:24, 17.53it/s] 44%|████▎     | 11780/26933 [13:30<14:28, 17.45it/s] 44%|████▍     | 11784/26933 [13:30<14:27, 17.47it/s] 44%|████▍     | 11788/26933 [13:30<14:26, 17.48it/s] 44%|████▍     | 11792/26933 [13:30<14:31, 17.38it/s] 44%|████▍     | 11796/26933 [13:31<14:28, 17.43it/s] 44%|████▍     | 11800/26933 [13:31<14:26, 17.47it/s] 44%|████▍     | 11804/26933 [13:31<14:24, 17.50it/s] 44%|████▍     | 11808/26933 [13:31<14:26, 17.45it/s] 44%|████▍     | 11812/26933 [13:31<14:24, 17.50it/s] 44%|████▍     | 11816/26933 [13:32<14:24, 17.49it/s] 44%|████▍     | 11820/26933 [13:32<14:26, 17.44it/s] 44%|████▍     | 11824/26933 [13:32<14:28, 17.40it/s] 44%|████▍     | 11828/26933 [13:32<14:28, 17.39it/s] 44%|████▍     | 11832/26933 [13:33<14:25, 17.44it/s] 44%|████▍     | 11836/26933 [13:33<14:22, 17.51it/s] 44%|████▍     | 11840/26933 [13:33<14:20, 17.53it/s] 44%|████▍     | 11844/26933 [13:33<14:23, 17.47it/s] 44%|████▍     | 11848/26933 [13:34<14:21, 17.51it/s] 44%|████▍     | 11852/26933 [13:34<14:20, 17.52it/s] 44%|████▍     | 11856/26933 [13:34<14:21, 17.51it/s] 44%|████▍     | 11860/26933 [13:34<14:19, 17.53it/s] 44%|████▍     | 11864/26933 [13:34<14:21, 17.50it/s] 44%|████▍     | 11868/26933 [13:35<14:19, 17.53it/s] 44%|████▍     | 11872/26933 [13:35<14:18, 17.53it/s] 44%|████▍     | 11876/26933 [13:35<14:17, 17.57it/s] 44%|████▍     | 11880/26933 [13:35<14:17, 17.56it/s] 44%|████▍     | 11884/26933 [13:36<14:17, 17.55it/s] 44%|████▍     | 11888/26933 [13:36<14:16, 17.57it/s] 44%|████▍     | 11892/26933 [13:36<14:15, 17.59it/s] 44%|████▍     | 11896/26933 [13:36<14:16, 17.56it/s] 44%|████▍     | 11900/26933 [13:36<14:15, 17.58it/s] 44%|████▍     | 11904/26933 [13:37<14:15, 17.57it/s] 44%|████▍     | 11908/26933 [13:37<14:15, 17.57it/s] 44%|████▍     | 11912/26933 [13:37<14:15, 17.56it/s] 44%|████▍     | 11916/26933 [13:37<14:18, 17.50it/s] 44%|████▍     | 11920/26933 [13:38<14:17, 17.50it/s] 44%|████▍     | 11924/26933 [13:38<14:17, 17.50it/s] 44%|████▍     | 11928/26933 [13:38<14:16, 17.51it/s] 44%|████▍     | 11932/26933 [13:38<14:18, 17.47it/s] 44%|████▍     | 11936/26933 [13:39<14:18, 17.46it/s] 44%|████▍     | 11940/26933 [13:39<14:17, 17.48it/s] 44%|████▍     | 11944/26933 [13:39<14:16, 17.50it/s] 44%|████▍     | 11948/26933 [13:39<14:15, 17.51it/s] 44%|████▍     | 11952/26933 [13:39<14:16, 17.50it/s] 44%|████▍     | 11956/26933 [13:40<14:15, 17.51it/s] 44%|████▍     | 11960/26933 [13:40<14:13, 17.55it/s] 44%|████▍     | 11964/26933 [13:40<14:11, 17.57it/s] 44%|████▍     | 11968/26933 [13:40<14:15, 17.49it/s] 44%|████▍     | 11972/26933 [13:41<14:19, 17.40it/s] 44%|████▍     | 11976/26933 [13:41<14:24, 17.29it/s] 44%|████▍     | 11980/26933 [13:41<14:28, 17.23it/s] 44%|████▍     | 11984/26933 [13:41<14:30, 17.17it/s] 45%|████▍     | 11988/26933 [13:42<14:30, 17.17it/s] 45%|████▍     | 11992/26933 [13:42<14:26, 17.23it/s] 45%|████▍     | 11996/26933 [13:42<14:21, 17.33it/s] 45%|████▍     | 12000/26933 [13:42<14:22, 17.32it/s] 45%|████▍     | 12004/26933 [13:42<14:22, 17.31it/s] 45%|████▍     | 12008/26933 [13:43<14:17, 17.40it/s] 45%|████▍     | 12012/26933 [13:43<14:16, 17.43it/s] 45%|████▍     | 12016/26933 [13:43<14:13, 17.47it/s] 45%|████▍     | 12020/26933 [13:43<14:13, 17.47it/s] 45%|████▍     | 12024/26933 [13:44<14:11, 17.50it/s] 45%|████▍     | 12028/26933 [13:44<14:10, 17.52it/s] 45%|████▍     | 12032/26933 [13:44<14:08, 17.57it/s] 45%|████▍     | 12036/26933 [13:44<14:09, 17.54it/s] 45%|████▍     | 12040/26933 [13:44<14:09, 17.54it/s] 45%|████▍     | 12044/26933 [13:45<14:07, 17.57it/s] 45%|████▍     | 12048/26933 [13:45<14:06, 17.59it/s] 45%|████▍     | 12052/26933 [13:45<14:09, 17.51it/s] 45%|████▍     | 12056/26933 [13:45<14:08, 17.52it/s] 45%|████▍     | 12060/26933 [13:46<14:08, 17.53it/s] 45%|████▍     | 12064/26933 [13:46<14:07, 17.54it/s] 45%|████▍     | 12068/26933 [13:46<14:05, 17.57it/s] 45%|████▍     | 12072/26933 [13:46<14:06, 17.56it/s] 45%|████▍     | 12076/26933 [13:47<14:07, 17.52it/s] 45%|████▍     | 12080/26933 [13:47<14:06, 17.54it/s] 45%|████▍     | 12084/26933 [13:47<14:05, 17.56it/s] 45%|████▍     | 12088/26933 [13:47<14:06, 17.54it/s] 45%|████▍     | 12092/26933 [13:47<14:04, 17.58it/s] 45%|████▍     | 12096/26933 [13:48<14:03, 17.59it/s] 45%|████▍     | 12100/26933 [13:48<14:01, 17.63it/s] 45%|████▍     | 12104/26933 [13:48<14:00, 17.64it/s] 45%|████▍     | 12108/26933 [13:48<14:01, 17.62it/s] 45%|████▍     | 12112/26933 [13:49<14:00, 17.64it/s] 45%|████▍     | 12116/26933 [13:49<13:59, 17.65it/s] 45%|████▌     | 12120/26933 [13:49<13:59, 17.65it/s] 45%|████▌     | 12124/26933 [13:49<14:02, 17.58it/s] 45%|████▌     | 12128/26933 [13:50<14:01, 17.59it/s] 45%|████▌     | 12132/26933 [13:50<14:02, 17.57it/s] 45%|████▌     | 12136/26933 [13:50<14:02, 17.56it/s] 45%|████▌     | 12140/26933 [13:50<14:05, 17.49it/s] 45%|████▌     | 12144/26933 [13:50<14:04, 17.51it/s] 45%|████▌     | 12148/26933 [13:51<14:00, 17.59it/s] 45%|████▌     | 12152/26933 [13:51<13:59, 17.60it/s] 45%|████▌     | 12156/26933 [13:51<13:57, 17.64it/s] 45%|████▌     | 12160/26933 [13:51<14:00, 17.58it/s] 45%|████▌     | 12164/26933 [13:52<13:59, 17.59it/s] 45%|████▌     | 12168/26933 [13:52<13:57, 17.62it/s] 45%|████▌     | 12172/26933 [13:52<13:56, 17.64it/s] 45%|████▌     | 12176/26933 [13:52<13:59, 17.57it/s] 45%|████▌     | 12180/26933 [13:52<13:58, 17.60it/s] 45%|████▌     | 12184/26933 [13:53<13:59, 17.58it/s] 45%|████▌     | 12188/26933 [13:53<13:56, 17.63it/s] 45%|████▌     | 12192/26933 [13:53<13:55, 17.65it/s] 45%|████▌     | 12196/26933 [13:53<13:57, 17.59it/s] 45%|████▌     | 12200/26933 [13:54<13:57, 17.59it/s] 45%|████▌     | 12204/26933 [13:54<13:55, 17.62it/s] 45%|████▌     | 12208/26933 [13:54<13:55, 17.62it/s] 45%|████▌     | 12212/26933 [13:54<13:57, 17.57it/s] 45%|████▌     | 12216/26933 [13:55<13:56, 17.59it/s] 45%|████▌     | 12220/26933 [13:55<13:57, 17.57it/s] 45%|████▌     | 12224/26933 [13:55<13:55, 17.61it/s] 45%|████▌     | 12228/26933 [13:55<13:55, 17.61it/s] 45%|████▌     | 12232/26933 [13:55<13:55, 17.59it/s] 45%|████▌     | 12236/26933 [13:56<13:55, 17.60it/s] 45%|████▌     | 12240/26933 [13:56<13:55, 17.59it/s] 45%|████▌     | 12244/26933 [13:56<13:55, 17.58it/s] 45%|████▌     | 12248/26933 [13:56<13:54, 17.59it/s] 45%|████▌     | 12252/26933 [13:57<13:52, 17.62it/s] 46%|████▌     | 12256/26933 [13:57<13:52, 17.63it/s] 46%|████▌     | 12260/26933 [13:57<13:51, 17.64it/s] 46%|████▌     | 12264/26933 [13:57<13:52, 17.62it/s] 46%|████▌     | 12268/26933 [13:57<13:51, 17.65it/s] 46%|████▌     | 12272/26933 [13:58<13:51, 17.63it/s] 46%|████▌     | 12276/26933 [13:58<13:50, 17.64it/s] 46%|████▌     | 12280/26933 [13:58<13:51, 17.63it/s] 46%|████▌     | 12284/26933 [13:58<13:51, 17.62it/s] 46%|████▌     | 12288/26933 [13:59<13:50, 17.63it/s] 46%|████▌     | 12292/26933 [13:59<13:50, 17.63it/s] 46%|████▌     | 12296/26933 [13:59<13:54, 17.55it/s] 46%|████▌     | 12300/26933 [13:59<13:55, 17.51it/s] 46%|████▌     | 12304/26933 [14:00<13:55, 17.51it/s] 46%|████▌     | 12308/26933 [14:00<13:56, 17.48it/s] 46%|████▌     | 12312/26933 [14:00<13:58, 17.44it/s] 46%|████▌     | 12316/26933 [14:00<13:57, 17.44it/s] 46%|████▌     | 12320/26933 [14:00<13:56, 17.46it/s] 46%|████▌     | 12324/26933 [14:01<14:07, 17.23it/s] 46%|████▌     | 12328/26933 [14:01<14:04, 17.29it/s] 46%|████▌     | 12332/26933 [14:01<13:57, 17.43it/s] 46%|████▌     | 12336/26933 [14:01<13:56, 17.45it/s] 46%|████▌     | 12340/26933 [14:02<13:53, 17.52it/s] 46%|████▌     | 12344/26933 [14:02<13:53, 17.51it/s] 46%|████▌     | 12348/26933 [14:02<13:52, 17.53it/s] 46%|████▌     | 12352/26933 [14:02<13:58, 17.39it/s] 46%|████▌     | 12356/26933 [14:02<13:55, 17.45it/s] 46%|████▌     | 12360/26933 [14:03<13:53, 17.48it/s] 46%|████▌     | 12364/26933 [14:03<13:52, 17.50it/s] 46%|████▌     | 12368/26933 [14:03<13:50, 17.54it/s] 46%|████▌     | 12372/26933 [14:03<13:51, 17.52it/s] 46%|████▌     | 12376/26933 [14:04<13:49, 17.54it/s] 46%|████▌     | 12380/26933 [14:04<13:51, 17.50it/s] 46%|████▌     | 12384/26933 [14:04<13:51, 17.51it/s] 46%|████▌     | 12388/26933 [14:04<13:52, 17.47it/s] 46%|████▌     | 12392/26933 [14:05<13:51, 17.48it/s] 46%|████▌     | 12396/26933 [14:05<13:50, 17.49it/s] 46%|████▌     | 12400/26933 [14:05<13:50, 17.51it/s] 46%|████▌     | 12404/26933 [14:05<13:50, 17.50it/s] 46%|████▌     | 12408/26933 [14:05<13:47, 17.55it/s] 46%|████▌     | 12412/26933 [14:06<13:47, 17.56it/s] 46%|████▌     | 12416/26933 [14:06<13:46, 17.57it/s] 46%|████▌     | 12420/26933 [14:06<13:47, 17.55it/s] 46%|████▌     | 12424/26933 [14:06<13:48, 17.52it/s] 46%|████▌     | 12428/26933 [14:07<13:46, 17.54it/s] 46%|████▌     | 12432/26933 [14:07<13:46, 17.54it/s] 46%|████▌     | 12436/26933 [14:07<13:46, 17.55it/s] 46%|████▌     | 12440/26933 [14:07<13:47, 17.52it/s] 46%|████▌     | 12444/26933 [14:08<13:47, 17.51it/s] 46%|████▌     | 12448/26933 [14:08<13:47, 17.50it/s] 46%|████▌     | 12452/26933 [14:08<13:45, 17.53it/s] 46%|████▌     | 12456/26933 [14:08<13:47, 17.49it/s] 46%|████▋     | 12460/26933 [14:08<13:51, 17.40it/s] 46%|████▋     | 12464/26933 [14:09<13:48, 17.47it/s] 46%|████▋     | 12468/26933 [14:09<13:47, 17.48it/s] 46%|████▋     | 12472/26933 [14:09<13:49, 17.44it/s] 46%|████▋     | 12476/26933 [14:09<13:50, 17.41it/s] 46%|████▋     | 12480/26933 [14:10<13:49, 17.42it/s] 46%|████▋     | 12484/26933 [14:10<13:51, 17.37it/s] 46%|████▋     | 12488/26933 [14:10<13:49, 17.41it/s] 46%|████▋     | 12492/26933 [14:10<13:52, 17.35it/s] 46%|████▋     | 12496/26933 [14:11<13:49, 17.39it/s] 46%|████▋     | 12500/26933 [14:11<13:48, 17.42it/s] 46%|████▋     | 12504/26933 [14:11<13:46, 17.46it/s] 46%|████▋     | 12508/26933 [14:11<13:45, 17.48it/s] 46%|████▋     | 12512/26933 [14:11<13:45, 17.46it/s] 46%|████▋     | 12516/26933 [14:12<13:41, 17.55it/s] 46%|████▋     | 12520/26933 [14:12<13:36, 17.65it/s] 47%|████▋     | 12524/26933 [14:12<13:33, 17.72it/s] 47%|████▋     | 12528/26933 [14:12<13:32, 17.74it/s] 47%|████▋     | 12532/26933 [14:13<13:29, 17.79it/s] 47%|████▋     | 12536/26933 [14:13<13:26, 17.84it/s] 47%|████▋     | 12540/26933 [14:13<13:25, 17.86it/s] 47%|████▋     | 12544/26933 [14:13<13:25, 17.86it/s] 47%|████▋     | 12548/26933 [14:13<13:26, 17.84it/s] 47%|████▋     | 12552/26933 [14:14<13:25, 17.84it/s] 47%|████▋     | 12556/26933 [14:14<13:26, 17.84it/s] 47%|████▋     | 12560/26933 [14:14<13:26, 17.83it/s] 47%|████▋     | 12564/26933 [14:14<13:27, 17.80it/s] 47%|████▋     | 12568/26933 [14:15<13:25, 17.84it/s] 47%|████▋     | 12572/26933 [14:15<13:24, 17.85it/s] 47%|████▋     | 12576/26933 [14:15<13:25, 17.83it/s] 47%|████▋     | 12580/26933 [14:15<13:27, 17.77it/s] 47%|████▋     | 12584/26933 [14:15<13:26, 17.80it/s] 47%|████▋     | 12588/26933 [14:16<13:25, 17.81it/s] 47%|████▋     | 12592/26933 [14:16<13:25, 17.81it/s] 47%|████▋     | 12596/26933 [14:16<13:25, 17.79it/s] 47%|████▋     | 12600/26933 [14:16<13:25, 17.80it/s] 47%|████▋     | 12604/26933 [14:17<13:24, 17.81it/s] 47%|████▋     | 12608/26933 [14:17<13:23, 17.82it/s] 47%|████▋     | 12612/26933 [14:17<13:22, 17.85it/s] 47%|████▋     | 12616/26933 [14:17<13:23, 17.83it/s] 47%|████▋     | 12620/26933 [14:17<13:21, 17.86it/s] 47%|████▋     | 12624/26933 [14:18<13:22, 17.83it/s] 47%|████▋     | 12628/26933 [14:18<13:23, 17.81it/s] 47%|████▋     | 12632/26933 [14:18<13:22, 17.83it/s] 47%|████▋     | 12636/26933 [14:18<13:23, 17.79it/s] 47%|████▋     | 12640/26933 [14:19<13:23, 17.80it/s] 47%|████▋     | 12644/26933 [14:19<13:21, 17.82it/s] 47%|████▋     | 12648/26933 [14:19<13:38, 17.46it/s] 47%|████▋     | 12652/26933 [14:19<13:35, 17.51it/s] 47%|████▋     | 12656/26933 [14:20<13:31, 17.60it/s] 47%|████▋     | 12660/26933 [14:20<13:27, 17.67it/s] 47%|████▋     | 12664/26933 [14:20<13:31, 17.58it/s] 47%|████▋     | 12668/26933 [14:20<13:27, 17.66it/s] 47%|████▋     | 12672/26933 [14:20<13:27, 17.66it/s] 47%|████▋     | 12676/26933 [14:21<13:24, 17.73it/s] 47%|████▋     | 12680/26933 [14:21<13:23, 17.74it/s] 47%|████▋     | 12684/26933 [14:21<13:18, 17.85it/s] 47%|████▋     | 12688/26933 [14:21<13:14, 17.94it/s] 47%|████▋     | 12692/26933 [14:22<13:10, 18.01it/s] 47%|████▋     | 12696/26933 [14:22<13:06, 18.10it/s] 47%|████▋     | 12700/26933 [14:22<13:05, 18.13it/s] 47%|████▋     | 12704/26933 [14:22<13:04, 18.14it/s] 47%|████▋     | 12708/26933 [14:22<13:06, 18.09it/s] 47%|████▋     | 12712/26933 [14:23<13:03, 18.15it/s] 47%|████▋     | 12716/26933 [14:23<13:02, 18.17it/s] 47%|████▋     | 12720/26933 [14:23<13:02, 18.17it/s] 47%|████▋     | 12724/26933 [14:23<13:04, 18.10it/s] 47%|████▋     | 12728/26933 [14:24<13:04, 18.12it/s] 47%|████▋     | 12732/26933 [14:24<13:03, 18.14it/s] 47%|████▋     | 12736/26933 [14:24<13:03, 18.13it/s] 47%|████▋     | 12740/26933 [14:24<13:09, 17.97it/s] 47%|████▋     | 12744/26933 [14:24<13:14, 17.85it/s] 47%|████▋     | 12748/26933 [14:25<13:16, 17.81it/s] 47%|████▋     | 12752/26933 [14:25<13:19, 17.73it/s] 47%|████▋     | 12756/26933 [14:25<13:21, 17.69it/s] 47%|████▋     | 12760/26933 [14:25<13:24, 17.62it/s] 47%|████▋     | 12764/26933 [14:26<13:23, 17.63it/s] 47%|████▋     | 12768/26933 [14:26<13:23, 17.63it/s] 47%|████▋     | 12772/26933 [14:26<13:24, 17.61it/s] 47%|████▋     | 12776/26933 [14:26<13:27, 17.52it/s] 47%|████▋     | 12780/26933 [14:26<13:28, 17.50it/s] 47%|████▋     | 12784/26933 [14:27<13:29, 17.47it/s] 47%|████▋     | 12788/26933 [14:27<13:29, 17.48it/s] 47%|████▋     | 12792/26933 [14:27<13:28, 17.50it/s] 48%|████▊     | 12796/26933 [14:27<13:30, 17.45it/s] 48%|████▊     | 12800/26933 [14:28<13:28, 17.47it/s] 48%|████▊     | 12804/26933 [14:28<13:25, 17.53it/s] 48%|████▊     | 12808/26933 [14:28<13:23, 17.58it/s] 48%|████▊     | 12812/26933 [14:28<13:24, 17.56it/s] 48%|████▊     | 12816/26933 [14:29<13:22, 17.59it/s] 48%|████▊     | 12820/26933 [14:29<13:22, 17.59it/s] 48%|████▊     | 12824/26933 [14:29<13:21, 17.61it/s] 48%|████▊     | 12828/26933 [14:29<13:22, 17.57it/s] 48%|████▊     | 12832/26933 [14:29<13:23, 17.55it/s] 48%|████▊     | 12836/26933 [14:30<13:24, 17.51it/s] 48%|████▊     | 12840/26933 [14:30<13:23, 17.54it/s] 48%|████▊     | 12844/26933 [14:30<13:21, 17.57it/s] 48%|████▊     | 12848/26933 [14:30<13:21, 17.57it/s] 48%|████▊     | 12852/26933 [14:31<13:20, 17.59it/s] 48%|████▊     | 12856/26933 [14:31<13:18, 17.62it/s] 48%|████▊     | 12860/26933 [14:31<13:17, 17.65it/s] 48%|████▊     | 12864/26933 [14:31<13:18, 17.61it/s] 48%|████▊     | 12868/26933 [14:31<13:18, 17.62it/s] 48%|████▊     | 12872/26933 [14:32<13:17, 17.63it/s] 48%|████▊     | 12876/26933 [14:32<13:18, 17.61it/s] 48%|████▊     | 12880/26933 [14:32<13:15, 17.66it/s] 48%|████▊     | 12884/26933 [14:32<13:17, 17.61it/s] 48%|████▊     | 12888/26933 [14:33<13:17, 17.61it/s] 48%|████▊     | 12892/26933 [14:33<13:17, 17.60it/s] 48%|████▊     | 12896/26933 [14:33<13:17, 17.60it/s] 48%|████▊     | 12900/26933 [14:33<13:19, 17.56it/s] 48%|████▊     | 12904/26933 [14:34<13:17, 17.60it/s] 48%|████▊     | 12908/26933 [14:34<13:14, 17.65it/s] 48%|████▊     | 12912/26933 [14:34<13:10, 17.74it/s] 48%|████▊     | 12916/26933 [14:34<13:07, 17.80it/s] 48%|████▊     | 12920/26933 [14:34<13:08, 17.77it/s] 48%|████▊     | 12924/26933 [14:35<13:05, 17.84it/s] 48%|████▊     | 12928/26933 [14:35<13:02, 17.89it/s] 48%|████▊     | 12932/26933 [14:35<13:00, 17.93it/s] 48%|████▊     | 12936/26933 [14:35<13:02, 17.90it/s] 48%|████▊     | 12940/26933 [14:36<12:59, 17.95it/s] 48%|████▊     | 12944/26933 [14:36<12:58, 17.96it/s] 48%|████▊     | 12948/26933 [14:36<12:58, 17.97it/s] 48%|████▊     | 12952/26933 [14:36<13:01, 17.89it/s] 48%|████▊     | 12956/26933 [14:36<13:00, 17.91it/s] 48%|████▊     | 12960/26933 [14:37<12:58, 17.94it/s] 48%|████▊     | 12964/26933 [14:37<12:59, 17.92it/s] 48%|████▊     | 12968/26933 [14:37<12:59, 17.92it/s] 48%|████▊     | 12972/26933 [14:37<13:00, 17.90it/s] 48%|████▊     | 12976/26933 [14:38<13:09, 17.69it/s] 48%|████▊     | 12980/26933 [14:38<13:15, 17.55it/s] 48%|████▊     | 12984/26933 [14:38<13:26, 17.29it/s] 48%|████▊     | 12988/26933 [14:38<13:30, 17.21it/s] 48%|████▊     | 12992/26933 [14:38<13:34, 17.11it/s] 48%|████▊     | 12996/26933 [14:39<13:31, 17.18it/s] 48%|████▊     | 13000/26933 [14:39<13:29, 17.21it/s] 48%|████▊     | 13004/26933 [14:39<13:25, 17.30it/s] 48%|████▊     | 13008/26933 [14:39<13:21, 17.38it/s] 48%|████▊     | 13012/26933 [14:40<13:19, 17.40it/s] 48%|████▊     | 13016/26933 [14:40<13:18, 17.43it/s] 48%|████▊     | 13020/26933 [14:40<13:18, 17.42it/s] 48%|████▊     | 13024/26933 [14:40<13:20, 17.37it/s] 48%|████▊     | 13028/26933 [14:41<13:19, 17.40it/s] 48%|████▊     | 13032/26933 [14:41<13:18, 17.41it/s] 48%|████▊     | 13036/26933 [14:41<13:17, 17.43it/s] 48%|████▊     | 13040/26933 [14:41<13:19, 17.37it/s] 48%|████▊     | 13044/26933 [14:41<13:17, 17.41it/s] 48%|████▊     | 13048/26933 [14:42<13:17, 17.41it/s] 48%|████▊     | 13052/26933 [14:42<13:15, 17.45it/s] 48%|████▊     | 13056/26933 [14:42<13:11, 17.53it/s] 48%|████▊     | 13060/26933 [14:42<13:11, 17.53it/s] 49%|████▊     | 13064/26933 [14:43<13:10, 17.54it/s] 49%|████▊     | 13068/26933 [14:43<13:06, 17.63it/s] 49%|████▊     | 13072/26933 [14:43<13:08, 17.58it/s] 49%|████▊     | 13076/26933 [14:43<13:06, 17.62it/s] 49%|████▊     | 13080/26933 [14:44<13:00, 17.74it/s] 49%|████▊     | 13084/26933 [14:44<12:57, 17.80it/s] 49%|████▊     | 13088/26933 [14:44<12:56, 17.84it/s] 49%|████▊     | 13092/26933 [14:44<12:55, 17.85it/s] 49%|████▊     | 13096/26933 [14:44<12:54, 17.87it/s] 49%|████▊     | 13100/26933 [14:45<12:51, 17.92it/s] 49%|████▊     | 13104/26933 [14:45<12:52, 17.89it/s] 49%|████▊     | 13108/26933 [14:45<12:52, 17.89it/s] 49%|████▊     | 13112/26933 [14:45<12:53, 17.88it/s] 49%|████▊     | 13116/26933 [14:46<12:51, 17.91it/s] 49%|████▊     | 13120/26933 [14:46<12:50, 17.93it/s] 49%|████▊     | 13124/26933 [14:46<12:49, 17.95it/s] 49%|████▊     | 13128/26933 [14:46<12:48, 17.95it/s] 49%|████▉     | 13132/26933 [14:46<12:50, 17.91it/s] 49%|████▉     | 13136/26933 [14:47<12:48, 17.94it/s] 49%|████▉     | 13140/26933 [14:47<12:48, 17.95it/s] 49%|████▉     | 13144/26933 [14:47<12:48, 17.94it/s] 49%|████▉     | 13148/26933 [14:47<12:48, 17.93it/s] 49%|████▉     | 13152/26933 [14:48<12:47, 17.96it/s] 49%|████▉     | 13156/26933 [14:48<12:46, 17.97it/s] 49%|████▉     | 13160/26933 [14:48<12:45, 17.99it/s] 49%|████▉     | 13164/26933 [14:48<12:45, 17.99it/s] 49%|████▉     | 13168/26933 [14:48<12:46, 17.95it/s] 49%|████▉     | 13172/26933 [14:49<12:46, 17.96it/s] 49%|████▉     | 13176/26933 [14:49<12:45, 17.96it/s] 49%|████▉     | 13180/26933 [14:49<12:46, 17.95it/s] 49%|████▉     | 13184/26933 [14:49<12:47, 17.91it/s] 49%|████▉     | 13188/26933 [14:50<12:46, 17.94it/s] 49%|████▉     | 13192/26933 [14:50<12:46, 17.94it/s] 49%|████▉     | 13196/26933 [14:50<12:45, 17.94it/s] 49%|████▉     | 13200/26933 [14:50<12:50, 17.82it/s] 49%|████▉     | 13204/26933 [14:50<12:57, 17.66it/s] 49%|████▉     | 13208/26933 [14:51<12:57, 17.64it/s] 49%|████▉     | 13212/26933 [14:51<12:57, 17.66it/s] 49%|████▉     | 13216/26933 [14:51<12:58, 17.63it/s] 49%|████▉     | 13220/26933 [14:51<12:57, 17.63it/s] 49%|████▉     | 13224/26933 [14:52<12:56, 17.65it/s] 49%|████▉     | 13228/26933 [14:52<12:57, 17.63it/s] 49%|████▉     | 13232/26933 [14:52<12:58, 17.60it/s] 49%|████▉     | 13236/26933 [14:52<12:58, 17.60it/s] 49%|████▉     | 13240/26933 [14:52<12:57, 17.60it/s] 49%|████▉     | 13244/26933 [14:53<12:56, 17.63it/s] 49%|████▉     | 13248/26933 [14:53<12:54, 17.66it/s] 49%|████▉     | 13252/26933 [14:53<12:54, 17.66it/s] 49%|████▉     | 13256/26933 [14:53<12:57, 17.59it/s] 49%|████▉     | 13260/26933 [14:54<12:57, 17.58it/s] 49%|████▉     | 13264/26933 [14:54<13:00, 17.52it/s] 49%|████▉     | 13268/26933 [14:54<12:59, 17.52it/s] 49%|████▉     | 13272/26933 [14:54<13:00, 17.51it/s] 49%|████▉     | 13276/26933 [14:55<12:58, 17.55it/s] 49%|████▉     | 13280/26933 [14:55<12:55, 17.61it/s] 49%|████▉     | 13284/26933 [14:55<12:52, 17.68it/s] 49%|████▉     | 13288/26933 [14:55<12:50, 17.70it/s] 49%|████▉     | 13292/26933 [14:55<12:51, 17.67it/s] 49%|████▉     | 13296/26933 [14:56<12:50, 17.71it/s] 49%|████▉     | 13300/26933 [14:56<12:49, 17.72it/s] 49%|████▉     | 13304/26933 [14:56<12:48, 17.73it/s] 49%|████▉     | 13308/26933 [14:56<12:51, 17.66it/s] 49%|████▉     | 13312/26933 [14:57<12:51, 17.65it/s] 49%|████▉     | 13316/26933 [14:57<12:51, 17.64it/s] 49%|████▉     | 13320/26933 [14:57<12:51, 17.65it/s] 49%|████▉     | 13324/26933 [14:57<12:51, 17.64it/s] 49%|████▉     | 13328/26933 [14:57<12:50, 17.66it/s] 50%|████▉     | 13332/26933 [14:58<12:50, 17.66it/s] 50%|████▉     | 13336/26933 [14:58<12:49, 17.66it/s] 50%|████▉     | 13340/26933 [14:58<12:48, 17.69it/s] 50%|████▉     | 13344/26933 [14:58<12:52, 17.59it/s] 50%|████▉     | 13348/26933 [14:59<12:51, 17.62it/s] 50%|████▉     | 13352/26933 [14:59<12:50, 17.62it/s] 50%|████▉     | 13356/26933 [14:59<12:51, 17.61it/s] 50%|████▉     | 13360/26933 [14:59<12:50, 17.61it/s] 50%|████▉     | 13364/26933 [15:00<12:50, 17.61it/s] 50%|████▉     | 13368/26933 [15:00<12:50, 17.61it/s] 50%|████▉     | 13372/26933 [15:00<12:49, 17.63it/s] 50%|████▉     | 13376/26933 [15:00<12:48, 17.64it/s] 50%|████▉     | 13380/26933 [15:00<12:50, 17.59it/s] 50%|████▉     | 13384/26933 [15:01<12:48, 17.63it/s] 50%|████▉     | 13388/26933 [15:01<12:47, 17.64it/s] 50%|████▉     | 13392/26933 [15:01<12:46, 17.67it/s] 50%|████▉     | 13396/26933 [15:01<12:47, 17.64it/s] 50%|████▉     | 13400/26933 [15:02<12:48, 17.60it/s] 50%|████▉     | 13404/26933 [15:02<12:47, 17.62it/s] 50%|████▉     | 13408/26933 [15:02<12:45, 17.66it/s] 50%|████▉     | 13412/26933 [15:02<12:47, 17.62it/s] 50%|████▉     | 13416/26933 [15:02<12:46, 17.64it/s] 50%|████▉     | 13420/26933 [15:03<12:46, 17.63it/s] 50%|████▉     | 13424/26933 [15:03<12:45, 17.65it/s] 50%|████▉     | 13428/26933 [15:03<12:44, 17.66it/s] 50%|████▉     | 13432/26933 [15:03<12:45, 17.64it/s] 50%|████▉     | 13436/26933 [15:04<12:45, 17.63it/s] 50%|████▉     | 13440/26933 [15:04<12:44, 17.65it/s] 50%|████▉     | 13444/26933 [15:04<12:44, 17.65it/s] 50%|████▉     | 13448/26933 [15:04<12:45, 17.62it/s] 50%|████▉     | 13452/26933 [15:04<12:43, 17.65it/s] 50%|████▉     | 13456/26933 [15:05<12:42, 17.67it/s] 50%|████▉     | 13460/26933 [15:05<12:40, 17.71it/s] 50%|████▉     | 13464/26933 [15:05<12:39, 17.73it/s] 50%|█████     | 13468/26933 [15:05<12:40, 17.71it/s] 50%|█████     | 13472/26933 [15:06<12:41, 17.68it/s] 50%|█████     | 13476/26933 [15:06<12:40, 17.69it/s] 50%|█████     | 13480/26933 [15:06<12:39, 17.70it/s] 50%|█████     | 13484/26933 [15:06<12:40, 17.68it/s] 50%|█████     | 13488/26933 [15:07<12:39, 17.69it/s] 50%|█████     | 13492/26933 [15:07<12:38, 17.73it/s] 50%|█████     | 13496/26933 [15:07<12:36, 17.76it/s] 50%|█████     | 13500/26933 [15:07<12:36, 17.76it/s] 50%|█████     | 13504/26933 [15:07<12:36, 17.75it/s] 50%|█████     | 13508/26933 [15:08<12:36, 17.74it/s] 50%|█████     | 13512/26933 [15:08<12:36, 17.73it/s] 50%|█████     | 13516/26933 [15:08<12:35, 17.75it/s] 50%|█████     | 13520/26933 [15:08<12:38, 17.69it/s] 50%|█████     | 13524/26933 [15:09<12:37, 17.70it/s] 50%|█████     | 13528/26933 [15:09<12:36, 17.71it/s] 50%|█████     | 13532/26933 [15:09<12:36, 17.71it/s] 50%|█████     | 13536/26933 [15:09<12:39, 17.63it/s] 50%|█████     | 13540/26933 [15:09<12:37, 17.68it/s] 50%|█████     | 13544/26933 [15:10<12:36, 17.70it/s] 50%|█████     | 13548/26933 [15:10<12:40, 17.60it/s] 50%|█████     | 13552/26933 [15:10<12:53, 17.29it/s] 50%|█████     | 13556/26933 [15:10<12:56, 17.22it/s] 50%|█████     | 13560/26933 [15:11<12:56, 17.23it/s] 50%|█████     | 13564/26933 [15:11<13:04, 17.03it/s] 50%|█████     | 13568/26933 [15:11<13:03, 17.06it/s] 50%|█████     | 13572/26933 [15:11<12:59, 17.13it/s] 50%|█████     | 13576/26933 [15:12<12:51, 17.32it/s] 50%|█████     | 13580/26933 [15:12<12:45, 17.44it/s] 50%|█████     | 13584/26933 [15:12<12:40, 17.56it/s] 50%|█████     | 13588/26933 [15:12<12:38, 17.58it/s] 50%|█████     | 13592/26933 [15:12<12:37, 17.61it/s] 50%|█████     | 13596/26933 [15:13<12:37, 17.62it/s] 50%|█████     | 13600/26933 [15:13<12:35, 17.66it/s] 51%|█████     | 13604/26933 [15:13<12:33, 17.69it/s] 51%|█████     | 13608/26933 [15:13<12:36, 17.62it/s] 51%|█████     | 13612/26933 [15:14<12:34, 17.65it/s] 51%|█████     | 13616/26933 [15:14<12:34, 17.66it/s] 51%|█████     | 13620/26933 [15:14<12:38, 17.56it/s] 51%|█████     | 13624/26933 [15:14<12:37, 17.57it/s] 51%|█████     | 13628/26933 [15:15<12:35, 17.60it/s] 51%|█████     | 13632/26933 [15:15<12:33, 17.65it/s] 51%|█████     | 13636/26933 [15:15<12:32, 17.67it/s] 51%|█████     | 13640/26933 [15:15<12:33, 17.64it/s] 51%|█████     | 13644/26933 [15:15<12:35, 17.59it/s] 51%|█████     | 13648/26933 [15:16<12:34, 17.60it/s] 51%|█████     | 13652/26933 [15:16<12:31, 17.68it/s] 51%|█████     | 13656/26933 [15:16<12:30, 17.69it/s] 51%|█████     | 13660/26933 [15:16<12:31, 17.66it/s] 51%|█████     | 13664/26933 [15:17<12:30, 17.68it/s] 51%|█████     | 13668/26933 [15:17<12:30, 17.69it/s] 51%|█████     | 13672/26933 [15:17<12:28, 17.71it/s] 51%|█████     | 13676/26933 [15:17<12:27, 17.73it/s] 51%|█████     | 13680/26933 [15:17<12:28, 17.70it/s] 51%|█████     | 13684/26933 [15:18<12:28, 17.71it/s] 51%|█████     | 13688/26933 [15:18<12:27, 17.72it/s] 51%|█████     | 13692/26933 [15:18<12:26, 17.74it/s] 51%|█████     | 13696/26933 [15:18<12:27, 17.71it/s] 51%|█████     | 13700/26933 [15:19<12:27, 17.70it/s] 51%|█████     | 13704/26933 [15:19<12:27, 17.69it/s] 51%|█████     | 13708/26933 [15:19<12:27, 17.70it/s] 51%|█████     | 13712/26933 [15:19<12:27, 17.69it/s] 51%|█████     | 13716/26933 [15:19<12:27, 17.69it/s] 51%|█████     | 13720/26933 [15:20<12:25, 17.71it/s] 51%|█████     | 13724/26933 [15:20<12:25, 17.72it/s] 51%|█████     | 13728/26933 [15:20<12:25, 17.71it/s] 51%|█████     | 13732/26933 [15:20<12:24, 17.74it/s] 51%|█████     | 13736/26933 [15:21<12:19, 17.85it/s] 51%|█████     | 13740/26933 [15:21<12:16, 17.92it/s] 51%|█████     | 13744/26933 [15:21<12:13, 17.99it/s] 51%|█████     | 13748/26933 [15:21<12:12, 18.00it/s] 51%|█████     | 13752/26933 [15:21<12:09, 18.06it/s] 51%|█████     | 13756/26933 [15:22<12:09, 18.07it/s] 51%|█████     | 13760/26933 [15:22<12:08, 18.07it/s] 51%|█████     | 13764/26933 [15:22<12:06, 18.12it/s] 51%|█████     | 13768/26933 [15:22<12:07, 18.10it/s] 51%|█████     | 13772/26933 [15:23<12:06, 18.11it/s] 51%|█████     | 13776/26933 [15:23<12:06, 18.11it/s] 51%|█████     | 13780/26933 [15:23<12:05, 18.12it/s] 51%|█████     | 13784/26933 [15:23<12:05, 18.13it/s] 51%|█████     | 13788/26933 [15:23<12:04, 18.14it/s] 51%|█████     | 13792/26933 [15:24<12:02, 18.19it/s] 51%|█████     | 13796/26933 [15:24<12:03, 18.17it/s] 51%|█████     | 13800/26933 [15:24<12:02, 18.19it/s] 51%|█████▏    | 13804/26933 [15:24<12:03, 18.14it/s] 51%|█████▏    | 13808/26933 [15:25<12:04, 18.12it/s] 51%|█████▏    | 13812/26933 [15:25<12:04, 18.11it/s] 51%|█████▏    | 13816/26933 [15:25<12:04, 18.10it/s] 51%|█████▏    | 13820/26933 [15:25<12:06, 18.04it/s] 51%|█████▏    | 13824/26933 [15:25<12:07, 18.03it/s] 51%|█████▏    | 13828/26933 [15:26<12:04, 18.09it/s] 51%|█████▏    | 13832/26933 [15:26<12:03, 18.11it/s] 51%|█████▏    | 13836/26933 [15:26<12:02, 18.12it/s] 51%|█████▏    | 13840/26933 [15:26<12:02, 18.12it/s] 51%|█████▏    | 13844/26933 [15:27<12:02, 18.12it/s] 51%|█████▏    | 13848/26933 [15:27<12:01, 18.13it/s] 51%|█████▏    | 13852/26933 [15:27<12:01, 18.12it/s] 51%|█████▏    | 13856/26933 [15:27<11:59, 18.16it/s] 51%|█████▏    | 13860/26933 [15:27<12:00, 18.13it/s] 51%|█████▏    | 13864/26933 [15:28<11:59, 18.16it/s] 51%|█████▏    | 13868/26933 [15:28<11:58, 18.17it/s] 52%|█████▏    | 13872/26933 [15:28<11:58, 18.17it/s] 52%|█████▏    | 13876/26933 [15:28<11:59, 18.15it/s] 52%|█████▏    | 13880/26933 [15:29<11:58, 18.16it/s] 52%|█████▏    | 13884/26933 [15:29<11:59, 18.15it/s] 52%|█████▏    | 13888/26933 [15:29<11:58, 18.17it/s] 52%|█████▏    | 13892/26933 [15:29<11:56, 18.19it/s] 52%|█████▏    | 13896/26933 [15:29<11:57, 18.17it/s] 52%|█████▏    | 13900/26933 [15:30<11:55, 18.21it/s] 52%|█████▏    | 13904/26933 [15:30<11:55, 18.22it/s] 52%|█████▏    | 13908/26933 [15:30<11:55, 18.20it/s] 52%|█████▏    | 13912/26933 [15:30<11:56, 18.17it/s] 52%|█████▏    | 13916/26933 [15:31<11:59, 18.10it/s] 52%|█████▏    | 13920/26933 [15:31<12:00, 18.05it/s] 52%|█████▏    | 13924/26933 [15:31<12:02, 18.00it/s] 52%|█████▏    | 13928/26933 [15:31<12:03, 17.99it/s] 52%|█████▏    | 13932/26933 [15:31<12:05, 17.93it/s] 52%|█████▏    | 13936/26933 [15:32<12:03, 17.96it/s] 52%|█████▏    | 13940/26933 [15:32<12:02, 17.97it/s] 52%|█████▏    | 13944/26933 [15:32<12:02, 17.98it/s] 52%|█████▏    | 13948/26933 [15:32<12:04, 17.93it/s] 52%|█████▏    | 13952/26933 [15:33<12:05, 17.89it/s] 52%|█████▏    | 13956/26933 [15:33<12:04, 17.91it/s] 52%|█████▏    | 13960/26933 [15:33<12:04, 17.91it/s] 52%|█████▏    | 13964/26933 [15:33<12:03, 17.91it/s] 52%|█████▏    | 13968/26933 [15:33<12:05, 17.88it/s] 52%|█████▏    | 13972/26933 [15:34<12:03, 17.91it/s] 52%|█████▏    | 13976/26933 [15:34<12:02, 17.95it/s] 52%|█████▏    | 13980/26933 [15:34<12:00, 17.97it/s] 52%|█████▏    | 13984/26933 [15:34<12:10, 17.72it/s] 52%|█████▏    | 13988/26933 [15:35<12:10, 17.71it/s] 52%|█████▏    | 13992/26933 [15:35<12:08, 17.76it/s] 52%|█████▏    | 13996/26933 [15:35<12:07, 17.79it/s] 52%|█████▏    | 14000/26933 [15:35<12:08, 17.75it/s] 52%|█████▏    | 14004/26933 [15:35<12:12, 17.65it/s] 52%|█████▏    | 14008/26933 [15:36<12:10, 17.69it/s] 52%|█████▏    | 14012/26933 [15:36<12:09, 17.70it/s] 52%|█████▏    | 14016/26933 [15:36<12:10, 17.68it/s] 52%|█████▏    | 14020/26933 [15:36<12:11, 17.66it/s] 52%|█████▏    | 14024/26933 [15:37<12:09, 17.69it/s] 52%|█████▏    | 14028/26933 [15:37<12:10, 17.66it/s] 52%|█████▏    | 14032/26933 [15:37<12:08, 17.71it/s] 52%|█████▏    | 14036/26933 [15:37<12:11, 17.64it/s] 52%|█████▏    | 14040/26933 [15:37<12:09, 17.67it/s] 52%|█████▏    | 14044/26933 [15:38<12:09, 17.67it/s] 52%|█████▏    | 14048/26933 [15:38<12:08, 17.68it/s] 52%|█████▏    | 14052/26933 [15:38<12:07, 17.70it/s] 52%|█████▏    | 14056/26933 [15:38<12:08, 17.68it/s] 52%|█████▏    | 14060/26933 [15:39<12:05, 17.73it/s] 52%|█████▏    | 14064/26933 [15:39<12:05, 17.73it/s] 52%|█████▏    | 14068/26933 [15:39<12:04, 17.75it/s] 52%|█████▏    | 14072/26933 [15:39<12:06, 17.71it/s] 52%|█████▏    | 14076/26933 [15:40<12:04, 17.74it/s] 52%|█████▏    | 14080/26933 [15:40<12:04, 17.75it/s] 52%|█████▏    | 14084/26933 [15:40<12:03, 17.76it/s] 52%|█████▏    | 14088/26933 [15:40<12:02, 17.77it/s] 52%|█████▏    | 14092/26933 [15:40<12:05, 17.70it/s] 52%|█████▏    | 14096/26933 [15:41<12:04, 17.72it/s] 52%|█████▏    | 14100/26933 [15:41<12:06, 17.66it/s] 52%|█████▏    | 14104/26933 [15:41<12:08, 17.62it/s] 52%|█████▏    | 14108/26933 [15:41<12:10, 17.55it/s] 52%|█████▏    | 14112/26933 [15:42<12:08, 17.61it/s] 52%|█████▏    | 14116/26933 [15:42<12:08, 17.60it/s] 52%|█████▏    | 14120/26933 [15:42<12:06, 17.64it/s] 52%|█████▏    | 14124/26933 [15:42<12:06, 17.62it/s] 52%|█████▏    | 14128/26933 [15:42<12:05, 17.64it/s] 52%|█████▏    | 14132/26933 [15:43<12:04, 17.67it/s] 52%|█████▏    | 14136/26933 [15:43<12:02, 17.70it/s] 53%|█████▎    | 14140/26933 [15:43<12:02, 17.71it/s] 53%|█████▎    | 14144/26933 [15:43<12:04, 17.66it/s] 53%|█████▎    | 14148/26933 [15:44<12:03, 17.67it/s] 53%|█████▎    | 14152/26933 [15:44<12:04, 17.65it/s] 53%|█████▎    | 14156/26933 [15:44<12:01, 17.70it/s] 53%|█████▎    | 14160/26933 [15:44<12:02, 17.68it/s] 53%|█████▎    | 14164/26933 [15:45<12:00, 17.73it/s] 53%|█████▎    | 14168/26933 [15:45<11:58, 17.77it/s] 53%|█████▎    | 14172/26933 [15:45<11:56, 17.81it/s] 53%|█████▎    | 14176/26933 [15:45<11:55, 17.83it/s] 53%|█████▎    | 14180/26933 [15:45<11:56, 17.79it/s] 53%|█████▎    | 14184/26933 [15:46<11:55, 17.82it/s] 53%|█████▎    | 14188/26933 [15:46<11:53, 17.86it/s] 53%|█████▎    | 14192/26933 [15:46<11:53, 17.87it/s] 53%|█████▎    | 14196/26933 [15:46<11:55, 17.79it/s] 53%|█████▎    | 14200/26933 [15:47<11:52, 17.86it/s] 53%|█████▎    | 14204/26933 [15:47<11:53, 17.84it/s] 53%|█████▎    | 14208/26933 [15:47<11:53, 17.84it/s] 53%|█████▎    | 14212/26933 [15:47<11:52, 17.86it/s] 53%|█████▎    | 14216/26933 [15:47<11:53, 17.82it/s] 53%|█████▎    | 14220/26933 [15:48<11:54, 17.80it/s] 53%|█████▎    | 14224/26933 [15:48<11:54, 17.79it/s] 53%|█████▎    | 14228/26933 [15:48<11:54, 17.79it/s] 53%|█████▎    | 14232/26933 [15:48<11:55, 17.76it/s] 53%|█████▎    | 14236/26933 [15:49<11:52, 17.83it/s] 53%|█████▎    | 14240/26933 [15:49<11:54, 17.77it/s] 53%|█████▎    | 14244/26933 [15:49<11:54, 17.76it/s] 53%|█████▎    | 14248/26933 [15:49<11:55, 17.74it/s] 53%|█████▎    | 14252/26933 [15:49<11:56, 17.69it/s] 53%|█████▎    | 14256/26933 [15:50<11:56, 17.69it/s] 53%|█████▎    | 14260/26933 [15:50<11:55, 17.72it/s] 53%|█████▎    | 14264/26933 [15:50<11:55, 17.71it/s] 53%|█████▎    | 14268/26933 [15:50<11:56, 17.67it/s] 53%|█████▎    | 14272/26933 [15:51<11:55, 17.71it/s] 53%|█████▎    | 14276/26933 [15:51<11:53, 17.74it/s] 53%|█████▎    | 14280/26933 [15:51<11:52, 17.76it/s] 53%|█████▎    | 14284/26933 [15:51<11:54, 17.71it/s] 53%|█████▎    | 14288/26933 [15:51<11:52, 17.74it/s] 53%|█████▎    | 14292/26933 [15:52<11:51, 17.76it/s] 53%|█████▎    | 14296/26933 [15:52<11:51, 17.76it/s] 53%|█████▎    | 14300/26933 [15:52<11:52, 17.73it/s] 53%|█████▎    | 14304/26933 [15:52<11:53, 17.70it/s] 53%|█████▎    | 14308/26933 [15:53<11:52, 17.72it/s] 53%|█████▎    | 14312/26933 [15:53<11:51, 17.75it/s] 53%|█████▎    | 14316/26933 [15:53<11:51, 17.72it/s] 53%|█████▎    | 14320/26933 [15:53<11:53, 17.69it/s] 53%|█████▎    | 14324/26933 [15:54<11:53, 17.68it/s] 53%|█████▎    | 14328/26933 [15:54<11:51, 17.72it/s] 53%|█████▎    | 14332/26933 [15:54<11:50, 17.73it/s] 53%|█████▎    | 14336/26933 [15:54<11:49, 17.75it/s] 53%|█████▎    | 14340/26933 [15:54<11:53, 17.65it/s] 53%|█████▎    | 14344/26933 [15:55<11:55, 17.59it/s] 53%|█████▎    | 14348/26933 [15:55<11:59, 17.48it/s] 53%|█████▎    | 14352/26933 [15:55<12:00, 17.46it/s] 53%|█████▎    | 14356/26933 [15:55<12:02, 17.40it/s] 53%|█████▎    | 14360/26933 [15:56<12:06, 17.30it/s] 53%|█████▎    | 14364/26933 [15:56<12:05, 17.33it/s] 53%|█████▎    | 14368/26933 [15:56<12:00, 17.44it/s] 53%|█████▎    | 14372/26933 [15:56<11:57, 17.50it/s] 53%|█████▎    | 14376/26933 [15:56<11:54, 17.58it/s] 53%|█████▎    | 14380/26933 [15:57<11:51, 17.65it/s] 53%|█████▎    | 14384/26933 [15:57<11:47, 17.73it/s] 53%|█████▎    | 14388/26933 [15:57<11:46, 17.77it/s] 53%|█████▎    | 14392/26933 [15:57<11:47, 17.73it/s] 53%|█████▎    | 14396/26933 [15:58<11:47, 17.73it/s] 53%|█████▎    | 14400/26933 [15:58<11:44, 17.78it/s] 53%|█████▎    | 14404/26933 [15:58<11:45, 17.77it/s] 53%|█████▎    | 14408/26933 [15:58<11:45, 17.75it/s] 54%|█████▎    | 14412/26933 [15:59<11:44, 17.78it/s] 54%|█████▎    | 14416/26933 [15:59<11:46, 17.72it/s] 54%|█████▎    | 14420/26933 [15:59<11:46, 17.72it/s] 54%|█████▎    | 14424/26933 [15:59<11:45, 17.72it/s] 54%|█████▎    | 14428/26933 [15:59<11:46, 17.71it/s] 54%|█████▎    | 14432/26933 [16:00<11:45, 17.73it/s] 54%|█████▎    | 14436/26933 [16:00<11:44, 17.74it/s] 54%|█████▎    | 14440/26933 [16:00<11:42, 17.79it/s] 54%|█████▎    | 14444/26933 [16:00<11:44, 17.74it/s] 54%|█████▎    | 14448/26933 [16:01<11:43, 17.76it/s] 54%|█████▎    | 14452/26933 [16:01<11:43, 17.74it/s] 54%|█████▎    | 14456/26933 [16:01<11:41, 17.79it/s] 54%|█████▎    | 14460/26933 [16:01<11:42, 17.76it/s] 54%|█████▎    | 14464/26933 [16:01<12:06, 17.16it/s] 54%|█████▎    | 14468/26933 [16:02<11:58, 17.34it/s] 54%|█████▎    | 14472/26933 [16:02<11:58, 17.34it/s] 54%|█████▎    | 14476/26933 [16:02<11:52, 17.48it/s] 54%|█████▍    | 14480/26933 [16:02<11:51, 17.51it/s] 54%|█████▍    | 14484/26933 [16:03<11:47, 17.61it/s] 54%|█████▍    | 14488/26933 [16:03<11:46, 17.61it/s] 54%|█████▍    | 14492/26933 [16:03<11:45, 17.64it/s] 54%|█████▍    | 14496/26933 [16:03<11:48, 17.56it/s] 54%|█████▍    | 14500/26933 [16:04<11:46, 17.60it/s] 54%|█████▍    | 14504/26933 [16:04<11:46, 17.59it/s] 54%|█████▍    | 14508/26933 [16:04<11:41, 17.71it/s] 54%|█████▍    | 14512/26933 [16:04<11:37, 17.80it/s] 54%|█████▍    | 14516/26933 [16:04<11:36, 17.84it/s] 54%|█████▍    | 14520/26933 [16:05<11:33, 17.89it/s] 54%|█████▍    | 14524/26933 [16:05<11:32, 17.91it/s] 54%|█████▍    | 14528/26933 [16:05<11:29, 17.99it/s] 54%|█████▍    | 14532/26933 [16:05<11:28, 18.00it/s] 54%|█████▍    | 14536/26933 [16:06<11:27, 18.03it/s] 54%|█████▍    | 14540/26933 [16:06<11:25, 18.07it/s] 54%|█████▍    | 14544/26933 [16:06<11:25, 18.08it/s] 54%|█████▍    | 14548/26933 [16:06<11:24, 18.10it/s] 54%|█████▍    | 14552/26933 [16:06<11:24, 18.08it/s] 54%|█████▍    | 14556/26933 [16:07<11:23, 18.10it/s] 54%|█████▍    | 14560/26933 [16:07<11:24, 18.08it/s] 54%|█████▍    | 14564/26933 [16:07<11:26, 18.03it/s] 54%|█████▍    | 14568/26933 [16:07<11:29, 17.93it/s] 54%|█████▍    | 14572/26933 [16:08<11:28, 17.94it/s] 54%|█████▍    | 14576/26933 [16:08<11:28, 17.94it/s] 54%|█████▍    | 14580/26933 [16:08<11:28, 17.95it/s] 54%|█████▍    | 14584/26933 [16:08<11:27, 17.97it/s] 54%|█████▍    | 14588/26933 [16:08<11:27, 17.96it/s] 54%|█████▍    | 14592/26933 [16:09<11:27, 17.96it/s] 54%|█████▍    | 14596/26933 [16:09<11:26, 17.97it/s] 54%|█████▍    | 14600/26933 [16:09<11:24, 18.02it/s] 54%|█████▍    | 14604/26933 [16:09<11:24, 18.01it/s] 54%|█████▍    | 14608/26933 [16:10<11:22, 18.06it/s] 54%|█████▍    | 14612/26933 [16:10<11:22, 18.05it/s] 54%|█████▍    | 14616/26933 [16:10<11:20, 18.10it/s] 54%|█████▍    | 14620/26933 [16:10<11:19, 18.13it/s] 54%|█████▍    | 14624/26933 [16:10<11:20, 18.09it/s] 54%|█████▍    | 14628/26933 [16:11<11:24, 17.99it/s] 54%|█████▍    | 14632/26933 [16:11<11:29, 17.85it/s] 54%|█████▍    | 14636/26933 [16:11<11:29, 17.83it/s] 54%|█████▍    | 14640/26933 [16:11<11:29, 17.83it/s] 54%|█████▍    | 14644/26933 [16:12<11:28, 17.85it/s] 54%|█████▍    | 14648/26933 [16:12<11:27, 17.87it/s] 54%|█████▍    | 14652/26933 [16:12<11:29, 17.80it/s] 54%|█████▍    | 14656/26933 [16:12<11:30, 17.77it/s] 54%|█████▍    | 14660/26933 [16:12<11:33, 17.71it/s] 54%|█████▍    | 14664/26933 [16:13<11:31, 17.73it/s] 54%|█████▍    | 14668/26933 [16:13<11:30, 17.77it/s] 54%|█████▍    | 14672/26933 [16:13<11:29, 17.78it/s] 54%|█████▍    | 14676/26933 [16:13<11:30, 17.75it/s] 55%|█████▍    | 14680/26933 [16:14<11:28, 17.80it/s] 55%|█████▍    | 14684/26933 [16:14<11:27, 17.82it/s] 55%|█████▍    | 14688/26933 [16:14<11:26, 17.84it/s] 55%|█████▍    | 14692/26933 [16:14<11:25, 17.85it/s] 55%|█████▍    | 14696/26933 [16:14<11:26, 17.82it/s] 55%|█████▍    | 14700/26933 [16:15<11:25, 17.84it/s] 55%|█████▍    | 14704/26933 [16:15<11:24, 17.86it/s] 55%|█████▍    | 14708/26933 [16:15<11:23, 17.88it/s] 55%|█████▍    | 14712/26933 [16:15<11:27, 17.79it/s] 55%|█████▍    | 14716/26933 [16:16<11:27, 17.78it/s] 55%|█████▍    | 14720/26933 [16:16<11:26, 17.79it/s] 55%|█████▍    | 14724/26933 [16:16<11:28, 17.74it/s] 55%|█████▍    | 14728/26933 [16:16<11:28, 17.73it/s] 55%|█████▍    | 14732/26933 [16:16<11:27, 17.75it/s] 55%|█████▍    | 14736/26933 [16:17<11:28, 17.72it/s] 55%|█████▍    | 14740/26933 [16:17<11:27, 17.73it/s] 55%|█████▍    | 14744/26933 [16:17<11:28, 17.70it/s] 55%|█████▍    | 14748/26933 [16:17<11:30, 17.65it/s] 55%|█████▍    | 14752/26933 [16:18<11:29, 17.68it/s] 55%|█████▍    | 14756/26933 [16:18<11:28, 17.70it/s] 55%|█████▍    | 14760/26933 [16:18<11:27, 17.70it/s] 55%|█████▍    | 14764/26933 [16:18<11:30, 17.63it/s] 55%|█████▍    | 14768/26933 [16:18<11:28, 17.67it/s] 55%|█████▍    | 14772/26933 [16:19<11:27, 17.68it/s] 55%|█████▍    | 14776/26933 [16:19<11:28, 17.65it/s] 55%|█████▍    | 14780/26933 [16:19<11:27, 17.69it/s] 55%|█████▍    | 14784/26933 [16:19<11:28, 17.65it/s] 55%|█████▍    | 14788/26933 [16:20<11:27, 17.68it/s] 55%|█████▍    | 14792/26933 [16:20<11:34, 17.48it/s] 55%|█████▍    | 14796/26933 [16:20<11:30, 17.58it/s] 55%|█████▍    | 14800/26933 [16:20<11:29, 17.59it/s] 55%|█████▍    | 14804/26933 [16:21<11:28, 17.60it/s] 55%|█████▍    | 14808/26933 [16:21<11:27, 17.64it/s] 55%|█████▍    | 14812/26933 [16:21<11:26, 17.66it/s] 55%|█████▌    | 14816/26933 [16:21<11:26, 17.66it/s] 55%|█████▌    | 14820/26933 [16:21<11:28, 17.61it/s] 55%|█████▌    | 14824/26933 [16:22<11:26, 17.63it/s] 55%|█████▌    | 14828/26933 [16:22<11:25, 17.67it/s] 55%|█████▌    | 14832/26933 [16:22<11:25, 17.66it/s] 55%|█████▌    | 14836/26933 [16:22<11:26, 17.62it/s] 55%|█████▌    | 14840/26933 [16:23<11:24, 17.66it/s] 55%|█████▌    | 14844/26933 [16:23<11:24, 17.66it/s] 55%|█████▌    | 14848/26933 [16:23<11:24, 17.65it/s] 55%|█████▌    | 14852/26933 [16:23<11:25, 17.63it/s] 55%|█████▌    | 14856/26933 [16:23<11:24, 17.65it/s] 55%|█████▌    | 14860/26933 [16:24<11:23, 17.67it/s] 55%|█████▌    | 14864/26933 [16:24<11:23, 17.67it/s] 55%|█████▌    | 14868/26933 [16:24<11:20, 17.72it/s] 55%|█████▌    | 14872/26933 [16:24<11:21, 17.70it/s] 55%|█████▌    | 14876/26933 [16:25<11:19, 17.76it/s] 55%|█████▌    | 14880/26933 [16:25<11:19, 17.75it/s] 55%|█████▌    | 14884/26933 [16:25<11:19, 17.72it/s] 55%|█████▌    | 14888/26933 [16:25<11:21, 17.68it/s] 55%|█████▌    | 14892/26933 [16:26<11:19, 17.71it/s] 55%|█████▌    | 14896/26933 [16:26<11:18, 17.75it/s] 55%|█████▌    | 14900/26933 [16:26<11:16, 17.78it/s] 55%|█████▌    | 14904/26933 [16:26<11:17, 17.74it/s] 55%|█████▌    | 14908/26933 [16:26<11:23, 17.60it/s] 55%|█████▌    | 14912/26933 [16:27<11:23, 17.60it/s] 55%|█████▌    | 14916/26933 [16:27<11:23, 17.59it/s] 55%|█████▌    | 14920/26933 [16:27<11:22, 17.59it/s] 55%|█████▌    | 14924/26933 [16:27<11:22, 17.59it/s] 55%|█████▌    | 14928/26933 [16:28<11:22, 17.58it/s] 55%|█████▌    | 14932/26933 [16:28<11:22, 17.59it/s] 55%|█████▌    | 14936/26933 [16:28<11:22, 17.58it/s] 55%|█████▌    | 14940/26933 [16:28<11:26, 17.47it/s] 55%|█████▌    | 14944/26933 [16:28<11:26, 17.46it/s] 56%|█████▌    | 14948/26933 [16:29<11:26, 17.47it/s] 56%|█████▌    | 14952/26933 [16:29<11:26, 17.46it/s] 56%|█████▌    | 14956/26933 [16:29<11:24, 17.49it/s] 56%|█████▌    | 14960/26933 [16:29<11:21, 17.57it/s] 56%|█████▌    | 14964/26933 [16:30<11:17, 17.65it/s] 56%|█████▌    | 14968/26933 [16:30<11:15, 17.71it/s] 56%|█████▌    | 14972/26933 [16:30<11:13, 17.77it/s] 56%|█████▌    | 14976/26933 [16:30<11:12, 17.77it/s] 56%|█████▌    | 14980/26933 [16:31<11:11, 17.79it/s] 56%|█████▌    | 14984/26933 [16:31<11:11, 17.79it/s] 56%|█████▌    | 14988/26933 [16:31<11:10, 17.81it/s] 56%|█████▌    | 14992/26933 [16:31<11:09, 17.84it/s] 56%|█████▌    | 14996/26933 [16:31<11:10, 17.80it/s] 56%|█████▌    | 15000/26933 [16:32<11:10, 17.79it/s] 56%|█████▌    | 15004/26933 [16:32<11:10, 17.80it/s] 56%|█████▌    | 15008/26933 [16:32<11:10, 17.79it/s] 56%|█████▌    | 15012/26933 [16:32<11:10, 17.78it/s] 56%|█████▌    | 15016/26933 [16:33<11:09, 17.79it/s] 56%|█████▌    | 15020/26933 [16:33<11:09, 17.81it/s] 56%|█████▌    | 15024/26933 [16:33<11:09, 17.80it/s] 56%|█████▌    | 15028/26933 [16:33<11:10, 17.75it/s] 56%|█████▌    | 15032/26933 [16:33<11:11, 17.71it/s] 56%|█████▌    | 15036/26933 [16:34<11:09, 17.76it/s] 56%|█████▌    | 15040/26933 [16:34<11:08, 17.80it/s] 56%|█████▌    | 15044/26933 [16:34<11:07, 17.82it/s] 56%|█████▌    | 15048/26933 [16:34<11:09, 17.75it/s] 56%|█████▌    | 15052/26933 [16:35<11:08, 17.78it/s] 56%|█████▌    | 15056/26933 [16:35<11:07, 17.81it/s] 56%|█████▌    | 15060/26933 [16:35<11:06, 17.81it/s] 56%|█████▌    | 15064/26933 [16:35<11:07, 17.78it/s] 56%|█████▌    | 15068/26933 [16:35<11:07, 17.79it/s] 56%|█████▌    | 15072/26933 [16:36<11:05, 17.81it/s] 56%|█████▌    | 15076/26933 [16:36<11:04, 17.85it/s] 56%|█████▌    | 15080/26933 [16:36<11:04, 17.84it/s] 56%|█████▌    | 15084/26933 [16:36<11:05, 17.80it/s] 56%|█████▌    | 15088/26933 [16:37<11:04, 17.83it/s] 56%|█████▌    | 15092/26933 [16:37<11:04, 17.81it/s] 56%|█████▌    | 15096/26933 [16:37<11:03, 17.83it/s] 56%|█████▌    | 15100/26933 [16:37<11:04, 17.81it/s] 56%|█████▌    | 15104/26933 [16:37<11:04, 17.79it/s] 56%|█████▌    | 15108/26933 [16:38<11:04, 17.80it/s] 56%|█████▌    | 15112/26933 [16:38<11:04, 17.79it/s] 56%|█████▌    | 15116/26933 [16:38<11:03, 17.81it/s] 56%|█████▌    | 15120/26933 [16:38<11:04, 17.77it/s] 56%|█████▌    | 15124/26933 [16:39<11:04, 17.78it/s] 56%|█████▌    | 15128/26933 [16:39<11:03, 17.78it/s] 56%|█████▌    | 15132/26933 [16:39<11:03, 17.79it/s] 56%|█████▌    | 15136/26933 [16:39<11:04, 17.76it/s] 56%|█████▌    | 15140/26933 [16:40<11:02, 17.80it/s] 56%|█████▌    | 15144/26933 [16:40<11:02, 17.79it/s] 56%|█████▌    | 15148/26933 [16:40<11:01, 17.82it/s] 56%|█████▋    | 15152/26933 [16:40<11:00, 17.82it/s] 56%|█████▋    | 15156/26933 [16:40<11:02, 17.78it/s] 56%|█████▋    | 15160/26933 [16:41<10:59, 17.84it/s] 56%|█████▋    | 15164/26933 [16:41<11:00, 17.83it/s] 56%|█████▋    | 15168/26933 [16:41<10:58, 17.87it/s] 56%|█████▋    | 15172/26933 [16:41<11:00, 17.80it/s] 56%|█████▋    | 15176/26933 [16:42<11:01, 17.78it/s] 56%|█████▋    | 15180/26933 [16:42<11:00, 17.79it/s] 56%|█████▋    | 15184/26933 [16:42<11:00, 17.79it/s] 56%|█████▋    | 15188/26933 [16:42<10:58, 17.84it/s] 56%|█████▋    | 15192/26933 [16:42<11:00, 17.77it/s] 56%|█████▋    | 15196/26933 [16:43<11:00, 17.77it/s] 56%|█████▋    | 15200/26933 [16:43<10:59, 17.80it/s] 56%|█████▋    | 15204/26933 [16:43<10:58, 17.81it/s] 56%|█████▋    | 15208/26933 [16:43<11:02, 17.69it/s] 56%|█████▋    | 15212/26933 [16:44<11:02, 17.69it/s] 56%|█████▋    | 15216/26933 [16:44<11:02, 17.68it/s] 57%|█████▋    | 15220/26933 [16:44<11:02, 17.69it/s] 57%|█████▋    | 15224/26933 [16:44<11:02, 17.66it/s] 57%|█████▋    | 15228/26933 [16:44<11:02, 17.67it/s] 57%|█████▋    | 15232/26933 [16:45<11:00, 17.71it/s] 57%|█████▋    | 15236/26933 [16:45<11:00, 17.71it/s] 57%|█████▋    | 15240/26933 [16:45<11:01, 17.68it/s] 57%|█████▋    | 15244/26933 [16:45<11:02, 17.65it/s] 57%|█████▋    | 15248/26933 [16:46<11:04, 17.59it/s] 57%|█████▋    | 15252/26933 [16:46<11:03, 17.61it/s] 57%|█████▋    | 15256/26933 [16:46<11:01, 17.65it/s] 57%|█████▋    | 15260/26933 [16:46<11:01, 17.63it/s] 57%|█████▋    | 15264/26933 [16:46<11:00, 17.66it/s] 57%|█████▋    | 15268/26933 [16:47<10:59, 17.69it/s] 57%|█████▋    | 15272/26933 [16:47<10:59, 17.67it/s] 57%|█████▋    | 15276/26933 [16:47<10:58, 17.70it/s] 57%|█████▋    | 15280/26933 [16:47<10:58, 17.69it/s] 57%|█████▋    | 15284/26933 [16:48<10:57, 17.70it/s] 57%|█████▋    | 15288/26933 [16:48<10:58, 17.68it/s] 57%|█████▋    | 15292/26933 [16:48<10:56, 17.73it/s] 57%|█████▋    | 15296/26933 [16:48<10:58, 17.67it/s] 57%|█████▋    | 15300/26933 [16:49<10:56, 17.72it/s] 57%|█████▋    | 15304/26933 [16:49<10:55, 17.74it/s] 57%|█████▋    | 15308/26933 [16:49<10:54, 17.77it/s] 57%|█████▋    | 15312/26933 [16:49<10:53, 17.78it/s] 57%|█████▋    | 15316/26933 [16:49<10:54, 17.75it/s] 57%|█████▋    | 15320/26933 [16:50<10:53, 17.76it/s] 57%|█████▋    | 15324/26933 [16:50<10:52, 17.79it/s] 57%|█████▋    | 15328/26933 [16:50<10:50, 17.84it/s] 57%|█████▋    | 15332/26933 [16:50<10:52, 17.79it/s] 57%|█████▋    | 15336/26933 [16:51<10:51, 17.81it/s] 57%|█████▋    | 15340/26933 [16:51<10:51, 17.80it/s] 57%|█████▋    | 15344/26933 [16:51<10:50, 17.82it/s] 57%|█████▋    | 15348/26933 [16:51<10:50, 17.81it/s] 57%|█████▋    | 15352/26933 [16:51<10:51, 17.77it/s] 57%|█████▋    | 15356/26933 [16:52<10:51, 17.77it/s] 57%|█████▋    | 15360/26933 [16:52<10:51, 17.77it/s] 57%|█████▋    | 15364/26933 [16:52<10:50, 17.78it/s] 57%|█████▋    | 15368/26933 [16:52<10:52, 17.74it/s] 57%|█████▋    | 15372/26933 [16:53<10:52, 17.73it/s] 57%|█████▋    | 15376/26933 [16:53<10:50, 17.76it/s] 57%|█████▋    | 15380/26933 [16:53<10:49, 17.78it/s] 57%|█████▋    | 15384/26933 [16:53<10:50, 17.76it/s] 57%|█████▋    | 15388/26933 [16:53<10:48, 17.82it/s] 57%|█████▋    | 15392/26933 [16:54<10:49, 17.77it/s] 57%|█████▋    | 15396/26933 [16:54<10:48, 17.78it/s] 57%|█████▋    | 15400/26933 [16:54<10:48, 17.80it/s] 57%|█████▋    | 15404/26933 [16:54<10:49, 17.74it/s] 57%|█████▋    | 15408/26933 [16:55<10:48, 17.77it/s] 57%|█████▋    | 15412/26933 [16:55<10:47, 17.78it/s] 57%|█████▋    | 15416/26933 [16:55<10:48, 17.77it/s] 57%|█████▋    | 15420/26933 [16:55<11:02, 17.39it/s] 57%|█████▋    | 15424/26933 [16:56<10:56, 17.53it/s] 57%|█████▋    | 15428/26933 [16:56<10:54, 17.57it/s] 57%|█████▋    | 15432/26933 [16:56<10:51, 17.64it/s] 57%|█████▋    | 15436/26933 [16:56<10:52, 17.61it/s] 57%|█████▋    | 15440/26933 [16:56<10:53, 17.59it/s] 57%|█████▋    | 15444/26933 [16:57<10:51, 17.64it/s] 57%|█████▋    | 15448/26933 [16:57<10:49, 17.69it/s] 57%|█████▋    | 15452/26933 [16:57<10:47, 17.73it/s] 57%|█████▋    | 15456/26933 [16:57<10:53, 17.55it/s] 57%|█████▋    | 15460/26933 [16:58<10:50, 17.62it/s] 57%|█████▋    | 15464/26933 [16:58<10:50, 17.62it/s] 57%|█████▋    | 15468/26933 [16:58<10:49, 17.64it/s] 57%|█████▋    | 15472/26933 [16:58<10:51, 17.59it/s] 57%|█████▋    | 15476/26933 [16:58<10:49, 17.64it/s] 57%|█████▋    | 15480/26933 [16:59<10:48, 17.66it/s] 57%|█████▋    | 15484/26933 [16:59<10:48, 17.67it/s] 58%|█████▊    | 15488/26933 [16:59<10:47, 17.68it/s] 58%|█████▊    | 15492/26933 [16:59<10:48, 17.63it/s] 58%|█████▊    | 15496/26933 [17:00<10:46, 17.70it/s] 58%|█████▊    | 15500/26933 [17:00<10:45, 17.72it/s] 58%|█████▊    | 15504/26933 [17:00<10:44, 17.74it/s] 58%|█████▊    | 15508/26933 [17:00<10:45, 17.69it/s] 58%|█████▊    | 15512/26933 [17:00<10:44, 17.72it/s] 58%|█████▊    | 15516/26933 [17:01<10:44, 17.73it/s] 58%|█████▊    | 15520/26933 [17:01<10:46, 17.67it/s] 58%|█████▊    | 15524/26933 [17:01<10:45, 17.68it/s] 58%|█████▊    | 15528/26933 [17:01<10:46, 17.64it/s] 58%|█████▊    | 15532/26933 [17:02<10:43, 17.71it/s] 58%|█████▊    | 15536/26933 [17:02<10:43, 17.71it/s] 58%|█████▊    | 15540/26933 [17:02<10:42, 17.74it/s] 58%|█████▊    | 15544/26933 [17:02<10:46, 17.61it/s] 58%|█████▊    | 15548/26933 [17:03<10:45, 17.63it/s] 58%|█████▊    | 15552/26933 [17:03<10:44, 17.67it/s] 58%|█████▊    | 15556/26933 [17:03<10:42, 17.70it/s] 58%|█████▊    | 15560/26933 [17:03<10:42, 17.71it/s] 58%|█████▊    | 15564/26933 [17:03<10:43, 17.66it/s] 58%|█████▊    | 15568/26933 [17:04<10:42, 17.69it/s] 58%|█████▊    | 15572/26933 [17:04<10:41, 17.70it/s] 58%|█████▊    | 15576/26933 [17:04<10:40, 17.73it/s] 58%|█████▊    | 15580/26933 [17:04<10:42, 17.68it/s] 58%|█████▊    | 15584/26933 [17:05<10:42, 17.67it/s] 58%|█████▊    | 15588/26933 [17:05<10:42, 17.65it/s] 58%|█████▊    | 15592/26933 [17:05<10:43, 17.63it/s] 58%|█████▊    | 15596/26933 [17:05<10:45, 17.56it/s] 58%|█████▊    | 15600/26933 [17:05<10:43, 17.62it/s] 58%|█████▊    | 15604/26933 [17:06<10:42, 17.64it/s] 58%|█████▊    | 15608/26933 [17:06<10:41, 17.66it/s] 58%|█████▊    | 15612/26933 [17:06<10:41, 17.65it/s] 58%|█████▊    | 15616/26933 [17:06<10:43, 17.59it/s] 58%|█████▊    | 15620/26933 [17:07<10:42, 17.62it/s] 58%|█████▊    | 15624/26933 [17:07<10:40, 17.64it/s] 58%|█████▊    | 15628/26933 [17:07<10:41, 17.61it/s] 58%|█████▊    | 15632/26933 [17:07<10:45, 17.50it/s] 58%|█████▊    | 15636/26933 [17:08<10:45, 17.49it/s] 58%|█████▊    | 15640/26933 [17:08<10:45, 17.50it/s] 58%|█████▊    | 15644/26933 [17:08<10:43, 17.55it/s] 58%|█████▊    | 15648/26933 [17:08<10:41, 17.60it/s] 58%|█████▊    | 15652/26933 [17:08<10:41, 17.58it/s] 58%|█████▊    | 15656/26933 [17:09<10:39, 17.63it/s] 58%|█████▊    | 15660/26933 [17:09<10:38, 17.66it/s] 58%|█████▊    | 15664/26933 [17:09<10:37, 17.67it/s] 58%|█████▊    | 15668/26933 [17:09<10:38, 17.66it/s] 58%|█████▊    | 15672/26933 [17:10<10:37, 17.66it/s] 58%|█████▊    | 15676/26933 [17:10<10:36, 17.68it/s] 58%|█████▊    | 15680/26933 [17:10<10:48, 17.35it/s] 58%|█████▊    | 15684/26933 [17:10<11:02, 16.98it/s] 58%|█████▊    | 15688/26933 [17:11<11:00, 17.03it/s] 58%|█████▊    | 15692/26933 [17:11<10:55, 17.15it/s] 58%|█████▊    | 15696/26933 [17:11<10:50, 17.27it/s] 58%|█████▊    | 15700/26933 [17:11<10:51, 17.23it/s] 58%|█████▊    | 15704/26933 [17:11<10:50, 17.26it/s] 58%|█████▊    | 15708/26933 [17:12<10:48, 17.31it/s] 58%|█████▊    | 15712/26933 [17:12<10:45, 17.37it/s] 58%|█████▊    | 15716/26933 [17:12<10:45, 17.38it/s] 58%|█████▊    | 15720/26933 [17:12<10:47, 17.32it/s] 58%|█████▊    | 15724/26933 [17:13<10:45, 17.36it/s] 58%|█████▊    | 15728/26933 [17:13<10:44, 17.40it/s] 58%|█████▊    | 15732/26933 [17:13<10:42, 17.43it/s] 58%|█████▊    | 15736/26933 [17:13<10:47, 17.28it/s] 58%|█████▊    | 15740/26933 [17:14<10:47, 17.30it/s] 58%|█████▊    | 15744/26933 [17:14<10:49, 17.24it/s] 58%|█████▊    | 15748/26933 [17:14<10:47, 17.27it/s] 58%|█████▊    | 15752/26933 [17:14<10:45, 17.32it/s] 59%|█████▊    | 15756/26933 [17:14<10:45, 17.32it/s] 59%|█████▊    | 15760/26933 [17:15<10:55, 17.04it/s] 59%|█████▊    | 15764/26933 [17:15<10:48, 17.21it/s] 59%|█████▊    | 15768/26933 [17:15<10:42, 17.38it/s] 59%|█████▊    | 15772/26933 [17:15<10:41, 17.40it/s] 59%|█████▊    | 15776/26933 [17:16<10:38, 17.47it/s] 59%|█████▊    | 15780/26933 [17:16<10:34, 17.59it/s] 59%|█████▊    | 15784/26933 [17:16<10:30, 17.69it/s] 59%|█████▊    | 15788/26933 [17:16<10:29, 17.71it/s] 59%|█████▊    | 15792/26933 [17:16<10:27, 17.76it/s] 59%|█████▊    | 15796/26933 [17:17<10:26, 17.79it/s] 59%|█████▊    | 15800/26933 [17:17<10:26, 17.77it/s] 59%|█████▊    | 15804/26933 [17:17<10:26, 17.77it/s] 59%|█████▊    | 15808/26933 [17:17<10:26, 17.75it/s] 59%|█████▊    | 15812/26933 [17:18<10:26, 17.75it/s] 59%|█████▊    | 15816/26933 [17:18<10:26, 17.75it/s] 59%|█████▊    | 15820/26933 [17:18<10:26, 17.73it/s] 59%|█████▉    | 15824/26933 [17:18<10:26, 17.73it/s] 59%|█████▉    | 15828/26933 [17:19<10:26, 17.72it/s] 59%|█████▉    | 15832/26933 [17:19<10:25, 17.75it/s] 59%|█████▉    | 15836/26933 [17:19<10:24, 17.78it/s] 59%|█████▉    | 15840/26933 [17:19<10:23, 17.79it/s] 59%|█████▉    | 15844/26933 [17:19<10:23, 17.77it/s] 59%|█████▉    | 15848/26933 [17:20<10:23, 17.77it/s] 59%|█████▉    | 15852/26933 [17:20<10:22, 17.79it/s] 59%|█████▉    | 15856/26933 [17:20<10:22, 17.79it/s] 59%|█████▉    | 15860/26933 [17:20<10:22, 17.78it/s] 59%|█████▉    | 15864/26933 [17:21<10:22, 17.79it/s] 59%|█████▉    | 15868/26933 [17:21<10:22, 17.76it/s] 59%|█████▉    | 15872/26933 [17:21<10:22, 17.75it/s] 59%|█████▉    | 15876/26933 [17:21<10:23, 17.74it/s] 59%|█████▉    | 15880/26933 [17:21<10:24, 17.71it/s] 59%|█████▉    | 15884/26933 [17:22<10:23, 17.73it/s] 59%|█████▉    | 15888/26933 [17:22<10:23, 17.73it/s] 59%|█████▉    | 15892/26933 [17:22<10:22, 17.73it/s] 59%|█████▉    | 15896/26933 [17:22<10:25, 17.64it/s] 59%|█████▉    | 15900/26933 [17:23<10:24, 17.67it/s] 59%|█████▉    | 15904/26933 [17:23<10:25, 17.64it/s] 59%|█████▉    | 15908/26933 [17:23<10:23, 17.68it/s] 59%|█████▉    | 15912/26933 [17:23<10:22, 17.69it/s] 59%|█████▉    | 15916/26933 [17:23<10:21, 17.72it/s] 59%|█████▉    | 15920/26933 [17:24<10:20, 17.75it/s] 59%|█████▉    | 15924/26933 [17:24<10:19, 17.76it/s] 59%|█████▉    | 15928/26933 [17:24<10:19, 17.76it/s] 59%|█████▉    | 15932/26933 [17:24<10:21, 17.70it/s] 59%|█████▉    | 15936/26933 [17:25<10:19, 17.76it/s] 59%|█████▉    | 15940/26933 [17:25<10:18, 17.76it/s] 59%|█████▉    | 15944/26933 [17:25<10:17, 17.79it/s] 59%|█████▉    | 15948/26933 [17:25<10:24, 17.60it/s] 59%|█████▉    | 15952/26933 [17:26<10:21, 17.67it/s] 59%|█████▉    | 15956/26933 [17:26<10:22, 17.62it/s] 59%|█████▉    | 15960/26933 [17:26<10:24, 17.58it/s] 59%|█████▉    | 15964/26933 [17:26<10:23, 17.60it/s] 59%|█████▉    | 15968/26933 [17:26<10:30, 17.38it/s] 59%|█████▉    | 15972/26933 [17:27<10:26, 17.50it/s] 59%|█████▉    | 15976/26933 [17:27<10:25, 17.51it/s] 59%|█████▉    | 15980/26933 [17:27<10:23, 17.58it/s] 59%|█████▉    | 15984/26933 [17:27<10:25, 17.50it/s] 59%|█████▉    | 15988/26933 [17:28<10:21, 17.62it/s] 59%|█████▉    | 15992/26933 [17:28<10:19, 17.66it/s] 59%|█████▉    | 15996/26933 [17:28<10:18, 17.68it/s] 59%|█████▉    | 16000/26933 [17:28<10:22, 17.56it/s] 59%|█████▉    | 16004/26933 [17:28<10:19, 17.64it/s] 59%|█████▉    | 16008/26933 [17:29<10:17, 17.69it/s] 59%|█████▉    | 16012/26933 [17:29<10:17, 17.68it/s] 59%|█████▉    | 16016/26933 [17:29<10:17, 17.69it/s] 59%|█████▉    | 16020/26933 [17:29<10:20, 17.58it/s] 59%|█████▉    | 16024/26933 [17:30<10:18, 17.64it/s] 60%|█████▉    | 16028/26933 [17:30<10:17, 17.67it/s] 60%|█████▉    | 16032/26933 [17:30<10:16, 17.68it/s] 60%|█████▉    | 16036/26933 [17:30<10:18, 17.61it/s] 60%|█████▉    | 16040/26933 [17:31<10:16, 17.68it/s] 60%|█████▉    | 16044/26933 [17:31<10:14, 17.72it/s] 60%|█████▉    | 16048/26933 [17:31<10:13, 17.73it/s] 60%|█████▉    | 16052/26933 [17:31<10:11, 17.78it/s] 60%|█████▉    | 16056/26933 [17:31<10:15, 17.68it/s] 60%|█████▉    | 16060/26933 [17:32<10:14, 17.70it/s] 60%|█████▉    | 16064/26933 [17:32<10:13, 17.73it/s] 60%|█████▉    | 16068/26933 [17:32<10:11, 17.76it/s] 60%|█████▉    | 16072/26933 [17:32<10:14, 17.67it/s] 60%|█████▉    | 16076/26933 [17:33<10:13, 17.68it/s] 60%|█████▉    | 16080/26933 [17:33<10:12, 17.71it/s] 60%|█████▉    | 16084/26933 [17:33<10:13, 17.69it/s] 60%|█████▉    | 16088/26933 [17:33<10:12, 17.70it/s] 60%|█████▉    | 16092/26933 [17:33<10:16, 17.58it/s] 60%|█████▉    | 16096/26933 [17:34<10:14, 17.64it/s] 60%|█████▉    | 16100/26933 [17:34<10:14, 17.64it/s] 60%|█████▉    | 16104/26933 [17:34<10:23, 17.36it/s] 60%|█████▉    | 16108/26933 [17:34<10:23, 17.36it/s] 60%|█████▉    | 16112/26933 [17:35<10:18, 17.49it/s] 60%|█████▉    | 16116/26933 [17:35<10:15, 17.59it/s] 60%|█████▉    | 16120/26933 [17:35<10:11, 17.68it/s] 60%|█████▉    | 16124/26933 [17:35<10:18, 17.49it/s] 60%|█████▉    | 16128/26933 [17:36<10:16, 17.54it/s] 60%|█████▉    | 16132/26933 [17:36<10:13, 17.60it/s] 60%|█████▉    | 16136/26933 [17:36<10:11, 17.67it/s] 60%|█████▉    | 16140/26933 [17:36<10:11, 17.64it/s] 60%|█████▉    | 16144/26933 [17:36<10:13, 17.59it/s] 60%|█████▉    | 16148/26933 [17:37<10:10, 17.66it/s] 60%|█████▉    | 16152/26933 [17:37<10:10, 17.66it/s] 60%|█████▉    | 16156/26933 [17:37<10:10, 17.66it/s] 60%|██████    | 16160/26933 [17:37<10:15, 17.51it/s] 60%|██████    | 16164/26933 [17:38<10:13, 17.56it/s] 60%|██████    | 16168/26933 [17:38<10:11, 17.60it/s] 60%|██████    | 16172/26933 [17:38<10:10, 17.62it/s] 60%|██████    | 16176/26933 [17:38<10:11, 17.60it/s] 60%|██████    | 16180/26933 [17:38<10:13, 17.53it/s] 60%|██████    | 16184/26933 [17:39<10:10, 17.61it/s] 60%|██████    | 16188/26933 [17:39<10:08, 17.66it/s] 60%|██████    | 16192/26933 [17:39<10:06, 17.70it/s] 60%|██████    | 16196/26933 [17:39<10:10, 17.60it/s] 60%|██████    | 16200/26933 [17:40<10:07, 17.66it/s] 60%|██████    | 16204/26933 [17:40<10:05, 17.72it/s] 60%|██████    | 16208/26933 [17:40<10:03, 17.77it/s] 60%|██████    | 16212/26933 [17:40<10:06, 17.69it/s] 60%|██████    | 16216/26933 [17:40<10:03, 17.75it/s] 60%|██████    | 16220/26933 [17:41<10:01, 17.82it/s] 60%|██████    | 16224/26933 [17:41<10:02, 17.78it/s] 60%|██████    | 16228/26933 [17:41<10:01, 17.80it/s] 60%|██████    | 16232/26933 [17:41<10:05, 17.67it/s] 60%|██████    | 16236/26933 [17:42<10:04, 17.69it/s] 60%|██████    | 16240/26933 [17:42<10:04, 17.70it/s] 60%|██████    | 16244/26933 [17:42<10:03, 17.71it/s] 60%|██████    | 16248/26933 [17:42<10:06, 17.60it/s] 60%|██████    | 16252/26933 [17:43<10:04, 17.67it/s] 60%|██████    | 16256/26933 [17:43<10:04, 17.66it/s] 60%|██████    | 16260/26933 [17:43<10:03, 17.68it/s] 60%|██████    | 16264/26933 [17:43<10:00, 17.75it/s] 60%|██████    | 16268/26933 [17:43<10:02, 17.71it/s] 60%|██████    | 16272/26933 [17:44<10:01, 17.72it/s] 60%|██████    | 16276/26933 [17:44<10:01, 17.72it/s] 60%|██████    | 16280/26933 [17:44<10:00, 17.74it/s] 60%|██████    | 16284/26933 [17:44<10:04, 17.60it/s] 60%|██████    | 16288/26933 [17:45<10:02, 17.67it/s] 60%|██████    | 16292/26933 [17:45<10:00, 17.72it/s] 61%|██████    | 16296/26933 [17:45<09:59, 17.73it/s] 61%|██████    | 16300/26933 [17:45<09:58, 17.78it/s] 61%|██████    | 16304/26933 [17:45<10:03, 17.60it/s] 61%|██████    | 16308/26933 [17:46<10:01, 17.66it/s] 61%|██████    | 16312/26933 [17:46<09:59, 17.70it/s] 61%|██████    | 16316/26933 [17:46<09:59, 17.71it/s] 61%|██████    | 16320/26933 [17:46<10:03, 17.58it/s] 61%|██████    | 16324/26933 [17:47<10:02, 17.60it/s] 61%|██████    | 16328/26933 [17:47<10:06, 17.49it/s] 61%|██████    | 16332/26933 [17:47<10:12, 17.31it/s] 61%|██████    | 16336/26933 [17:47<10:13, 17.28it/s] 61%|██████    | 16340/26933 [17:48<10:08, 17.40it/s] 61%|██████    | 16344/26933 [17:48<10:05, 17.48it/s] 61%|██████    | 16348/26933 [17:48<10:03, 17.54it/s] 61%|██████    | 16352/26933 [17:48<10:01, 17.59it/s] 61%|██████    | 16356/26933 [17:48<10:03, 17.54it/s] 61%|██████    | 16360/26933 [17:49<10:00, 17.61it/s] 61%|██████    | 16364/26933 [17:49<09:59, 17.64it/s] 61%|██████    | 16368/26933 [17:49<09:58, 17.65it/s] 61%|██████    | 16372/26933 [17:49<10:00, 17.60it/s] 61%|██████    | 16376/26933 [17:50<09:58, 17.64it/s] 61%|██████    | 16380/26933 [17:50<09:57, 17.67it/s] 61%|██████    | 16384/26933 [17:50<09:56, 17.68it/s] 61%|██████    | 16388/26933 [17:50<09:58, 17.62it/s] 61%|██████    | 16392/26933 [17:50<09:57, 17.65it/s] 61%|██████    | 16396/26933 [17:51<09:57, 17.62it/s] 61%|██████    | 16400/26933 [17:51<10:00, 17.55it/s] 61%|██████    | 16404/26933 [17:51<09:58, 17.60it/s] 61%|██████    | 16408/26933 [17:51<10:01, 17.51it/s] 61%|██████    | 16412/26933 [17:52<09:58, 17.59it/s] 61%|██████    | 16416/26933 [17:52<09:56, 17.63it/s] 61%|██████    | 16420/26933 [17:52<09:54, 17.69it/s] 61%|██████    | 16424/26933 [17:52<09:58, 17.56it/s] 61%|██████    | 16428/26933 [17:53<09:55, 17.63it/s] 61%|██████    | 16432/26933 [17:53<09:54, 17.65it/s] 61%|██████    | 16436/26933 [17:53<09:54, 17.67it/s] 61%|██████    | 16440/26933 [17:53<09:52, 17.70it/s] 61%|██████    | 16444/26933 [17:53<09:54, 17.64it/s] 61%|██████    | 16448/26933 [17:54<09:52, 17.71it/s] 61%|██████    | 16452/26933 [17:54<09:50, 17.74it/s] 61%|██████    | 16456/26933 [17:54<09:49, 17.77it/s] 61%|██████    | 16460/26933 [17:54<09:52, 17.69it/s] 61%|██████    | 16464/26933 [17:55<09:50, 17.74it/s] 61%|██████    | 16468/26933 [17:55<09:49, 17.76it/s] 61%|██████    | 16472/26933 [17:55<09:48, 17.76it/s] 61%|██████    | 16476/26933 [17:55<09:46, 17.82it/s] 61%|██████    | 16480/26933 [17:55<09:50, 17.69it/s] 61%|██████    | 16484/26933 [17:56<09:48, 17.75it/s] 61%|██████    | 16488/26933 [17:56<09:47, 17.78it/s] 61%|██████    | 16492/26933 [17:56<09:45, 17.82it/s] 61%|██████    | 16496/26933 [17:56<09:48, 17.73it/s] 61%|██████▏   | 16500/26933 [17:57<09:47, 17.76it/s] 61%|██████▏   | 16504/26933 [17:57<09:46, 17.79it/s] 61%|██████▏   | 16508/26933 [17:57<09:44, 17.82it/s] 61%|██████▏   | 16512/26933 [17:57<09:47, 17.73it/s] 61%|██████▏   | 16516/26933 [17:57<09:45, 17.80it/s] 61%|██████▏   | 16520/26933 [17:58<09:44, 17.81it/s] 61%|██████▏   | 16524/26933 [17:58<09:43, 17.83it/s] 61%|██████▏   | 16528/26933 [17:58<09:43, 17.83it/s] 61%|██████▏   | 16532/26933 [17:58<09:47, 17.71it/s] 61%|██████▏   | 16536/26933 [17:59<09:46, 17.73it/s] 61%|██████▏   | 16540/26933 [17:59<09:45, 17.76it/s] 61%|██████▏   | 16544/26933 [17:59<09:44, 17.79it/s] 61%|██████▏   | 16548/26933 [17:59<09:47, 17.68it/s] 61%|██████▏   | 16552/26933 [17:59<09:46, 17.71it/s] 61%|██████▏   | 16556/26933 [18:00<09:45, 17.74it/s] 61%|██████▏   | 16560/26933 [18:00<09:44, 17.74it/s] 62%|██████▏   | 16564/26933 [18:00<09:44, 17.75it/s] 62%|██████▏   | 16568/26933 [18:00<09:46, 17.68it/s] 62%|██████▏   | 16572/26933 [18:01<09:43, 17.76it/s] 62%|██████▏   | 16576/26933 [18:01<09:42, 17.77it/s] 62%|██████▏   | 16580/26933 [18:01<09:42, 17.78it/s] 62%|██████▏   | 16584/26933 [18:01<09:44, 17.69it/s] 62%|██████▏   | 16588/26933 [18:02<09:44, 17.70it/s] 62%|██████▏   | 16592/26933 [18:02<09:42, 17.76it/s] 62%|██████▏   | 16596/26933 [18:02<09:40, 17.81it/s] 62%|██████▏   | 16600/26933 [18:02<09:41, 17.78it/s] 62%|██████▏   | 16604/26933 [18:02<09:44, 17.68it/s] 62%|██████▏   | 16608/26933 [18:03<09:42, 17.72it/s] 62%|██████▏   | 16612/26933 [18:03<09:41, 17.75it/s] 62%|██████▏   | 16616/26933 [18:03<09:40, 17.77it/s] 62%|██████▏   | 16620/26933 [18:03<09:41, 17.72it/s] 62%|██████▏   | 16624/26933 [18:04<09:41, 17.73it/s] 62%|██████▏   | 16628/26933 [18:04<09:39, 17.78it/s] 62%|██████▏   | 16632/26933 [18:04<09:38, 17.79it/s] 62%|██████▏   | 16636/26933 [18:04<09:37, 17.83it/s] 62%|██████▏   | 16640/26933 [18:04<09:39, 17.77it/s] 62%|██████▏   | 16644/26933 [18:05<09:43, 17.62it/s] 62%|██████▏   | 16648/26933 [18:05<09:42, 17.67it/s] 62%|██████▏   | 16652/26933 [18:05<09:39, 17.74it/s] 62%|██████▏   | 16656/26933 [18:05<09:39, 17.73it/s] 62%|██████▏   | 16660/26933 [18:06<09:34, 17.89it/s] 62%|██████▏   | 16664/26933 [18:06<09:30, 17.99it/s] 62%|██████▏   | 16668/26933 [18:06<09:28, 18.06it/s] 62%|██████▏   | 16672/26933 [18:06<09:29, 18.01it/s] 62%|██████▏   | 16676/26933 [18:06<09:27, 18.08it/s] 62%|██████▏   | 16680/26933 [18:07<09:25, 18.12it/s] 62%|██████▏   | 16684/26933 [18:07<09:24, 18.17it/s] 62%|██████▏   | 16688/26933 [18:07<09:23, 18.17it/s] 62%|██████▏   | 16692/26933 [18:07<09:24, 18.13it/s] 62%|██████▏   | 16696/26933 [18:08<09:22, 18.19it/s] 62%|██████▏   | 16700/26933 [18:08<09:22, 18.20it/s] 62%|██████▏   | 16704/26933 [18:08<09:20, 18.24it/s] 62%|██████▏   | 16708/26933 [18:08<09:20, 18.25it/s] 62%|██████▏   | 16712/26933 [18:08<09:22, 18.16it/s] 62%|██████▏   | 16716/26933 [18:09<09:22, 18.17it/s] 62%|██████▏   | 16720/26933 [18:09<09:23, 18.11it/s] 62%|██████▏   | 16724/26933 [18:09<09:23, 18.12it/s] 62%|██████▏   | 16728/26933 [18:09<09:28, 17.95it/s] 62%|██████▏   | 16732/26933 [18:10<09:31, 17.85it/s] 62%|██████▏   | 16736/26933 [18:10<09:32, 17.82it/s] 62%|██████▏   | 16740/26933 [18:10<09:32, 17.80it/s] 62%|██████▏   | 16744/26933 [18:10<09:33, 17.77it/s] 62%|██████▏   | 16748/26933 [18:10<09:36, 17.68it/s] 62%|██████▏   | 16752/26933 [18:11<09:36, 17.66it/s] 62%|██████▏   | 16756/26933 [18:11<09:35, 17.69it/s] 62%|██████▏   | 16760/26933 [18:11<09:34, 17.71it/s] 62%|██████▏   | 16764/26933 [18:11<09:37, 17.61it/s] 62%|██████▏   | 16768/26933 [18:12<09:36, 17.62it/s] 62%|██████▏   | 16772/26933 [18:12<09:36, 17.64it/s] 62%|██████▏   | 16776/26933 [18:12<09:36, 17.63it/s] 62%|██████▏   | 16780/26933 [18:12<09:39, 17.53it/s] 62%|██████▏   | 16784/26933 [18:13<09:39, 17.52it/s] 62%|██████▏   | 16788/26933 [18:13<09:38, 17.55it/s] 62%|██████▏   | 16792/26933 [18:13<09:37, 17.57it/s] 62%|██████▏   | 16796/26933 [18:13<09:36, 17.59it/s] 62%|██████▏   | 16800/26933 [18:13<09:37, 17.53it/s] 62%|██████▏   | 16804/26933 [18:14<09:35, 17.60it/s] 62%|██████▏   | 16808/26933 [18:14<09:34, 17.62it/s] 62%|██████▏   | 16812/26933 [18:14<09:32, 17.69it/s] 62%|██████▏   | 16816/26933 [18:14<09:34, 17.62it/s] 62%|██████▏   | 16820/26933 [18:15<09:33, 17.64it/s] 62%|██████▏   | 16824/26933 [18:15<09:32, 17.67it/s] 62%|██████▏   | 16828/26933 [18:15<09:30, 17.70it/s] 62%|██████▏   | 16832/26933 [18:15<09:29, 17.74it/s] 63%|██████▎   | 16836/26933 [18:15<09:32, 17.63it/s] 63%|██████▎   | 16840/26933 [18:16<09:32, 17.63it/s] 63%|██████▎   | 16844/26933 [18:16<09:32, 17.61it/s] 63%|██████▎   | 16848/26933 [18:16<09:33, 17.60it/s] 63%|██████▎   | 16852/26933 [18:16<09:36, 17.50it/s] 63%|██████▎   | 16856/26933 [18:17<09:35, 17.50it/s] 63%|██████▎   | 16860/26933 [18:17<09:35, 17.51it/s] 63%|██████▎   | 16864/26933 [18:17<09:33, 17.57it/s] 63%|██████▎   | 16868/26933 [18:17<09:34, 17.51it/s] 63%|██████▎   | 16872/26933 [18:18<09:32, 17.57it/s] 63%|██████▎   | 16876/26933 [18:18<09:30, 17.62it/s] 63%|██████▎   | 16880/26933 [18:18<09:29, 17.64it/s] 63%|██████▎   | 16884/26933 [18:18<09:28, 17.68it/s] 63%|██████▎   | 16888/26933 [18:18<09:29, 17.63it/s] 63%|██████▎   | 16892/26933 [18:19<09:27, 17.70it/s] 63%|██████▎   | 16896/26933 [18:19<09:26, 17.71it/s] 63%|██████▎   | 16900/26933 [18:19<09:26, 17.71it/s] 63%|██████▎   | 16904/26933 [18:19<09:29, 17.62it/s] 63%|██████▎   | 16908/26933 [18:20<09:27, 17.66it/s] 63%|██████▎   | 16912/26933 [18:20<09:27, 17.65it/s] 63%|██████▎   | 16916/26933 [18:20<09:27, 17.65it/s] 63%|██████▎   | 16920/26933 [18:20<09:27, 17.64it/s] 63%|██████▎   | 16924/26933 [18:20<09:30, 17.56it/s] 63%|██████▎   | 16928/26933 [18:21<09:28, 17.59it/s] 63%|██████▎   | 16932/26933 [18:21<09:26, 17.64it/s] 63%|██████▎   | 16936/26933 [18:21<09:26, 17.65it/s] 63%|██████▎   | 16940/26933 [18:21<09:28, 17.59it/s] 63%|██████▎   | 16944/26933 [18:22<09:25, 17.65it/s] 63%|██████▎   | 16948/26933 [18:22<09:24, 17.67it/s] 63%|██████▎   | 16952/26933 [18:22<09:23, 17.70it/s] 63%|██████▎   | 16956/26933 [18:22<09:24, 17.66it/s] 63%|██████▎   | 16960/26933 [18:22<09:23, 17.70it/s] 63%|██████▎   | 16964/26933 [18:23<09:23, 17.70it/s] 63%|██████▎   | 16968/26933 [18:23<09:23, 17.69it/s] 63%|██████▎   | 16972/26933 [18:23<09:21, 17.73it/s] 63%|██████▎   | 16976/26933 [18:23<09:24, 17.64it/s] 63%|██████▎   | 16980/26933 [18:24<09:23, 17.67it/s] 63%|██████▎   | 16984/26933 [18:24<09:23, 17.65it/s] 63%|██████▎   | 16988/26933 [18:24<09:23, 17.65it/s] 63%|██████▎   | 16992/26933 [18:24<09:25, 17.58it/s] 63%|██████▎   | 16996/26933 [18:25<09:24, 17.61it/s] 63%|██████▎   | 17000/26933 [18:25<09:22, 17.65it/s] 63%|██████▎   | 17004/26933 [18:25<09:21, 17.68it/s] 63%|██████▎   | 17008/26933 [18:25<09:20, 17.70it/s] 63%|██████▎   | 17012/26933 [18:25<09:21, 17.67it/s] 63%|██████▎   | 17016/26933 [18:26<09:20, 17.70it/s] 63%|██████▎   | 17020/26933 [18:26<09:19, 17.71it/s] 63%|██████▎   | 17024/26933 [18:26<09:19, 17.71it/s] 63%|██████▎   | 17028/26933 [18:26<09:21, 17.63it/s] 63%|██████▎   | 17032/26933 [18:27<09:19, 17.69it/s] 63%|██████▎   | 17036/26933 [18:27<09:19, 17.70it/s] 63%|██████▎   | 17040/26933 [18:27<09:17, 17.73it/s] 63%|██████▎   | 17044/26933 [18:27<09:18, 17.72it/s] 63%|██████▎   | 17048/26933 [18:27<09:17, 17.75it/s] 63%|██████▎   | 17052/26933 [18:28<09:16, 17.75it/s] 63%|██████▎   | 17056/26933 [18:28<09:16, 17.74it/s] 63%|██████▎   | 17060/26933 [18:28<09:16, 17.76it/s] 63%|██████▎   | 17064/26933 [18:28<09:17, 17.69it/s] 63%|██████▎   | 17068/26933 [18:29<09:16, 17.72it/s] 63%|██████▎   | 17072/26933 [18:29<09:15, 17.74it/s] 63%|██████▎   | 17076/26933 [18:29<09:14, 17.78it/s] 63%|██████▎   | 17080/26933 [18:29<09:15, 17.73it/s] 63%|██████▎   | 17084/26933 [18:29<09:14, 17.75it/s] 63%|██████▎   | 17088/26933 [18:30<09:15, 17.74it/s] 63%|██████▎   | 17092/26933 [18:30<09:13, 17.77it/s] 63%|██████▎   | 17096/26933 [18:30<09:12, 17.82it/s] 63%|██████▎   | 17100/26933 [18:30<09:14, 17.73it/s] 64%|██████▎   | 17104/26933 [18:31<09:14, 17.74it/s] 64%|██████▎   | 17108/26933 [18:31<09:14, 17.73it/s] 64%|██████▎   | 17112/26933 [18:31<09:12, 17.77it/s] 64%|██████▎   | 17116/26933 [18:31<09:13, 17.72it/s] 64%|██████▎   | 17120/26933 [18:32<09:13, 17.72it/s] 64%|██████▎   | 17124/26933 [18:32<09:13, 17.73it/s] 64%|██████▎   | 17128/26933 [18:32<09:12, 17.75it/s] 64%|██████▎   | 17132/26933 [18:32<09:11, 17.76it/s] 64%|██████▎   | 17136/26933 [18:32<09:14, 17.65it/s] 64%|██████▎   | 17140/26933 [18:33<09:14, 17.67it/s] 64%|██████▎   | 17144/26933 [18:33<09:13, 17.68it/s] 64%|██████▎   | 17148/26933 [18:33<09:12, 17.72it/s] 64%|██████▎   | 17152/26933 [18:33<09:13, 17.66it/s] 64%|██████▎   | 17156/26933 [18:34<09:12, 17.69it/s] 64%|██████▎   | 17160/26933 [18:34<09:11, 17.71it/s] 64%|██████▎   | 17164/26933 [18:34<09:11, 17.72it/s] 64%|██████▎   | 17168/26933 [18:34<09:11, 17.71it/s] 64%|██████▍   | 17172/26933 [18:34<09:10, 17.73it/s] 64%|██████▍   | 17176/26933 [18:35<09:10, 17.73it/s] 64%|██████▍   | 17180/26933 [18:35<09:10, 17.73it/s] 64%|██████▍   | 17184/26933 [18:35<09:10, 17.72it/s] 64%|██████▍   | 17188/26933 [18:35<09:11, 17.65it/s] 64%|██████▍   | 17192/26933 [18:36<09:11, 17.66it/s] 64%|██████▍   | 17196/26933 [18:36<09:10, 17.68it/s] 64%|██████▍   | 17200/26933 [18:36<09:11, 17.65it/s] 64%|██████▍   | 17204/26933 [18:36<09:12, 17.62it/s] 64%|██████▍   | 17208/26933 [18:36<09:11, 17.65it/s] 64%|██████▍   | 17212/26933 [18:37<09:09, 17.69it/s] 64%|██████▍   | 17216/26933 [18:37<09:09, 17.67it/s] 64%|██████▍   | 17220/26933 [18:37<09:08, 17.71it/s] 64%|██████▍   | 17224/26933 [18:37<09:08, 17.69it/s] 64%|██████▍   | 17228/26933 [18:38<09:07, 17.73it/s] 64%|██████▍   | 17232/26933 [18:38<09:06, 17.74it/s] 64%|██████▍   | 17236/26933 [18:38<09:05, 17.79it/s] 64%|██████▍   | 17240/26933 [18:38<09:04, 17.81it/s] 64%|██████▍   | 17244/26933 [18:39<09:03, 17.84it/s] 64%|██████▍   | 17248/26933 [18:39<09:02, 17.84it/s] 64%|██████▍   | 17252/26933 [18:39<09:01, 17.87it/s] 64%|██████▍   | 17256/26933 [18:39<09:01, 17.86it/s] 64%|██████▍   | 17260/26933 [18:39<09:05, 17.73it/s] 64%|██████▍   | 17264/26933 [18:40<09:02, 17.81it/s] 64%|██████▍   | 17268/26933 [18:40<09:01, 17.85it/s] 64%|██████▍   | 17272/26933 [18:40<09:00, 17.87it/s] 64%|██████▍   | 17276/26933 [18:40<09:01, 17.84it/s] 64%|██████▍   | 17280/26933 [18:41<09:00, 17.86it/s] 64%|██████▍   | 17284/26933 [18:41<09:01, 17.83it/s] 64%|██████▍   | 17288/26933 [18:41<08:59, 17.87it/s] 64%|██████▍   | 17292/26933 [18:41<08:58, 17.89it/s] 64%|██████▍   | 17296/26933 [18:41<09:00, 17.83it/s] 64%|██████▍   | 17300/26933 [18:42<08:58, 17.88it/s] 64%|██████▍   | 17304/26933 [18:42<08:59, 17.86it/s] 64%|██████▍   | 17308/26933 [18:42<08:58, 17.88it/s] 64%|██████▍   | 17312/26933 [18:42<08:59, 17.83it/s] 64%|██████▍   | 17316/26933 [18:43<08:59, 17.84it/s] 64%|██████▍   | 17320/26933 [18:43<08:58, 17.84it/s] 64%|██████▍   | 17324/26933 [18:43<08:56, 17.90it/s] 64%|██████▍   | 17328/26933 [18:43<08:56, 17.90it/s] 64%|██████▍   | 17332/26933 [18:43<08:58, 17.83it/s] 64%|██████▍   | 17336/26933 [18:44<08:57, 17.85it/s] 64%|██████▍   | 17340/26933 [18:44<08:57, 17.83it/s] 64%|██████▍   | 17344/26933 [18:44<08:57, 17.85it/s] 64%|██████▍   | 17348/26933 [18:44<08:57, 17.84it/s] 64%|██████▍   | 17352/26933 [18:45<08:56, 17.86it/s] 64%|██████▍   | 17356/26933 [18:45<08:56, 17.85it/s] 64%|██████▍   | 17360/26933 [18:45<08:55, 17.88it/s] 64%|██████▍   | 17364/26933 [18:45<08:56, 17.85it/s] 64%|██████▍   | 17368/26933 [18:45<08:56, 17.84it/s] 65%|██████▍   | 17372/26933 [18:46<08:56, 17.82it/s] 65%|██████▍   | 17376/26933 [18:46<08:57, 17.77it/s] 65%|██████▍   | 17380/26933 [18:46<08:57, 17.76it/s] 65%|██████▍   | 17384/26933 [18:46<08:58, 17.74it/s] 65%|██████▍   | 17388/26933 [18:47<08:57, 17.76it/s] 65%|██████▍   | 17392/26933 [18:47<08:58, 17.73it/s] 65%|██████▍   | 17396/26933 [18:47<08:57, 17.76it/s] 65%|██████▍   | 17400/26933 [18:47<08:59, 17.68it/s] 65%|██████▍   | 17404/26933 [18:47<08:57, 17.73it/s] 65%|██████▍   | 17408/26933 [18:48<08:57, 17.72it/s] 65%|██████▍   | 17412/26933 [18:48<08:56, 17.75it/s] 65%|██████▍   | 17416/26933 [18:48<08:57, 17.72it/s] 65%|██████▍   | 17420/26933 [18:48<08:59, 17.64it/s] 65%|██████▍   | 17424/26933 [18:49<08:58, 17.65it/s] 65%|██████▍   | 17428/26933 [18:49<08:57, 17.68it/s] 65%|██████▍   | 17432/26933 [18:49<08:57, 17.66it/s] 65%|██████▍   | 17436/26933 [18:49<08:58, 17.65it/s] 65%|██████▍   | 17440/26933 [18:50<08:57, 17.65it/s] 65%|██████▍   | 17444/26933 [18:50<08:56, 17.67it/s] 65%|██████▍   | 17448/26933 [18:50<08:55, 17.70it/s] 65%|██████▍   | 17452/26933 [18:50<08:55, 17.72it/s] 65%|██████▍   | 17456/26933 [18:50<08:55, 17.68it/s] 65%|██████▍   | 17460/26933 [18:51<08:55, 17.69it/s] 65%|██████▍   | 17464/26933 [18:51<08:55, 17.69it/s] 65%|██████▍   | 17468/26933 [18:51<08:55, 17.67it/s] 65%|██████▍   | 17472/26933 [18:51<08:57, 17.59it/s] 65%|██████▍   | 17476/26933 [18:52<08:56, 17.64it/s] 65%|██████▍   | 17480/26933 [18:52<08:55, 17.66it/s] 65%|██████▍   | 17484/26933 [18:52<08:54, 17.68it/s] 65%|██████▍   | 17488/26933 [18:52<08:55, 17.64it/s] 65%|██████▍   | 17492/26933 [18:52<08:54, 17.66it/s] 65%|██████▍   | 17496/26933 [18:53<08:53, 17.68it/s] 65%|██████▍   | 17500/26933 [18:53<08:53, 17.68it/s] 65%|██████▍   | 17504/26933 [18:53<08:53, 17.66it/s] 65%|██████▌   | 17508/26933 [18:53<08:53, 17.65it/s] 65%|██████▌   | 17512/26933 [18:54<08:53, 17.67it/s] 65%|██████▌   | 17516/26933 [18:54<08:54, 17.62it/s] 65%|██████▌   | 17520/26933 [18:54<08:54, 17.61it/s] 65%|██████▌   | 17524/26933 [18:54<08:53, 17.62it/s] 65%|██████▌   | 17528/26933 [18:55<08:52, 17.67it/s] 65%|██████▌   | 17532/26933 [18:55<08:51, 17.70it/s] 65%|██████▌   | 17536/26933 [18:55<08:51, 17.69it/s] 65%|██████▌   | 17540/26933 [18:55<08:50, 17.72it/s] 65%|██████▌   | 17544/26933 [18:55<08:51, 17.68it/s] 65%|██████▌   | 17548/26933 [18:56<08:50, 17.70it/s] 65%|██████▌   | 17552/26933 [18:56<08:49, 17.70it/s] 65%|██████▌   | 17556/26933 [18:56<08:49, 17.72it/s] 65%|██████▌   | 17560/26933 [18:56<08:49, 17.70it/s] 65%|██████▌   | 17564/26933 [18:57<08:49, 17.70it/s] 65%|██████▌   | 17568/26933 [18:57<08:48, 17.72it/s] 65%|██████▌   | 17572/26933 [18:57<08:47, 17.74it/s] 65%|██████▌   | 17576/26933 [18:57<08:47, 17.73it/s] 65%|██████▌   | 17580/26933 [18:57<08:47, 17.73it/s] 65%|██████▌   | 17584/26933 [18:58<08:44, 17.83it/s] 65%|██████▌   | 17588/26933 [18:58<08:46, 17.76it/s] 65%|██████▌   | 17592/26933 [18:58<08:45, 17.76it/s] 65%|██████▌   | 17596/26933 [18:58<08:46, 17.74it/s] 65%|██████▌   | 17600/26933 [18:59<08:45, 17.77it/s] 65%|██████▌   | 17604/26933 [18:59<08:45, 17.76it/s] 65%|██████▌   | 17608/26933 [18:59<08:44, 17.79it/s] 65%|██████▌   | 17612/26933 [18:59<08:45, 17.74it/s] 65%|██████▌   | 17616/26933 [18:59<08:45, 17.74it/s] 65%|██████▌   | 17620/26933 [19:00<08:46, 17.70it/s] 65%|██████▌   | 17624/26933 [19:00<08:45, 17.71it/s] 65%|██████▌   | 17628/26933 [19:00<08:45, 17.71it/s] 65%|██████▌   | 17632/26933 [19:00<08:45, 17.68it/s] 65%|██████▌   | 17636/26933 [19:01<08:48, 17.60it/s] 65%|██████▌   | 17640/26933 [19:01<08:46, 17.64it/s] 66%|██████▌   | 17644/26933 [19:01<08:45, 17.68it/s] 66%|██████▌   | 17648/26933 [19:01<08:45, 17.67it/s] 66%|██████▌   | 17652/26933 [19:02<08:45, 17.67it/s] 66%|██████▌   | 17656/26933 [19:02<08:45, 17.66it/s] 66%|██████▌   | 17660/26933 [19:02<08:45, 17.66it/s] 66%|██████▌   | 17664/26933 [19:02<08:45, 17.63it/s] 66%|██████▌   | 17668/26933 [19:02<08:46, 17.61it/s] 66%|██████▌   | 17672/26933 [19:03<08:45, 17.64it/s] 66%|██████▌   | 17676/26933 [19:03<08:43, 17.68it/s] 66%|██████▌   | 17680/26933 [19:03<08:42, 17.72it/s] 66%|██████▌   | 17684/26933 [19:03<08:43, 17.66it/s] 66%|██████▌   | 17688/26933 [19:04<08:43, 17.68it/s] 66%|██████▌   | 17692/26933 [19:04<08:41, 17.71it/s] 66%|██████▌   | 17696/26933 [19:04<08:41, 17.70it/s] 66%|██████▌   | 17700/26933 [19:04<08:43, 17.63it/s] 66%|██████▌   | 17704/26933 [19:04<08:42, 17.66it/s] 66%|██████▌   | 17708/26933 [19:05<08:41, 17.68it/s] 66%|██████▌   | 17712/26933 [19:05<08:40, 17.71it/s] 66%|██████▌   | 17716/26933 [19:05<08:40, 17.72it/s] 66%|██████▌   | 17720/26933 [19:05<08:41, 17.67it/s] 66%|██████▌   | 17724/26933 [19:06<08:40, 17.69it/s] 66%|██████▌   | 17728/26933 [19:06<08:39, 17.72it/s] 66%|██████▌   | 17732/26933 [19:06<08:39, 17.72it/s] 66%|██████▌   | 17736/26933 [19:06<08:39, 17.69it/s] 66%|██████▌   | 17740/26933 [19:06<08:38, 17.73it/s] 66%|██████▌   | 17744/26933 [19:07<08:38, 17.74it/s] 66%|██████▌   | 17748/26933 [19:07<08:37, 17.75it/s] 66%|██████▌   | 17752/26933 [19:07<08:37, 17.75it/s] 66%|██████▌   | 17756/26933 [19:07<08:37, 17.73it/s] 66%|██████▌   | 17760/26933 [19:08<08:36, 17.76it/s] 66%|██████▌   | 17764/26933 [19:08<08:36, 17.76it/s] 66%|██████▌   | 17768/26933 [19:08<08:35, 17.77it/s] 66%|██████▌   | 17772/26933 [19:08<08:35, 17.76it/s] 66%|██████▌   | 17776/26933 [19:09<08:35, 17.75it/s] 66%|██████▌   | 17780/26933 [19:09<08:35, 17.77it/s] 66%|██████▌   | 17784/26933 [19:09<08:34, 17.77it/s] 66%|██████▌   | 17788/26933 [19:09<08:34, 17.77it/s] 66%|██████▌   | 17792/26933 [19:09<08:35, 17.73it/s] 66%|██████▌   | 17796/26933 [19:10<08:34, 17.76it/s] 66%|██████▌   | 17800/26933 [19:10<08:33, 17.78it/s] 66%|██████▌   | 17804/26933 [19:10<08:32, 17.80it/s] 66%|██████▌   | 17808/26933 [19:10<08:33, 17.76it/s] 66%|██████▌   | 17812/26933 [19:11<08:32, 17.79it/s] 66%|██████▌   | 17816/26933 [19:11<08:32, 17.80it/s] 66%|██████▌   | 17820/26933 [19:11<08:29, 17.88it/s] 66%|██████▌   | 17824/26933 [19:11<08:27, 17.93it/s] 66%|██████▌   | 17828/26933 [19:11<08:27, 17.95it/s] 66%|██████▌   | 17832/26933 [19:12<08:25, 18.00it/s] 66%|██████▌   | 17836/26933 [19:12<08:24, 18.04it/s] 66%|██████▌   | 17840/26933 [19:12<08:23, 18.05it/s] 66%|██████▋   | 17844/26933 [19:12<08:24, 18.01it/s] 66%|██████▋   | 17848/26933 [19:13<08:22, 18.07it/s] 66%|██████▋   | 17852/26933 [19:13<08:23, 18.05it/s] 66%|██████▋   | 17856/26933 [19:13<08:21, 18.09it/s] 66%|██████▋   | 17860/26933 [19:13<08:21, 18.08it/s] 66%|██████▋   | 17864/26933 [19:13<08:23, 18.02it/s] 66%|██████▋   | 17868/26933 [19:14<08:22, 18.03it/s] 66%|██████▋   | 17872/26933 [19:14<08:21, 18.06it/s] 66%|██████▋   | 17876/26933 [19:14<08:21, 18.08it/s] 66%|██████▋   | 17880/26933 [19:14<08:21, 18.04it/s] 66%|██████▋   | 17884/26933 [19:15<08:22, 18.02it/s] 66%|██████▋   | 17888/26933 [19:15<08:32, 17.66it/s] 66%|██████▋   | 17892/26933 [19:15<08:28, 17.78it/s] 66%|██████▋   | 17896/26933 [19:15<08:26, 17.84it/s] 66%|██████▋   | 17900/26933 [19:15<08:25, 17.86it/s] 66%|██████▋   | 17904/26933 [19:16<08:23, 17.92it/s] 66%|██████▋   | 17908/26933 [19:16<08:20, 18.02it/s] 67%|██████▋   | 17912/26933 [19:16<08:19, 18.07it/s] 67%|██████▋   | 17916/26933 [19:16<08:20, 18.00it/s] 67%|██████▋   | 17920/26933 [19:17<08:20, 18.01it/s] 67%|██████▋   | 17924/26933 [19:17<08:20, 18.00it/s] 67%|██████▋   | 17928/26933 [19:17<08:23, 17.90it/s] 67%|██████▋   | 17932/26933 [19:17<08:24, 17.85it/s] 67%|██████▋   | 17936/26933 [19:17<08:25, 17.81it/s] 67%|██████▋   | 17940/26933 [19:18<08:22, 17.90it/s] 67%|██████▋   | 17944/26933 [19:18<08:20, 17.95it/s] 67%|██████▋   | 17948/26933 [19:18<08:19, 17.98it/s] 67%|██████▋   | 17952/26933 [19:18<08:20, 17.95it/s] 67%|██████▋   | 17956/26933 [19:19<08:18, 17.99it/s] 67%|██████▋   | 17960/26933 [19:19<08:18, 18.01it/s] 67%|██████▋   | 17964/26933 [19:19<08:16, 18.06it/s] 67%|██████▋   | 17968/26933 [19:19<08:16, 18.06it/s] 67%|██████▋   | 17972/26933 [19:19<08:17, 18.02it/s] 67%|██████▋   | 17976/26933 [19:20<08:16, 18.05it/s] 67%|██████▋   | 17980/26933 [19:20<08:14, 18.09it/s] 67%|██████▋   | 17984/26933 [19:20<08:14, 18.12it/s] 67%|██████▋   | 17988/26933 [19:20<08:17, 17.98it/s] 67%|██████▋   | 17992/26933 [19:21<08:19, 17.91it/s] 67%|██████▋   | 17996/26933 [19:21<08:20, 17.87it/s] 67%|██████▋   | 18000/26933 [19:21<08:20, 17.86it/s] 67%|██████▋   | 18004/26933 [19:21<08:21, 17.80it/s] 67%|██████▋   | 18008/26933 [19:21<08:22, 17.76it/s] 67%|██████▋   | 18012/26933 [19:22<08:21, 17.80it/s] 67%|██████▋   | 18016/26933 [19:22<08:20, 17.81it/s] 67%|██████▋   | 18020/26933 [19:22<08:19, 17.84it/s] 67%|██████▋   | 18024/26933 [19:22<08:21, 17.78it/s] 67%|██████▋   | 18028/26933 [19:23<08:20, 17.78it/s] 67%|██████▋   | 18032/26933 [19:23<08:20, 17.78it/s] 67%|██████▋   | 18036/26933 [19:23<08:19, 17.80it/s] 67%|██████▋   | 18040/26933 [19:23<08:21, 17.75it/s] 67%|██████▋   | 18044/26933 [19:23<08:20, 17.76it/s] 67%|██████▋   | 18048/26933 [19:24<08:19, 17.79it/s] 67%|██████▋   | 18052/26933 [19:24<08:19, 17.78it/s] 67%|██████▋   | 18056/26933 [19:24<08:18, 17.79it/s] 67%|██████▋   | 18060/26933 [19:24<08:19, 17.76it/s] 67%|██████▋   | 18064/26933 [19:25<08:19, 17.76it/s] 67%|██████▋   | 18068/26933 [19:25<08:19, 17.76it/s] 67%|██████▋   | 18072/26933 [19:25<08:19, 17.75it/s] 67%|██████▋   | 18076/26933 [19:25<08:19, 17.73it/s] 67%|██████▋   | 18080/26933 [19:26<08:20, 17.70it/s] 67%|██████▋   | 18084/26933 [19:26<08:22, 17.62it/s] 67%|██████▋   | 18088/26933 [19:26<08:32, 17.27it/s] 67%|██████▋   | 18092/26933 [19:26<08:28, 17.38it/s] 67%|██████▋   | 18096/26933 [19:26<08:27, 17.42it/s] 67%|██████▋   | 18100/26933 [19:27<08:25, 17.49it/s] 67%|██████▋   | 18104/26933 [19:27<08:23, 17.55it/s] 67%|██████▋   | 18108/26933 [19:27<08:21, 17.59it/s] 67%|██████▋   | 18112/26933 [19:27<08:21, 17.58it/s] 67%|██████▋   | 18116/26933 [19:28<08:20, 17.62it/s] 67%|██████▋   | 18120/26933 [19:28<08:18, 17.67it/s] 67%|██████▋   | 18124/26933 [19:28<08:18, 17.68it/s] 67%|██████▋   | 18128/26933 [19:28<08:19, 17.64it/s] 67%|██████▋   | 18132/26933 [19:28<08:17, 17.70it/s] 67%|██████▋   | 18136/26933 [19:29<08:18, 17.66it/s] 67%|██████▋   | 18140/26933 [19:29<08:18, 17.63it/s] 67%|██████▋   | 18144/26933 [19:29<08:18, 17.63it/s] 67%|██████▋   | 18148/26933 [19:29<08:19, 17.59it/s] 67%|██████▋   | 18152/26933 [19:30<08:17, 17.64it/s] 67%|██████▋   | 18156/26933 [19:30<08:17, 17.64it/s] 67%|██████▋   | 18160/26933 [19:30<08:16, 17.67it/s] 67%|██████▋   | 18164/26933 [19:30<08:17, 17.62it/s] 67%|██████▋   | 18168/26933 [19:31<08:16, 17.65it/s] 67%|██████▋   | 18172/26933 [19:31<08:15, 17.66it/s] 67%|██████▋   | 18176/26933 [19:31<08:15, 17.67it/s] 68%|██████▊   | 18180/26933 [19:31<08:14, 17.69it/s] 68%|██████▊   | 18184/26933 [19:31<08:16, 17.60it/s] 68%|██████▊   | 18188/26933 [19:32<08:16, 17.63it/s] 68%|██████▊   | 18192/26933 [19:32<08:27, 17.21it/s] 68%|██████▊   | 18196/26933 [19:32<08:23, 17.35it/s] 68%|██████▊   | 18200/26933 [19:32<08:21, 17.43it/s] 68%|██████▊   | 18204/26933 [19:33<08:18, 17.52it/s] 68%|██████▊   | 18208/26933 [19:33<08:15, 17.62it/s] 68%|██████▊   | 18212/26933 [19:33<08:13, 17.67it/s] 68%|██████▊   | 18216/26933 [19:33<08:13, 17.65it/s] 68%|██████▊   | 18220/26933 [19:33<08:12, 17.69it/s] 68%|██████▊   | 18224/26933 [19:34<08:11, 17.72it/s] 68%|██████▊   | 18228/26933 [19:34<08:10, 17.73it/s] 68%|██████▊   | 18232/26933 [19:34<08:10, 17.75it/s] 68%|██████▊   | 18236/26933 [19:34<08:10, 17.73it/s] 68%|██████▊   | 18240/26933 [19:35<08:09, 17.75it/s] 68%|██████▊   | 18244/26933 [19:35<08:09, 17.76it/s] 68%|██████▊   | 18248/26933 [19:35<08:08, 17.78it/s] 68%|██████▊   | 18252/26933 [19:35<08:09, 17.74it/s] 68%|██████▊   | 18256/26933 [19:36<08:08, 17.75it/s] 68%|██████▊   | 18260/26933 [19:36<08:08, 17.74it/s] 68%|██████▊   | 18264/26933 [19:36<08:16, 17.47it/s] 68%|██████▊   | 18268/26933 [19:36<08:12, 17.59it/s] 68%|██████▊   | 18272/26933 [19:36<08:11, 17.61it/s] 68%|██████▊   | 18276/26933 [19:37<08:09, 17.67it/s] 68%|██████▊   | 18280/26933 [19:37<08:09, 17.67it/s] 68%|██████▊   | 18284/26933 [19:37<08:09, 17.68it/s] 68%|██████▊   | 18288/26933 [19:37<08:09, 17.66it/s] 68%|██████▊   | 18292/26933 [19:38<08:08, 17.69it/s] 68%|██████▊   | 18296/26933 [19:38<08:06, 17.74it/s] 68%|██████▊   | 18300/26933 [19:38<08:07, 17.72it/s] 68%|██████▊   | 18304/26933 [19:38<08:07, 17.71it/s] 68%|██████▊   | 18308/26933 [19:38<08:08, 17.66it/s] 68%|██████▊   | 18312/26933 [19:39<08:07, 17.67it/s] 68%|██████▊   | 18316/26933 [19:39<08:08, 17.66it/s] 68%|██████▊   | 18320/26933 [19:39<08:07, 17.67it/s] 68%|██████▊   | 18324/26933 [19:39<08:07, 17.67it/s] 68%|██████▊   | 18328/26933 [19:40<08:06, 17.69it/s] 68%|██████▊   | 18332/26933 [19:40<08:05, 17.72it/s] 68%|██████▊   | 18336/26933 [19:40<08:05, 17.72it/s] 68%|██████▊   | 18340/26933 [19:40<08:05, 17.69it/s] 68%|██████▊   | 18344/26933 [19:40<08:04, 17.74it/s] 68%|██████▊   | 18348/26933 [19:41<08:03, 17.76it/s] 68%|██████▊   | 18352/26933 [19:41<08:03, 17.75it/s] 68%|██████▊   | 18356/26933 [19:41<08:03, 17.76it/s] 68%|██████▊   | 18360/26933 [19:41<08:02, 17.75it/s] 68%|██████▊   | 18364/26933 [19:42<08:02, 17.75it/s] 68%|██████▊   | 18368/26933 [19:42<08:02, 17.75it/s] 68%|██████▊   | 18372/26933 [19:42<08:02, 17.75it/s] 68%|██████▊   | 18376/26933 [19:42<08:02, 17.73it/s] 68%|██████▊   | 18380/26933 [19:43<08:01, 17.75it/s] 68%|██████▊   | 18384/26933 [19:43<08:02, 17.70it/s] 68%|██████▊   | 18388/26933 [19:43<08:02, 17.72it/s] 68%|██████▊   | 18392/26933 [19:43<08:01, 17.73it/s] 68%|██████▊   | 18396/26933 [19:43<08:01, 17.73it/s] 68%|██████▊   | 18400/26933 [19:44<07:59, 17.78it/s] 68%|██████▊   | 18404/26933 [19:44<07:59, 17.80it/s] 68%|██████▊   | 18408/26933 [19:44<07:59, 17.79it/s] 68%|██████▊   | 18412/26933 [19:44<07:59, 17.76it/s] 68%|██████▊   | 18416/26933 [19:45<07:59, 17.78it/s] 68%|██████▊   | 18420/26933 [19:45<07:59, 17.76it/s] 68%|██████▊   | 18424/26933 [19:45<07:58, 17.77it/s] 68%|██████▊   | 18428/26933 [19:45<07:59, 17.73it/s] 68%|██████▊   | 18432/26933 [19:45<08:00, 17.70it/s] 68%|██████▊   | 18436/26933 [19:46<07:58, 17.75it/s] 68%|██████▊   | 18440/26933 [19:46<07:59, 17.71it/s] 68%|██████▊   | 18444/26933 [19:46<08:00, 17.66it/s] 68%|██████▊   | 18448/26933 [19:46<08:02, 17.60it/s] 69%|██████▊   | 18452/26933 [19:47<08:01, 17.63it/s] 69%|██████▊   | 18456/26933 [19:47<08:02, 17.57it/s] 69%|██████▊   | 18460/26933 [19:47<08:04, 17.50it/s] 69%|██████▊   | 18464/26933 [19:47<08:05, 17.46it/s] 69%|██████▊   | 18468/26933 [19:47<08:04, 17.47it/s] 69%|██████▊   | 18472/26933 [19:48<08:06, 17.40it/s] 69%|██████▊   | 18476/26933 [19:48<08:06, 17.40it/s] 69%|██████▊   | 18480/26933 [19:48<08:06, 17.37it/s] 69%|██████▊   | 18484/26933 [19:48<08:06, 17.37it/s] 69%|██████▊   | 18488/26933 [19:49<08:04, 17.43it/s] 69%|██████▊   | 18492/26933 [19:49<08:04, 17.43it/s] 69%|██████▊   | 18496/26933 [19:49<08:04, 17.42it/s] 69%|██████▊   | 18500/26933 [19:49<08:06, 17.35it/s] 69%|██████▊   | 18504/26933 [19:50<08:05, 17.35it/s] 69%|██████▊   | 18508/26933 [19:50<08:12, 17.11it/s] 69%|██████▊   | 18512/26933 [19:50<08:10, 17.15it/s] 69%|██████▊   | 18516/26933 [19:50<08:11, 17.14it/s] 69%|██████▉   | 18520/26933 [19:51<08:11, 17.13it/s] 69%|██████▉   | 18524/26933 [19:51<08:09, 17.16it/s] 69%|██████▉   | 18528/26933 [19:51<08:08, 17.21it/s] 69%|██████▉   | 18532/26933 [19:51<08:08, 17.20it/s] 69%|██████▉   | 18536/26933 [19:51<08:05, 17.29it/s] 69%|██████▉   | 18540/26933 [19:52<08:03, 17.37it/s] 69%|██████▉   | 18544/26933 [19:52<08:01, 17.44it/s] 69%|██████▉   | 18548/26933 [19:52<07:57, 17.54it/s] 69%|██████▉   | 18552/26933 [19:52<07:58, 17.53it/s] 69%|██████▉   | 18556/26933 [19:53<07:56, 17.57it/s] 69%|██████▉   | 18560/26933 [19:53<07:56, 17.58it/s] 69%|██████▉   | 18564/26933 [19:53<07:55, 17.61it/s] 69%|██████▉   | 18568/26933 [19:53<07:55, 17.59it/s] 69%|██████▉   | 18572/26933 [19:53<07:54, 17.61it/s] 69%|██████▉   | 18576/26933 [19:54<07:55, 17.59it/s] 69%|██████▉   | 18580/26933 [19:54<07:54, 17.62it/s] 69%|██████▉   | 18584/26933 [19:54<07:53, 17.64it/s] 69%|██████▉   | 18588/26933 [19:54<07:54, 17.58it/s] 69%|██████▉   | 18592/26933 [19:55<07:54, 17.59it/s] 69%|██████▉   | 18596/26933 [19:55<07:53, 17.62it/s] 69%|██████▉   | 18600/26933 [19:55<07:52, 17.65it/s] 69%|██████▉   | 18604/26933 [19:55<07:56, 17.49it/s] 69%|██████▉   | 18608/26933 [19:56<07:55, 17.49it/s] 69%|██████▉   | 18612/26933 [19:56<07:56, 17.47it/s] 69%|██████▉   | 18616/26933 [19:56<07:54, 17.52it/s] 69%|██████▉   | 18620/26933 [19:56<07:52, 17.58it/s] 69%|██████▉   | 18624/26933 [19:56<07:55, 17.49it/s] 69%|██████▉   | 18628/26933 [19:57<07:54, 17.51it/s] 69%|██████▉   | 18632/26933 [19:57<07:56, 17.42it/s] 69%|██████▉   | 18636/26933 [19:57<07:57, 17.36it/s] 69%|██████▉   | 18640/26933 [19:57<07:56, 17.42it/s] 69%|██████▉   | 18644/26933 [19:58<07:53, 17.52it/s] 69%|██████▉   | 18648/26933 [19:58<07:52, 17.55it/s] 69%|██████▉   | 18652/26933 [19:58<07:50, 17.61it/s] 69%|██████▉   | 18656/26933 [19:58<07:49, 17.62it/s] 69%|██████▉   | 18660/26933 [19:58<07:49, 17.63it/s] 69%|██████▉   | 18664/26933 [19:59<07:49, 17.61it/s] 69%|██████▉   | 18668/26933 [19:59<07:50, 17.58it/s] 69%|██████▉   | 18672/26933 [19:59<07:48, 17.62it/s] 69%|██████▉   | 18676/26933 [19:59<07:49, 17.60it/s] 69%|██████▉   | 18680/26933 [20:00<07:48, 17.62it/s] 69%|██████▉   | 18684/26933 [20:00<07:47, 17.63it/s] 69%|██████▉   | 18688/26933 [20:00<07:46, 17.66it/s] 69%|██████▉   | 18692/26933 [20:00<07:47, 17.63it/s] 69%|██████▉   | 18696/26933 [20:01<07:46, 17.65it/s] 69%|██████▉   | 18700/26933 [20:01<07:46, 17.64it/s] 69%|██████▉   | 18704/26933 [20:01<07:45, 17.67it/s] 69%|██████▉   | 18708/26933 [20:01<07:45, 17.66it/s] 69%|██████▉   | 18712/26933 [20:01<07:45, 17.66it/s] 69%|██████▉   | 18716/26933 [20:02<07:44, 17.68it/s] 70%|██████▉   | 18720/26933 [20:02<07:44, 17.69it/s] 70%|██████▉   | 18724/26933 [20:02<07:42, 17.73it/s] 70%|██████▉   | 18728/26933 [20:02<07:42, 17.74it/s] 70%|██████▉   | 18732/26933 [20:03<07:42, 17.71it/s] 70%|██████▉   | 18736/26933 [20:03<07:42, 17.72it/s] 70%|██████▉   | 18740/26933 [20:03<07:42, 17.73it/s] 70%|██████▉   | 18744/26933 [20:03<07:43, 17.66it/s] 70%|██████▉   | 18748/26933 [20:03<07:43, 17.65it/s] 70%|██████▉   | 18752/26933 [20:04<07:44, 17.60it/s] 70%|██████▉   | 18756/26933 [20:04<07:44, 17.60it/s] 70%|██████▉   | 18760/26933 [20:04<07:44, 17.61it/s] 70%|██████▉   | 18764/26933 [20:04<07:45, 17.56it/s] 70%|██████▉   | 18768/26933 [20:05<07:43, 17.60it/s] 70%|██████▉   | 18772/26933 [20:05<07:42, 17.65it/s] 70%|██████▉   | 18776/26933 [20:05<07:41, 17.69it/s] 70%|██████▉   | 18780/26933 [20:05<07:41, 17.65it/s] 70%|██████▉   | 18784/26933 [20:06<07:41, 17.66it/s] 70%|██████▉   | 18788/26933 [20:06<07:40, 17.71it/s] 70%|██████▉   | 18792/26933 [20:06<07:39, 17.71it/s] 70%|██████▉   | 18796/26933 [20:06<07:39, 17.70it/s] 70%|██████▉   | 18800/26933 [20:06<07:40, 17.65it/s] 70%|██████▉   | 18804/26933 [20:07<07:40, 17.67it/s] 70%|██████▉   | 18808/26933 [20:07<07:49, 17.32it/s] 70%|██████▉   | 18812/26933 [20:07<07:49, 17.31it/s] 70%|██████▉   | 18816/26933 [20:07<07:47, 17.36it/s] 70%|██████▉   | 18820/26933 [20:08<07:43, 17.50it/s] 70%|██████▉   | 18824/26933 [20:08<07:41, 17.56it/s] 70%|██████▉   | 18828/26933 [20:08<07:39, 17.64it/s] 70%|██████▉   | 18832/26933 [20:08<07:38, 17.66it/s] 70%|██████▉   | 18836/26933 [20:08<07:36, 17.74it/s] 70%|██████▉   | 18840/26933 [20:09<07:34, 17.79it/s] 70%|██████▉   | 18844/26933 [20:09<07:34, 17.79it/s] 70%|██████▉   | 18848/26933 [20:09<07:33, 17.84it/s] 70%|██████▉   | 18852/26933 [20:09<07:33, 17.81it/s] 70%|███████   | 18856/26933 [20:10<07:33, 17.82it/s] 70%|███████   | 18860/26933 [20:10<07:33, 17.82it/s] 70%|███████   | 18864/26933 [20:10<07:32, 17.84it/s] 70%|███████   | 18868/26933 [20:10<07:32, 17.82it/s] 70%|███████   | 18872/26933 [20:10<07:31, 17.85it/s] 70%|███████   | 18876/26933 [20:11<07:31, 17.85it/s] 70%|███████   | 18880/26933 [20:11<07:31, 17.85it/s] 70%|███████   | 18884/26933 [20:11<07:30, 17.85it/s] 70%|███████   | 18888/26933 [20:11<07:32, 17.78it/s] 70%|███████   | 18892/26933 [20:12<07:32, 17.77it/s] 70%|███████   | 18896/26933 [20:12<07:32, 17.77it/s] 70%|███████   | 18900/26933 [20:12<07:31, 17.80it/s] 70%|███████   | 18904/26933 [20:12<07:31, 17.79it/s] 70%|███████   | 18908/26933 [20:13<07:30, 17.80it/s] 70%|███████   | 18912/26933 [20:13<07:31, 17.75it/s] 70%|███████   | 18916/26933 [20:13<07:31, 17.77it/s] 70%|███████   | 18920/26933 [20:13<07:31, 17.75it/s] 70%|███████   | 18924/26933 [20:13<07:32, 17.72it/s] 70%|███████   | 18928/26933 [20:14<07:32, 17.69it/s] 70%|███████   | 18932/26933 [20:14<07:31, 17.72it/s] 70%|███████   | 18936/26933 [20:14<07:31, 17.72it/s] 70%|███████   | 18940/26933 [20:14<07:31, 17.69it/s] 70%|███████   | 18944/26933 [20:15<07:30, 17.73it/s] 70%|███████   | 18948/26933 [20:15<07:30, 17.72it/s] 70%|███████   | 18952/26933 [20:15<07:29, 17.74it/s] 70%|███████   | 18956/26933 [20:15<07:29, 17.75it/s] 70%|███████   | 18960/26933 [20:15<07:30, 17.71it/s] 70%|███████   | 18964/26933 [20:16<07:29, 17.74it/s] 70%|███████   | 18968/26933 [20:16<07:29, 17.74it/s] 70%|███████   | 18972/26933 [20:16<07:29, 17.73it/s] 70%|███████   | 18976/26933 [20:16<07:29, 17.70it/s] 70%|███████   | 18980/26933 [20:17<07:29, 17.70it/s] 70%|███████   | 18984/26933 [20:17<07:26, 17.82it/s] 71%|███████   | 18988/26933 [20:17<07:23, 17.90it/s] 71%|███████   | 18992/26933 [20:17<07:23, 17.92it/s] 71%|███████   | 18996/26933 [20:17<07:21, 17.99it/s] 71%|███████   | 19000/26933 [20:18<07:20, 18.03it/s] 71%|███████   | 19004/26933 [20:18<07:19, 18.03it/s] 71%|███████   | 19008/26933 [20:18<07:18, 18.06it/s] 71%|███████   | 19012/26933 [20:18<07:18, 18.05it/s] 71%|███████   | 19016/26933 [20:19<07:18, 18.06it/s] 71%|███████   | 19020/26933 [20:19<07:18, 18.06it/s] 71%|███████   | 19024/26933 [20:19<07:17, 18.07it/s] 71%|███████   | 19028/26933 [20:19<07:17, 18.08it/s] 71%|███████   | 19032/26933 [20:19<07:18, 18.03it/s] 71%|███████   | 19036/26933 [20:20<07:18, 18.03it/s] 71%|███████   | 19040/26933 [20:20<07:17, 18.03it/s] 71%|███████   | 19044/26933 [20:20<07:15, 18.10it/s] 71%|███████   | 19048/26933 [20:20<07:16, 18.05it/s] 71%|███████   | 19052/26933 [20:21<07:15, 18.09it/s] 71%|███████   | 19056/26933 [20:21<07:15, 18.11it/s] 71%|███████   | 19060/26933 [20:21<07:14, 18.14it/s] 71%|███████   | 19064/26933 [20:21<07:16, 18.03it/s] 71%|███████   | 19068/26933 [20:21<07:18, 17.92it/s] 71%|███████   | 19072/26933 [20:22<07:20, 17.84it/s] 71%|███████   | 19076/26933 [20:22<07:22, 17.77it/s] 71%|███████   | 19080/26933 [20:22<07:21, 17.78it/s] 71%|███████   | 19084/26933 [20:22<07:22, 17.75it/s] 71%|███████   | 19088/26933 [20:23<07:21, 17.75it/s] 71%|███████   | 19092/26933 [20:23<07:21, 17.75it/s] 71%|███████   | 19096/26933 [20:23<07:23, 17.65it/s] 71%|███████   | 19100/26933 [20:23<07:25, 17.59it/s] 71%|███████   | 19104/26933 [20:23<07:26, 17.52it/s] 71%|███████   | 19108/26933 [20:24<07:27, 17.50it/s] 71%|███████   | 19112/26933 [20:24<07:25, 17.56it/s] 71%|███████   | 19116/26933 [20:24<07:24, 17.61it/s] 71%|███████   | 19120/26933 [20:24<07:24, 17.56it/s] 71%|███████   | 19124/26933 [20:25<07:23, 17.59it/s] 71%|███████   | 19128/26933 [20:25<07:23, 17.61it/s] 71%|███████   | 19132/26933 [20:25<07:22, 17.62it/s] 71%|███████   | 19136/26933 [20:25<07:22, 17.62it/s] 71%|███████   | 19140/26933 [20:26<07:21, 17.65it/s] 71%|███████   | 19144/26933 [20:26<07:20, 17.68it/s] 71%|███████   | 19148/26933 [20:26<07:20, 17.69it/s] 71%|███████   | 19152/26933 [20:26<07:21, 17.64it/s] 71%|███████   | 19156/26933 [20:26<07:21, 17.63it/s] 71%|███████   | 19160/26933 [20:27<07:20, 17.66it/s] 71%|███████   | 19164/26933 [20:27<07:18, 17.71it/s] 71%|███████   | 19168/26933 [20:27<07:18, 17.72it/s] 71%|███████   | 19172/26933 [20:27<07:20, 17.64it/s] 71%|███████   | 19176/26933 [20:28<07:19, 17.63it/s] 71%|███████   | 19180/26933 [20:28<07:18, 17.67it/s] 71%|███████   | 19184/26933 [20:28<07:17, 17.69it/s] 71%|███████   | 19188/26933 [20:28<07:19, 17.62it/s] 71%|███████▏  | 19192/26933 [20:28<07:18, 17.65it/s] 71%|███████▏  | 19196/26933 [20:29<07:17, 17.66it/s] 71%|███████▏  | 19200/26933 [20:29<07:17, 17.68it/s] 71%|███████▏  | 19204/26933 [20:29<07:17, 17.68it/s] 71%|███████▏  | 19208/26933 [20:29<07:17, 17.67it/s] 71%|███████▏  | 19212/26933 [20:30<07:17, 17.64it/s] 71%|███████▏  | 19216/26933 [20:30<07:15, 17.70it/s] 71%|███████▏  | 19220/26933 [20:30<07:14, 17.73it/s] 71%|███████▏  | 19224/26933 [20:30<07:16, 17.68it/s] 71%|███████▏  | 19228/26933 [20:31<07:15, 17.70it/s] 71%|███████▏  | 19232/26933 [20:31<07:15, 17.69it/s] 71%|███████▏  | 19236/26933 [20:31<07:17, 17.59it/s] 71%|███████▏  | 19240/26933 [20:31<07:17, 17.58it/s] 71%|███████▏  | 19244/26933 [20:31<07:16, 17.62it/s] 71%|███████▏  | 19248/26933 [20:32<07:14, 17.68it/s] 71%|███████▏  | 19252/26933 [20:32<07:13, 17.71it/s] 71%|███████▏  | 19256/26933 [20:32<07:12, 17.76it/s] 72%|███████▏  | 19260/26933 [20:32<07:13, 17.72it/s] 72%|███████▏  | 19264/26933 [20:33<07:12, 17.71it/s] 72%|███████▏  | 19268/26933 [20:33<07:13, 17.70it/s] 72%|███████▏  | 19272/26933 [20:33<07:12, 17.73it/s] 72%|███████▏  | 19276/26933 [20:33<07:10, 17.77it/s] 72%|███████▏  | 19280/26933 [20:33<07:11, 17.72it/s] 72%|███████▏  | 19284/26933 [20:34<07:11, 17.74it/s] 72%|███████▏  | 19288/26933 [20:34<07:10, 17.77it/s] 72%|███████▏  | 19292/26933 [20:34<07:09, 17.78it/s] 72%|███████▏  | 19296/26933 [20:34<07:11, 17.70it/s] 72%|███████▏  | 19300/26933 [20:35<07:09, 17.76it/s] 72%|███████▏  | 19304/26933 [20:35<07:10, 17.72it/s] 72%|███████▏  | 19308/26933 [20:35<07:10, 17.72it/s] 72%|███████▏  | 19312/26933 [20:35<07:19, 17.32it/s] 72%|███████▏  | 19316/26933 [20:36<07:16, 17.44it/s] 72%|███████▏  | 19320/26933 [20:36<07:13, 17.55it/s] 72%|███████▏  | 19324/26933 [20:36<07:12, 17.61it/s] 72%|███████▏  | 19328/26933 [20:36<07:09, 17.70it/s] 72%|███████▏  | 19332/26933 [20:36<07:08, 17.72it/s] 72%|███████▏  | 19336/26933 [20:37<07:07, 17.77it/s] 72%|███████▏  | 19340/26933 [20:37<07:06, 17.80it/s] 72%|███████▏  | 19344/26933 [20:37<07:06, 17.78it/s] 72%|███████▏  | 19348/26933 [20:37<07:07, 17.75it/s] 72%|███████▏  | 19352/26933 [20:38<07:06, 17.77it/s] 72%|███████▏  | 19356/26933 [20:38<07:05, 17.79it/s] 72%|███████▏  | 19360/26933 [20:38<07:04, 17.82it/s] 72%|███████▏  | 19364/26933 [20:38<07:04, 17.82it/s] 72%|███████▏  | 19368/26933 [20:38<07:05, 17.78it/s] 72%|███████▏  | 19372/26933 [20:39<07:04, 17.79it/s] 72%|███████▏  | 19376/26933 [20:39<07:11, 17.52it/s] 72%|███████▏  | 19380/26933 [20:39<07:10, 17.56it/s] 72%|███████▏  | 19384/26933 [20:39<07:10, 17.52it/s] 72%|███████▏  | 19388/26933 [20:40<07:09, 17.56it/s] 72%|███████▏  | 19392/26933 [20:40<07:07, 17.62it/s] 72%|███████▏  | 19396/26933 [20:40<07:07, 17.64it/s] 72%|███████▏  | 19400/26933 [20:40<07:07, 17.61it/s] 72%|███████▏  | 19404/26933 [20:40<07:05, 17.68it/s] 72%|███████▏  | 19408/26933 [20:41<07:04, 17.73it/s] 72%|███████▏  | 19412/26933 [20:41<07:03, 17.74it/s] 72%|███████▏  | 19416/26933 [20:41<07:03, 17.74it/s] 72%|███████▏  | 19420/26933 [20:41<07:03, 17.75it/s] 72%|███████▏  | 19424/26933 [20:42<07:02, 17.79it/s] 72%|███████▏  | 19428/26933 [20:42<07:01, 17.78it/s] 72%|███████▏  | 19432/26933 [20:42<07:03, 17.72it/s] 72%|███████▏  | 19436/26933 [20:42<07:03, 17.70it/s] 72%|███████▏  | 19440/26933 [20:42<07:02, 17.72it/s] 72%|███████▏  | 19444/26933 [20:43<07:02, 17.73it/s] 72%|███████▏  | 19448/26933 [20:43<07:01, 17.76it/s] 72%|███████▏  | 19452/26933 [20:43<07:00, 17.77it/s] 72%|███████▏  | 19456/26933 [20:43<07:01, 17.74it/s] 72%|███████▏  | 19460/26933 [20:44<07:00, 17.77it/s] 72%|███████▏  | 19464/26933 [20:44<07:01, 17.73it/s] 72%|███████▏  | 19468/26933 [20:44<07:00, 17.76it/s] 72%|███████▏  | 19472/26933 [20:44<07:00, 17.73it/s] 72%|███████▏  | 19476/26933 [20:45<07:00, 17.75it/s] 72%|███████▏  | 19480/26933 [20:45<07:00, 17.74it/s] 72%|███████▏  | 19484/26933 [20:45<07:00, 17.73it/s] 72%|███████▏  | 19488/26933 [20:45<06:57, 17.85it/s] 72%|███████▏  | 19492/26933 [20:45<06:55, 17.93it/s] 72%|███████▏  | 19496/26933 [20:46<06:53, 18.00it/s] 72%|███████▏  | 19500/26933 [20:46<06:51, 18.05it/s] 72%|███████▏  | 19504/26933 [20:46<06:51, 18.06it/s] 72%|███████▏  | 19508/26933 [20:46<06:51, 18.04it/s] 72%|███████▏  | 19512/26933 [20:47<06:50, 18.10it/s] 72%|███████▏  | 19516/26933 [20:47<06:50, 18.05it/s] 72%|███████▏  | 19520/26933 [20:47<06:50, 18.07it/s] 72%|███████▏  | 19524/26933 [20:47<06:49, 18.07it/s] 73%|███████▎  | 19528/26933 [20:47<06:50, 18.06it/s] 73%|███████▎  | 19532/26933 [20:48<06:49, 18.09it/s] 73%|███████▎  | 19536/26933 [20:48<06:49, 18.08it/s] 73%|███████▎  | 19540/26933 [20:48<06:49, 18.03it/s] 73%|███████▎  | 19544/26933 [20:48<06:50, 18.00it/s] 73%|███████▎  | 19548/26933 [20:49<06:48, 18.06it/s] 73%|███████▎  | 19552/26933 [20:49<06:47, 18.09it/s] 73%|███████▎  | 19556/26933 [20:49<06:46, 18.13it/s] 73%|███████▎  | 19560/26933 [20:49<06:46, 18.12it/s] 73%|███████▎  | 19564/26933 [20:49<06:47, 18.08it/s] 73%|███████▎  | 19568/26933 [20:50<06:47, 18.08it/s] 73%|███████▎  | 19572/26933 [20:50<06:46, 18.13it/s] 73%|███████▎  | 19576/26933 [20:50<06:46, 18.12it/s] 73%|███████▎  | 19580/26933 [20:50<06:47, 18.06it/s] 73%|███████▎  | 19584/26933 [20:51<06:47, 18.03it/s] 73%|███████▎  | 19588/26933 [20:51<06:47, 18.04it/s] 73%|███████▎  | 19592/26933 [20:51<06:46, 18.05it/s] 73%|███████▎  | 19596/26933 [20:51<06:47, 17.99it/s] 73%|███████▎  | 19600/26933 [20:51<06:51, 17.84it/s] 73%|███████▎  | 19604/26933 [20:52<06:51, 17.83it/s] 73%|███████▎  | 19608/26933 [20:52<06:51, 17.82it/s] 73%|███████▎  | 19612/26933 [20:52<06:51, 17.78it/s] 73%|███████▎  | 19616/26933 [20:52<06:52, 17.75it/s] 73%|███████▎  | 19620/26933 [20:53<06:51, 17.76it/s] 73%|███████▎  | 19624/26933 [20:53<06:51, 17.76it/s] 73%|███████▎  | 19628/26933 [20:53<06:50, 17.78it/s] 73%|███████▎  | 19632/26933 [20:53<06:50, 17.77it/s] 73%|███████▎  | 19636/26933 [20:53<06:51, 17.72it/s] 73%|███████▎  | 19640/26933 [20:54<06:50, 17.75it/s] 73%|███████▎  | 19644/26933 [20:54<06:52, 17.69it/s] 73%|███████▎  | 19648/26933 [20:54<06:53, 17.61it/s] 73%|███████▎  | 19652/26933 [20:54<06:53, 17.60it/s] 73%|███████▎  | 19656/26933 [20:55<06:52, 17.65it/s] 73%|███████▎  | 19660/26933 [20:55<06:51, 17.66it/s] 73%|███████▎  | 19664/26933 [20:55<06:52, 17.64it/s] 73%|███████▎  | 19668/26933 [20:55<06:53, 17.59it/s] 73%|███████▎  | 19672/26933 [20:55<06:54, 17.53it/s] 73%|███████▎  | 19676/26933 [20:56<06:55, 17.46it/s] 73%|███████▎  | 19680/26933 [20:56<06:56, 17.43it/s] 73%|███████▎  | 19684/26933 [20:56<06:56, 17.41it/s] 73%|███████▎  | 19688/26933 [20:56<06:54, 17.46it/s] 73%|███████▎  | 19692/26933 [20:57<06:53, 17.53it/s] 73%|███████▎  | 19696/26933 [20:57<06:52, 17.54it/s] 73%|███████▎  | 19700/26933 [20:57<06:50, 17.64it/s] 73%|███████▎  | 19704/26933 [20:57<06:48, 17.68it/s] 73%|███████▎  | 19708/26933 [20:58<06:47, 17.71it/s] 73%|███████▎  | 19712/26933 [20:58<06:49, 17.65it/s] 73%|███████▎  | 19716/26933 [20:58<06:49, 17.61it/s] 73%|███████▎  | 19720/26933 [20:58<06:49, 17.60it/s] 73%|███████▎  | 19724/26933 [20:58<06:49, 17.60it/s] 73%|███████▎  | 19728/26933 [20:59<06:49, 17.60it/s] 73%|███████▎  | 19732/26933 [20:59<06:50, 17.55it/s] 73%|███████▎  | 19736/26933 [20:59<06:51, 17.48it/s] 73%|███████▎  | 19740/26933 [20:59<06:52, 17.45it/s] 73%|███████▎  | 19744/26933 [21:00<06:51, 17.49it/s] 73%|███████▎  | 19748/26933 [21:00<06:49, 17.55it/s] 73%|███████▎  | 19752/26933 [21:00<06:46, 17.66it/s] 73%|███████▎  | 19756/26933 [21:00<06:46, 17.65it/s] 73%|███████▎  | 19760/26933 [21:00<06:46, 17.63it/s] 73%|███████▎  | 19764/26933 [21:01<06:46, 17.64it/s] 73%|███████▎  | 19768/26933 [21:01<06:46, 17.64it/s] 73%|███████▎  | 19772/26933 [21:01<06:45, 17.67it/s] 73%|███████▎  | 19776/26933 [21:01<06:45, 17.63it/s] 73%|███████▎  | 19780/26933 [21:02<06:44, 17.66it/s] 73%|███████▎  | 19784/26933 [21:02<06:44, 17.69it/s] 73%|███████▎  | 19788/26933 [21:02<06:43, 17.69it/s] 73%|███████▎  | 19792/26933 [21:02<06:45, 17.62it/s] 74%|███████▎  | 19796/26933 [21:03<06:44, 17.63it/s] 74%|███████▎  | 19800/26933 [21:03<06:43, 17.67it/s] 74%|███████▎  | 19804/26933 [21:03<06:44, 17.64it/s] 74%|███████▎  | 19808/26933 [21:03<06:43, 17.65it/s] 74%|███████▎  | 19812/26933 [21:03<06:45, 17.58it/s] 74%|███████▎  | 19816/26933 [21:04<06:44, 17.60it/s] 74%|███████▎  | 19820/26933 [21:04<06:43, 17.64it/s] 74%|███████▎  | 19824/26933 [21:04<06:42, 17.66it/s] 74%|███████▎  | 19828/26933 [21:04<06:43, 17.63it/s] 74%|███████▎  | 19832/26933 [21:05<06:42, 17.64it/s] 74%|███████▎  | 19836/26933 [21:05<06:42, 17.64it/s] 74%|███████▎  | 19840/26933 [21:05<06:42, 17.62it/s] 74%|███████▎  | 19844/26933 [21:05<06:42, 17.60it/s] 74%|███████▎  | 19848/26933 [21:05<06:41, 17.65it/s] 74%|███████▎  | 19852/26933 [21:06<06:42, 17.59it/s] 74%|███████▎  | 19856/26933 [21:06<06:41, 17.61it/s] 74%|███████▎  | 19860/26933 [21:06<06:41, 17.60it/s] 74%|███████▍  | 19864/26933 [21:06<06:41, 17.60it/s] 74%|███████▍  | 19868/26933 [21:07<06:42, 17.57it/s] 74%|███████▍  | 19872/26933 [21:07<06:42, 17.54it/s] 74%|███████▍  | 19876/26933 [21:07<06:42, 17.55it/s] 74%|███████▍  | 19880/26933 [21:07<06:42, 17.53it/s] 74%|███████▍  | 19884/26933 [21:08<06:41, 17.57it/s] 74%|███████▍  | 19888/26933 [21:08<06:40, 17.60it/s] 74%|███████▍  | 19892/26933 [21:08<06:39, 17.62it/s] 74%|███████▍  | 19896/26933 [21:08<06:38, 17.64it/s] 74%|███████▍  | 19900/26933 [21:08<06:39, 17.62it/s] 74%|███████▍  | 19904/26933 [21:09<06:38, 17.62it/s] 74%|███████▍  | 19908/26933 [21:09<06:38, 17.62it/s] 74%|███████▍  | 19912/26933 [21:09<06:37, 17.65it/s] 74%|███████▍  | 19916/26933 [21:09<06:37, 17.63it/s] 74%|███████▍  | 19920/26933 [21:10<06:37, 17.65it/s] 74%|███████▍  | 19924/26933 [21:10<06:37, 17.63it/s] 74%|███████▍  | 19928/26933 [21:10<06:36, 17.65it/s] 74%|███████▍  | 19932/26933 [21:10<06:37, 17.62it/s] 74%|███████▍  | 19936/26933 [21:10<06:36, 17.64it/s] 74%|███████▍  | 19940/26933 [21:11<06:37, 17.60it/s] 74%|███████▍  | 19944/26933 [21:11<06:36, 17.63it/s] 74%|███████▍  | 19948/26933 [21:11<06:35, 17.67it/s] 74%|███████▍  | 19952/26933 [21:11<06:34, 17.68it/s] 74%|███████▍  | 19956/26933 [21:12<06:33, 17.74it/s] 74%|███████▍  | 19960/26933 [21:12<06:32, 17.77it/s] 74%|███████▍  | 19964/26933 [21:12<06:32, 17.77it/s] 74%|███████▍  | 19968/26933 [21:12<06:33, 17.69it/s] 74%|███████▍  | 19972/26933 [21:13<06:32, 17.72it/s] 74%|███████▍  | 19976/26933 [21:13<06:32, 17.74it/s] 74%|███████▍  | 19980/26933 [21:13<06:31, 17.77it/s] 74%|███████▍  | 19984/26933 [21:13<06:30, 17.78it/s] 74%|███████▍  | 19988/26933 [21:13<06:31, 17.76it/s] 74%|███████▍  | 19992/26933 [21:14<06:32, 17.71it/s] 74%|███████▍  | 19996/26933 [21:14<06:32, 17.67it/s] 74%|███████▍  | 20000/26933 [21:14<06:32, 17.68it/s] 74%|███████▍  | 20004/26933 [21:14<06:32, 17.65it/s] 74%|███████▍  | 20008/26933 [21:15<06:31, 17.71it/s] 74%|███████▍  | 20012/26933 [21:15<06:31, 17.70it/s] 74%|███████▍  | 20016/26933 [21:15<06:31, 17.68it/s] 74%|███████▍  | 20020/26933 [21:15<06:30, 17.69it/s] 74%|███████▍  | 20024/26933 [21:15<06:30, 17.68it/s] 74%|███████▍  | 20028/26933 [21:16<06:30, 17.69it/s] 74%|███████▍  | 20032/26933 [21:16<06:29, 17.70it/s] 74%|███████▍  | 20036/26933 [21:16<06:28, 17.74it/s] 74%|███████▍  | 20040/26933 [21:16<06:29, 17.69it/s] 74%|███████▍  | 20044/26933 [21:17<06:28, 17.72it/s] 74%|███████▍  | 20048/26933 [21:17<06:28, 17.72it/s] 74%|███████▍  | 20052/26933 [21:17<06:27, 17.74it/s] 74%|███████▍  | 20056/26933 [21:17<06:28, 17.70it/s] 74%|███████▍  | 20060/26933 [21:17<06:27, 17.76it/s] 74%|███████▍  | 20064/26933 [21:18<06:27, 17.75it/s] 75%|███████▍  | 20068/26933 [21:18<06:25, 17.80it/s] 75%|███████▍  | 20072/26933 [21:18<06:24, 17.82it/s] 75%|███████▍  | 20076/26933 [21:18<06:25, 17.77it/s] 75%|███████▍  | 20080/26933 [21:19<06:25, 17.80it/s] 75%|███████▍  | 20084/26933 [21:19<06:24, 17.79it/s] 75%|███████▍  | 20088/26933 [21:19<06:25, 17.78it/s] 75%|███████▍  | 20092/26933 [21:19<06:25, 17.73it/s] 75%|███████▍  | 20096/26933 [21:19<06:26, 17.71it/s] 75%|███████▍  | 20100/26933 [21:20<06:26, 17.70it/s] 75%|███████▍  | 20104/26933 [21:20<06:25, 17.73it/s] 75%|███████▍  | 20108/26933 [21:20<06:23, 17.78it/s] 75%|███████▍  | 20112/26933 [21:20<06:24, 17.74it/s] 75%|███████▍  | 20116/26933 [21:21<06:24, 17.73it/s] 75%|███████▍  | 20120/26933 [21:21<06:23, 17.76it/s] 75%|███████▍  | 20124/26933 [21:21<06:22, 17.80it/s] 75%|███████▍  | 20128/26933 [21:21<06:24, 17.71it/s] 75%|███████▍  | 20132/26933 [21:22<06:23, 17.74it/s] 75%|███████▍  | 20136/26933 [21:22<06:23, 17.72it/s] 75%|███████▍  | 20140/26933 [21:22<06:23, 17.74it/s] 75%|███████▍  | 20144/26933 [21:22<06:22, 17.75it/s] 75%|███████▍  | 20148/26933 [21:22<06:24, 17.64it/s] 75%|███████▍  | 20152/26933 [21:23<06:23, 17.68it/s] 75%|███████▍  | 20156/26933 [21:23<06:24, 17.65it/s] 75%|███████▍  | 20160/26933 [21:23<06:22, 17.71it/s] 75%|███████▍  | 20164/26933 [21:23<06:23, 17.65it/s] 75%|███████▍  | 20168/26933 [21:24<06:21, 17.71it/s] 75%|███████▍  | 20172/26933 [21:24<06:20, 17.75it/s] 75%|███████▍  | 20176/26933 [21:24<06:19, 17.80it/s] 75%|███████▍  | 20180/26933 [21:24<06:19, 17.81it/s] 75%|███████▍  | 20184/26933 [21:24<06:18, 17.81it/s] 75%|███████▍  | 20188/26933 [21:25<06:18, 17.84it/s] 75%|███████▍  | 20192/26933 [21:25<06:18, 17.80it/s] 75%|███████▍  | 20196/26933 [21:25<06:17, 17.83it/s] 75%|███████▌  | 20200/26933 [21:25<06:18, 17.77it/s] 75%|███████▌  | 20204/26933 [21:26<06:17, 17.81it/s] 75%|███████▌  | 20208/26933 [21:26<06:17, 17.81it/s] 75%|███████▌  | 20212/26933 [21:26<06:17, 17.81it/s] 75%|███████▌  | 20216/26933 [21:26<06:18, 17.77it/s] 75%|███████▌  | 20220/26933 [21:26<06:17, 17.78it/s] 75%|███████▌  | 20224/26933 [21:27<06:18, 17.72it/s] 75%|███████▌  | 20228/26933 [21:27<06:19, 17.65it/s] 75%|███████▌  | 20232/26933 [21:27<06:20, 17.63it/s] 75%|███████▌  | 20236/26933 [21:27<06:20, 17.58it/s] 75%|███████▌  | 20240/26933 [21:28<06:18, 17.67it/s] 75%|███████▌  | 20244/26933 [21:28<06:15, 17.80it/s] 75%|███████▌  | 20248/26933 [21:28<06:13, 17.90it/s] 75%|███████▌  | 20252/26933 [21:28<06:12, 17.91it/s] 75%|███████▌  | 20256/26933 [21:29<06:10, 18.00it/s] 75%|███████▌  | 20260/26933 [21:29<06:09, 18.04it/s] 75%|███████▌  | 20264/26933 [21:29<06:08, 18.09it/s] 75%|███████▌  | 20268/26933 [21:29<06:07, 18.12it/s] 75%|███████▌  | 20272/26933 [21:29<06:09, 18.05it/s] 75%|███████▌  | 20276/26933 [21:30<06:08, 18.05it/s] 75%|███████▌  | 20280/26933 [21:30<06:08, 18.08it/s] 75%|███████▌  | 20284/26933 [21:30<06:06, 18.13it/s] 75%|███████▌  | 20288/26933 [21:30<06:06, 18.11it/s] 75%|███████▌  | 20292/26933 [21:30<06:09, 17.99it/s] 75%|███████▌  | 20296/26933 [21:31<06:10, 17.93it/s] 75%|███████▌  | 20300/26933 [21:31<06:10, 17.92it/s] 75%|███████▌  | 20304/26933 [21:31<06:10, 17.88it/s] 75%|███████▌  | 20308/26933 [21:31<06:12, 17.78it/s] 75%|███████▌  | 20312/26933 [21:32<06:13, 17.74it/s] 75%|███████▌  | 20316/26933 [21:32<06:12, 17.75it/s] 75%|███████▌  | 20320/26933 [21:32<06:12, 17.76it/s] 75%|███████▌  | 20324/26933 [21:32<06:13, 17.71it/s] 75%|███████▌  | 20328/26933 [21:33<06:12, 17.74it/s] 75%|███████▌  | 20332/26933 [21:33<06:12, 17.74it/s] 76%|███████▌  | 20336/26933 [21:33<06:10, 17.80it/s] 76%|███████▌  | 20340/26933 [21:33<06:09, 17.82it/s] 76%|███████▌  | 20344/26933 [21:33<06:11, 17.72it/s] 76%|███████▌  | 20348/26933 [21:34<06:11, 17.74it/s] 76%|███████▌  | 20352/26933 [21:34<06:10, 17.75it/s] 76%|███████▌  | 20356/26933 [21:34<06:09, 17.80it/s] 76%|███████▌  | 20360/26933 [21:34<06:10, 17.75it/s] 76%|███████▌  | 20364/26933 [21:35<06:08, 17.83it/s] 76%|███████▌  | 20368/26933 [21:35<06:07, 17.85it/s] 76%|███████▌  | 20372/26933 [21:35<06:07, 17.86it/s] 76%|███████▌  | 20376/26933 [21:35<06:06, 17.88it/s] 76%|███████▌  | 20380/26933 [21:35<06:07, 17.83it/s] 76%|███████▌  | 20384/26933 [21:36<06:07, 17.84it/s] 76%|███████▌  | 20388/26933 [21:36<06:05, 17.89it/s] 76%|███████▌  | 20392/26933 [21:36<06:05, 17.88it/s] 76%|███████▌  | 20396/26933 [21:36<06:06, 17.85it/s] 76%|███████▌  | 20400/26933 [21:37<06:05, 17.87it/s] 76%|███████▌  | 20404/26933 [21:37<06:05, 17.88it/s] 76%|███████▌  | 20408/26933 [21:37<06:04, 17.89it/s] 76%|███████▌  | 20412/26933 [21:37<06:04, 17.90it/s] 76%|███████▌  | 20416/26933 [21:37<06:04, 17.87it/s] 76%|███████▌  | 20420/26933 [21:38<06:03, 17.92it/s] 76%|███████▌  | 20424/26933 [21:38<06:03, 17.92it/s] 76%|███████▌  | 20428/26933 [21:38<06:02, 17.94it/s] 76%|███████▌  | 20432/26933 [21:38<06:03, 17.88it/s] 76%|███████▌  | 20436/26933 [21:39<06:03, 17.89it/s] 76%|███████▌  | 20440/26933 [21:39<06:03, 17.85it/s] 76%|███████▌  | 20444/26933 [21:39<06:03, 17.86it/s] 76%|███████▌  | 20448/26933 [21:39<06:03, 17.85it/s] 76%|███████▌  | 20452/26933 [21:39<06:02, 17.86it/s] 76%|███████▌  | 20456/26933 [21:40<06:02, 17.86it/s] 76%|███████▌  | 20460/26933 [21:40<06:02, 17.83it/s] 76%|███████▌  | 20464/26933 [21:40<06:03, 17.82it/s] 76%|███████▌  | 20468/26933 [21:40<06:05, 17.70it/s] 76%|███████▌  | 20472/26933 [21:41<06:05, 17.66it/s] 76%|███████▌  | 20476/26933 [21:41<06:05, 17.66it/s] 76%|███████▌  | 20480/26933 [21:41<06:04, 17.68it/s] 76%|███████▌  | 20484/26933 [21:41<06:05, 17.64it/s] 76%|███████▌  | 20488/26933 [21:42<06:03, 17.73it/s] 76%|███████▌  | 20492/26933 [21:42<06:03, 17.74it/s] 76%|███████▌  | 20496/26933 [21:42<06:02, 17.77it/s] 76%|███████▌  | 20500/26933 [21:42<06:01, 17.78it/s] 76%|███████▌  | 20504/26933 [21:42<06:02, 17.74it/s] 76%|███████▌  | 20508/26933 [21:43<06:02, 17.73it/s] 76%|███████▌  | 20512/26933 [21:43<06:01, 17.75it/s] 76%|███████▌  | 20516/26933 [21:43<06:01, 17.73it/s] 76%|███████▌  | 20520/26933 [21:43<06:02, 17.69it/s] 76%|███████▌  | 20524/26933 [21:44<06:01, 17.73it/s] 76%|███████▌  | 20528/26933 [21:44<06:01, 17.73it/s] 76%|███████▌  | 20532/26933 [21:44<06:00, 17.74it/s] 76%|███████▌  | 20536/26933 [21:44<06:00, 17.72it/s] 76%|███████▋  | 20540/26933 [21:44<06:00, 17.71it/s] 76%|███████▋  | 20544/26933 [21:45<05:59, 17.75it/s] 76%|███████▋  | 20548/26933 [21:45<05:59, 17.76it/s] 76%|███████▋  | 20552/26933 [21:45<05:59, 17.73it/s] 76%|███████▋  | 20556/26933 [21:45<06:00, 17.70it/s] 76%|███████▋  | 20560/26933 [21:46<05:59, 17.75it/s] 76%|███████▋  | 20564/26933 [21:46<05:58, 17.76it/s] 76%|███████▋  | 20568/26933 [21:46<05:59, 17.70it/s] 76%|███████▋  | 20572/26933 [21:46<05:59, 17.71it/s] 76%|███████▋  | 20576/26933 [21:46<05:59, 17.67it/s] 76%|███████▋  | 20580/26933 [21:47<05:58, 17.71it/s] 76%|███████▋  | 20584/26933 [21:47<05:57, 17.74it/s] 76%|███████▋  | 20588/26933 [21:47<05:56, 17.78it/s] 76%|███████▋  | 20592/26933 [21:47<05:56, 17.81it/s] 76%|███████▋  | 20596/26933 [21:48<05:55, 17.82it/s] 76%|███████▋  | 20600/26933 [21:48<05:54, 17.85it/s] 77%|███████▋  | 20604/26933 [21:48<05:53, 17.88it/s] 77%|███████▋  | 20608/26933 [21:48<05:53, 17.88it/s] 77%|███████▋  | 20612/26933 [21:48<05:53, 17.90it/s] 77%|███████▋  | 20616/26933 [21:49<05:52, 17.91it/s] 77%|███████▋  | 20620/26933 [21:49<05:52, 17.89it/s] 77%|███████▋  | 20624/26933 [21:49<05:52, 17.91it/s] 77%|███████▋  | 20628/26933 [21:49<05:52, 17.87it/s] 77%|███████▋  | 20632/26933 [21:50<05:52, 17.88it/s] 77%|███████▋  | 20636/26933 [21:50<05:52, 17.88it/s] 77%|███████▋  | 20640/26933 [21:50<05:52, 17.88it/s] 77%|███████▋  | 20644/26933 [21:50<05:53, 17.81it/s] 77%|███████▋  | 20648/26933 [21:50<05:52, 17.85it/s] 77%|███████▋  | 20652/26933 [21:51<05:50, 17.91it/s] 77%|███████▋  | 20656/26933 [21:51<05:48, 18.00it/s] 77%|███████▋  | 20660/26933 [21:51<05:46, 18.09it/s] 77%|███████▋  | 20664/26933 [21:51<05:45, 18.13it/s] 77%|███████▋  | 20668/26933 [21:52<05:44, 18.18it/s] 77%|███████▋  | 20672/26933 [21:52<05:43, 18.23it/s] 77%|███████▋  | 20676/26933 [21:52<05:42, 18.27it/s] 77%|███████▋  | 20680/26933 [21:52<05:42, 18.24it/s] 77%|███████▋  | 20684/26933 [21:52<05:42, 18.27it/s] 77%|███████▋  | 20688/26933 [21:53<05:41, 18.27it/s] 77%|███████▋  | 20692/26933 [21:53<05:41, 18.26it/s] 77%|███████▋  | 20696/26933 [21:53<05:42, 18.19it/s] 77%|███████▋  | 20700/26933 [21:53<05:47, 17.93it/s] 77%|███████▋  | 20704/26933 [21:54<05:48, 17.88it/s] 77%|███████▋  | 20708/26933 [21:54<05:49, 17.82it/s] 77%|███████▋  | 20712/26933 [21:54<05:49, 17.79it/s] 77%|███████▋  | 20716/26933 [21:54<05:50, 17.72it/s] 77%|███████▋  | 20720/26933 [21:54<05:50, 17.73it/s] 77%|███████▋  | 20724/26933 [21:55<05:50, 17.72it/s] 77%|███████▋  | 20728/26933 [21:55<05:50, 17.71it/s] 77%|███████▋  | 20732/26933 [21:55<05:50, 17.68it/s] 77%|███████▋  | 20736/26933 [21:55<05:52, 17.59it/s] 77%|███████▋  | 20740/26933 [21:56<05:54, 17.48it/s] 77%|███████▋  | 20744/26933 [21:56<05:55, 17.39it/s] 77%|███████▋  | 20748/26933 [21:56<05:56, 17.33it/s] 77%|███████▋  | 20752/26933 [21:56<05:58, 17.26it/s] 77%|███████▋  | 20756/26933 [21:57<05:57, 17.26it/s] 77%|███████▋  | 20760/26933 [21:57<05:57, 17.26it/s] 77%|███████▋  | 20764/26933 [21:57<05:58, 17.22it/s] 77%|███████▋  | 20768/26933 [21:57<06:00, 17.08it/s] 77%|███████▋  | 20772/26933 [21:57<06:00, 17.11it/s] 77%|███████▋  | 20776/26933 [21:58<06:01, 17.05it/s] 77%|███████▋  | 20780/26933 [21:58<06:02, 16.99it/s] 77%|███████▋  | 20784/26933 [21:58<06:04, 16.88it/s] 77%|███████▋  | 20788/26933 [21:58<06:04, 16.85it/s] 77%|███████▋  | 20792/26933 [21:59<05:59, 17.07it/s] 77%|███████▋  | 20796/26933 [21:59<05:56, 17.24it/s] 77%|███████▋  | 20800/26933 [21:59<05:51, 17.47it/s] 77%|███████▋  | 20804/26933 [21:59<05:51, 17.44it/s] 77%|███████▋  | 20808/26933 [22:00<05:46, 17.66it/s] 77%|███████▋  | 20812/26933 [22:00<05:43, 17.82it/s] 77%|███████▋  | 20816/26933 [22:00<05:41, 17.90it/s] 77%|███████▋  | 20820/26933 [22:00<05:40, 17.97it/s] 77%|███████▋  | 20824/26933 [22:00<05:40, 17.95it/s] 77%|███████▋  | 20828/26933 [22:01<05:40, 17.94it/s] 77%|███████▋  | 20832/26933 [22:01<05:39, 17.99it/s] 77%|███████▋  | 20836/26933 [22:01<05:38, 18.03it/s] 77%|███████▋  | 20840/26933 [22:01<05:37, 18.06it/s] 77%|███████▋  | 20844/26933 [22:02<05:36, 18.11it/s] 77%|███████▋  | 20848/26933 [22:02<05:35, 18.16it/s] 77%|███████▋  | 20852/26933 [22:02<05:34, 18.20it/s] 77%|███████▋  | 20856/26933 [22:02<05:33, 18.20it/s] 77%|███████▋  | 20860/26933 [22:02<05:34, 18.18it/s] 77%|███████▋  | 20864/26933 [22:03<05:32, 18.23it/s] 77%|███████▋  | 20868/26933 [22:03<05:32, 18.24it/s] 77%|███████▋  | 20872/26933 [22:03<05:32, 18.24it/s] 78%|███████▊  | 20876/26933 [22:03<05:33, 18.18it/s] 78%|███████▊  | 20880/26933 [22:04<05:32, 18.20it/s] 78%|███████▊  | 20884/26933 [22:04<05:31, 18.26it/s] 78%|███████▊  | 20888/26933 [22:04<05:30, 18.28it/s] 78%|███████▊  | 20892/26933 [22:04<05:30, 18.26it/s] 78%|███████▊  | 20896/26933 [22:04<05:31, 18.23it/s] 78%|███████▊  | 20900/26933 [22:05<05:30, 18.23it/s] 78%|███████▊  | 20904/26933 [22:05<05:30, 18.24it/s] 78%|███████▊  | 20908/26933 [22:05<05:29, 18.27it/s] 78%|███████▊  | 20912/26933 [22:05<05:29, 18.25it/s] 78%|███████▊  | 20916/26933 [22:06<05:29, 18.27it/s] 78%|███████▊  | 20920/26933 [22:06<05:28, 18.29it/s] 78%|███████▊  | 20924/26933 [22:06<05:28, 18.29it/s] 78%|███████▊  | 20928/26933 [22:06<05:28, 18.30it/s] 78%|███████▊  | 20932/26933 [22:06<05:28, 18.26it/s] 78%|███████▊  | 20936/26933 [22:07<05:33, 17.99it/s] 78%|███████▊  | 20940/26933 [22:07<05:31, 18.08it/s] 78%|███████▊  | 20944/26933 [22:07<05:29, 18.15it/s] 78%|███████▊  | 20948/26933 [22:07<05:29, 18.15it/s] 78%|███████▊  | 20952/26933 [22:07<05:28, 18.21it/s] 78%|███████▊  | 20956/26933 [22:08<05:27, 18.27it/s] 78%|███████▊  | 20960/26933 [22:08<05:26, 18.29it/s] 78%|███████▊  | 20964/26933 [22:08<05:26, 18.27it/s] 78%|███████▊  | 20968/26933 [22:08<05:26, 18.25it/s] 78%|███████▊  | 20972/26933 [22:09<05:26, 18.27it/s] 78%|███████▊  | 20976/26933 [22:09<05:25, 18.28it/s] 78%|███████▊  | 20980/26933 [22:09<05:25, 18.29it/s] 78%|███████▊  | 20984/26933 [22:09<05:25, 18.29it/s] 78%|███████▊  | 20988/26933 [22:09<05:24, 18.29it/s] 78%|███████▊  | 20992/26933 [22:10<05:25, 18.26it/s] 78%|███████▊  | 20996/26933 [22:10<05:24, 18.28it/s] 78%|███████▊  | 21000/26933 [22:10<05:24, 18.27it/s] 78%|███████▊  | 21004/26933 [22:10<05:25, 18.23it/s] 78%|███████▊  | 21008/26933 [22:11<05:24, 18.26it/s] 78%|███████▊  | 21012/26933 [22:11<05:23, 18.28it/s] 78%|███████▊  | 21016/26933 [22:11<05:23, 18.28it/s] 78%|███████▊  | 21020/26933 [22:11<05:22, 18.34it/s] 78%|███████▊  | 21024/26933 [22:11<05:22, 18.31it/s] 78%|███████▊  | 21028/26933 [22:12<05:21, 18.35it/s] 78%|███████▊  | 21032/26933 [22:12<05:21, 18.35it/s] 78%|███████▊  | 21036/26933 [22:12<05:22, 18.29it/s] 78%|███████▊  | 21040/26933 [22:12<05:24, 18.18it/s] 78%|███████▊  | 21044/26933 [22:13<05:23, 18.22it/s] 78%|███████▊  | 21048/26933 [22:13<05:22, 18.25it/s] 78%|███████▊  | 21052/26933 [22:13<05:21, 18.27it/s] 78%|███████▊  | 21056/26933 [22:13<05:21, 18.29it/s] 78%|███████▊  | 21060/26933 [22:13<05:21, 18.26it/s] 78%|███████▊  | 21064/26933 [22:14<05:21, 18.25it/s] 78%|███████▊  | 21068/26933 [22:14<05:21, 18.27it/s] 78%|███████▊  | 21072/26933 [22:14<05:20, 18.29it/s] 78%|███████▊  | 21076/26933 [22:14<05:21, 18.24it/s] 78%|███████▊  | 21080/26933 [22:14<05:21, 18.23it/s] 78%|███████▊  | 21084/26933 [22:15<05:20, 18.24it/s] 78%|███████▊  | 21088/26933 [22:15<05:19, 18.29it/s] 78%|███████▊  | 21092/26933 [22:15<05:19, 18.30it/s] 78%|███████▊  | 21096/26933 [22:15<05:19, 18.26it/s] 78%|███████▊  | 21100/26933 [22:16<05:19, 18.27it/s] 78%|███████▊  | 21104/26933 [22:16<05:18, 18.30it/s] 78%|███████▊  | 21108/26933 [22:16<05:18, 18.28it/s] 78%|███████▊  | 21112/26933 [22:16<05:19, 18.20it/s] 78%|███████▊  | 21116/26933 [22:16<05:19, 18.23it/s] 78%|███████▊  | 21120/26933 [22:17<05:18, 18.26it/s] 78%|███████▊  | 21124/26933 [22:17<05:17, 18.27it/s] 78%|███████▊  | 21128/26933 [22:17<05:17, 18.29it/s] 78%|███████▊  | 21132/26933 [22:17<05:21, 18.05it/s] 78%|███████▊  | 21136/26933 [22:18<05:23, 17.94it/s] 78%|███████▊  | 21140/26933 [22:18<05:22, 17.94it/s] 79%|███████▊  | 21144/26933 [22:18<05:23, 17.91it/s] 79%|███████▊  | 21148/26933 [22:18<05:24, 17.83it/s] 79%|███████▊  | 21152/26933 [22:18<05:24, 17.82it/s] 79%|███████▊  | 21156/26933 [22:19<05:24, 17.81it/s] 79%|███████▊  | 21160/26933 [22:19<05:24, 17.79it/s] 79%|███████▊  | 21164/26933 [22:19<05:24, 17.76it/s] 79%|███████▊  | 21168/26933 [22:19<05:26, 17.65it/s] 79%|███████▊  | 21172/26933 [22:20<05:25, 17.67it/s] 79%|███████▊  | 21176/26933 [22:20<05:25, 17.70it/s] 79%|███████▊  | 21180/26933 [22:20<05:24, 17.71it/s] 79%|███████▊  | 21184/26933 [22:20<05:25, 17.65it/s] 79%|███████▊  | 21188/26933 [22:21<05:25, 17.64it/s] 79%|███████▊  | 21192/26933 [22:21<05:25, 17.66it/s] 79%|███████▊  | 21196/26933 [22:21<05:24, 17.67it/s] 79%|███████▊  | 21200/26933 [22:21<05:23, 17.70it/s] 79%|███████▊  | 21204/26933 [22:21<05:24, 17.65it/s] 79%|███████▊  | 21208/26933 [22:22<05:24, 17.66it/s] 79%|███████▉  | 21212/26933 [22:22<05:23, 17.69it/s] 79%|███████▉  | 21216/26933 [22:22<05:22, 17.72it/s] 79%|███████▉  | 21220/26933 [22:22<05:23, 17.67it/s] 79%|███████▉  | 21224/26933 [22:23<05:22, 17.71it/s] 79%|███████▉  | 21228/26933 [22:23<05:21, 17.72it/s] 79%|███████▉  | 21232/26933 [22:23<05:21, 17.72it/s] 79%|███████▉  | 21236/26933 [22:23<05:21, 17.74it/s] 79%|███████▉  | 21240/26933 [22:23<05:22, 17.67it/s] 79%|███████▉  | 21244/26933 [22:24<05:21, 17.68it/s] 79%|███████▉  | 21248/26933 [22:24<05:22, 17.61it/s] 79%|███████▉  | 21252/26933 [22:24<05:21, 17.67it/s] 79%|███████▉  | 21256/26933 [22:24<05:20, 17.69it/s] 79%|███████▉  | 21260/26933 [22:25<05:22, 17.62it/s] 79%|███████▉  | 21264/26933 [22:25<05:23, 17.52it/s] 79%|███████▉  | 21268/26933 [22:25<05:21, 17.63it/s] 79%|███████▉  | 21272/26933 [22:25<05:20, 17.69it/s] 79%|███████▉  | 21276/26933 [22:25<05:18, 17.74it/s] 79%|███████▉  | 21280/26933 [22:26<05:18, 17.75it/s] 79%|███████▉  | 21284/26933 [22:26<05:17, 17.78it/s] 79%|███████▉  | 21288/26933 [22:26<05:17, 17.77it/s] 79%|███████▉  | 21292/26933 [22:26<05:18, 17.72it/s] 79%|███████▉  | 21296/26933 [22:27<05:17, 17.75it/s] 79%|███████▉  | 21300/26933 [22:27<05:16, 17.80it/s] 79%|███████▉  | 21304/26933 [22:27<05:15, 17.85it/s] 79%|███████▉  | 21308/26933 [22:27<05:16, 17.76it/s] 79%|███████▉  | 21312/26933 [22:28<05:16, 17.74it/s] 79%|███████▉  | 21316/26933 [22:28<05:15, 17.79it/s] 79%|███████▉  | 21320/26933 [22:28<05:15, 17.78it/s] 79%|███████▉  | 21324/26933 [22:28<05:15, 17.80it/s] 79%|███████▉  | 21328/26933 [22:28<05:16, 17.72it/s] 79%|███████▉  | 21332/26933 [22:29<05:15, 17.74it/s] 79%|███████▉  | 21336/26933 [22:29<05:15, 17.76it/s] 79%|███████▉  | 21340/26933 [22:29<05:14, 17.80it/s] 79%|███████▉  | 21344/26933 [22:29<05:16, 17.69it/s] 79%|███████▉  | 21348/26933 [22:30<05:15, 17.69it/s] 79%|███████▉  | 21352/26933 [22:30<05:14, 17.73it/s] 79%|███████▉  | 21356/26933 [22:30<05:13, 17.77it/s] 79%|███████▉  | 21360/26933 [22:30<05:13, 17.79it/s] 79%|███████▉  | 21364/26933 [22:30<05:14, 17.71it/s] 79%|███████▉  | 21368/26933 [22:31<05:12, 17.79it/s] 79%|███████▉  | 21372/26933 [22:31<05:12, 17.79it/s] 79%|███████▉  | 21376/26933 [22:31<05:12, 17.81it/s] 79%|███████▉  | 21380/26933 [22:31<05:12, 17.78it/s] 79%|███████▉  | 21384/26933 [22:32<05:11, 17.80it/s] 79%|███████▉  | 21388/26933 [22:32<05:11, 17.82it/s] 79%|███████▉  | 21392/26933 [22:32<05:10, 17.85it/s] 79%|███████▉  | 21396/26933 [22:32<05:09, 17.88it/s] 79%|███████▉  | 21400/26933 [22:32<05:10, 17.80it/s] 79%|███████▉  | 21404/26933 [22:33<05:09, 17.84it/s] 79%|███████▉  | 21408/26933 [22:33<05:09, 17.85it/s] 80%|███████▉  | 21412/26933 [22:33<05:09, 17.87it/s] 80%|███████▉  | 21416/26933 [22:33<05:10, 17.77it/s] 80%|███████▉  | 21420/26933 [22:34<05:09, 17.79it/s] 80%|███████▉  | 21424/26933 [22:34<05:09, 17.80it/s] 80%|███████▉  | 21428/26933 [22:34<05:08, 17.82it/s] 80%|███████▉  | 21432/26933 [22:34<05:09, 17.75it/s] 80%|███████▉  | 21436/26933 [22:34<05:09, 17.77it/s] 80%|███████▉  | 21440/26933 [22:35<05:08, 17.81it/s] 80%|███████▉  | 21444/26933 [22:35<05:07, 17.86it/s] 80%|███████▉  | 21448/26933 [22:35<05:05, 17.96it/s] 80%|███████▉  | 21452/26933 [22:35<05:06, 17.90it/s] 80%|███████▉  | 21456/26933 [22:36<05:04, 17.96it/s] 80%|███████▉  | 21460/26933 [22:36<05:03, 18.04it/s] 80%|███████▉  | 21464/26933 [22:36<05:02, 18.09it/s] 80%|███████▉  | 21468/26933 [22:36<05:04, 17.97it/s] 80%|███████▉  | 21472/26933 [22:36<05:04, 17.95it/s] 80%|███████▉  | 21476/26933 [22:37<05:03, 17.98it/s] 80%|███████▉  | 21480/26933 [22:37<05:03, 17.98it/s] 80%|███████▉  | 21484/26933 [22:37<05:02, 17.99it/s] 80%|███████▉  | 21488/26933 [22:37<05:03, 17.91it/s] 80%|███████▉  | 21492/26933 [22:38<05:04, 17.85it/s] 80%|███████▉  | 21496/26933 [22:38<05:03, 17.90it/s] 80%|███████▉  | 21500/26933 [22:38<05:02, 17.98it/s] 80%|███████▉  | 21504/26933 [22:38<05:01, 18.00it/s] 80%|███████▉  | 21508/26933 [22:38<05:00, 18.07it/s] 80%|███████▉  | 21512/26933 [22:39<04:58, 18.13it/s] 80%|███████▉  | 21516/26933 [22:39<04:57, 18.19it/s] 80%|███████▉  | 21520/26933 [22:39<04:57, 18.18it/s] 80%|███████▉  | 21524/26933 [22:39<04:58, 18.12it/s] 80%|███████▉  | 21528/26933 [22:40<04:58, 18.12it/s] 80%|███████▉  | 21532/26933 [22:40<04:57, 18.14it/s] 80%|███████▉  | 21536/26933 [22:40<04:56, 18.20it/s] 80%|███████▉  | 21540/26933 [22:40<04:57, 18.11it/s] 80%|███████▉  | 21544/26933 [22:40<04:57, 18.14it/s] 80%|████████  | 21548/26933 [22:41<04:56, 18.14it/s] 80%|████████  | 21552/26933 [22:41<04:56, 18.14it/s] 80%|████████  | 21556/26933 [22:41<04:57, 18.08it/s] 80%|████████  | 21560/26933 [22:41<04:57, 18.05it/s] 80%|████████  | 21564/26933 [22:42<04:56, 18.13it/s] 80%|████████  | 21568/26933 [22:42<04:56, 18.12it/s] 80%|████████  | 21572/26933 [22:42<04:55, 18.14it/s] 80%|████████  | 21576/26933 [22:42<04:55, 18.14it/s] 80%|████████  | 21580/26933 [22:42<04:55, 18.09it/s] 80%|████████  | 21584/26933 [22:43<04:54, 18.15it/s] 80%|████████  | 21588/26933 [22:43<04:54, 18.18it/s] 80%|████████  | 21592/26933 [22:43<04:53, 18.22it/s] 80%|████████  | 21596/26933 [22:43<04:53, 18.16it/s] 80%|████████  | 21600/26933 [22:44<04:53, 18.16it/s] 80%|████████  | 21604/26933 [22:44<04:52, 18.20it/s] 80%|████████  | 21608/26933 [22:44<04:53, 18.14it/s] 80%|████████  | 21612/26933 [22:44<04:54, 18.08it/s] 80%|████████  | 21616/26933 [22:44<04:55, 17.96it/s] 80%|████████  | 21620/26933 [22:45<04:56, 17.94it/s] 80%|████████  | 21624/26933 [22:45<04:55, 17.95it/s] 80%|████████  | 21628/26933 [22:45<04:56, 17.90it/s] 80%|████████  | 21632/26933 [22:45<04:57, 17.82it/s] 80%|████████  | 21636/26933 [22:46<04:56, 17.88it/s] 80%|████████  | 21640/26933 [22:46<04:55, 17.91it/s] 80%|████████  | 21644/26933 [22:46<04:56, 17.85it/s] 80%|████████  | 21648/26933 [22:46<04:55, 17.86it/s] 80%|████████  | 21652/26933 [22:46<04:56, 17.83it/s] 80%|████████  | 21656/26933 [22:47<04:56, 17.82it/s] 80%|████████  | 21660/26933 [22:47<04:56, 17.79it/s] 80%|████████  | 21664/26933 [22:47<04:55, 17.86it/s] 80%|████████  | 21668/26933 [22:47<04:54, 17.87it/s] 80%|████████  | 21672/26933 [22:48<04:52, 17.97it/s] 80%|████████  | 21676/26933 [22:48<04:51, 18.03it/s] 80%|████████  | 21680/26933 [22:48<04:50, 18.07it/s] 81%|████████  | 21684/26933 [22:48<04:49, 18.11it/s] 81%|████████  | 21688/26933 [22:48<04:50, 18.07it/s] 81%|████████  | 21692/26933 [22:49<04:48, 18.14it/s] 81%|████████  | 21696/26933 [22:49<04:48, 18.13it/s] 81%|████████  | 21700/26933 [22:49<04:48, 18.15it/s] 81%|████████  | 21704/26933 [22:49<04:50, 18.01it/s] 81%|████████  | 21708/26933 [22:50<04:50, 17.96it/s] 81%|████████  | 21712/26933 [22:50<04:51, 17.89it/s] 81%|████████  | 21716/26933 [22:50<04:52, 17.82it/s] 81%|████████  | 21720/26933 [22:50<04:53, 17.75it/s] 81%|████████  | 21724/26933 [22:50<04:53, 17.73it/s] 81%|████████  | 21728/26933 [22:51<04:52, 17.77it/s] 81%|████████  | 21732/26933 [22:51<04:52, 17.76it/s] 81%|████████  | 21736/26933 [22:51<04:52, 17.77it/s] 81%|████████  | 21740/26933 [22:51<04:53, 17.71it/s] 81%|████████  | 21744/26933 [22:52<04:52, 17.74it/s] 81%|████████  | 21748/26933 [22:52<04:52, 17.75it/s] 81%|████████  | 21752/26933 [22:52<04:51, 17.77it/s] 81%|████████  | 21756/26933 [22:52<04:51, 17.74it/s] 81%|████████  | 21760/26933 [22:53<04:51, 17.73it/s] 81%|████████  | 21764/26933 [22:53<04:49, 17.86it/s] 81%|████████  | 21768/26933 [22:53<04:47, 17.99it/s] 81%|████████  | 21772/26933 [22:53<04:44, 18.13it/s] 81%|████████  | 21776/26933 [22:53<04:44, 18.11it/s] 81%|████████  | 21780/26933 [22:54<04:44, 18.08it/s] 81%|████████  | 21784/26933 [22:54<04:44, 18.07it/s] 81%|████████  | 21788/26933 [22:54<04:44, 18.10it/s] 81%|████████  | 21792/26933 [22:54<04:47, 17.91it/s] 81%|████████  | 21796/26933 [22:54<04:45, 18.02it/s] 81%|████████  | 21800/26933 [22:55<04:44, 18.07it/s] 81%|████████  | 21804/26933 [22:55<04:43, 18.09it/s] 81%|████████  | 21808/26933 [22:55<04:42, 18.12it/s] 81%|████████  | 21812/26933 [22:55<04:42, 18.11it/s] 81%|████████  | 21816/26933 [22:56<04:42, 18.14it/s] 81%|████████  | 21820/26933 [22:56<04:41, 18.14it/s] 81%|████████  | 21824/26933 [22:56<04:41, 18.15it/s] 81%|████████  | 21828/26933 [22:56<04:43, 18.03it/s] 81%|████████  | 21832/26933 [22:56<04:43, 18.00it/s] 81%|████████  | 21836/26933 [22:57<04:43, 17.96it/s] 81%|████████  | 21840/26933 [22:57<04:44, 17.90it/s] 81%|████████  | 21844/26933 [22:57<04:44, 17.90it/s] 81%|████████  | 21848/26933 [22:57<04:45, 17.84it/s] 81%|████████  | 21852/26933 [22:58<04:44, 17.83it/s] 81%|████████  | 21856/26933 [22:58<04:44, 17.85it/s] 81%|████████  | 21860/26933 [22:58<04:43, 17.87it/s] 81%|████████  | 21864/26933 [22:58<04:44, 17.83it/s] 81%|████████  | 21868/26933 [22:59<04:43, 17.87it/s] 81%|████████  | 21872/26933 [22:59<04:43, 17.87it/s] 81%|████████  | 21876/26933 [22:59<04:42, 17.90it/s] 81%|████████  | 21880/26933 [22:59<04:42, 17.89it/s] 81%|████████▏ | 21884/26933 [22:59<04:43, 17.84it/s] 81%|████████▏ | 21888/26933 [23:00<04:42, 17.85it/s] 81%|████████▏ | 21892/26933 [23:00<04:42, 17.86it/s] 81%|████████▏ | 21896/26933 [23:00<04:42, 17.84it/s] 81%|████████▏ | 21900/26933 [23:00<04:42, 17.82it/s] 81%|████████▏ | 21904/26933 [23:01<04:41, 17.84it/s] 81%|████████▏ | 21908/26933 [23:01<04:41, 17.86it/s] 81%|████████▏ | 21912/26933 [23:01<04:41, 17.84it/s] 81%|████████▏ | 21916/26933 [23:01<04:40, 17.87it/s] 81%|████████▏ | 21920/26933 [23:01<04:41, 17.81it/s] 81%|████████▏ | 21924/26933 [23:02<04:40, 17.84it/s] 81%|████████▏ | 21928/26933 [23:02<04:40, 17.85it/s] 81%|████████▏ | 21932/26933 [23:02<04:39, 17.86it/s] 81%|████████▏ | 21936/26933 [23:02<04:40, 17.82it/s] 81%|████████▏ | 21940/26933 [23:03<04:40, 17.82it/s] 81%|████████▏ | 21944/26933 [23:03<04:39, 17.84it/s] 81%|████████▏ | 21948/26933 [23:03<04:40, 17.80it/s] 82%|████████▏ | 21952/26933 [23:03<04:40, 17.78it/s] 82%|████████▏ | 21956/26933 [23:03<04:41, 17.70it/s] 82%|████████▏ | 21960/26933 [23:04<04:40, 17.72it/s] 82%|████████▏ | 21964/26933 [23:04<04:39, 17.77it/s] 82%|████████▏ | 21968/26933 [23:04<04:39, 17.78it/s] 82%|████████▏ | 21972/26933 [23:04<04:40, 17.71it/s] 82%|████████▏ | 21976/26933 [23:05<04:39, 17.71it/s] 82%|████████▏ | 21980/26933 [23:05<04:39, 17.69it/s] 82%|████████▏ | 21984/26933 [23:05<04:39, 17.70it/s] 82%|████████▏ | 21988/26933 [23:05<04:40, 17.61it/s] 82%|████████▏ | 21992/26933 [23:05<04:40, 17.64it/s] 82%|████████▏ | 21996/26933 [23:06<04:39, 17.68it/s] 82%|████████▏ | 22000/26933 [23:06<04:38, 17.73it/s] 82%|████████▏ | 22004/26933 [23:06<04:37, 17.74it/s] 82%|████████▏ | 22008/26933 [23:06<04:38, 17.66it/s] 82%|████████▏ | 22012/26933 [23:07<04:37, 17.70it/s] 82%|████████▏ | 22016/26933 [23:07<04:37, 17.70it/s] 82%|████████▏ | 22020/26933 [23:07<04:37, 17.70it/s] 82%|████████▏ | 22024/26933 [23:07<04:38, 17.64it/s] 82%|████████▏ | 22028/26933 [23:08<04:37, 17.69it/s] 82%|████████▏ | 22032/26933 [23:08<04:36, 17.71it/s] 82%|████████▏ | 22036/26933 [23:08<04:36, 17.73it/s] 82%|████████▏ | 22040/26933 [23:08<04:33, 17.86it/s] 82%|████████▏ | 22044/26933 [23:08<04:33, 17.87it/s] 82%|████████▏ | 22048/26933 [23:09<04:32, 17.93it/s] 82%|████████▏ | 22052/26933 [23:09<04:31, 17.99it/s] 82%|████████▏ | 22056/26933 [23:09<04:30, 18.05it/s] 82%|████████▏ | 22060/26933 [23:09<04:30, 18.00it/s] 82%|████████▏ | 22064/26933 [23:10<04:29, 18.07it/s] 82%|████████▏ | 22068/26933 [23:10<04:28, 18.10it/s] 82%|████████▏ | 22072/26933 [23:10<04:28, 18.10it/s] 82%|████████▏ | 22076/26933 [23:10<04:28, 18.12it/s] 82%|████████▏ | 22080/26933 [23:10<04:28, 18.08it/s] 82%|████████▏ | 22084/26933 [23:11<04:27, 18.10it/s] 82%|████████▏ | 22088/26933 [23:11<04:27, 18.09it/s] 82%|████████▏ | 22092/26933 [23:11<04:27, 18.13it/s] 82%|████████▏ | 22096/26933 [23:11<04:27, 18.09it/s] 82%|████████▏ | 22100/26933 [23:11<04:27, 18.10it/s] 82%|████████▏ | 22104/26933 [23:12<04:26, 18.11it/s] 82%|████████▏ | 22108/26933 [23:12<04:26, 18.08it/s] 82%|████████▏ | 22112/26933 [23:12<04:26, 18.09it/s] 82%|████████▏ | 22116/26933 [23:12<04:27, 18.04it/s] 82%|████████▏ | 22120/26933 [23:13<04:26, 18.06it/s] 82%|████████▏ | 22124/26933 [23:13<04:25, 18.10it/s] 82%|████████▏ | 22128/26933 [23:13<04:25, 18.12it/s] 82%|████████▏ | 22132/26933 [23:13<04:25, 18.06it/s] 82%|████████▏ | 22136/26933 [23:13<04:24, 18.11it/s] 82%|████████▏ | 22140/26933 [23:14<04:24, 18.10it/s] 82%|████████▏ | 22144/26933 [23:14<04:24, 18.13it/s] 82%|████████▏ | 22148/26933 [23:14<04:24, 18.12it/s] 82%|████████▏ | 22152/26933 [23:14<04:24, 18.08it/s] 82%|████████▏ | 22156/26933 [23:15<04:24, 18.09it/s] 82%|████████▏ | 22160/26933 [23:15<04:23, 18.08it/s] 82%|████████▏ | 22164/26933 [23:15<04:23, 18.11it/s] 82%|████████▏ | 22168/26933 [23:15<04:23, 18.05it/s] 82%|████████▏ | 22172/26933 [23:15<04:23, 18.10it/s] 82%|████████▏ | 22176/26933 [23:16<04:22, 18.13it/s] 82%|████████▏ | 22180/26933 [23:16<04:22, 18.12it/s] 82%|████████▏ | 22184/26933 [23:16<04:22, 18.12it/s] 82%|████████▏ | 22188/26933 [23:16<04:23, 18.02it/s] 82%|████████▏ | 22192/26933 [23:17<04:22, 18.04it/s] 82%|████████▏ | 22196/26933 [23:17<04:22, 18.03it/s] 82%|████████▏ | 22200/26933 [23:17<04:21, 18.07it/s] 82%|████████▏ | 22204/26933 [23:17<04:24, 17.91it/s] 82%|████████▏ | 22208/26933 [23:17<04:24, 17.85it/s] 82%|████████▏ | 22212/26933 [23:18<04:25, 17.79it/s] 82%|████████▏ | 22216/26933 [23:18<04:25, 17.77it/s] 83%|████████▎ | 22220/26933 [23:18<04:25, 17.74it/s] 83%|████████▎ | 22224/26933 [23:18<04:26, 17.70it/s] 83%|████████▎ | 22228/26933 [23:19<04:26, 17.65it/s] 83%|████████▎ | 22232/26933 [23:19<04:26, 17.66it/s] 83%|████████▎ | 22236/26933 [23:19<04:25, 17.66it/s] 83%|████████▎ | 22240/26933 [23:19<04:26, 17.60it/s] 83%|████████▎ | 22244/26933 [23:20<04:25, 17.66it/s] 83%|████████▎ | 22248/26933 [23:20<04:24, 17.68it/s] 83%|████████▎ | 22252/26933 [23:20<04:24, 17.71it/s] 83%|████████▎ | 22256/26933 [23:20<04:24, 17.70it/s] 83%|████████▎ | 22260/26933 [23:20<04:24, 17.70it/s] 83%|████████▎ | 22264/26933 [23:21<04:23, 17.70it/s] 83%|████████▎ | 22268/26933 [23:21<04:23, 17.70it/s] 83%|████████▎ | 22272/26933 [23:21<04:23, 17.66it/s] 83%|████████▎ | 22276/26933 [23:21<04:24, 17.63it/s] 83%|████████▎ | 22280/26933 [23:22<04:23, 17.65it/s] 83%|████████▎ | 22284/26933 [23:22<04:23, 17.66it/s] 83%|████████▎ | 22288/26933 [23:22<04:23, 17.66it/s] 83%|████████▎ | 22292/26933 [23:22<04:22, 17.67it/s] 83%|████████▎ | 22296/26933 [23:22<04:22, 17.66it/s] 83%|████████▎ | 22300/26933 [23:23<04:21, 17.68it/s] 83%|████████▎ | 22304/26933 [23:23<04:21, 17.71it/s] 83%|████████▎ | 22308/26933 [23:23<04:20, 17.77it/s] 83%|████████▎ | 22312/26933 [23:23<04:21, 17.68it/s] 83%|████████▎ | 22316/26933 [23:24<04:21, 17.65it/s] 83%|████████▎ | 22320/26933 [23:24<04:21, 17.62it/s] 83%|████████▎ | 22324/26933 [23:24<04:21, 17.65it/s] 83%|████████▎ | 22328/26933 [23:24<04:21, 17.62it/s] 83%|████████▎ | 22332/26933 [23:25<04:20, 17.66it/s] 83%|████████▎ | 22336/26933 [23:25<04:20, 17.66it/s] 83%|████████▎ | 22340/26933 [23:25<04:20, 17.64it/s] 83%|████████▎ | 22344/26933 [23:25<04:19, 17.68it/s] 83%|████████▎ | 22348/26933 [23:25<04:19, 17.64it/s] 83%|████████▎ | 22352/26933 [23:26<04:19, 17.63it/s] 83%|████████▎ | 22356/26933 [23:26<04:20, 17.60it/s] 83%|████████▎ | 22360/26933 [23:26<04:19, 17.64it/s] 83%|████████▎ | 22364/26933 [23:26<04:19, 17.60it/s] 83%|████████▎ | 22368/26933 [23:27<04:18, 17.66it/s] 83%|████████▎ | 22372/26933 [23:27<04:17, 17.69it/s] 83%|████████▎ | 22376/26933 [23:27<04:17, 17.70it/s] 83%|████████▎ | 22380/26933 [23:27<04:17, 17.70it/s] 83%|████████▎ | 22384/26933 [23:27<04:17, 17.67it/s] 83%|████████▎ | 22388/26933 [23:28<04:17, 17.67it/s] 83%|████████▎ | 22392/26933 [23:28<04:23, 17.24it/s] 83%|████████▎ | 22396/26933 [23:28<04:20, 17.41it/s] 83%|████████▎ | 22400/26933 [23:28<04:19, 17.50it/s] 83%|████████▎ | 22404/26933 [23:29<04:17, 17.58it/s] 83%|████████▎ | 22408/26933 [23:29<04:16, 17.66it/s] 83%|████████▎ | 22412/26933 [23:29<04:15, 17.69it/s] 83%|████████▎ | 22416/26933 [23:29<04:16, 17.63it/s] 83%|████████▎ | 22420/26933 [23:29<04:15, 17.65it/s] 83%|████████▎ | 22424/26933 [23:30<04:14, 17.69it/s] 83%|████████▎ | 22428/26933 [23:30<04:14, 17.72it/s] 83%|████████▎ | 22432/26933 [23:30<04:13, 17.73it/s] 83%|████████▎ | 22436/26933 [23:30<04:13, 17.71it/s] 83%|████████▎ | 22440/26933 [23:31<04:13, 17.72it/s] 83%|████████▎ | 22444/26933 [23:31<04:13, 17.71it/s] 83%|████████▎ | 22448/26933 [23:31<04:12, 17.73it/s] 83%|████████▎ | 22452/26933 [23:31<04:13, 17.68it/s] 83%|████████▎ | 22456/26933 [23:32<04:12, 17.74it/s] 83%|████████▎ | 22460/26933 [23:32<04:11, 17.78it/s] 83%|████████▎ | 22464/26933 [23:32<04:11, 17.78it/s] 83%|████████▎ | 22468/26933 [23:32<04:11, 17.75it/s] 83%|████████▎ | 22472/26933 [23:32<04:11, 17.73it/s] 83%|████████▎ | 22476/26933 [23:33<04:10, 17.77it/s] 83%|████████▎ | 22480/26933 [23:33<04:10, 17.76it/s] 83%|████████▎ | 22484/26933 [23:33<04:10, 17.76it/s] 83%|████████▎ | 22488/26933 [23:33<04:15, 17.43it/s] 84%|████████▎ | 22492/26933 [23:34<04:13, 17.49it/s] 84%|████████▎ | 22496/26933 [23:34<04:13, 17.53it/s] 84%|████████▎ | 22500/26933 [23:34<04:11, 17.61it/s] 84%|████████▎ | 22504/26933 [23:34<04:12, 17.55it/s] 84%|████████▎ | 22508/26933 [23:34<04:10, 17.64it/s] 84%|████████▎ | 22512/26933 [23:35<04:09, 17.70it/s] 84%|████████▎ | 22516/26933 [23:35<04:11, 17.60it/s] 84%|████████▎ | 22520/26933 [23:35<04:16, 17.21it/s] 84%|████████▎ | 22524/26933 [23:35<04:13, 17.37it/s] 84%|████████▎ | 22528/26933 [23:36<04:11, 17.53it/s] 84%|████████▎ | 22532/26933 [23:36<04:09, 17.63it/s] 84%|████████▎ | 22536/26933 [23:36<04:08, 17.69it/s] 84%|████████▎ | 22540/26933 [23:36<04:07, 17.76it/s] 84%|████████▎ | 22544/26933 [23:37<04:05, 17.86it/s] 84%|████████▎ | 22548/26933 [23:37<04:04, 17.96it/s] 84%|████████▎ | 22552/26933 [23:37<04:03, 18.02it/s] 84%|████████▎ | 22556/26933 [23:37<04:02, 18.07it/s] 84%|████████▍ | 22560/26933 [23:37<04:01, 18.08it/s] 84%|████████▍ | 22564/26933 [23:38<04:01, 18.11it/s] 84%|████████▍ | 22568/26933 [23:38<04:00, 18.17it/s] 84%|████████▍ | 22572/26933 [23:38<04:00, 18.16it/s] 84%|████████▍ | 22576/26933 [23:38<04:04, 17.85it/s] 84%|████████▍ | 22580/26933 [23:39<04:02, 17.92it/s] 84%|████████▍ | 22584/26933 [23:39<04:02, 17.95it/s] 84%|████████▍ | 22588/26933 [23:39<04:01, 17.96it/s] 84%|████████▍ | 22592/26933 [23:39<04:02, 17.90it/s] 84%|████████▍ | 22596/26933 [23:39<04:03, 17.81it/s] 84%|████████▍ | 22600/26933 [23:40<04:02, 17.88it/s] 84%|████████▍ | 22604/26933 [23:40<04:01, 17.93it/s] 84%|████████▍ | 22608/26933 [23:40<04:00, 18.01it/s] 84%|████████▍ | 22612/26933 [23:40<04:04, 17.67it/s] 84%|████████▍ | 22616/26933 [23:41<04:02, 17.77it/s] 84%|████████▍ | 22620/26933 [23:41<04:02, 17.80it/s] 84%|████████▍ | 22624/26933 [23:41<04:01, 17.87it/s] 84%|████████▍ | 22628/26933 [23:41<04:01, 17.83it/s] 84%|████████▍ | 22632/26933 [23:41<04:06, 17.44it/s] 84%|████████▍ | 22636/26933 [23:42<04:05, 17.48it/s] 84%|████████▍ | 22640/26933 [23:42<04:07, 17.31it/s] 84%|████████▍ | 22644/26933 [23:42<04:05, 17.45it/s] 84%|████████▍ | 22648/26933 [23:42<04:08, 17.24it/s] 84%|████████▍ | 22652/26933 [23:43<04:05, 17.41it/s] 84%|████████▍ | 22656/26933 [23:43<04:03, 17.54it/s] 84%|████████▍ | 22660/26933 [23:43<04:04, 17.46it/s] 84%|████████▍ | 22664/26933 [23:43<04:07, 17.25it/s] 84%|████████▍ | 22668/26933 [23:44<04:04, 17.43it/s] 84%|████████▍ | 22672/26933 [23:44<04:03, 17.52it/s] 84%|████████▍ | 22676/26933 [23:44<04:01, 17.62it/s] 84%|████████▍ | 22680/26933 [23:44<04:00, 17.68it/s] 84%|████████▍ | 22684/26933 [23:44<04:03, 17.43it/s] 84%|████████▍ | 22688/26933 [23:45<04:01, 17.56it/s] 84%|████████▍ | 22692/26933 [23:45<04:00, 17.65it/s] 84%|████████▍ | 22696/26933 [23:45<03:59, 17.70it/s] 84%|████████▍ | 22700/26933 [23:45<04:02, 17.45it/s] 84%|████████▍ | 22704/26933 [23:46<04:00, 17.59it/s] 84%|████████▍ | 22708/26933 [23:46<03:59, 17.64it/s] 84%|████████▍ | 22712/26933 [23:46<03:58, 17.68it/s] 84%|████████▍ | 22716/26933 [23:46<03:57, 17.74it/s] 84%|████████▍ | 22720/26933 [23:46<04:01, 17.46it/s] 84%|████████▍ | 22724/26933 [23:47<03:59, 17.59it/s] 84%|████████▍ | 22728/26933 [23:47<03:58, 17.62it/s] 84%|████████▍ | 22732/26933 [23:47<03:57, 17.65it/s] 84%|████████▍ | 22736/26933 [23:47<04:01, 17.39it/s] 84%|████████▍ | 22740/26933 [23:48<03:59, 17.52it/s] 84%|████████▍ | 22744/26933 [23:48<03:58, 17.60it/s] 84%|████████▍ | 22748/26933 [23:48<03:57, 17.65it/s] 84%|████████▍ | 22752/26933 [23:48<03:59, 17.45it/s] 84%|████████▍ | 22756/26933 [23:49<03:57, 17.57it/s] 85%|████████▍ | 22760/26933 [23:49<03:56, 17.68it/s] 85%|████████▍ | 22764/26933 [23:49<03:55, 17.73it/s] 85%|████████▍ | 22768/26933 [23:49<03:54, 17.78it/s] 85%|████████▍ | 22772/26933 [23:49<03:57, 17.54it/s] 85%|████████▍ | 22776/26933 [23:50<03:55, 17.64it/s] 85%|████████▍ | 22780/26933 [23:50<03:54, 17.73it/s] 85%|████████▍ | 22784/26933 [23:50<03:53, 17.75it/s] 85%|████████▍ | 22788/26933 [23:50<03:56, 17.50it/s] 85%|████████▍ | 22792/26933 [23:51<03:54, 17.63it/s] 85%|████████▍ | 22796/26933 [23:51<03:54, 17.65it/s] 85%|████████▍ | 22800/26933 [23:51<03:53, 17.66it/s] 85%|████████▍ | 22804/26933 [23:51<03:53, 17.69it/s] 85%|████████▍ | 22808/26933 [23:51<03:56, 17.45it/s] 85%|████████▍ | 22812/26933 [23:52<03:54, 17.54it/s] 85%|████████▍ | 22816/26933 [23:52<03:53, 17.60it/s] 85%|████████▍ | 22820/26933 [23:52<03:53, 17.62it/s] 85%|████████▍ | 22824/26933 [23:52<03:55, 17.43it/s] 85%|████████▍ | 22828/26933 [23:53<03:54, 17.52it/s] 85%|████████▍ | 22832/26933 [23:53<03:53, 17.58it/s] 85%|████████▍ | 22836/26933 [23:53<03:52, 17.62it/s] 85%|████████▍ | 22840/26933 [23:53<03:55, 17.41it/s] 85%|████████▍ | 22844/26933 [23:54<03:53, 17.52it/s] 85%|████████▍ | 22848/26933 [23:54<03:51, 17.62it/s] 85%|████████▍ | 22852/26933 [23:54<03:50, 17.67it/s] 85%|████████▍ | 22856/26933 [23:54<03:50, 17.71it/s] 85%|████████▍ | 22860/26933 [23:54<03:52, 17.50it/s] 85%|████████▍ | 22864/26933 [23:55<03:50, 17.67it/s] 85%|████████▍ | 22868/26933 [23:55<03:48, 17.77it/s] 85%|████████▍ | 22872/26933 [23:55<03:48, 17.78it/s] 85%|████████▍ | 22876/26933 [23:55<03:51, 17.50it/s] 85%|████████▍ | 22880/26933 [23:56<03:50, 17.60it/s] 85%|████████▍ | 22884/26933 [23:56<03:49, 17.67it/s] 85%|████████▍ | 22888/26933 [23:56<03:48, 17.72it/s] 85%|████████▍ | 22892/26933 [23:56<03:47, 17.73it/s] 85%|████████▌ | 22896/26933 [23:56<03:51, 17.46it/s] 85%|████████▌ | 22900/26933 [23:57<03:49, 17.54it/s] 85%|████████▌ | 22904/26933 [23:57<03:48, 17.62it/s] 85%|████████▌ | 22908/26933 [23:57<03:47, 17.67it/s] 85%|████████▌ | 22912/26933 [23:57<03:50, 17.41it/s] 85%|████████▌ | 22916/26933 [23:58<03:48, 17.56it/s] 85%|████████▌ | 22920/26933 [23:58<03:47, 17.64it/s] 85%|████████▌ | 22924/26933 [23:58<03:46, 17.68it/s] 85%|████████▌ | 22928/26933 [23:58<03:49, 17.45it/s] 85%|████████▌ | 22932/26933 [23:59<03:48, 17.54it/s] 85%|████████▌ | 22936/26933 [23:59<03:46, 17.62it/s] 85%|████████▌ | 22940/26933 [23:59<03:45, 17.71it/s] 85%|████████▌ | 22944/26933 [23:59<03:44, 17.78it/s] 85%|████████▌ | 22948/26933 [23:59<03:47, 17.51it/s] 85%|████████▌ | 22952/26933 [24:00<03:46, 17.60it/s] 85%|████████▌ | 22956/26933 [24:00<03:45, 17.66it/s] 85%|████████▌ | 22960/26933 [24:00<03:44, 17.70it/s] 85%|████████▌ | 22964/26933 [24:00<03:47, 17.44it/s] 85%|████████▌ | 22968/26933 [24:01<03:45, 17.56it/s] 85%|████████▌ | 22972/26933 [24:01<03:44, 17.65it/s] 85%|████████▌ | 22976/26933 [24:01<03:43, 17.72it/s] 85%|████████▌ | 22980/26933 [24:01<03:42, 17.75it/s] 85%|████████▌ | 22984/26933 [24:01<03:45, 17.49it/s] 85%|████████▌ | 22988/26933 [24:02<03:44, 17.57it/s] 85%|████████▌ | 22992/26933 [24:02<03:43, 17.64it/s] 85%|████████▌ | 22996/26933 [24:02<03:43, 17.65it/s] 85%|████████▌ | 23000/26933 [24:02<03:45, 17.41it/s] 85%|████████▌ | 23004/26933 [24:03<03:44, 17.52it/s] 85%|████████▌ | 23008/26933 [24:03<03:42, 17.62it/s] 85%|████████▌ | 23012/26933 [24:03<03:41, 17.68it/s] 85%|████████▌ | 23016/26933 [24:03<03:44, 17.45it/s] 85%|████████▌ | 23020/26933 [24:04<03:42, 17.56it/s] 85%|████████▌ | 23024/26933 [24:04<03:42, 17.60it/s] 86%|████████▌ | 23028/26933 [24:04<03:41, 17.65it/s] 86%|████████▌ | 23032/26933 [24:04<03:40, 17.67it/s] 86%|████████▌ | 23036/26933 [24:04<03:43, 17.40it/s] 86%|████████▌ | 23040/26933 [24:05<03:42, 17.51it/s] 86%|████████▌ | 23044/26933 [24:05<03:41, 17.59it/s] 86%|████████▌ | 23048/26933 [24:05<03:39, 17.66it/s] 86%|████████▌ | 23052/26933 [24:05<03:42, 17.46it/s] 86%|████████▌ | 23056/26933 [24:06<03:40, 17.55it/s] 86%|████████▌ | 23060/26933 [24:06<03:39, 17.64it/s] 86%|████████▌ | 23064/26933 [24:06<03:38, 17.67it/s] 86%|████████▌ | 23068/26933 [24:06<03:38, 17.71it/s] 86%|████████▌ | 23072/26933 [24:06<03:41, 17.40it/s] 86%|████████▌ | 23076/26933 [24:07<03:39, 17.53it/s] 86%|████████▌ | 23080/26933 [24:07<03:39, 17.59it/s] 86%|████████▌ | 23084/26933 [24:07<03:37, 17.66it/s] 86%|████████▌ | 23088/26933 [24:07<03:40, 17.41it/s] 86%|████████▌ | 23092/26933 [24:08<03:39, 17.53it/s] 86%|████████▌ | 23096/26933 [24:08<03:37, 17.60it/s] 86%|████████▌ | 23100/26933 [24:08<03:37, 17.66it/s] 86%|████████▌ | 23104/26933 [24:08<03:40, 17.40it/s] 86%|████████▌ | 23108/26933 [24:09<03:38, 17.52it/s] 86%|████████▌ | 23112/26933 [24:09<03:37, 17.59it/s] 86%|████████▌ | 23116/26933 [24:09<03:36, 17.65it/s] 86%|████████▌ | 23120/26933 [24:09<03:35, 17.71it/s] 86%|████████▌ | 23124/26933 [24:09<03:37, 17.48it/s] 86%|████████▌ | 23128/26933 [24:10<03:36, 17.59it/s] 86%|████████▌ | 23132/26933 [24:10<03:35, 17.64it/s] 86%|████████▌ | 23136/26933 [24:10<03:34, 17.69it/s] 86%|████████▌ | 23140/26933 [24:10<03:37, 17.44it/s] 86%|████████▌ | 23144/26933 [24:11<03:35, 17.55it/s] 86%|████████▌ | 23148/26933 [24:11<03:34, 17.62it/s] 86%|████████▌ | 23152/26933 [24:11<03:34, 17.65it/s] 86%|████████▌ | 23156/26933 [24:11<03:33, 17.71it/s] 86%|████████▌ | 23160/26933 [24:11<03:35, 17.49it/s] 86%|████████▌ | 23164/26933 [24:12<03:34, 17.58it/s] 86%|████████▌ | 23168/26933 [24:12<03:33, 17.62it/s] 86%|████████▌ | 23172/26933 [24:12<03:32, 17.70it/s] 86%|████████▌ | 23176/26933 [24:12<03:35, 17.43it/s] 86%|████████▌ | 23180/26933 [24:13<03:33, 17.55it/s] 86%|████████▌ | 23184/26933 [24:13<03:31, 17.69it/s] 86%|████████▌ | 23188/26933 [24:13<03:31, 17.75it/s] 86%|████████▌ | 23192/26933 [24:13<03:33, 17.51it/s] 86%|████████▌ | 23196/26933 [24:14<03:32, 17.57it/s] 86%|████████▌ | 23200/26933 [24:14<03:32, 17.56it/s] 86%|████████▌ | 23204/26933 [24:14<03:31, 17.65it/s] 86%|████████▌ | 23208/26933 [24:14<03:30, 17.72it/s] 86%|████████▌ | 23212/26933 [24:14<03:33, 17.46it/s] 86%|████████▌ | 23216/26933 [24:15<03:31, 17.56it/s] 86%|████████▌ | 23220/26933 [24:15<03:30, 17.62it/s] 86%|████████▌ | 23224/26933 [24:15<03:29, 17.68it/s] 86%|████████▌ | 23228/26933 [24:15<03:32, 17.43it/s] 86%|████████▋ | 23232/26933 [24:16<03:31, 17.53it/s] 86%|████████▋ | 23236/26933 [24:16<03:30, 17.60it/s] 86%|████████▋ | 23240/26933 [24:16<03:29, 17.62it/s] 86%|████████▋ | 23244/26933 [24:16<03:28, 17.69it/s] 86%|████████▋ | 23248/26933 [24:16<03:31, 17.44it/s] 86%|████████▋ | 23252/26933 [24:17<03:29, 17.55it/s] 86%|████████▋ | 23256/26933 [24:17<03:28, 17.63it/s] 86%|████████▋ | 23260/26933 [24:17<03:27, 17.70it/s] 86%|████████▋ | 23264/26933 [24:17<03:29, 17.53it/s] 86%|████████▋ | 23268/26933 [24:18<03:27, 17.66it/s] 86%|████████▋ | 23272/26933 [24:18<03:26, 17.73it/s] 86%|████████▋ | 23276/26933 [24:18<03:26, 17.74it/s] 86%|████████▋ | 23280/26933 [24:18<03:28, 17.50it/s] 86%|████████▋ | 23284/26933 [24:19<03:27, 17.58it/s] 86%|████████▋ | 23288/26933 [24:19<03:26, 17.63it/s] 86%|████████▋ | 23292/26933 [24:19<03:26, 17.61it/s] 86%|████████▋ | 23296/26933 [24:19<03:25, 17.66it/s] 87%|████████▋ | 23300/26933 [24:19<03:28, 17.43it/s] 87%|████████▋ | 23304/26933 [24:20<03:27, 17.49it/s] 87%|████████▋ | 23308/26933 [24:20<03:26, 17.53it/s] 87%|████████▋ | 23312/26933 [24:20<03:25, 17.59it/s] 87%|████████▋ | 23316/26933 [24:20<03:27, 17.42it/s] 87%|████████▋ | 23320/26933 [24:21<03:26, 17.54it/s] 87%|████████▋ | 23324/26933 [24:21<03:24, 17.63it/s] 87%|████████▋ | 23328/26933 [24:21<03:23, 17.73it/s] 87%|████████▋ | 23332/26933 [24:21<03:22, 17.80it/s] 87%|████████▋ | 23336/26933 [24:21<03:25, 17.54it/s] 87%|████████▋ | 23340/26933 [24:22<03:23, 17.63it/s] 87%|████████▋ | 23344/26933 [24:22<03:23, 17.67it/s] 87%|████████▋ | 23348/26933 [24:22<03:22, 17.67it/s] 87%|████████▋ | 23352/26933 [24:22<03:25, 17.42it/s] 87%|████████▋ | 23356/26933 [24:23<03:23, 17.55it/s] 87%|████████▋ | 23360/26933 [24:23<03:22, 17.64it/s] 87%|████████▋ | 23364/26933 [24:23<03:21, 17.72it/s] 87%|████████▋ | 23368/26933 [24:23<03:23, 17.49it/s] 87%|████████▋ | 23372/26933 [24:24<03:22, 17.59it/s] 87%|████████▋ | 23376/26933 [24:24<03:21, 17.66it/s] 87%|████████▋ | 23380/26933 [24:24<03:20, 17.72it/s] 87%|████████▋ | 23384/26933 [24:24<03:19, 17.77it/s] 87%|████████▋ | 23388/26933 [24:24<03:22, 17.54it/s] 87%|████████▋ | 23392/26933 [24:25<03:20, 17.64it/s] 87%|████████▋ | 23396/26933 [24:25<03:19, 17.71it/s] 87%|████████▋ | 23400/26933 [24:25<03:18, 17.78it/s] 87%|████████▋ | 23404/26933 [24:25<03:20, 17.58it/s] 87%|████████▋ | 23408/26933 [24:26<03:19, 17.71it/s] 87%|████████▋ | 23412/26933 [24:26<03:18, 17.76it/s] 87%|████████▋ | 23416/26933 [24:26<03:18, 17.73it/s] 87%|████████▋ | 23420/26933 [24:26<03:17, 17.76it/s] 87%|████████▋ | 23424/26933 [24:26<03:19, 17.56it/s] 87%|████████▋ | 23428/26933 [24:27<03:18, 17.67it/s] 87%|████████▋ | 23432/26933 [24:27<03:17, 17.71it/s] 87%|████████▋ | 23436/26933 [24:27<03:16, 17.76it/s] 87%|████████▋ | 23440/26933 [24:27<03:18, 17.56it/s] 87%|████████▋ | 23444/26933 [24:28<03:17, 17.68it/s] 87%|████████▋ | 23448/26933 [24:28<03:16, 17.74it/s] 87%|████████▋ | 23452/26933 [24:28<03:16, 17.70it/s] 87%|████████▋ | 23456/26933 [24:28<03:18, 17.51it/s] 87%|████████▋ | 23460/26933 [24:28<03:16, 17.64it/s] 87%|████████▋ | 23464/26933 [24:29<03:15, 17.73it/s] 87%|████████▋ | 23468/26933 [24:29<03:14, 17.78it/s] 87%|████████▋ | 23472/26933 [24:29<03:13, 17.85it/s] 87%|████████▋ | 23476/26933 [24:29<03:16, 17.60it/s] 87%|████████▋ | 23480/26933 [24:30<03:15, 17.63it/s] 87%|████████▋ | 23484/26933 [24:30<03:15, 17.66it/s] 87%|████████▋ | 23488/26933 [24:30<03:15, 17.66it/s] 87%|████████▋ | 23492/26933 [24:30<03:17, 17.40it/s] 87%|████████▋ | 23496/26933 [24:31<03:16, 17.45it/s] 87%|████████▋ | 23500/26933 [24:31<03:15, 17.55it/s] 87%|████████▋ | 23504/26933 [24:31<03:14, 17.62it/s] 87%|████████▋ | 23508/26933 [24:31<03:13, 17.68it/s] 87%|████████▋ | 23512/26933 [24:31<03:15, 17.47it/s] 87%|████████▋ | 23516/26933 [24:32<03:14, 17.58it/s] 87%|████████▋ | 23520/26933 [24:32<03:13, 17.65it/s] 87%|████████▋ | 23524/26933 [24:32<03:12, 17.71it/s] 87%|████████▋ | 23528/26933 [24:32<03:14, 17.48it/s] 87%|████████▋ | 23532/26933 [24:33<03:13, 17.56it/s] 87%|████████▋ | 23536/26933 [24:33<03:12, 17.62it/s] 87%|████████▋ | 23540/26933 [24:33<03:12, 17.66it/s] 87%|████████▋ | 23544/26933 [24:33<03:14, 17.43it/s] 87%|████████▋ | 23548/26933 [24:33<03:13, 17.51it/s] 87%|████████▋ | 23552/26933 [24:34<03:11, 17.61it/s] 87%|████████▋ | 23556/26933 [24:34<03:11, 17.67it/s] 87%|████████▋ | 23560/26933 [24:34<03:10, 17.70it/s] 87%|████████▋ | 23564/26933 [24:34<03:12, 17.47it/s] 88%|████████▊ | 23568/26933 [24:35<03:11, 17.59it/s] 88%|████████▊ | 23572/26933 [24:35<03:10, 17.68it/s] 88%|████████▊ | 23576/26933 [24:35<03:09, 17.74it/s] 88%|████████▊ | 23580/26933 [24:35<03:11, 17.48it/s] 88%|████████▊ | 23584/26933 [24:36<03:10, 17.57it/s] 88%|████████▊ | 23588/26933 [24:36<03:09, 17.63it/s] 88%|████████▊ | 23592/26933 [24:36<03:09, 17.67it/s] 88%|████████▊ | 23596/26933 [24:36<03:07, 17.79it/s] 88%|████████▊ | 23600/26933 [24:36<03:08, 17.64it/s] 88%|████████▊ | 23604/26933 [24:37<03:07, 17.80it/s] 88%|████████▊ | 23608/26933 [24:37<03:05, 17.90it/s] 88%|████████▊ | 23612/26933 [24:37<03:04, 18.03it/s] 88%|████████▊ | 23616/26933 [24:37<03:05, 17.85it/s] 88%|████████▊ | 23620/26933 [24:38<03:04, 17.98it/s] 88%|████████▊ | 23624/26933 [24:38<03:03, 18.01it/s] 88%|████████▊ | 23628/26933 [24:38<03:03, 18.00it/s] 88%|████████▊ | 23632/26933 [24:38<03:02, 18.04it/s] 88%|████████▊ | 23636/26933 [24:38<03:04, 17.87it/s] 88%|████████▊ | 23640/26933 [24:39<03:03, 17.96it/s] 88%|████████▊ | 23644/26933 [24:39<03:02, 18.02it/s] 88%|████████▊ | 23648/26933 [24:39<03:02, 18.04it/s] 88%|████████▊ | 23652/26933 [24:39<03:03, 17.87it/s] 88%|████████▊ | 23656/26933 [24:40<03:02, 17.97it/s] 88%|████████▊ | 23660/26933 [24:40<03:01, 18.02it/s] 88%|████████▊ | 23664/26933 [24:40<03:00, 18.07it/s] 88%|████████▊ | 23668/26933 [24:40<03:00, 18.13it/s] 88%|████████▊ | 23672/26933 [24:40<03:01, 17.95it/s] 88%|████████▊ | 23676/26933 [24:41<03:00, 18.05it/s] 88%|████████▊ | 23680/26933 [24:41<02:59, 18.08it/s] 88%|████████▊ | 23684/26933 [24:41<02:59, 18.11it/s] 88%|████████▊ | 23688/26933 [24:41<03:01, 17.90it/s] 88%|████████▊ | 23692/26933 [24:42<02:59, 18.01it/s] 88%|████████▊ | 23696/26933 [24:42<02:59, 18.06it/s] 88%|████████▊ | 23700/26933 [24:42<02:58, 18.14it/s] 88%|████████▊ | 23704/26933 [24:42<02:57, 18.17it/s] 88%|████████▊ | 23708/26933 [24:42<02:59, 17.95it/s] 88%|████████▊ | 23712/26933 [24:43<02:58, 18.06it/s] 88%|████████▊ | 23716/26933 [24:43<02:57, 18.11it/s] 88%|████████▊ | 23720/26933 [24:43<02:56, 18.18it/s] 88%|████████▊ | 23724/26933 [24:43<02:58, 18.00it/s] 88%|████████▊ | 23728/26933 [24:44<02:57, 18.07it/s] 88%|████████▊ | 23732/26933 [24:44<02:56, 18.14it/s] 88%|████████▊ | 23736/26933 [24:44<02:56, 18.15it/s] 88%|████████▊ | 23740/26933 [24:44<02:55, 18.17it/s] 88%|████████▊ | 23744/26933 [24:44<02:57, 17.97it/s] 88%|████████▊ | 23748/26933 [24:45<02:56, 18.03it/s] 88%|████████▊ | 23752/26933 [24:45<02:55, 18.11it/s] 88%|████████▊ | 23756/26933 [24:45<02:54, 18.18it/s] 88%|████████▊ | 23760/26933 [24:45<02:56, 18.02it/s] 88%|████████▊ | 23764/26933 [24:46<02:55, 18.01it/s] 88%|████████▊ | 23768/26933 [24:46<02:55, 17.98it/s] 88%|████████▊ | 23772/26933 [24:46<02:55, 18.00it/s] 88%|████████▊ | 23776/26933 [24:46<02:55, 17.97it/s] 88%|████████▊ | 23780/26933 [24:46<02:57, 17.76it/s] 88%|████████▊ | 23784/26933 [24:47<02:56, 17.84it/s] 88%|████████▊ | 23788/26933 [24:47<02:55, 17.91it/s] 88%|████████▊ | 23792/26933 [24:47<02:55, 17.95it/s] 88%|████████▊ | 23796/26933 [24:47<02:56, 17.74it/s] 88%|████████▊ | 23800/26933 [24:48<02:55, 17.82it/s] 88%|████████▊ | 23804/26933 [24:48<02:55, 17.83it/s] 88%|████████▊ | 23808/26933 [24:48<02:55, 17.85it/s] 88%|████████▊ | 23812/26933 [24:48<02:54, 17.91it/s] 88%|████████▊ | 23816/26933 [24:48<02:55, 17.72it/s] 88%|████████▊ | 23820/26933 [24:49<02:54, 17.81it/s] 88%|████████▊ | 23824/26933 [24:49<02:54, 17.82it/s] 88%|████████▊ | 23828/26933 [24:49<02:53, 17.87it/s] 88%|████████▊ | 23832/26933 [24:49<02:55, 17.69it/s] 89%|████████▊ | 23836/26933 [24:50<02:54, 17.79it/s] 89%|████████▊ | 23840/26933 [24:50<02:53, 17.80it/s] 89%|████████▊ | 23844/26933 [24:50<02:53, 17.85it/s] 89%|████████▊ | 23848/26933 [24:50<02:52, 17.90it/s] 89%|████████▊ | 23852/26933 [24:50<02:54, 17.66it/s] 89%|████████▊ | 23856/26933 [24:51<02:53, 17.73it/s] 89%|████████▊ | 23860/26933 [24:51<02:52, 17.79it/s] 89%|████████▊ | 23864/26933 [24:51<02:51, 17.85it/s] 89%|████████▊ | 23868/26933 [24:51<02:53, 17.70it/s] 89%|████████▊ | 23872/26933 [24:52<02:52, 17.78it/s] 89%|████████▊ | 23876/26933 [24:52<02:51, 17.86it/s] 89%|████████▊ | 23880/26933 [24:52<02:50, 17.86it/s] 89%|████████▊ | 23884/26933 [24:52<02:53, 17.62it/s] 89%|████████▊ | 23888/26933 [24:52<02:51, 17.71it/s] 89%|████████▊ | 23892/26933 [24:53<02:51, 17.77it/s] 89%|████████▊ | 23896/26933 [24:53<02:50, 17.84it/s] 89%|████████▊ | 23900/26933 [24:53<02:49, 17.87it/s] 89%|████████▉ | 23904/26933 [24:53<02:51, 17.67it/s] 89%|████████▉ | 23908/26933 [24:54<02:50, 17.73it/s] 89%|████████▉ | 23912/26933 [24:54<02:49, 17.81it/s] 89%|████████▉ | 23916/26933 [24:54<02:49, 17.83it/s] 89%|████████▉ | 23920/26933 [24:54<02:50, 17.67it/s] 89%|████████▉ | 23924/26933 [24:55<02:49, 17.74it/s] 89%|████████▉ | 23928/26933 [24:55<02:49, 17.78it/s] 89%|████████▉ | 23932/26933 [24:55<02:48, 17.81it/s] 89%|████████▉ | 23936/26933 [24:55<02:48, 17.80it/s] 89%|████████▉ | 23940/26933 [24:55<02:49, 17.63it/s] 89%|████████▉ | 23944/26933 [24:56<02:48, 17.71it/s] 89%|████████▉ | 23948/26933 [24:56<02:48, 17.76it/s] 89%|████████▉ | 23952/26933 [24:56<02:47, 17.80it/s] 89%|████████▉ | 23956/26933 [24:56<02:49, 17.59it/s] 89%|████████▉ | 23960/26933 [24:57<02:48, 17.66it/s] 89%|████████▉ | 23964/26933 [24:57<02:47, 17.75it/s] 89%|████████▉ | 23968/26933 [24:57<02:46, 17.82it/s] 89%|████████▉ | 23972/26933 [24:57<02:45, 17.87it/s] 89%|████████▉ | 23976/26933 [24:57<02:47, 17.68it/s] 89%|████████▉ | 23980/26933 [24:58<02:46, 17.77it/s] 89%|████████▉ | 23984/26933 [24:58<02:45, 17.86it/s] 89%|████████▉ | 23988/26933 [24:58<02:44, 17.91it/s] 89%|████████▉ | 23992/26933 [24:58<02:46, 17.69it/s] 89%|████████▉ | 23996/26933 [24:59<02:45, 17.75it/s] 89%|████████▉ | 24000/26933 [24:59<02:44, 17.82it/s] 89%|████████▉ | 24004/26933 [24:59<02:43, 17.88it/s] 89%|████████▉ | 24008/26933 [24:59<02:43, 17.93it/s] 89%|████████▉ | 24012/26933 [24:59<02:44, 17.71it/s] 89%|████████▉ | 24016/26933 [25:00<02:44, 17.76it/s] 89%|████████▉ | 24020/26933 [25:00<02:43, 17.78it/s] 89%|████████▉ | 24024/26933 [25:00<02:43, 17.78it/s] 89%|████████▉ | 24028/26933 [25:00<02:44, 17.61it/s] 89%|████████▉ | 24032/26933 [25:01<02:43, 17.73it/s] 89%|████████▉ | 24036/26933 [25:01<02:42, 17.78it/s] 89%|████████▉ | 24040/26933 [25:01<02:42, 17.80it/s] 89%|████████▉ | 24044/26933 [25:01<02:44, 17.61it/s] 89%|████████▉ | 24048/26933 [25:02<02:43, 17.69it/s] 89%|████████▉ | 24052/26933 [25:02<02:42, 17.74it/s] 89%|████████▉ | 24056/26933 [25:02<02:41, 17.78it/s] 89%|████████▉ | 24060/26933 [25:02<02:41, 17.79it/s] 89%|████████▉ | 24064/26933 [25:02<02:42, 17.64it/s] 89%|████████▉ | 24068/26933 [25:03<02:41, 17.74it/s] 89%|████████▉ | 24072/26933 [25:03<02:40, 17.81it/s] 89%|████████▉ | 24076/26933 [25:03<02:40, 17.82it/s] 89%|████████▉ | 24080/26933 [25:03<02:41, 17.68it/s] 89%|████████▉ | 24084/26933 [25:04<02:40, 17.75it/s] 89%|████████▉ | 24088/26933 [25:04<02:39, 17.80it/s] 89%|████████▉ | 24092/26933 [25:04<02:39, 17.85it/s] 89%|████████▉ | 24096/26933 [25:04<02:38, 17.90it/s] 89%|████████▉ | 24100/26933 [25:04<02:39, 17.75it/s] 89%|████████▉ | 24104/26933 [25:05<02:38, 17.81it/s] 90%|████████▉ | 24108/26933 [25:05<02:38, 17.84it/s] 90%|████████▉ | 24112/26933 [25:05<02:38, 17.85it/s] 90%|████████▉ | 24116/26933 [25:05<02:39, 17.69it/s] 90%|████████▉ | 24120/26933 [25:06<02:38, 17.74it/s] 90%|████████▉ | 24124/26933 [25:06<02:37, 17.78it/s] 90%|████████▉ | 24128/26933 [25:06<02:37, 17.79it/s] 90%|████████▉ | 24132/26933 [25:06<02:37, 17.78it/s] 90%|████████▉ | 24136/26933 [25:06<02:42, 17.24it/s] 90%|████████▉ | 24140/26933 [25:07<02:40, 17.35it/s] 90%|████████▉ | 24144/26933 [25:07<02:39, 17.53it/s] 90%|████████▉ | 24148/26933 [25:07<02:37, 17.65it/s] 90%|████████▉ | 24152/26933 [25:07<02:38, 17.55it/s] 90%|████████▉ | 24156/26933 [25:08<02:37, 17.66it/s] 90%|████████▉ | 24160/26933 [25:08<02:36, 17.70it/s] 90%|████████▉ | 24164/26933 [25:08<02:35, 17.77it/s] 90%|████████▉ | 24168/26933 [25:08<02:36, 17.67it/s] 90%|████████▉ | 24172/26933 [25:09<02:35, 17.80it/s] 90%|████████▉ | 24176/26933 [25:09<02:34, 17.85it/s] 90%|████████▉ | 24180/26933 [25:09<02:33, 17.91it/s] 90%|████████▉ | 24184/26933 [25:09<02:33, 17.94it/s] 90%|████████▉ | 24188/26933 [25:09<02:34, 17.78it/s] 90%|████████▉ | 24192/26933 [25:10<02:33, 17.83it/s] 90%|████████▉ | 24196/26933 [25:10<02:33, 17.81it/s] 90%|████████▉ | 24200/26933 [25:10<02:32, 17.87it/s] 90%|████████▉ | 24204/26933 [25:10<02:33, 17.73it/s] 90%|████████▉ | 24208/26933 [25:11<02:33, 17.80it/s] 90%|████████▉ | 24212/26933 [25:11<02:32, 17.82it/s] 90%|████████▉ | 24216/26933 [25:11<02:32, 17.80it/s] 90%|████████▉ | 24220/26933 [25:11<02:31, 17.86it/s] 90%|████████▉ | 24224/26933 [25:11<02:33, 17.66it/s] 90%|████████▉ | 24228/26933 [25:12<02:32, 17.77it/s] 90%|████████▉ | 24232/26933 [25:12<02:31, 17.84it/s] 90%|████████▉ | 24236/26933 [25:12<02:30, 17.90it/s] 90%|█████████ | 24240/26933 [25:12<02:31, 17.73it/s] 90%|█████████ | 24244/26933 [25:13<02:30, 17.81it/s] 90%|█████████ | 24248/26933 [25:13<02:30, 17.84it/s] 90%|█████████ | 24252/26933 [25:13<02:29, 17.90it/s] 90%|█████████ | 24256/26933 [25:13<02:29, 17.90it/s] 90%|█████████ | 24260/26933 [25:13<02:30, 17.75it/s] 90%|█████████ | 24264/26933 [25:14<02:30, 17.78it/s] 90%|█████████ | 24268/26933 [25:14<02:29, 17.78it/s] 90%|█████████ | 24272/26933 [25:14<02:29, 17.83it/s] 90%|█████████ | 24276/26933 [25:14<02:30, 17.67it/s] 90%|█████████ | 24280/26933 [25:15<02:29, 17.77it/s] 90%|█████████ | 24284/26933 [25:15<02:28, 17.79it/s] 90%|█████████ | 24288/26933 [25:15<02:28, 17.78it/s] 90%|█████████ | 24292/26933 [25:15<02:27, 17.85it/s] 90%|█████████ | 24296/26933 [25:15<02:29, 17.65it/s] 90%|█████████ | 24300/26933 [25:16<02:29, 17.65it/s] 90%|█████████ | 24304/26933 [25:16<02:28, 17.67it/s] 90%|█████████ | 24308/26933 [25:16<02:28, 17.70it/s] 90%|█████████ | 24312/26933 [25:16<02:29, 17.52it/s] 90%|█████████ | 24316/26933 [25:17<02:29, 17.55it/s] 90%|█████████ | 24320/26933 [25:17<02:28, 17.61it/s] 90%|█████████ | 24324/26933 [25:17<02:27, 17.68it/s] 90%|█████████ | 24328/26933 [25:17<02:28, 17.51it/s] 90%|█████████ | 24332/26933 [25:18<02:27, 17.59it/s] 90%|█████████ | 24336/26933 [25:18<02:27, 17.67it/s] 90%|█████████ | 24340/26933 [25:18<02:26, 17.70it/s] 90%|█████████ | 24344/26933 [25:18<02:25, 17.76it/s] 90%|█████████ | 24348/26933 [25:18<02:27, 17.57it/s] 90%|█████████ | 24352/26933 [25:19<02:26, 17.64it/s] 90%|█████████ | 24356/26933 [25:19<02:28, 17.40it/s] 90%|█████████ | 24360/26933 [25:19<02:26, 17.51it/s] 90%|█████████ | 24364/26933 [25:19<02:27, 17.47it/s] 90%|█████████ | 24368/26933 [25:20<02:26, 17.52it/s] 90%|█████████ | 24372/26933 [25:20<02:26, 17.46it/s] 91%|█████████ | 24376/26933 [25:20<02:25, 17.52it/s] 91%|█████████ | 24380/26933 [25:20<02:26, 17.38it/s] 91%|█████████ | 24384/26933 [25:20<02:26, 17.43it/s] 91%|█████████ | 24388/26933 [25:21<02:25, 17.44it/s] 91%|█████████ | 24392/26933 [25:21<02:25, 17.48it/s] 91%|█████████ | 24396/26933 [25:21<02:24, 17.53it/s] 91%|█████████ | 24400/26933 [25:21<02:25, 17.36it/s] 91%|█████████ | 24404/26933 [25:22<02:25, 17.42it/s] 91%|█████████ | 24408/26933 [25:22<02:24, 17.49it/s] 91%|█████████ | 24412/26933 [25:22<02:23, 17.62it/s] 91%|█████████ | 24416/26933 [25:22<02:23, 17.57it/s] 91%|█████████ | 24420/26933 [25:23<02:22, 17.64it/s] 91%|█████████ | 24424/26933 [25:23<02:21, 17.70it/s] 91%|█████████ | 24428/26933 [25:23<02:21, 17.72it/s] 91%|█████████ | 24432/26933 [25:23<02:20, 17.76it/s] 91%|█████████ | 24436/26933 [25:23<02:22, 17.58it/s] 91%|█████████ | 24440/26933 [25:24<02:21, 17.61it/s] 91%|█████████ | 24444/26933 [25:24<02:21, 17.62it/s] 91%|█████████ | 24448/26933 [25:24<02:20, 17.66it/s] 91%|█████████ | 24452/26933 [25:24<02:21, 17.55it/s] 91%|█████████ | 24456/26933 [25:25<02:20, 17.65it/s] 91%|█████████ | 24460/26933 [25:25<02:19, 17.71it/s] 91%|█████████ | 24464/26933 [25:25<02:20, 17.63it/s] 91%|█████████ | 24468/26933 [25:25<02:20, 17.54it/s] 91%|█████████ | 24472/26933 [25:25<02:19, 17.64it/s] 91%|█████████ | 24476/26933 [25:26<02:18, 17.73it/s] 91%|█████████ | 24480/26933 [25:26<02:18, 17.75it/s] 91%|█████████ | 24484/26933 [25:26<02:17, 17.76it/s] 91%|█████████ | 24488/26933 [25:26<02:18, 17.61it/s] 91%|█████████ | 24492/26933 [25:27<02:18, 17.66it/s] 91%|█████████ | 24496/26933 [25:27<02:17, 17.68it/s] 91%|█████████ | 24500/26933 [25:27<02:17, 17.71it/s] 91%|█████████ | 24504/26933 [25:27<02:18, 17.59it/s] 91%|█████████ | 24508/26933 [25:28<02:17, 17.68it/s] 91%|█████████ | 24512/26933 [25:28<02:16, 17.73it/s] 91%|█████████ | 24516/26933 [25:28<02:15, 17.79it/s] 91%|█████████ | 24520/26933 [25:28<02:15, 17.82it/s] 91%|█████████ | 24524/26933 [25:28<02:16, 17.64it/s] 91%|█████████ | 24528/26933 [25:29<02:15, 17.73it/s] 91%|█████████ | 24532/26933 [25:29<02:15, 17.76it/s] 91%|█████████ | 24536/26933 [25:29<02:14, 17.81it/s] 91%|█████████ | 24540/26933 [25:29<02:15, 17.66it/s] 91%|█████████ | 24544/26933 [25:30<02:14, 17.75it/s] 91%|█████████ | 24548/26933 [25:30<02:14, 17.79it/s] 91%|█████████ | 24552/26933 [25:30<02:13, 17.79it/s] 91%|█████████ | 24556/26933 [25:30<02:13, 17.85it/s] 91%|█████████ | 24560/26933 [25:30<02:14, 17.66it/s] 91%|█████████ | 24564/26933 [25:31<02:13, 17.68it/s] 91%|█████████ | 24568/26933 [25:31<02:13, 17.67it/s] 91%|█████████ | 24572/26933 [25:31<02:13, 17.73it/s] 91%|█████████ | 24576/26933 [25:31<02:14, 17.57it/s] 91%|█████████▏| 24580/26933 [25:32<02:13, 17.64it/s] 91%|█████████▏| 24584/26933 [25:32<02:12, 17.70it/s] 91%|█████████▏| 24588/26933 [25:32<02:12, 17.76it/s] 91%|█████████▏| 24592/26933 [25:32<02:12, 17.64it/s] 91%|█████████▏| 24596/26933 [25:32<02:11, 17.72it/s] 91%|█████████▏| 24600/26933 [25:33<02:11, 17.76it/s] 91%|█████████▏| 24604/26933 [25:33<02:10, 17.78it/s] 91%|█████████▏| 24608/26933 [25:33<02:10, 17.80it/s] 91%|█████████▏| 24612/26933 [25:33<02:11, 17.65it/s] 91%|█████████▏| 24616/26933 [25:34<02:10, 17.70it/s] 91%|█████████▏| 24620/26933 [25:34<02:10, 17.77it/s] 91%|█████████▏| 24624/26933 [25:34<02:09, 17.82it/s] 91%|█████████▏| 24628/26933 [25:34<02:10, 17.63it/s] 91%|█████████▏| 24632/26933 [25:35<02:10, 17.70it/s] 91%|█████████▏| 24636/26933 [25:35<02:08, 17.83it/s] 91%|█████████▏| 24640/26933 [25:35<02:07, 17.96it/s] 92%|█████████▏| 24644/26933 [25:35<02:07, 18.01it/s] 92%|█████████▏| 24648/26933 [25:35<02:07, 17.86it/s] 92%|█████████▏| 24652/26933 [25:36<02:07, 17.95it/s] 92%|█████████▏| 24656/26933 [25:36<02:06, 18.03it/s] 92%|█████████▏| 24660/26933 [25:36<02:05, 18.09it/s] 92%|█████████▏| 24664/26933 [25:36<02:06, 17.97it/s] 92%|█████████▏| 24668/26933 [25:37<02:05, 18.05it/s] 92%|█████████▏| 24672/26933 [25:37<02:04, 18.12it/s] 92%|█████████▏| 24676/26933 [25:37<02:04, 18.13it/s] 92%|█████████▏| 24680/26933 [25:37<02:04, 18.08it/s] 92%|█████████▏| 24684/26933 [25:37<02:06, 17.79it/s] 92%|█████████▏| 24688/26933 [25:38<02:06, 17.80it/s] 92%|█████████▏| 24692/26933 [25:38<02:06, 17.75it/s] 92%|█████████▏| 24696/26933 [25:38<02:05, 17.80it/s] 92%|█████████▏| 24700/26933 [25:38<02:06, 17.66it/s] 92%|█████████▏| 24704/26933 [25:39<02:06, 17.69it/s] 92%|█████████▏| 24708/26933 [25:39<02:07, 17.40it/s] 92%|█████████▏| 24712/26933 [25:39<02:07, 17.38it/s] 92%|█████████▏| 24716/26933 [25:39<02:07, 17.38it/s] 92%|█████████▏| 24720/26933 [25:39<02:08, 17.20it/s] 92%|█████████▏| 24724/26933 [25:40<02:08, 17.24it/s] 92%|█████████▏| 24728/26933 [25:40<02:07, 17.32it/s] 92%|█████████▏| 24732/26933 [25:40<02:05, 17.50it/s] 92%|█████████▏| 24736/26933 [25:40<02:06, 17.41it/s] 92%|█████████▏| 24740/26933 [25:41<02:05, 17.52it/s] 92%|█████████▏| 24744/26933 [25:41<02:04, 17.59it/s] 92%|█████████▏| 24748/26933 [25:41<02:03, 17.65it/s] 92%|█████████▏| 24752/26933 [25:41<02:04, 17.53it/s] 92%|█████████▏| 24756/26933 [25:42<02:03, 17.63it/s] 92%|█████████▏| 24760/26933 [25:42<02:02, 17.70it/s] 92%|█████████▏| 24764/26933 [25:42<02:02, 17.74it/s] 92%|█████████▏| 24768/26933 [25:42<02:01, 17.77it/s] 92%|█████████▏| 24772/26933 [25:42<02:02, 17.61it/s] 92%|█████████▏| 24776/26933 [25:43<02:01, 17.69it/s] 92%|█████████▏| 24780/26933 [25:43<02:01, 17.73it/s] 92%|█████████▏| 24784/26933 [25:43<02:01, 17.71it/s] 92%|█████████▏| 24788/26933 [25:43<02:02, 17.56it/s] 92%|█████████▏| 24792/26933 [25:44<02:01, 17.60it/s] 92%|█████████▏| 24796/26933 [25:44<02:00, 17.67it/s] 92%|█████████▏| 24800/26933 [25:44<02:00, 17.65it/s] 92%|█████████▏| 24804/26933 [25:44<02:00, 17.64it/s] 92%|█████████▏| 24808/26933 [25:44<02:01, 17.44it/s] 92%|█████████▏| 24812/26933 [25:45<02:00, 17.54it/s] 92%|█████████▏| 24816/26933 [25:45<02:00, 17.55it/s] 92%|█████████▏| 24820/26933 [25:45<01:59, 17.63it/s] 92%|█████████▏| 24824/26933 [25:45<02:00, 17.50it/s] 92%|█████████▏| 24828/26933 [25:46<01:59, 17.56it/s] 92%|█████████▏| 24832/26933 [25:46<01:59, 17.56it/s] 92%|█████████▏| 24836/26933 [25:46<01:59, 17.57it/s] 92%|█████████▏| 24840/26933 [25:46<01:59, 17.48it/s] 92%|█████████▏| 24844/26933 [25:47<01:58, 17.56it/s] 92%|█████████▏| 24848/26933 [25:47<01:58, 17.63it/s] 92%|█████████▏| 24852/26933 [25:47<01:57, 17.70it/s] 92%|█████████▏| 24856/26933 [25:47<01:57, 17.71it/s] 92%|█████████▏| 24860/26933 [25:47<01:58, 17.43it/s] 92%|█████████▏| 24864/26933 [25:48<01:58, 17.49it/s] 92%|█████████▏| 24868/26933 [25:48<01:57, 17.51it/s] 92%|█████████▏| 24872/26933 [25:48<01:57, 17.52it/s] 92%|█████████▏| 24876/26933 [25:48<01:58, 17.42it/s] 92%|█████████▏| 24880/26933 [25:49<01:57, 17.52it/s] 92%|█████████▏| 24884/26933 [25:49<01:56, 17.59it/s] 92%|█████████▏| 24888/26933 [25:49<01:55, 17.64it/s] 92%|█████████▏| 24892/26933 [25:49<01:55, 17.69it/s] 92%|█████████▏| 24896/26933 [25:49<01:56, 17.55it/s] 92%|█████████▏| 24900/26933 [25:50<01:55, 17.60it/s] 92%|█████████▏| 24904/26933 [25:50<01:54, 17.67it/s] 92%|█████████▏| 24908/26933 [25:50<01:54, 17.68it/s] 92%|█████████▏| 24912/26933 [25:50<01:55, 17.53it/s] 93%|█████████▎| 24916/26933 [25:51<01:54, 17.60it/s] 93%|█████████▎| 24920/26933 [25:51<01:53, 17.68it/s] 93%|█████████▎| 24924/26933 [25:51<01:53, 17.70it/s] 93%|█████████▎| 24928/26933 [25:51<01:54, 17.58it/s] 93%|█████████▎| 24932/26933 [25:52<01:53, 17.63it/s] 93%|█████████▎| 24936/26933 [25:52<01:53, 17.64it/s] 93%|█████████▎| 24940/26933 [25:52<01:52, 17.69it/s] 93%|█████████▎| 24944/26933 [25:52<01:52, 17.72it/s] 93%|█████████▎| 24948/26933 [25:52<01:52, 17.59it/s] 93%|█████████▎| 24952/26933 [25:53<01:52, 17.65it/s] 93%|█████████▎| 24956/26933 [25:53<01:51, 17.68it/s] 93%|█████████▎| 24960/26933 [25:53<01:51, 17.71it/s] 93%|█████████▎| 24964/26933 [25:53<01:51, 17.59it/s] 93%|█████████▎| 24968/26933 [25:54<01:51, 17.64it/s] 93%|█████████▎| 24972/26933 [25:54<01:50, 17.67it/s] 93%|█████████▎| 24976/26933 [25:54<01:50, 17.65it/s] 93%|█████████▎| 24980/26933 [25:54<01:50, 17.70it/s] 93%|█████████▎| 24984/26933 [25:54<01:50, 17.59it/s] 93%|█████████▎| 24988/26933 [25:55<01:50, 17.63it/s] 93%|█████████▎| 24992/26933 [25:55<01:49, 17.69it/s] 93%|█████████▎| 24996/26933 [25:55<01:50, 17.59it/s] 93%|█████████▎| 25000/26933 [25:55<01:52, 17.20it/s] 93%|█████████▎| 25004/26933 [25:56<01:51, 17.38it/s] 93%|█████████▎| 25008/26933 [25:56<01:50, 17.49it/s] 93%|█████████▎| 25012/26933 [25:56<01:49, 17.58it/s] 93%|█████████▎| 25016/26933 [25:56<01:49, 17.51it/s] 93%|█████████▎| 25020/26933 [25:57<01:48, 17.58it/s] 93%|█████████▎| 25024/26933 [25:57<01:48, 17.64it/s] 93%|█████████▎| 25028/26933 [25:57<01:47, 17.65it/s] 93%|█████████▎| 25032/26933 [25:57<01:47, 17.71it/s] 93%|█████████▎| 25036/26933 [25:57<01:48, 17.54it/s] 93%|█████████▎| 25040/26933 [25:58<01:47, 17.62it/s] 93%|█████████▎| 25044/26933 [25:58<01:46, 17.66it/s] 93%|█████████▎| 25048/26933 [25:58<01:46, 17.65it/s] 93%|█████████▎| 25052/26933 [25:58<01:47, 17.51it/s] 93%|█████████▎| 25056/26933 [25:59<01:47, 17.53it/s] 93%|█████████▎| 25060/26933 [25:59<01:46, 17.53it/s] 93%|█████████▎| 25064/26933 [25:59<01:46, 17.54it/s] 93%|█████████▎| 25068/26933 [25:59<01:45, 17.61it/s] 93%|█████████▎| 25072/26933 [25:59<01:46, 17.53it/s] 93%|█████████▎| 25076/26933 [26:00<01:45, 17.58it/s] 93%|█████████▎| 25080/26933 [26:00<01:45, 17.61it/s] 93%|█████████▎| 25084/26933 [26:00<01:44, 17.69it/s] 93%|█████████▎| 25088/26933 [26:00<01:44, 17.59it/s] 93%|█████████▎| 25092/26933 [26:01<01:44, 17.64it/s] 93%|█████████▎| 25096/26933 [26:01<01:44, 17.65it/s] 93%|█████████▎| 25100/26933 [26:01<01:43, 17.70it/s] 93%|█████████▎| 25104/26933 [26:01<01:44, 17.58it/s] 93%|█████████▎| 25108/26933 [26:02<01:43, 17.64it/s] 93%|█████████▎| 25112/26933 [26:02<01:42, 17.68it/s] 93%|█████████▎| 25116/26933 [26:02<01:42, 17.70it/s] 93%|█████████▎| 25120/26933 [26:02<01:42, 17.71it/s] 93%|█████████▎| 25124/26933 [26:02<01:42, 17.57it/s] 93%|█████████▎| 25128/26933 [26:03<01:42, 17.63it/s] 93%|█████████▎| 25132/26933 [26:03<01:42, 17.65it/s] 93%|█████████▎| 25136/26933 [26:03<01:41, 17.68it/s] 93%|█████████▎| 25140/26933 [26:03<01:42, 17.54it/s] 93%|█████████▎| 25144/26933 [26:04<01:41, 17.61it/s] 93%|█████████▎| 25148/26933 [26:04<01:41, 17.66it/s] 93%|█████████▎| 25152/26933 [26:04<01:40, 17.69it/s] 93%|█████████▎| 25156/26933 [26:04<01:40, 17.70it/s] 93%|█████████▎| 25160/26933 [26:04<01:40, 17.57it/s] 93%|█████████▎| 25164/26933 [26:05<01:40, 17.66it/s] 93%|█████████▎| 25168/26933 [26:05<01:39, 17.71it/s] 93%|█████████▎| 25172/26933 [26:05<01:39, 17.72it/s] 93%|█████████▎| 25176/26933 [26:05<01:39, 17.62it/s] 93%|█████████▎| 25180/26933 [26:06<01:39, 17.68it/s] 94%|█████████▎| 25184/26933 [26:06<01:38, 17.72it/s] 94%|█████████▎| 25188/26933 [26:06<01:38, 17.73it/s] 94%|█████████▎| 25192/26933 [26:06<01:38, 17.63it/s] 94%|█████████▎| 25196/26933 [26:06<01:38, 17.70it/s] 94%|█████████▎| 25200/26933 [26:07<01:37, 17.72it/s] 94%|█████████▎| 25204/26933 [26:07<01:37, 17.74it/s] 94%|█████████▎| 25208/26933 [26:07<01:37, 17.76it/s] 94%|█████████▎| 25212/26933 [26:07<01:37, 17.65it/s] 94%|█████████▎| 25216/26933 [26:08<01:36, 17.71it/s] 94%|█████████▎| 25220/26933 [26:08<01:36, 17.76it/s] 94%|█████████▎| 25224/26933 [26:08<01:36, 17.78it/s] 94%|█████████▎| 25228/26933 [26:08<01:36, 17.67it/s] 94%|█████████▎| 25232/26933 [26:09<01:35, 17.72it/s] 94%|█████████▎| 25236/26933 [26:09<01:35, 17.75it/s] 94%|█████████▎| 25240/26933 [26:09<01:35, 17.79it/s] 94%|█████████▎| 25244/26933 [26:09<01:34, 17.81it/s] 94%|█████████▎| 25248/26933 [26:09<01:35, 17.70it/s] 94%|█████████▍| 25252/26933 [26:10<01:34, 17.76it/s] 94%|█████████▍| 25256/26933 [26:10<01:34, 17.78it/s] 94%|█████████▍| 25260/26933 [26:10<01:34, 17.79it/s] 94%|█████████▍| 25264/26933 [26:10<01:34, 17.67it/s] 94%|█████████▍| 25268/26933 [26:11<01:33, 17.73it/s] 94%|█████████▍| 25272/26933 [26:11<01:33, 17.78it/s] 94%|█████████▍| 25276/26933 [26:11<01:33, 17.63it/s] 94%|█████████▍| 25280/26933 [26:11<01:35, 17.37it/s] 94%|█████████▍| 25284/26933 [26:11<01:35, 17.35it/s] 94%|█████████▍| 25288/26933 [26:12<01:34, 17.43it/s] 94%|█████████▍| 25292/26933 [26:12<01:33, 17.46it/s] 94%|█████████▍| 25296/26933 [26:12<01:33, 17.57it/s] 94%|█████████▍| 25300/26933 [26:12<01:33, 17.48it/s] 94%|█████████▍| 25304/26933 [26:13<01:32, 17.52it/s] 94%|█████████▍| 25308/26933 [26:13<01:32, 17.61it/s] 94%|█████████▍| 25312/26933 [26:13<01:31, 17.64it/s] 94%|█████████▍| 25316/26933 [26:13<01:32, 17.49it/s] 94%|█████████▍| 25320/26933 [26:14<01:31, 17.55it/s] 94%|█████████▍| 25324/26933 [26:14<01:31, 17.63it/s] 94%|█████████▍| 25328/26933 [26:14<01:30, 17.64it/s] 94%|█████████▍| 25332/26933 [26:14<01:30, 17.67it/s] 94%|█████████▍| 25336/26933 [26:14<01:30, 17.56it/s] 94%|█████████▍| 25340/26933 [26:15<01:30, 17.61it/s] 94%|█████████▍| 25344/26933 [26:15<01:30, 17.65it/s] 94%|█████████▍| 25348/26933 [26:15<01:29, 17.73it/s] 94%|█████████▍| 25352/26933 [26:15<01:29, 17.61it/s] 94%|█████████▍| 25356/26933 [26:16<01:29, 17.66it/s] 94%|█████████▍| 25360/26933 [26:16<01:30, 17.31it/s] 94%|█████████▍| 25364/26933 [26:16<01:29, 17.45it/s] 94%|█████████▍| 25368/26933 [26:16<01:29, 17.55it/s] 94%|█████████▍| 25372/26933 [26:16<01:29, 17.51it/s] 94%|█████████▍| 25376/26933 [26:17<01:28, 17.59it/s] 94%|█████████▍| 25380/26933 [26:17<01:27, 17.66it/s] 94%|█████████▍| 25384/26933 [26:17<01:27, 17.68it/s] 94%|█████████▍| 25388/26933 [26:17<01:27, 17.59it/s] 94%|█████████▍| 25392/26933 [26:18<01:27, 17.66it/s] 94%|█████████▍| 25396/26933 [26:18<01:26, 17.71it/s] 94%|█████████▍| 25400/26933 [26:18<01:26, 17.74it/s] 94%|█████████▍| 25404/26933 [26:18<01:26, 17.62it/s] 94%|█████████▍| 25408/26933 [26:19<01:26, 17.66it/s] 94%|█████████▍| 25412/26933 [26:19<01:25, 17.72it/s] 94%|█████████▍| 25416/26933 [26:19<01:25, 17.71it/s] 94%|█████████▍| 25420/26933 [26:19<01:25, 17.72it/s] 94%|█████████▍| 25424/26933 [26:19<01:25, 17.62it/s] 94%|█████████▍| 25428/26933 [26:20<01:25, 17.67it/s] 94%|█████████▍| 25432/26933 [26:20<01:24, 17.71it/s] 94%|█████████▍| 25436/26933 [26:20<01:24, 17.72it/s] 94%|█████████▍| 25440/26933 [26:20<01:24, 17.65it/s] 94%|█████████▍| 25444/26933 [26:21<01:24, 17.69it/s] 94%|█████████▍| 25448/26933 [26:21<01:23, 17.71it/s] 95%|█████████▍| 25452/26933 [26:21<01:23, 17.71it/s] 95%|█████████▍| 25456/26933 [26:21<01:23, 17.71it/s] 95%|█████████▍| 25460/26933 [26:21<01:23, 17.60it/s] 95%|█████████▍| 25464/26933 [26:22<01:23, 17.61it/s] 95%|█████████▍| 25468/26933 [26:22<01:23, 17.62it/s] 95%|█████████▍| 25472/26933 [26:22<01:22, 17.65it/s] 95%|█████████▍| 25476/26933 [26:22<01:22, 17.57it/s] 95%|█████████▍| 25480/26933 [26:23<01:22, 17.62it/s] 95%|█████████▍| 25484/26933 [26:23<01:22, 17.67it/s] 95%|█████████▍| 25488/26933 [26:23<01:21, 17.71it/s] 95%|█████████▍| 25492/26933 [26:23<01:21, 17.66it/s] 95%|█████████▍| 25496/26933 [26:23<01:21, 17.69it/s] 95%|█████████▍| 25500/26933 [26:24<01:20, 17.72it/s] 95%|█████████▍| 25504/26933 [26:24<01:20, 17.74it/s] 95%|█████████▍| 25508/26933 [26:24<01:20, 17.77it/s] 95%|█████████▍| 25512/26933 [26:24<01:20, 17.68it/s] 95%|█████████▍| 25516/26933 [26:25<01:19, 17.74it/s] 95%|█████████▍| 25520/26933 [26:25<01:19, 17.78it/s] 95%|█████████▍| 25524/26933 [26:25<01:19, 17.79it/s] 95%|█████████▍| 25528/26933 [26:25<01:19, 17.64it/s] 95%|█████████▍| 25532/26933 [26:26<01:19, 17.59it/s] 95%|█████████▍| 25536/26933 [26:26<01:19, 17.57it/s] 95%|█████████▍| 25540/26933 [26:26<01:19, 17.63it/s] 95%|█████████▍| 25544/26933 [26:26<01:18, 17.64it/s] 95%|█████████▍| 25548/26933 [26:26<01:18, 17.57it/s] 95%|█████████▍| 25552/26933 [26:27<01:18, 17.66it/s] 95%|█████████▍| 25556/26933 [26:27<01:17, 17.70it/s] 95%|█████████▍| 25560/26933 [26:27<01:17, 17.71it/s] 95%|█████████▍| 25564/26933 [26:27<01:17, 17.62it/s] 95%|█████████▍| 25568/26933 [26:28<01:17, 17.65it/s] 95%|█████████▍| 25572/26933 [26:28<01:16, 17.68it/s] 95%|█████████▍| 25576/26933 [26:28<01:16, 17.69it/s] 95%|█████████▍| 25580/26933 [26:28<01:16, 17.75it/s] 95%|█████████▍| 25584/26933 [26:28<01:16, 17.67it/s] 95%|█████████▌| 25588/26933 [26:29<01:15, 17.73it/s] 95%|█████████▌| 25592/26933 [26:29<01:15, 17.76it/s] 95%|█████████▌| 25596/26933 [26:29<01:15, 17.80it/s] 95%|█████████▌| 25600/26933 [26:29<01:15, 17.67it/s] 95%|█████████▌| 25604/26933 [26:30<01:14, 17.74it/s] 95%|█████████▌| 25608/26933 [26:30<01:14, 17.77it/s] 95%|█████████▌| 25612/26933 [26:30<01:14, 17.74it/s] 95%|█████████▌| 25616/26933 [26:30<01:14, 17.67it/s] 95%|█████████▌| 25620/26933 [26:31<01:14, 17.66it/s] 95%|█████████▌| 25624/26933 [26:31<01:13, 17.70it/s] 95%|█████████▌| 25628/26933 [26:31<01:13, 17.74it/s] 95%|█████████▌| 25632/26933 [26:31<01:13, 17.77it/s] 95%|█████████▌| 25636/26933 [26:31<01:13, 17.62it/s] 95%|█████████▌| 25640/26933 [26:32<01:13, 17.58it/s] 95%|█████████▌| 25644/26933 [26:32<01:13, 17.55it/s] 95%|█████████▌| 25648/26933 [26:32<01:12, 17.61it/s] 95%|█████████▌| 25652/26933 [26:32<01:13, 17.51it/s] 95%|█████████▌| 25656/26933 [26:33<01:12, 17.56it/s] 95%|█████████▌| 25660/26933 [26:33<01:12, 17.58it/s] 95%|█████████▌| 25664/26933 [26:33<01:11, 17.65it/s] 95%|█████████▌| 25668/26933 [26:33<01:11, 17.70it/s] 95%|█████████▌| 25672/26933 [26:33<01:11, 17.60it/s] 95%|█████████▌| 25676/26933 [26:34<01:11, 17.64it/s] 95%|█████████▌| 25680/26933 [26:34<01:10, 17.66it/s] 95%|█████████▌| 25684/26933 [26:34<01:10, 17.71it/s] 95%|█████████▌| 25688/26933 [26:34<01:10, 17.55it/s] 95%|█████████▌| 25692/26933 [26:35<01:10, 17.56it/s] 95%|█████████▌| 25696/26933 [26:35<01:10, 17.53it/s] 95%|█████████▌| 25700/26933 [26:35<01:10, 17.55it/s] 95%|█████████▌| 25704/26933 [26:35<01:10, 17.42it/s] 95%|█████████▌| 25708/26933 [26:36<01:10, 17.43it/s] 95%|█████████▌| 25712/26933 [26:36<01:09, 17.48it/s] 95%|█████████▌| 25716/26933 [26:36<01:09, 17.52it/s] 95%|█████████▌| 25720/26933 [26:36<01:08, 17.59it/s] 96%|█████████▌| 25724/26933 [26:36<01:09, 17.42it/s] 96%|█████████▌| 25728/26933 [26:37<01:09, 17.35it/s] 96%|█████████▌| 25732/26933 [26:37<01:08, 17.49it/s] 96%|█████████▌| 25736/26933 [26:37<01:08, 17.58it/s] 96%|█████████▌| 25740/26933 [26:37<01:07, 17.57it/s] 96%|█████████▌| 25744/26933 [26:38<01:07, 17.62it/s] 96%|█████████▌| 25748/26933 [26:38<01:07, 17.67it/s] 96%|█████████▌| 25752/26933 [26:38<01:06, 17.69it/s] 96%|█████████▌| 25756/26933 [26:38<01:06, 17.72it/s] 96%|█████████▌| 25760/26933 [26:38<01:06, 17.67it/s] 96%|█████████▌| 25764/26933 [26:39<01:05, 17.72it/s] 96%|█████████▌| 25768/26933 [26:39<01:08, 17.05it/s] 96%|█████████▌| 25772/26933 [26:39<01:07, 17.14it/s] 96%|█████████▌| 25776/26933 [26:39<01:06, 17.34it/s] 96%|█████████▌| 25780/26933 [26:40<01:05, 17.55it/s] 96%|█████████▌| 25784/26933 [26:40<01:04, 17.73it/s] 96%|█████████▌| 25788/26933 [26:40<01:04, 17.86it/s] 96%|█████████▌| 25792/26933 [26:40<01:04, 17.79it/s] 96%|█████████▌| 25796/26933 [26:41<01:03, 17.77it/s] 96%|█████████▌| 25800/26933 [26:41<01:03, 17.80it/s] 96%|█████████▌| 25804/26933 [26:41<01:03, 17.77it/s] 96%|█████████▌| 25808/26933 [26:41<01:03, 17.79it/s] 96%|█████████▌| 25812/26933 [26:41<01:03, 17.71it/s] 96%|█████████▌| 25816/26933 [26:42<01:02, 17.73it/s] 96%|█████████▌| 25820/26933 [26:42<01:02, 17.77it/s] 96%|█████████▌| 25824/26933 [26:42<01:02, 17.79it/s] 96%|█████████▌| 25828/26933 [26:42<01:02, 17.70it/s] 96%|█████████▌| 25832/26933 [26:43<01:02, 17.68it/s] 96%|█████████▌| 25836/26933 [26:43<01:01, 17.71it/s] 96%|█████████▌| 25840/26933 [26:43<01:01, 17.74it/s] 96%|█████████▌| 25844/26933 [26:43<01:01, 17.76it/s] 96%|█████████▌| 25848/26933 [26:43<01:01, 17.68it/s] 96%|█████████▌| 25852/26933 [26:44<01:01, 17.70it/s] 96%|█████████▌| 25856/26933 [26:44<01:00, 17.74it/s] 96%|█████████▌| 25860/26933 [26:44<01:00, 17.60it/s] 96%|█████████▌| 25864/26933 [26:44<01:00, 17.58it/s] 96%|█████████▌| 25868/26933 [26:45<01:00, 17.65it/s] 96%|█████████▌| 25872/26933 [26:45<01:00, 17.68it/s] 96%|█████████▌| 25876/26933 [26:45<00:59, 17.73it/s] 96%|█████████▌| 25880/26933 [26:45<00:59, 17.75it/s] 96%|█████████▌| 25884/26933 [26:45<00:59, 17.76it/s] 96%|█████████▌| 25888/26933 [26:46<00:58, 17.79it/s] 96%|█████████▌| 25892/26933 [26:46<00:58, 17.84it/s] 96%|█████████▌| 25896/26933 [26:46<00:58, 17.80it/s] 96%|█████████▌| 25900/26933 [26:46<00:58, 17.74it/s] 96%|█████████▌| 25904/26933 [26:47<00:57, 17.79it/s] 96%|█████████▌| 25908/26933 [26:47<00:57, 17.81it/s] 96%|█████████▌| 25912/26933 [26:47<00:57, 17.82it/s] 96%|█████████▌| 25916/26933 [26:47<00:57, 17.74it/s] 96%|█████████▌| 25920/26933 [26:48<00:56, 17.77it/s] 96%|█████████▋| 25924/26933 [26:48<00:56, 17.81it/s] 96%|█████████▋| 25928/26933 [26:48<00:56, 17.80it/s] 96%|█████████▋| 25932/26933 [26:48<00:56, 17.80it/s] 96%|█████████▋| 25936/26933 [26:48<00:56, 17.73it/s] 96%|█████████▋| 25940/26933 [26:49<00:56, 17.71it/s] 96%|█████████▋| 25944/26933 [26:49<00:55, 17.73it/s] 96%|█████████▋| 25948/26933 [26:49<00:55, 17.72it/s] 96%|█████████▋| 25952/26933 [26:49<00:55, 17.66it/s] 96%|█████████▋| 25956/26933 [26:50<00:55, 17.71it/s] 96%|█████████▋| 25960/26933 [26:50<00:54, 17.72it/s] 96%|█████████▋| 25964/26933 [26:50<00:54, 17.65it/s] 96%|█████████▋| 25968/26933 [26:50<00:54, 17.63it/s] 96%|█████████▋| 25972/26933 [26:50<00:54, 17.53it/s] 96%|█████████▋| 25976/26933 [26:51<00:54, 17.60it/s] 96%|█████████▋| 25980/26933 [26:51<00:54, 17.57it/s] 96%|█████████▋| 25984/26933 [26:51<00:54, 17.56it/s] 96%|█████████▋| 25988/26933 [26:51<00:54, 17.48it/s] 97%|█████████▋| 25992/26933 [26:52<00:53, 17.47it/s] 97%|█████████▋| 25996/26933 [26:52<00:53, 17.50it/s] 97%|█████████▋| 26000/26933 [26:52<00:53, 17.53it/s] 97%|█████████▋| 26004/26933 [26:52<00:53, 17.46it/s] 97%|█████████▋| 26008/26933 [26:53<00:52, 17.46it/s] 97%|█████████▋| 26012/26933 [26:53<00:52, 17.50it/s] 97%|█████████▋| 26016/26933 [26:53<00:52, 17.57it/s] 97%|█████████▋| 26020/26933 [26:53<00:51, 17.58it/s] 97%|█████████▋| 26024/26933 [26:53<00:51, 17.49it/s] 97%|█████████▋| 26028/26933 [26:54<00:51, 17.56it/s] 97%|█████████▋| 26032/26933 [26:54<00:51, 17.61it/s] 97%|█████████▋| 26036/26933 [26:54<00:50, 17.62it/s] 97%|█████████▋| 26040/26933 [26:54<00:51, 17.50it/s] 97%|█████████▋| 26044/26933 [26:55<00:50, 17.54it/s] 97%|█████████▋| 26048/26933 [26:55<00:50, 17.54it/s] 97%|█████████▋| 26052/26933 [26:55<00:50, 17.58it/s] 97%|█████████▋| 26056/26933 [26:55<00:49, 17.62it/s] 97%|█████████▋| 26060/26933 [26:55<00:49, 17.57it/s] 97%|█████████▋| 26064/26933 [26:56<00:49, 17.64it/s] 97%|█████████▋| 26068/26933 [26:56<00:49, 17.64it/s] 97%|█████████▋| 26072/26933 [26:56<00:48, 17.67it/s] 97%|█████████▋| 26076/26933 [26:56<00:48, 17.58it/s] 97%|█████████▋| 26080/26933 [26:57<00:48, 17.63it/s] 97%|█████████▋| 26084/26933 [26:57<00:48, 17.65it/s] 97%|█████████▋| 26088/26933 [26:57<00:47, 17.65it/s] 97%|█████████▋| 26092/26933 [26:57<00:47, 17.59it/s] 97%|█████████▋| 26096/26933 [26:58<00:48, 17.34it/s] 97%|█████████▋| 26100/26933 [26:58<00:47, 17.45it/s] 97%|█████████▋| 26104/26933 [26:58<00:47, 17.53it/s] 97%|█████████▋| 26108/26933 [26:58<00:46, 17.56it/s] 97%|█████████▋| 26112/26933 [26:58<00:46, 17.52it/s] 97%|█████████▋| 26116/26933 [26:59<00:46, 17.56it/s] 97%|█████████▋| 26120/26933 [26:59<00:46, 17.60it/s] 97%|█████████▋| 26124/26933 [26:59<00:46, 17.57it/s] 97%|█████████▋| 26128/26933 [26:59<00:45, 17.50it/s] 97%|█████████▋| 26132/26933 [27:00<00:45, 17.51it/s] 97%|█████████▋| 26136/26933 [27:00<00:45, 17.57it/s] 97%|█████████▋| 26140/26933 [27:00<00:45, 17.61it/s] 97%|█████████▋| 26144/26933 [27:00<00:44, 17.66it/s] 97%|█████████▋| 26148/26933 [27:00<00:44, 17.59it/s] 97%|█████████▋| 26152/26933 [27:01<00:44, 17.69it/s] 97%|█████████▋| 26156/26933 [27:01<00:43, 17.73it/s] 97%|█████████▋| 26160/26933 [27:01<00:43, 17.79it/s] 97%|█████████▋| 26164/26933 [27:01<00:43, 17.70it/s] 97%|█████████▋| 26168/26933 [27:02<00:43, 17.75it/s] 97%|█████████▋| 26172/26933 [27:02<00:42, 17.74it/s] 97%|█████████▋| 26176/26933 [27:02<00:42, 17.75it/s] 97%|█████████▋| 26180/26933 [27:02<00:42, 17.67it/s] 97%|█████████▋| 26184/26933 [27:02<00:42, 17.74it/s] 97%|█████████▋| 26188/26933 [27:03<00:41, 17.76it/s] 97%|█████████▋| 26192/26933 [27:03<00:41, 17.82it/s] 97%|█████████▋| 26196/26933 [27:03<00:41, 17.96it/s] 97%|█████████▋| 26200/26933 [27:03<00:40, 17.99it/s] 97%|█████████▋| 26204/26933 [27:04<00:40, 18.10it/s] 97%|█████████▋| 26208/26933 [27:04<00:39, 18.21it/s] 97%|█████████▋| 26212/26933 [27:04<00:39, 18.24it/s] 97%|█████████▋| 26216/26933 [27:04<00:39, 18.27it/s] 97%|█████████▋| 26220/26933 [27:04<00:39, 18.24it/s] 97%|█████████▋| 26224/26933 [27:05<00:38, 18.22it/s] 97%|█████████▋| 26228/26933 [27:05<00:38, 18.27it/s] 97%|█████████▋| 26232/26933 [27:05<00:38, 18.31it/s] 97%|█████████▋| 26236/26933 [27:05<00:38, 18.25it/s] 97%|█████████▋| 26240/26933 [27:06<00:37, 18.31it/s] 97%|█████████▋| 26244/26933 [27:06<00:37, 18.32it/s] 97%|█████████▋| 26248/26933 [27:06<00:37, 18.33it/s] 97%|█████████▋| 26252/26933 [27:06<00:37, 18.31it/s] 97%|█████████▋| 26256/26933 [27:06<00:37, 18.24it/s] 98%|█████████▊| 26260/26933 [27:07<00:36, 18.27it/s] 98%|█████████▊| 26264/26933 [27:07<00:36, 18.27it/s] 98%|█████████▊| 26268/26933 [27:07<00:36, 18.24it/s] 98%|█████████▊| 26272/26933 [27:07<00:36, 18.21it/s] 98%|█████████▊| 26276/26933 [27:08<00:35, 18.29it/s] 98%|█████████▊| 26280/26933 [27:08<00:35, 18.31it/s] 98%|█████████▊| 26284/26933 [27:08<00:35, 18.31it/s] 98%|█████████▊| 26288/26933 [27:08<00:35, 18.30it/s] 98%|█████████▊| 26292/26933 [27:08<00:35, 18.23it/s] 98%|█████████▊| 26296/26933 [27:09<00:34, 18.26it/s] 98%|█████████▊| 26300/26933 [27:09<00:34, 18.27it/s] 98%|█████████▊| 26304/26933 [27:09<00:34, 18.27it/s] 98%|█████████▊| 26308/26933 [27:09<00:34, 18.26it/s] 98%|█████████▊| 26312/26933 [27:10<00:33, 18.30it/s] 98%|█████████▊| 26316/26933 [27:10<00:33, 18.30it/s] 98%|█████████▊| 26320/26933 [27:10<00:33, 18.31it/s] 98%|█████████▊| 26324/26933 [27:10<00:33, 18.31it/s] 98%|█████████▊| 26328/26933 [27:10<00:33, 18.22it/s] 98%|█████████▊| 26332/26933 [27:11<00:32, 18.23it/s] 98%|█████████▊| 26336/26933 [27:11<00:32, 18.24it/s] 98%|█████████▊| 26340/26933 [27:11<00:32, 18.29it/s] 98%|█████████▊| 26344/26933 [27:11<00:32, 18.22it/s] 98%|█████████▊| 26348/26933 [27:11<00:32, 18.26it/s] 98%|█████████▊| 26352/26933 [27:12<00:31, 18.27it/s] 98%|█████████▊| 26356/26933 [27:12<00:31, 18.28it/s] 98%|█████████▊| 26360/26933 [27:12<00:31, 18.27it/s] 98%|█████████▊| 26364/26933 [27:12<00:31, 18.10it/s] 98%|█████████▊| 26368/26933 [27:13<00:31, 18.14it/s] 98%|█████████▊| 26372/26933 [27:13<00:30, 18.17it/s] 98%|█████████▊| 26376/26933 [27:13<00:30, 18.25it/s] 98%|█████████▊| 26380/26933 [27:13<00:30, 18.24it/s] 98%|█████████▊| 26384/26933 [27:13<00:30, 18.16it/s] 98%|█████████▊| 26388/26933 [27:14<00:29, 18.22it/s] 98%|█████████▊| 26392/26933 [27:14<00:29, 18.23it/s] 98%|█████████▊| 26396/26933 [27:14<00:29, 18.23it/s] 98%|█████████▊| 26400/26933 [27:14<00:29, 18.20it/s] 98%|█████████▊| 26404/26933 [27:15<00:29, 18.20it/s] 98%|█████████▊| 26408/26933 [27:15<00:28, 18.22it/s] 98%|█████████▊| 26412/26933 [27:15<00:28, 18.24it/s] 98%|█████████▊| 26416/26933 [27:15<00:28, 18.29it/s] 98%|█████████▊| 26420/26933 [27:15<00:28, 18.22it/s] 98%|█████████▊| 26424/26933 [27:16<00:27, 18.22it/s] 98%|█████████▊| 26428/26933 [27:16<00:27, 18.23it/s] 98%|█████████▊| 26432/26933 [27:16<00:27, 18.25it/s] 98%|█████████▊| 26436/26933 [27:16<00:27, 18.23it/s] 98%|█████████▊| 26440/26933 [27:17<00:27, 18.07it/s] 98%|█████████▊| 26444/26933 [27:17<00:27, 17.94it/s] 98%|█████████▊| 26448/26933 [27:17<00:27, 17.83it/s] 98%|█████████▊| 26452/26933 [27:17<00:27, 17.80it/s] 98%|█████████▊| 26456/26933 [27:17<00:26, 17.67it/s] 98%|█████████▊| 26460/26933 [27:18<00:26, 17.73it/s] 98%|█████████▊| 26464/26933 [27:18<00:26, 17.75it/s] 98%|█████████▊| 26468/26933 [27:18<00:26, 17.79it/s] 98%|█████████▊| 26472/26933 [27:18<00:26, 17.64it/s] 98%|█████████▊| 26476/26933 [27:19<00:25, 17.67it/s] 98%|█████████▊| 26480/26933 [27:19<00:25, 17.57it/s] 98%|█████████▊| 26484/26933 [27:19<00:25, 17.52it/s] 98%|█████████▊| 26488/26933 [27:19<00:25, 17.22it/s] 98%|█████████▊| 26492/26933 [27:20<00:25, 17.33it/s] 98%|█████████▊| 26496/26933 [27:20<00:25, 17.42it/s] 98%|█████████▊| 26500/26933 [27:20<00:24, 17.52it/s] 98%|█████████▊| 26504/26933 [27:20<00:24, 17.60it/s] 98%|█████████▊| 26508/26933 [27:20<00:24, 17.60it/s] 98%|█████████▊| 26512/26933 [27:21<00:23, 17.65it/s] 98%|█████████▊| 26516/26933 [27:21<00:23, 17.70it/s] 98%|█████████▊| 26520/26933 [27:21<00:23, 17.76it/s] 98%|█████████▊| 26524/26933 [27:21<00:23, 17.70it/s] 98%|█████████▊| 26528/26933 [27:22<00:22, 17.76it/s] 99%|█████████▊| 26532/26933 [27:22<00:22, 17.77it/s] 99%|█████████▊| 26536/26933 [27:22<00:22, 17.80it/s] 99%|█████████▊| 26540/26933 [27:22<00:22, 17.82it/s] 99%|█████████▊| 26544/26933 [27:22<00:21, 17.74it/s] 99%|█████████▊| 26548/26933 [27:23<00:21, 17.75it/s] 99%|█████████▊| 26552/26933 [27:23<00:21, 17.77it/s] 99%|█████████▊| 26556/26933 [27:23<00:21, 17.82it/s] 99%|█████████▊| 26560/26933 [27:23<00:21, 17.71it/s] 99%|█████████▊| 26564/26933 [27:24<00:20, 17.75it/s] 99%|█████████▊| 26568/26933 [27:24<00:20, 17.74it/s] 99%|█████████▊| 26572/26933 [27:24<00:20, 17.74it/s] 99%|█████████▊| 26576/26933 [27:24<00:20, 17.75it/s] 99%|█████████▊| 26580/26933 [27:24<00:19, 17.70it/s] 99%|█████████▊| 26584/26933 [27:25<00:20, 17.36it/s] 99%|█████████▊| 26588/26933 [27:25<00:19, 17.48it/s] 99%|█████████▊| 26592/26933 [27:25<00:19, 17.62it/s] 99%|█████████▊| 26596/26933 [27:25<00:19, 17.63it/s] 99%|█████████▉| 26600/26933 [27:26<00:18, 17.69it/s] 99%|█████████▉| 26604/26933 [27:26<00:18, 17.71it/s] 99%|█████████▉| 26608/26933 [27:26<00:18, 17.75it/s] 99%|█████████▉| 26612/26933 [27:26<00:18, 17.74it/s] 99%|█████████▉| 26616/26933 [27:26<00:17, 17.79it/s] 99%|█████████▉| 26620/26933 [27:27<00:17, 17.81it/s] 99%|█████████▉| 26624/26933 [27:27<00:17, 17.80it/s] 99%|█████████▉| 26628/26933 [27:27<00:17, 17.85it/s] 99%|█████████▉| 26632/26933 [27:27<00:16, 17.85it/s] 99%|█████████▉| 26636/26933 [27:28<00:16, 17.88it/s] 99%|█████████▉| 26640/26933 [27:28<00:16, 17.87it/s] 99%|█████████▉| 26644/26933 [27:28<00:16, 17.86it/s] 99%|█████████▉| 26648/26933 [27:28<00:16, 17.80it/s] 99%|█████████▉| 26652/26933 [27:29<00:15, 17.80it/s] 99%|█████████▉| 26656/26933 [27:29<00:15, 17.80it/s] 99%|█████████▉| 26660/26933 [27:29<00:15, 17.77it/s] 99%|█████████▉| 26664/26933 [27:29<00:15, 17.81it/s] 99%|█████████▉| 26668/26933 [27:29<00:14, 17.74it/s] 99%|█████████▉| 26672/26933 [27:30<00:14, 17.76it/s] 99%|█████████▉| 26676/26933 [27:30<00:14, 17.76it/s] 99%|█████████▉| 26680/26933 [27:30<00:14, 17.82it/s] 99%|█████████▉| 26684/26933 [27:30<00:13, 17.79it/s] 99%|█████████▉| 26688/26933 [27:31<00:13, 17.82it/s] 99%|█████████▉| 26692/26933 [27:31<00:13, 17.79it/s] 99%|█████████▉| 26696/26933 [27:31<00:13, 17.81it/s] 99%|█████████▉| 26700/26933 [27:31<00:13, 17.82it/s] 99%|█████████▉| 26704/26933 [27:31<00:12, 17.77it/s] 99%|█████████▉| 26708/26933 [27:32<00:12, 17.83it/s] 99%|█████████▉| 26712/26933 [27:32<00:12, 17.79it/s] 99%|█████████▉| 26716/26933 [27:32<00:12, 17.80it/s] 99%|█████████▉| 26720/26933 [27:32<00:11, 17.81it/s] 99%|█████████▉| 26724/26933 [27:33<00:11, 17.92it/s] 99%|█████████▉| 26728/26933 [27:33<00:11, 17.89it/s] 99%|█████████▉| 26732/26933 [27:33<00:11, 17.88it/s] 99%|█████████▉| 26736/26933 [27:33<00:10, 17.92it/s] 99%|█████████▉| 26740/26933 [27:33<00:10, 17.91it/s] 99%|█████████▉| 26744/26933 [27:34<00:10, 17.92it/s] 99%|█████████▉| 26748/26933 [27:34<00:10, 17.90it/s] 99%|█████████▉| 26752/26933 [27:34<00:10, 17.90it/s] 99%|█████████▉| 26756/26933 [27:34<00:09, 17.79it/s] 99%|█████████▉| 26760/26933 [27:35<00:09, 17.81it/s] 99%|█████████▉| 26764/26933 [27:35<00:09, 17.80it/s] 99%|█████████▉| 26768/26933 [27:35<00:09, 17.84it/s] 99%|█████████▉| 26772/26933 [27:35<00:09, 17.88it/s] 99%|█████████▉| 26776/26933 [27:35<00:08, 17.74it/s] 99%|█████████▉| 26780/26933 [27:36<00:08, 17.78it/s] 99%|█████████▉| 26784/26933 [27:36<00:08, 17.90it/s] 99%|█████████▉| 26788/26933 [27:36<00:08, 17.99it/s] 99%|█████████▉| 26792/26933 [27:36<00:07, 17.99it/s] 99%|█████████▉| 26796/26933 [27:37<00:07, 18.08it/s]100%|█████████▉| 26800/26933 [27:37<00:07, 18.15it/s]100%|█████████▉| 26804/26933 [27:37<00:07, 18.14it/s]100%|█████████▉| 26808/26933 [27:37<00:06, 18.16it/s]100%|█████████▉| 26812/26933 [27:37<00:06, 18.12it/s]100%|█████████▉| 26816/26933 [27:38<00:06, 18.15it/s]100%|█████████▉| 26820/26933 [27:38<00:06, 17.64it/s]100%|█████████▉| 26824/26933 [27:38<00:06, 17.65it/s]100%|█████████▉| 26828/26933 [27:38<00:05, 17.64it/s]100%|█████████▉| 26832/26933 [27:39<00:05, 17.60it/s]100%|█████████▉| 26836/26933 [27:39<00:05, 17.61it/s]100%|█████████▉| 26840/26933 [27:39<00:05, 17.63it/s]100%|█████████▉| 26844/26933 [27:39<00:05, 17.63it/s]100%|█████████▉| 26848/26933 [27:40<00:04, 17.65it/s]100%|█████████▉| 26852/26933 [27:40<00:04, 17.66it/s]100%|█████████▉| 26856/26933 [27:40<00:04, 17.64it/s]100%|█████████▉| 26860/26933 [27:40<00:04, 17.68it/s]100%|█████████▉| 26864/26933 [27:40<00:03, 17.66it/s]100%|█████████▉| 26868/26933 [27:41<00:03, 17.70it/s]100%|█████████▉| 26872/26933 [27:41<00:03, 17.71it/s]100%|█████████▉| 26876/26933 [27:41<00:03, 17.72it/s]100%|█████████▉| 26880/26933 [27:41<00:03, 17.63it/s]100%|█████████▉| 26884/26933 [27:42<00:02, 17.69it/s]100%|█████████▉| 26888/26933 [27:42<00:02, 17.69it/s]100%|█████████▉| 26892/26933 [27:42<00:02, 17.71it/s]100%|█████████▉| 26896/26933 [27:42<00:02, 17.73it/s]100%|█████████▉| 26900/26933 [27:42<00:01, 17.71it/s]100%|█████████▉| 26904/26933 [27:43<00:01, 17.77it/s]100%|█████████▉| 26908/26933 [27:43<00:01, 17.79it/s]100%|█████████▉| 26912/26933 [27:43<00:01, 17.81it/s]100%|█████████▉| 26916/26933 [27:43<00:00, 17.75it/s]100%|█████████▉| 26920/26933 [27:44<00:00, 17.41it/s]100%|█████████▉| 26924/26933 [27:44<00:00, 17.50it/s]100%|█████████▉| 26928/26933 [27:44<00:00, 17.60it/s]100%|█████████▉| 26932/26933 [27:44<00:00, 17.60it/s]100%|██████████| 26933/26933 [27:44<00:00, 16.18it/s]
|    Task     |Version| Metric |Value |   |Stderr|
|-------------|------:|--------|-----:|---|-----:|
|arc_challenge|      0|acc     |0.3294|±  |0.0137|
|             |       |acc_norm|0.3584|±  |0.0140|
|arc_easy     |      0|acc     |0.6473|±  |0.0098|
|             |       |acc_norm|0.4882|±  |0.0103|
|boolq        |      1|acc     |0.6728|±  |0.0082|
|piqa         |      0|acc     |0.7410|±  |0.0102|
|             |       |acc_norm|0.7367|±  |0.0103|
|winogrande   |      0|acc     |0.6369|±  |0.0135|

W0908 04:15:50.272278 104337 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 04:15:50.296042 104337 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 04:15:50.409110 104337 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 04:15:50.409229 104337 utils.py:164] NumExpr defaulting to 16 threads.
I0908 04:15:50.579999 104337 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 04:15:50.828095 104337 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 04:16:12.368641 104337 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 04:16:35.318666 104337 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:56<00:56, 56.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 34.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:15<00:00, 37.54s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0908 04:22:24.718077 104753 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 04:22:24.750474 104753 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 04:22:25.995751 104753 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 04:22:25.995887 104753 utils.py:164] NumExpr defaulting to 16 threads.
I0908 04:22:26.437879 104753 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 04:22:27.606009 104753 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 04:22:50.601817 104753 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 04:23:14.360412 104753 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 29.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.37s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0908 04:35:09.198590 105149 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 04:35:09.232144 105149 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 04:35:10.463732 105149 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 04:35:10.463877 105149 utils.py:164] NumExpr defaulting to 16 threads.
I0908 04:35:10.918018 105149 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 04:35:12.076424 105149 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 04:35:35.162364 105149 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

> /home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py(98)main()
-> orig_model = AutoModelForCausalLM.from_pretrained(args.base_model,
(Pdb) --KeyboardInterrupt--
(Pdb) (Pdb) Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 152, in <module>
    main(args)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 98, in main
    orig_model = AutoModelForCausalLM.from_pretrained(args.base_model,
  File "/home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 98, in main
    orig_model = AutoModelForCausalLM.from_pretrained(args.base_model,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/user/miniconda3/envs/quip/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
W0908 04:36:38.882466 105439 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 04:36:38.906609 105439 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 04:36:39.018542 105439 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 04:36:39.018634 105439 utils.py:164] NumExpr defaulting to 16 threads.
I0908 04:36:39.176574 105439 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 04:36:39.418880 105439 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 04:37:01.052920 105439 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

> /home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py(98)main()
-> orig_model = AutoModelForCausalLM.from_pretrained(args.base_model,
(Pdb) > /home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py(99)main()
-> torch_dtype='auto',
(Pdb) *** NameError: name 'orig_model' is not defined
(Pdb) Breakpoint 1 at /home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py:102
(Pdb) I0908 04:38:13.647301 105439 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:59<00:59, 59.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 35.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 38.95s/it]
> /home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py(102)main()
-> orig_logits = utils.calculate_logits(orig_model, devset, args.batch_size)
(Pdb) LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
(Pdb) /home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0908 04:43:12.209303 106270 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 04:43:12.240129 106270 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 04:43:13.257511 106270 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 04:43:13.257677 106270 utils.py:164] NumExpr defaulting to 16 threads.
I0908 04:43:13.779354 106270 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 04:43:14.869000 106270 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 04:43:37.559154 106270 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 04:43:54.235524 106270 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:51<00:51, 51.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:12<00:00, 33.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:12<00:00, 36.27s/it]
> /home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py(103)main()
-> orig_logits = utils.calculate_logits(orig_model, devset, args.batch_size)
(Pdb) torch.Size([384, 4096])
(Pdb) args = Namespace(seed=0, num_cpu_threads=8, batch_size=16, devset_size=384, ctx_size=4096, sample_proc=1, base_model='meta-llama/Llama-2-7b-hf', hf_path='hf/2_7b_2bit', ckpt_path='ckpt/2_7b_2bit', ft_lr=1e-05, ft_susv_lr=0.0001, ft_bs=1, ft_update_freq=2, ft_epochs=8, ft_valid_freq=1, ft_valid_size=128.0, ft_early_stop=3, ft_train_mode=True, ft_grad_ckpt=False, ft_nshards=-1)
(Pdb) 16
(Pdb) args = Namespace(seed=0, num_cpu_threads=8, batch_size=16, devset_size=384, ctx_size=4096, sample_proc=1, base_model='meta-llama/Llama-2-7b-hf', hf_path='hf/2_7b_2bit', ckpt_path='ckpt/2_7b_2bit', ft_lr=1e-05, ft_susv_lr=0.0001, ft_bs=1, ft_update_freq=2, ft_epochs=8, ft_valid_freq=1, ft_valid_size=128.0, ft_early_stop=3, ft_train_mode=True, ft_grad_ckpt=False, ft_nshards=-1)
(Pdb) 16
(Pdb) args = Namespace(seed=0, num_cpu_threads=8, batch_size=16, devset_size=384, ctx_size=4096, sample_proc=1, base_model='meta-llama/Llama-2-7b-hf', hf_path='hf/2_7b_2bit', ckpt_path='ckpt/2_7b_2bit', ft_lr=1e-05, ft_susv_lr=0.0001, ft_bs=1, ft_update_freq=2, ft_epochs=8, ft_valid_freq=1, ft_valid_size=128.0, ft_early_stop=3, ft_train_mode=True, ft_grad_ckpt=False, ft_nshards=-1)
(Pdb) args = Namespace(seed=0, num_cpu_threads=8, batch_size=16, devset_size=384, ctx_size=4096, sample_proc=1, base_model='meta-llama/Llama-2-7b-hf', hf_path='hf/2_7b_2bit', ckpt_path='ckpt/2_7b_2bit', ft_lr=1e-05, ft_susv_lr=0.0001, ft_bs=1, ft_update_freq=2, ft_epochs=8, ft_valid_freq=1, ft_valid_size=128.0, ft_early_stop=3, ft_train_mode=True, ft_grad_ckpt=False, ft_nshards=-1)
(Pdb) *** SyntaxError: expression cannot contain assignment, perhaps you meant "=="?
(Pdb) args = Namespace(seed=0, num_cpu_threads=8, batch_size=16, devset_size=384, ctx_size=4096, sample_proc=1, base_model='meta-llama/Llama-2-7b-hf', hf_path='hf/2_7b_2bit', ckpt_path='ckpt/2_7b_2bit', ft_lr=1e-05, ft_susv_lr=0.0001, ft_bs=1, ft_update_freq=2, ft_epochs=8, ft_valid_freq=1, ft_valid_size=128.0, ft_early_stop=3, ft_train_mode=True, ft_grad_ckpt=False, ft_nshards=-1)
(Pdb) (Pdb) args = Namespace(seed=0, num_cpu_threads=8, batch_size=4, devset_size=384, ctx_size=4096, sample_proc=1, base_model='meta-llama/Llama-2-7b-hf', hf_path='hf/2_7b_2bit', ckpt_path='ckpt/2_7b_2bit', ft_lr=1e-05, ft_susv_lr=0.0001, ft_bs=1, ft_update_freq=2, ft_epochs=8, ft_valid_freq=1, ft_valid_size=128.0, ft_early_stop=3, ft_train_mode=True, ft_grad_ckpt=False, ft_nshards=-1)
(Pdb) Breakpoint 1 at /home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py:104
(Pdb) /home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0908 05:04:00.166392 108048 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 05:04:00.200886 108048 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 05:04:01.654545 108048 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 05:04:01.654693 108048 utils.py:164] NumExpr defaulting to 16 threads.
I0908 05:04:02.185030 108048 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 05:04:03.520685 108048 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 05:04:26.543561 108048 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 05:04:49.837833 108048 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:58<00:58, 59.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 35.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:17<00:00, 38.91s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0908 05:16:19.366452 108934 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 05:16:19.398571 108934 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 05:16:20.610319 108934 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 05:16:20.610478 108934 utils.py:164] NumExpr defaulting to 16 threads.
I0908 05:16:21.046022 108934 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 05:16:22.318134 108934 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/user/miniconda3/envs/quip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/user/benchmarks/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 22, in <module>
    from lib import codebook, utils
  File "/home/user/benchmarks/quip-sharp/lib/codebook/__init__.py", line 40, in <module>
    from . import (latticee8_padded12, latticee8_padded12_rvq3bit,
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12_rvq3bit.py", line 108, in <module>
    _E8P_GRID, _E8P_GRID_IDX, _PARITY_IDX = get_full_grid(_E8P_PACKED_ABS_CACHED)
  File "/home/user/benchmarks/quip-sharp/lib/codebook/latticee8_padded12_rvq3bit.py", line -1, in get_full_grid
KeyboardInterrupt
W0908 05:16:44.254076 109189 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 05:16:44.278100 109189 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 05:16:44.387534 109189 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 05:16:44.387635 109189 utils.py:164] NumExpr defaulting to 16 threads.
I0908 05:16:44.545409 109189 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 05:16:44.780689 109189 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 05:17:04.149007 109189 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 05:17:24.169537 109189 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:56<00:56, 56.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:16<00:00, 35.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:16<00:00, 38.27s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0908 05:40:35.416797 113805 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 05:40:35.449326 113805 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 05:40:36.872960 113805 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 05:40:36.873110 113805 utils.py:164] NumExpr defaulting to 16 threads.
I0908 05:40:37.338602 113805 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 05:40:38.599293 113805 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 05:41:02.234838 113805 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 05:41:25.417616 113805 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:58<00:58, 58.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:16<00:00, 34.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:16<00:00, 38.38s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0908 05:54:14.518483 116578 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 05:54:14.577390 116578 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 05:54:15.774027 116578 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 05:54:15.774184 116578 utils.py:164] NumExpr defaulting to 16 threads.
I0908 05:54:16.228290 116578 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 05:54:17.330051 116578 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 05:54:38.290604 116578 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 05:55:01.206715 116578 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:59<00:59, 59.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:20<00:00, 36.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:20<00:00, 40.11s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0908 06:05:37.114006 118994 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 06:05:37.146555 118994 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 06:05:38.472733 118994 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 06:05:38.472853 118994 utils.py:164] NumExpr defaulting to 16 threads.
I0908 06:05:38.899747 118994 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 06:05:40.180251 118994 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 06:06:01.488793 118994 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 06:06:22.630028 118994 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:02<01:02, 62.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:22<00:00, 37.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:22<00:00, 41.13s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0908 07:49:51.798968 127721 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0908 07:49:51.832717 127721 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0908 07:49:53.250447 127721 utils.py:151] Note: NumExpr detected 26 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0908 07:49:53.250588 127721 utils.py:164] NumExpr defaulting to 16 threads.
I0908 07:49:53.751262 127721 config.py:54] PyTorch version 2.8.0+cu126 available.
W0908 07:49:54.894487 127721 warnings.py:109] /home/user/benchmarks/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0908 07:50:16.239684 127721 warnings.py:109] /home/user/miniconda3/envs/quip/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

I0908 07:50:41.398454 127721 modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:03<01:03, 63.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:25<00:00, 38.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:25<00:00, 42.58s/it]
/home/user/miniconda3/envs/quip/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
